<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 44]
- [cs.AI](#cs.AI) [Total: 20]
- [cs.IR](#cs.IR) [Total: 8]
- [cs.LG](#cs.LG) [Total: 53]
- [math-ph](#math-ph) [Total: 1]
- [math.NA](#math.NA) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [cs.NI](#cs.NI) [Total: 1]
- [cs.DB](#cs.DB) [Total: 2]
- [stat.ML](#stat.ML) [Total: 5]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.AR](#cs.AR) [Total: 2]
- [cs.SI](#cs.SI) [Total: 2]
- [cs.DS](#cs.DS) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [physics.data-an](#physics.data-an) [Total: 1]
- [eess.IV](#eess.IV) [Total: 6]
- [cs.CY](#cs.CY) [Total: 2]
- [cs.HC](#cs.HC) [Total: 2]
- [cs.GR](#cs.GR) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.PF](#cs.PF) [Total: 1]
- [cs.CV](#cs.CV) [Total: 37]
- [quant-ph](#quant-ph) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [cs.DC](#cs.DC) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [cs.SE](#cs.SE) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [Adaptive Linguistic Prompting (ALP) Enhances Phishing Webpage Detection in Multimodal Large Language Models](https://arxiv.org/abs/2507.13357)
*Atharva Bhargude,Ishan Gonehal,Chandler Haney,Dave Yoon,Kevin Zhu,Aaron Sandoval,Sean O'Brien,Kaustubh Vinnakota*

Main category: cs.CL

TL;DR: 本文提出了一种名为ALP的少样本自适应语言提示方法，利用多模态大型语言模型（如GPT-4o和Gemini 1.5 Pro）检测钓鱼网页，显著提升了检测准确率。


<details>
  <summary>Details</summary>
Motivation: 钓鱼攻击是严重的网络安全威胁，需要自适应检测技术。

Method: ALP是一种结构化语义推理方法，通过分析语言模式、紧迫性线索和操纵性措辞，结合文本、视觉和URL分析，构建统一模型。

Result: 实验表明，ALP显著提升了钓鱼检测准确率，F1分数达到0.93，优于传统方法。

Conclusion: ALP为基于语言的钓鱼检测系统提供了更鲁棒、可解释和自适应的基础。

Abstract: Phishing attacks represent a significant cybersecurity threat, necessitating
adaptive detection techniques. This study explores few-shot Adaptive Linguistic
Prompting (ALP) in detecting phishing webpages through the multimodal
capabilities of state-of-the-art large language models (LLMs) such as GPT-4o
and Gemini 1.5 Pro. ALP is a structured semantic reasoning method that guides
LLMs to analyze textual deception by breaking down linguistic patterns,
detecting urgency cues, and identifying manipulative diction commonly found in
phishing content. By integrating textual, visual, and URL-based analysis, we
propose a unified model capable of identifying sophisticated phishing attempts.
Our experiments demonstrate that ALP significantly enhances phishing detection
accuracy by guiding LLMs through structured reasoning and contextual analysis.
The findings highlight the potential of ALP-integrated multimodal LLMs to
advance phishing detection frameworks, achieving an F1-score of 0.93,
surpassing traditional approaches. These results establish a foundation for
more robust, interpretable, and adaptive linguistic-based phishing detection
systems using LLMs.

</details>


### [2] [Persona-Based Synthetic Data Generation Using Multi-Stage Conditioning with Large Language Models for Emotion Recognition](https://arxiv.org/abs/2507.13380)
*Keito Inoshita,Rushia Harada*

Main category: cs.CL

TL;DR: PersonaGen是一个基于大型语言模型的多阶段人格条件框架，用于生成情感丰富的文本，以解决情感识别领域高质量数据集稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 情感识别领域缺乏高质量、多样化的情感数据集，情感表达受个体特质、社会文化背景和情境因素影响，数据收集困难。

Method: PersonaGen通过结合人口属性、社会文化背景和详细情境上下文构建虚拟人格，指导情感表达生成。

Result: 实验表明，PersonaGen在生成多样、连贯且具有区分性的情感表达方面显著优于基线方法。

Conclusion: PersonaGen可作为增强或替代真实情感数据集的强大工具。

Abstract: In the field of emotion recognition, the development of high-performance
models remains a challenge due to the scarcity of high-quality, diverse
emotional datasets. Emotional expressions are inherently subjective, shaped by
individual personality traits, socio-cultural backgrounds, and contextual
factors, making large-scale, generalizable data collection both ethically and
practically difficult. To address this issue, we introduce PersonaGen, a novel
framework for generating emotionally rich text using a Large Language Model
(LLM) through multi-stage persona-based conditioning. PersonaGen constructs
layered virtual personas by combining demographic attributes, socio-cultural
backgrounds, and detailed situational contexts, which are then used to guide
emotion expression generation. We conduct comprehensive evaluations of the
generated synthetic data, assessing semantic diversity through clustering and
distributional metrics, human-likeness via LLM-based quality scoring, realism
through comparison with real-world emotion corpora, and practical utility in
downstream emotion classification tasks. Experimental results show that
PersonaGen significantly outperforms baseline methods in generating diverse,
coherent, and discriminative emotion expressions, demonstrating its potential
as a robust alternative for augmenting or replacing real-world emotional
datasets.

</details>


### [3] [SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation](https://arxiv.org/abs/2507.13381)
*Rafiq Kamel,Filippo Guerranti,Simon Geisler,Stephan Günnemann*

Main category: cs.CL

TL;DR: SAFT是一种结构感知的微调方法，通过注入图拓扑信息到预训练的大语言模型（LLMs）中，显著提升了AMR到文本生成的性能。


<details>
  <summary>Details</summary>
Motivation: 当前方法在将抽象意义表示（AMRs）线性化时丢弃了关键结构信息，或依赖与标准LLMs不兼容的架构。

Method: SAFT通过计算方向敏感的位置编码（基于磁拉普拉斯变换的AMRs）并将其投影到LLM的嵌入空间中，无需改变模型架构。

Result: SAFT在AMR 3.0上实现了3.5 BLEU的提升，性能增益随图复杂度增加而显著。

Conclusion: SAFT为连接结构化数据和语言模型提供了一种通用且有效的途径。

Abstract: Large Language Models (LLMs) are increasingly applied to tasks involving
structured inputs such as graphs. Abstract Meaning Representations (AMRs),
which encode rich semantics as directed graphs, offer a rigorous testbed for
evaluating LLMs on text generation from such structures. Yet, current methods
often arbitrarily linearize AMRs, discarding key structural cues, or rely on
architectures incompatible with standard LLMs. We introduce SAFT, a
structure-aware fine-tuning approach that injects graph topology into
pretrained LLMs without architectural changes. We compute direction-sensitive
positional encodings from the magnetic Laplacian of transformed AMRs and
project them into the embedding space of the LLM. While possibly applicable to
any graph-structured inputs, we focus on AMR-to-text generation as a
representative and challenging benchmark. SAFT sets a new state-of-the-art on
AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph
complexity, highlighting the value of structure-aware representations in
enhancing LLM performance. SAFT offers a general and effective pathway for
bridging structured data and language models.

</details>


### [4] [Context-Based Fake News Detection using Graph Based Approach: ACOVID-19 Use-case](https://arxiv.org/abs/2507.13382)
*Chandrashekar Muniyappa,Sirisha Velampalli*

Main category: cs.CL

TL;DR: 提出了一种基于图的新方法，利用NLP技术将新闻文章转换为图结构，并使用MDL-GBAD算法检测假新闻。


<details>
  <summary>Details</summary>
Motivation: 假新闻在数字世界中快速传播，需要有效的方法来应对这一挑战。

Method: 从Kaggle获取数据集，结合COVID-19相关新闻，利用NLP将新闻转换为图结构，并应用MDL-GBAD算法进行异常检测。

Result: 该方法能够识别数据集中的规范模式，并发现偏离这些模式的异常模式。

Conclusion: 基于图的方法在处理复杂上下文数据时表现出色，能够有效检测假新闻。

Abstract: In today\'s digital world, fake news is spreading with immense speed. Its a
significant concern to address. In this work, we addressed that challenge using
novel graph based approach. We took dataset from Kaggle that contains real and
fake news articles. To test our approach we incorporated recent covid-19
related news articles that contains both genuine and fake news that are
relevant to this problem. This further enhances the dataset as well instead of
relying completely on the original dataset. We propose a contextual graph-based
approach to detect fake news articles. We need to convert news articles into
appropriate schema, so we leverage Natural Language Processing (NLP) techniques
to transform news articles into contextual graph structures. We then apply the
Minimum Description Length (MDL)-based Graph-Based Anomaly Detection (GBAD)
algorithm for graph mining. Graph-based methods are particularly effective for
handling rich contextual data, as they enable the discovery of complex patterns
that traditional query-based or statistical techniques might overlook. Our
proposed approach identifies normative patterns within the dataset and
subsequently uncovers anomalous patterns that deviate from these established
norms.

</details>


### [5] [PARAM-1 BharatGen 2.9B Model](https://arxiv.org/abs/2507.13390)
*Kundeshwar Pundalik,Piyush Sawarkar,Nihar Sahoo,Abhishek Shinde,Prateek Chanda,Vedant Goswami,Ajay Nagpal,Atul Singh,Viraj Thakur,Vijay Dewane,Aamod Thakur,Bhargav Patel,Smita Gautam,Bhagwan Panditi,Shyam Pawar,Madhav Kotcha,Suraj Racha,Saral Sureka,Pankaj Singh,Rishi Bal,Rohit Saluja,Ganesh Ramakrishnan*

Main category: cs.CL

TL;DR: PARAM-1是一个专注于印度语言多样性的2.9B参数语言模型，通过双语（印地语和英语）训练，强调公平表示、适应印度形态的标记化和文化对齐评估。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLMs）以英语为中心的设计对印度等语言多样性地区的结构性忽视问题。

Method: 采用2.9B参数的解码器架构，训练双语数据集（印地语和英语），强调公平表示（25%语料分配）、适应印度形态的标记化和文化对齐评估。

Result: PARAM-1既是一个通用的语言模型，也是印度中心应用的稳健基线。

Conclusion: PARAM-1提供了一个以多样性为先的预训练设计蓝图，为公平的基础模型提供了示范。

Abstract: Large Language Models (LLMs) have emerged as powerful general-purpose
reasoning systems, yet their development remains dominated by English-centric
data, architectures, and optimization paradigms. This exclusionary design
results in structural under-representation of linguistically diverse regions
such as India, where over 20 official languages and 100+ dialects coexist
alongside phenomena like code-switching and diglossia. We introduce PARAM-1, a
2.9B parameter decoder-only, text-only language model trained from scratch with
an explicit architectural and linguistic focus on Indian diversity. PARAM-1 is
trained on a bilingual dataset consisting of only Hindi and English,
constructed with a strong focus on fact-rich, high-quality content. It is
guided by three core principles: equitable representation of Indic languages
through a 25% corpus allocation; tokenization fairness via a SentencePiece
tokenizer adapted to Indian morphological structures; and culturally aligned
evaluation benchmarks across IndicQA, code-mixed reasoning, and
socio-linguistic robustness tasks. By embedding diversity at the pretraining
level-rather than deferring it to post-hoc alignment-PARAM-1 offers a
design-first blueprint for equitable foundation modeling. Our results
demonstrate that it serves as both a competent general-purpose model and a
robust baseline for India-centric applications.

</details>


### [6] [TopicImpact: Improving Customer Feedback Analysis with Opinion Units for Topic Modeling and Star-Rating Prediction](https://arxiv.org/abs/2507.13392)
*Emil Häglund,Johanna Björklund*

Main category: cs.CL

TL;DR: 通过重构主题建模流程，基于意见单元提取客户评论的见解，提升主题建模性能，并关联情感与业务指标。


<details>
  <summary>Details</summary>
Motivation: 改进客户评论分析，提取更连贯和可解释的主题，同时捕捉情感，以关联业务指标。

Method: 利用大语言模型提取意见单元，重构主题建模流程，结合情感评分。

Result: 主题建模性能提升，生成连贯且可解释的主题，并能准确预测星级评分。

Conclusion: 该方法有效整合主题与情感模态，为业务决策提供更精准的见解。

Abstract: We improve the extraction of insights from customer reviews by restructuring
the topic modelling pipeline to operate on opinion units - distinct statements
that include relevant text excerpts and associated sentiment scores. Prior work
has demonstrated that such units can be reliably extracted using large language
models. The result is a heightened performance of the subsequent topic
modeling, leading to coherent and interpretable topics while also capturing the
sentiment associated with each topic. By correlating the topics and sentiments
with business metrics, such as star ratings, we can gain insights on how
specific customer concerns impact business outcomes. We present our system's
implementation, use cases, and advantages over other topic modeling and
classification solutions. We also evaluate its effectiveness in creating
coherent topics and assess methods for integrating topic and sentiment
modalities for accurate star-rating prediction.

</details>


### [7] [Mitigating Stylistic Biases of Machine Translation Systems via Monolingual Corpora Only](https://arxiv.org/abs/2507.13395)
*Xuanqi Gao,Weipeng Jiang,Juan Zhai,Shiqing Ma,Siyi Xie,Xinyang Yin,Chao Shen*

Main category: cs.CL

TL;DR: Babel框架通过单语料库提升神经机器翻译的风格保真度，无需平行语料库或修改现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖平行语料库保留风格，限制了应用范围。Babel旨在仅用单语料库解决风格保留问题。

Method: Babel包含风格检测器和扩散式风格应用器，作为后处理模块集成到现有NMT系统中。

Result: 在五个领域实验中，Babel风格不一致检测精度达88.21%，风格保留提升150%，语义相似度0.92。

Conclusion: Babel有效提升翻译风格保真度，同时保持语义完整性和流畅性。

Abstract: The advent of neural machine translation (NMT) has revolutionized
cross-lingual communication, yet preserving stylistic nuances remains a
significant challenge. While existing approaches often require parallel corpora
for style preservation, we introduce Babel, a novel framework that enhances
stylistic fidelity in NMT using only monolingual corpora. Babel employs two key
components: (1) a style detector based on contextual embeddings that identifies
stylistic disparities between source and target texts, and (2) a
diffusion-based style applicator that rectifies stylistic inconsistencies while
maintaining semantic integrity. Our framework integrates with existing NMT
systems as a post-processing module, enabling style-aware translation without
requiring architectural modifications or parallel stylistic data. Extensive
experiments on five diverse domains (law, literature, scientific writing,
medicine, and educational content) demonstrate Babel's effectiveness: it
identifies stylistic inconsistencies with 88.21% precision and improves
stylistic preservation by 150% while maintaining a high semantic similarity
score of 0.92. Human evaluation confirms that translations refined by Babel
better preserve source text style while maintaining fluency and adequacy.

</details>


### [8] [Causal Language Control in Multilingual Transformers via Sparse Feature Steering](https://arxiv.org/abs/2507.13410)
*Cheng-Ting Chou,George Liu,Jessica Sun,Cole Blondin,Kevin Zhu,Vasu Sharma,Sean O'Brien*

Main category: cs.CL

TL;DR: 利用稀疏自编码器（SAE）特征控制多语言大模型（LLM）的生成语言，通过修改单个SAE特征实现高达90%的语言切换成功率。


<details>
  <summary>Details</summary>
Motivation: 解决在多语言大模型中零样本设置下无法通过显式语言提示或微调控制生成语言的挑战。

Method: 利用预训练的SAE分析Gemma-2B和Gemma-9B的残差流，识别与目标语言（中、日、西、法）相关的特征，并通过修改单个特征实现语言切换。

Result: 在FastText语言分类中达到90%的成功率，同时保持LaBSE语义相似性。语言控制在模型中后期层效果最佳。

Conclusion: 稀疏特征调控是一种轻量且可解释的多语言生成控制机制。

Abstract: Deterministically controlling the target generation language of large
multilingual language models (LLMs) remains a fundamental challenge,
particularly in zero-shot settings where neither explicit language prompts nor
fine-tuning are available. In this work, we investigate whether sparse
autoencoder (SAE) features, previously shown to correlate with interpretable
model behaviors, can be leveraged to steer the generated language of LLMs
during inference. Leveraging pretrained SAEs on the residual streams of
Gemma-2B and Gemma-9B, we identify features whose activations differ most
significantly between English and four target languages: Chinese, Japanese,
Spanish, and French. By modifying just a single SAE feature at one transformer
layer, we achieve controlled language shifts with up to 90\% success, as
measured by FastText language classification, while preserving semantic
fidelity according to LaBSE (Language-Agnostic BERT Sentence Embedding)
similarity. Our analysis reveals that language steering is most effective in
mid-to-late transformer layers and is amplified by specific attention heads
disproportionately associated with language-sensitive SAE features. These
results demonstrate the promise of sparse feature steering as a lightweight and
interpretable mechanism for controllable multilingual generation.

</details>


### [9] [Aligning Knowledge Graphs and Language Models for Factual Accuracy](https://arxiv.org/abs/2507.13411)
*Nur A Zarin Nishat,Andrea Coletta,Luigi Bellomarini,Kossi Amouzouvi,Jens Lehmann,Sahar Vahdati*

Main category: cs.CL

TL;DR: ALIGNed-LLM通过将知识图谱嵌入语言模型的潜在空间，显著提高了语言模型的事实性，减少了幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在问答等任务中容易产生幻觉的问题，通过引入知识图谱提供结构化、可靠的外部信息。

Method: 使用预训练的知识图谱嵌入模型（如TransE）和可训练投影层，将实体与文本嵌入对齐。

Result: 在三个问答基准数据集和实际金融用例中，模型表现显著提升。

Conclusion: ALIGNed-LLM是一种简单有效的方法，能够显著增强语言模型的事实性。

Abstract: Large language models like GPT-4, Gemini, and Claude have transformed natural
language processing (NLP) tasks such as question answering, dialogue
generation, summarization, and so forth; yet their susceptibility to
hallucination stands as one of the major challenges. Among numerous approaches
to overcome this challenge, integration of Knowledge Graphs (KGs) into language
models has emerged as a promising solution as it provides structured, reliable,
domain-specific, and up-to-date external information to the language models. In
this paper, we introduce ALIGNed-LLM, a simple yet effective approach to
improve language models' factuality via a lean strategy to infuse KGs into the
latent space of language models inspired by LLaVA where visual and textual
information is infused. We use embeddings from a pre-trained Knowledge Graph
Embedding (KGE) model, such as TransE, and a trainable projection layer to
align entity and text embeddings. This alignment enables the language model to
distinguish between similar entities improving factual grounding and reducing
hallucination. We tested our approach on three popular questions-answering
benchmark datasets alongside language models of varying sizes, showing
significant improvement. Furthermore, we applied our approach to a real-world
financial use case from a large central bank in Europe, which demands high
accuracy and precision, demonstrating a substantial improvement of the LLM
answers.

</details>


### [10] [Paper Summary Attack: Jailbreaking LLMs through LLM Safety Papers](https://arxiv.org/abs/2507.13474)
*Liang Lin,Zhihao Xu,Xuehai Tang,Shi Liu,Biyu Zhou,Fuqing Zhu,Jizhong Han,Songlin Hu*

Main category: cs.CL

TL;DR: 论文提出了一种名为Paper Summary Attack（PSA）的新型攻击方法，利用LLMs对权威来源的信任，通过合成攻击或防御性论文内容构造对抗性提示模板，成功攻击了包括Deepseek-R1在内的先进模型。


<details>
  <summary>Details</summary>
Motivation: 研究发现LLMs倾向于信任权威来源（如学术论文），这可能带来新的安全漏洞。

Method: 提出PSA方法，通过合成攻击或防御性论文内容构造对抗性提示模板，并在预定义子部分中填充有害查询作为对抗性载荷。

Result: PSA在Claude3.5-Sonnet和Deepseek-R1上的攻击成功率分别达到97%和98%，并揭示了不同模型间的漏洞偏差。

Conclusion: PSA揭示了LLMs的安全漏洞，为对抗性方法和安全对齐提供了未来研究方向。

Abstract: The safety of large language models (LLMs) has garnered significant research
attention. In this paper, we argue that previous empirical studies demonstrate
LLMs exhibit a propensity to trust information from authoritative sources, such
as academic papers, implying new possible vulnerabilities. To verify this
possibility, a preliminary analysis is designed to illustrate our two findings.
Based on this insight, a novel jailbreaking method, Paper Summary Attack
(\llmname{PSA}), is proposed. It systematically synthesizes content from either
attack-focused or defense-focused LLM safety paper to construct an adversarial
prompt template, while strategically infilling harmful query as adversarial
payloads within predefined subsections. Extensive experiments show significant
vulnerabilities not only in base LLMs, but also in state-of-the-art reasoning
model like Deepseek-R1. PSA achieves a 97\% attack success rate (ASR) on
well-aligned models like Claude3.5-Sonnet and an even higher 98\% ASR on
Deepseek-R1. More intriguingly, our work has further revealed diametrically
opposed vulnerability bias across different base models, and even between
different versions of the same model, when exposed to either attack-focused or
defense-focused papers. This phenomenon potentially indicates future research
clues for both adversarial methodologies and safety alignment.Code is available
at https://github.com/233liang/Paper-Summary-Attack

</details>


### [11] [Revisiting LLM Value Probing Strategies: Are They Robust and Expressive?](https://arxiv.org/abs/2507.13490)
*Siqi Shen,Mehar Singh,Lajanugen Logeswaran,Moontae Lee,Honglak Lee,Rada Mihalcea*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）价值取向的鲁棒性和表达能力，发现现有方法在输入扰动下表现不稳定，且模型的价值与其实际行为关联较弱。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs价值取向的评估方法，填补现有研究在系统性比较和实际行为关联上的空白。

Method: 比较三种广泛使用的探测策略，通过提示和选项的变体测试鲁棒性，并设计两项任务研究价值对人口统计背景的响应及其与模型行为的关联。

Result: 所有方法在输入扰动下表现出较大方差；人口统计背景对自由文本生成影响小，模型价值与其行为偏好关联弱。

Conclusion: 需更谨慎地评估LLM价值探测方法，并意识到其局限性。

Abstract: There has been extensive research on assessing the value orientation of Large
Language Models (LLMs) as it can shape user experiences across demographic
groups. However, several challenges remain. First, while the Multiple Choice
Question (MCQ) setting has been shown to be vulnerable to perturbations, there
is no systematic comparison of probing methods for value probing. Second, it is
unclear to what extent the probed values capture in-context information and
reflect models' preferences for real-world actions. In this paper, we evaluate
the robustness and expressiveness of value representations across three widely
used probing strategies. We use variations in prompts and options, showing that
all methods exhibit large variances under input perturbations. We also
introduce two tasks studying whether the values are responsive to demographic
context, and how well they align with the models' behaviors in value-related
scenarios. We show that the demographic context has little effect on the
free-text generation, and the models' values only weakly correlate with their
preference for value-based actions. Our work highlights the need for a more
careful examination of LLM value probing and awareness of its limitations.

</details>


### [12] [Encoding syntactic objects and Merge operations in function spaces](https://arxiv.org/abs/2507.13501)
*Matilde Marcolli,Robert C. Berwick*

Main category: cs.CL

TL;DR: 论文通过数学论证表明，将词汇项表示为函数空间中的函数（如小波），可以构造出任意句法对象的忠实表示。该空间可通过第二Renyi熵赋予交换非结合半环结构，结果表示与magma结构兼容。函数集为operad上的代数，operad中的操作模拟将输入波形转换为编码句法结构的输出电路。Merge操作通过余积和Hopf代数马尔可夫链作用于这些电路。研究为句法核心计算结构的神经计算实现提供了理论可能性。


<details>
  <summary>Details</summary>
Motivation: 探索句法结构的数学表示及其神经计算实现的可行性。

Method: 将词汇项表示为函数空间中的函数，构建句法对象的表示，赋予半环结构，并通过operad和Hopf代数实现Merge操作。

Result: 证明了句法核心计算结构的神经计算实现的理论可能性，并通过正弦波的跨频相位同步具体实现。

Conclusion: 研究为句法结构的数学和神经计算实现提供了理论支持，并揭示了Merge与算术后继函数的相似性。

Abstract: We provide a mathematical argument showing that, given a representation of
lexical items as functions (wavelets, for instance) in some function space, it
is possible to construct a faithful representation of arbitrary syntactic
objects in the same function space. This space can be endowed with a
commutative non-associative semiring structure built using the second Renyi
entropy. The resulting representation of syntactic objects is compatible with
the magma structure. The resulting set of functions is an algebra over an
operad, where the operations in the operad model circuits that transform the
input wave forms into a combined output that encodes the syntactic structure.
The action of Merge on workspaces is faithfully implemented as action on these
circuits, through a coproduct and a Hopf algebra Markov chain. The results
obtained here provide a constructive argument showing the theoretical
possibility of a neurocomputational realization of the core computational
structure of syntax. We also present a particular case of this general
construction where this type of realization of Merge is implemented as a cross
frequency phase synchronization on sinusoidal waves. This also shows that Merge
can be expressed in terms of the successor function of a semiring, thus
clarifying the well known observation of its similarities with the successor
function of arithmetic.

</details>


### [13] [A Computational Approach to Modeling Conversational Systems: Analyzing Large-Scale Quasi-Patterned Dialogue Flows](https://arxiv.org/abs/2507.13544)
*Mohamed Achref Ben Ammar,Mohamed Taha Bennani*

Main category: cs.CL

TL;DR: 提出了一种用于构建和分析松散对话（准模式对话）的新型计算框架，通过Filter & Reconnect方法优化对话图的结构和语义。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型的系统在多场景中与用户交互，分析对话动态变得日益重要。

Method: 引入Filter & Reconnect方法，一种图简化技术，减少噪声同时保持语义连贯性和结构完整性。

Result: 结合大语言模型和该方法，语义指标S提升了2.06倍，同时实现了树状结构和0δ-双曲性，优化了对话建模的清晰度。

Conclusion: 该工作为大规模对话数据集分析提供了计算方法，可应用于监控聊天机器人、对话管理工具和用户行为分析等场景。

Abstract: The analysis of conversational dynamics has gained increasing importance with
the rise of large language model-based systems, which interact with users
across diverse contexts. In this work, we propose a novel computational
framework for constructing conversational graphs that capture the flow and
structure of loosely organized dialogues, referred to as quasi-patterned
conversations. We introduce the Filter & Reconnect method, a novel graph
simplification technique that minimizes noise while preserving semantic
coherence and structural integrity of conversational graphs. Through
comparative analysis, we demonstrate that the use of large language models
combined with our graph simplification technique has resulted in semantic
metric S increasing by a factor of 2.06 compared to previous approaches while
simultaneously enforcing a tree-like structure with 0 {\delta}-hyperbolicity,
ensuring optimal clarity in conversation modeling. This work provides a
computational method for analyzing large-scale dialogue datasets, with
practical applications related to monitoring automated systems such as
chatbots, dialogue management tools, and user behavior analytics.

</details>


### [14] [Reading Between the Lines: Combining Pause Dynamics and Semantic Coherence for Automated Assessment of Thought Disorder](https://arxiv.org/abs/2507.13551)
*Feng Chen,Weizhe Xu,Changye Li,Serguei Pakhomov,Alex Cohen,Simran Bhola,Sandy Yin,Sunny X Tang,Michael Mackinley,Lena Palaniyappan,Dror Ben-Zeev,Trevor Cohen*

Main category: cs.CL

TL;DR: 研究通过结合停顿特征和语义一致性指标，利用自动语音识别技术评估精神分裂症谱系障碍中的形式思维障碍（FTD）严重程度，发现停顿特征能有效预测FTD，且与语义指标结合可进一步提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 传统临床评定量表资源密集且缺乏可扩展性，自动语音分析技术（ASR）提供了一种客观、可扩展的替代方案，但其在评估FTD严重程度中的实用性需进一步验证。

Method: 研究整合了停顿特征和语义一致性指标，使用支持向量回归（SVR）预测临床FTD评分，分析了三个数据集（AVH、TOPSY、PsyCL）。

Result: 停顿特征单独使用时能有效预测FTD严重程度，与语义指标结合后预测性能进一步提升，相关性最高达0.649，AUC为83.71%。

Conclusion: 结合时间和语义分析框架为改进紊乱言语评估提供了方向，推动了精神病自动语音分析的发展。

Abstract: Formal thought disorder (FTD), a hallmark of schizophrenia spectrum
disorders, manifests as incoherent speech and poses challenges for clinical
assessment. Traditional clinical rating scales, though validated, are
resource-intensive and lack scalability. Automated speech analysis with
automatic speech recognition (ASR) allows for objective quantification of
linguistic and temporal features of speech, offering scalable alternatives. The
use of utterance timestamps in ASR captures pause dynamics, which are thought
to reflect the cognitive processes underlying speech production. However, the
utility of integrating these ASR-derived features for assessing FTD severity
requires further evaluation. This study integrates pause features with semantic
coherence metrics across three datasets: naturalistic self-recorded diaries
(AVH, n = 140), structured picture descriptions (TOPSY, n = 72), and dream
narratives (PsyCL, n = 43). We evaluated pause related features alongside
established coherence measures, using support vector regression (SVR) to
predict clinical FTD scores. Key findings demonstrate that pause features alone
robustly predict the severity of FTD. Integrating pause features with semantic
coherence metrics enhanced predictive performance compared to semantic-only
models, with integration of independent models achieving correlations up to
\r{ho} = 0.649 and AUC = 83.71% for severe cases detection (TOPSY, with best
\r{ho} = 0.584 and AUC = 79.23% for semantic-only models). The performance
gains from semantic and pause features integration held consistently across all
contexts, though the nature of pause patterns was dataset-dependent. These
findings suggest that frameworks combining temporal and semantic analyses
provide a roadmap for refining the assessment of disorganized speech and
advance automated speech analysis in psychosis.

</details>


### [15] [Consistent Explainers or Unreliable Narrators? Understanding LLM-generated Group Recommendations](https://arxiv.org/abs/2507.13705)
*Cedric Waterschoot,Nava Tintarev,Francesco Barile*

Main category: cs.CL

TL;DR: 论文评估了LLMs在群体推荐系统（GRS）中的表现，发现其推荐与ADD聚合相似，但解释常涉及额外标准，且透明性不足。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs作为GRS的决策和解释生成工具的有效性，并与传统社会选择聚合策略对比。

Method: 比较LLM生成的推荐和解释与Additive Utilitarian（ADD）聚合策略的结果。

Result: LLM推荐类似ADD聚合，但解释常涉及额外标准（如多样性、相似性），且透明性不足。

Conclusion: LLMs在GRS中的应用需改进透明性和解释一致性，传统聚合方法在大规模项目集上可能效率不足。

Abstract: Large Language Models (LLMs) are increasingly being implemented as joint
decision-makers and explanation generators for Group Recommender Systems (GRS).
In this paper, we evaluate these recommendations and explanations by comparing
them to social choice-based aggregation strategies. Our results indicate that
LLM-generated recommendations often resembled those produced by Additive
Utilitarian (ADD) aggregation. However, the explanations typically referred to
averaging ratings (resembling but not identical to ADD aggregation). Group
structure, uniform or divergent, did not impact the recommendations.
Furthermore, LLMs regularly claimed additional criteria such as user or item
similarity, diversity, or used undefined popularity metrics or thresholds. Our
findings have important implications for LLMs in the GRS pipeline as well as
standard aggregation strategies. Additional criteria in explanations were
dependent on the number of ratings in the group scenario, indicating potential
inefficiency of standard aggregation methods at larger item set sizes.
Additionally, inconsistent and ambiguous explanations undermine transparency
and explainability, which are key motivations behind the use of LLMs for GRS.

</details>


### [16] [A Data-Centric Framework for Addressing Phonetic and Prosodic Challenges in Russian Speech Generative Models](https://arxiv.org/abs/2507.13563)
*Kirill Borodin,Nikita Vasiliev,Vasiliy Kudryavtsev,Maxim Maslov,Mikhail Gorodnichev,Oleg Rogov,Grach Mkrtchian*

Main category: cs.CL

TL;DR: 论文介绍了Balalaika数据集，用于解决俄语语音合成中的挑战，实验表明其优于现有数据集。


<details>
  <summary>Details</summary>
Motivation: 俄语语音合成面临独特挑战，如元音弱化、辅音清化等，现有数据集不足。

Method: 构建了包含2000多小时高质量俄语语音的Balalaika数据集，并详细标注文本。

Result: 基于Balalaika训练的模型在语音合成和增强任务中表现显著优于现有数据集。

Conclusion: Balalaika数据集为俄语语音合成提供了高质量资源，解决了现有问题。

Abstract: Russian speech synthesis presents distinctive challenges, including vowel
reduction, consonant devoicing, variable stress patterns, homograph ambiguity,
and unnatural intonation. This paper introduces Balalaika, a novel dataset
comprising more than 2,000 hours of studio-quality Russian speech with
comprehensive textual annotations, including punctuation and stress markings.
Experimental results show that models trained on Balalaika significantly
outperform those trained on existing datasets in both speech synthesis and
enhancement tasks. We detail the dataset construction pipeline, annotation
methodology, and results of comparative evaluations.

</details>


### [17] [Question-Answer Extraction from Scientific Articles Using Knowledge Graphs and Large Language Models](https://arxiv.org/abs/2507.13827)
*Hosein Azarbonyad,Zi Long Zhu,Georgios Cheirmpos,Zubair Afzal,Vikrant Yadav,Georgios Tsatsaronis*

Main category: cs.CL

TL;DR: 论文提出两种生成问答对的方法：一种基于文章内容，另一种基于知识图谱，后者能更有效捕捉文章核心思想。


<details>
  <summary>Details</summary>
Motivation: 帮助学者快速识别和理解科学文章的主要思想和贡献。

Method: 1. 基于内容的方法：选择重要段落，用LLM生成问题并排序，再生成答案。2. 基于知识图谱的方法：构建知识图谱，提取关键三元组生成问答对。

Result: 基于知识图谱的方法能更有效捕捉文章核心思想，且微调ER提取模型对高质量三元组提取至关重要。

Conclusion: 知识图谱方法优于纯内容方法，适用于科学文章的问答对生成。

Abstract: When deciding to read an article or incorporate it into their research,
scholars often seek to quickly identify and understand its main ideas. In this
paper, we aim to extract these key concepts and contributions from scientific
articles in the form of Question and Answer (QA) pairs. We propose two distinct
approaches for generating QAs. The first approach involves selecting salient
paragraphs, using a Large Language Model (LLM) to generate questions, ranking
these questions by the likelihood of obtaining meaningful answers, and
subsequently generating answers. This method relies exclusively on the content
of the articles. However, assessing an article's novelty typically requires
comparison with the existing literature. Therefore, our second approach
leverages a Knowledge Graph (KG) for QA generation. We construct a KG by
fine-tuning an Entity Relationship (ER) extraction model on scientific articles
and using it to build the graph. We then employ a salient triplet extraction
method to select the most pertinent ERs per article, utilizing metrics such as
the centrality of entities based on a triplet TF-IDF-like measure. This measure
assesses the saliency of a triplet based on its importance within the article
compared to its prevalence in the literature. For evaluation, we generate QAs
using both approaches and have them assessed by Subject Matter Experts (SMEs)
through a set of predefined metrics to evaluate the quality of both questions
and answers. Our evaluations demonstrate that the KG-based approach effectively
captures the main ideas discussed in the articles. Furthermore, our findings
indicate that fine-tuning the ER extraction model on our scientific corpus is
crucial for extracting high-quality triplets from such documents.

</details>


### [18] [Linguistic and Embedding-Based Profiling of Texts generated by Humans and Large Language Models](https://arxiv.org/abs/2507.13614)
*Sergio E. Zanotto,Segun Aroyehun*

Main category: cs.CL

TL;DR: 该研究通过多层面语言特征分析人类与机器生成文本的差异，发现人类文本句法更简单、语义更多样，且新模型生成文本趋于同质化。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于利用语言特征而非简单分类，深入分析人类与机器生成文本的差异，填补现有研究的空白。

Method: 方法包括选择多领域数据集，计算形态、句法和语义等语言特征，并应用统计分析和风格嵌入测试变异性。

Result: 结果显示人类文本句法更简单、语义更多样；新模型生成文本变异性降低，趋于同质化。

Conclusion: 结论指出机器生成文本在风格上逐渐趋同，而人类文本仍保持更高的多样性。

Abstract: The rapid advancements in large language models (LLMs) have significantly
improved their ability to generate natural language, making texts generated by
LLMs increasingly indistinguishable from human-written texts. While recent
research has primarily focused on using LLMs to classify text as either
human-written and machine-generated texts, our study focus on characterizing
these texts using a set of linguistic features across different linguistic
levels such as morphology, syntax, and semantics. We select a dataset of
human-written and machine-generated texts spanning 8 domains and produced by 11
different LLMs. We calculate different linguistic features such as dependency
length and emotionality and we use them for characterizing human-written and
machine-generated texts along with different sampling strategies, repetition
controls and model release date. Our statistical analysis reveals that
human-written texts tend to exhibit simpler syntactic structures and more
diverse semantic content. Furthermore, we calculate the variability of our set
of features across models and domains. Both human and machine texts show
stylistic diversity across domains, with humans displaying greater variation in
our features. Finally, we apply style embeddings to further test variability
among human-written and machine-generated texts. Notably, newer models output
text that is similarly variable, pointing to an homogenization of
machine-generated texts.

</details>


### [19] [Seed-X: Building Strong Multilingual Translation LLM with 7B Parameters](https://arxiv.org/abs/2507.13618)
*Shanbo Cheng,Yu Bao,Qian Cao,Luyang Huang,Liyan Kang,Zhicheng Liu,Yu Lu,Wenhao Zhu,Zhichao Huang,Tao Li,Sitong Liu,Ningxin Peng,Shuaijie She,Lu Xu,Nuo Xu,Sen Yang,Runsheng Yu,Yiming Yu,Liehao Zou,Hang Li,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: Seed-X是一系列开源大语言模型，通过预训练和微调提升多语言翻译能力，性能媲美闭源模型。


<details>
  <summary>Details</summary>
Motivation: 解决多语言翻译中复杂的语言模式和生硬翻译问题。

Method: 预训练基础模型，微调指令模型并使用强化学习优化。

Result: 在28种语言中表现媲美顶尖闭源模型，显著优于其他开源模型。

Conclusion: Seed-X为翻译研究和应用提供了高效的开源解决方案。

Abstract: Multilingual translation stands as a challenging task for large language
models (LLMs) to handle intricate language patterns and stilted translations
that arise in automated translations. In this paper, we introduce Seed-X, a
family of open-source LLMs comprising instruct and reasoning models, pushing
the limits of translation capability with 7B parameter size. The base model is
pre-trained on a diverse, high-quality dataset encompassing both monolingual
and bilingual content across 28 languages, harnessing the full potential of
multilingual data. The instruct model is then finetuned to translate by
Chain-of-Thought (CoT) reasoning and further enhanced through reinforcement
learning (RL) to achieve better generalization across diverse language pairs.
Seed-X achieves performance comparable to leading closed-source models,
including Gemini-2.5 and GPT-4o, across 28 languages, and significantly
outperforms larger open-source models in both automatic metrics and human
evaluations. We share the best practices through our optimization process, and
make the parameter public available for advancing translation research and
applications.

</details>


### [20] [DENSE: Longitudinal Progress Note Generation with Temporal Modeling of Heterogeneous Clinical Notes Across Hospital Visits](https://arxiv.org/abs/2507.14079)
*Garapati Keerthana,Manik Gupta*

Main category: cs.CL

TL;DR: DENSE系统通过模拟医生参考过往记录的方式，生成临床连贯且时间敏感的进展记录，填补了电子健康记录中进展记录的缺失。


<details>
  <summary>Details</summary>
Motivation: 进展记录在电子健康记录中具有重要意义，但在大规模数据集中严重缺失，影响患者纵向叙事的完整性。

Method: DENSE采用细粒度分类和时间对齐机制，结合临床检索策略和大语言模型，生成连贯的进展记录。

Result: 生成的记录在时间对齐和纵向连贯性上优于原始记录，支持下游任务如总结和预测建模。

Conclusion: DENSE为实际医疗环境中基于大语言模型的记录合成提供了可扩展的解决方案。

Abstract: Progress notes are among the most clinically meaningful artifacts in an
Electronic Health Record (EHR), offering temporally grounded insights into a
patient's evolving condition, treatments, and care decisions. Despite their
importance, they are severely underrepresented in large-scale EHR datasets. For
instance, in the widely used Medical Information Mart for Intensive Care III
(MIMIC-III) dataset, only about $8.56\%$ of hospital visits include progress
notes, leaving gaps in longitudinal patient narratives. In contrast, the
dataset contains a diverse array of other note types, each capturing different
aspects of care.
  We present DENSE (Documenting Evolving Progress Notes from Scattered
Evidence), a system designed to align with clinical documentation workflows by
simulating how physicians reference past encounters while drafting progress
notes. The system introduces a fine-grained note categorization and a temporal
alignment mechanism that organizes heterogeneous notes across visits into
structured, chronological inputs. At its core, DENSE leverages a clinically
informed retrieval strategy to identify temporally and semantically relevant
content from both current and prior visits. This retrieved evidence is used to
prompt a large language model (LLM) to generate clinically coherent and
temporally aware progress notes.
  We evaluate DENSE on a curated cohort of patients with multiple visits and
complete progress note documentation. The generated notes demonstrate strong
longitudinal fidelity, achieving a temporal alignment ratio of $1.089$,
surpassing the continuity observed in original notes. By restoring narrative
coherence across fragmented documentation, our system supports improved
downstream tasks such as summarization, predictive modeling, and clinical
decision support, offering a scalable solution for LLM-driven note synthesis in
real-world healthcare settings.

</details>


### [21] [CU-ICU: Customizing Unsupervised Instruction-Finetuned Language Models for ICU Datasets via Text-to-Text Transfer Transformer](https://arxiv.org/abs/2507.13655)
*Teerapong Panboonyuen*

Main category: cs.CL

TL;DR: CU-ICU是一种针对ICU数据集的无监督指令微调方法，通过稀疏微调和少样本提示，显著提升了预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在医疗领域（如ICU）的领域适应性和标记数据不足的问题。

Method: 基于T5架构，结合稀疏微调和选择性参数更新，实现高效适应。

Result: 在脓毒症早期检测、死亡率预测和临床笔记生成等任务中，CU-ICU比标准微调方法提升了15%的准确性和20%的解释性，且仅更新不到1%的参数。

Conclusion: CU-ICU是一种可扩展、低开销的解决方案，适用于ICU环境中的临床决策支持。

Abstract: Integrating large language models into specialized domains like healthcare
presents unique challenges, including domain adaptation and limited labeled
data. We introduce CU-ICU, a method for customizing unsupervised
instruction-finetuned language models for ICU datasets by leveraging the
Text-to-Text Transfer Transformer (T5) architecture. CU-ICU employs a sparse
fine-tuning approach that combines few-shot prompting with selective parameter
updates, enabling efficient adaptation with minimal supervision. Our evaluation
across critical ICU tasks--early sepsis detection, mortality prediction, and
clinical note generation--demonstrates that CU-ICU consistently improves
predictive accuracy and interpretability over standard fine-tuning methods.
Notably, CU-ICU achieves up to a 15% increase in sepsis detection accuracy and
a 20% enhancement in generating clinically relevant explanations while updating
fewer than 1% of model parameters in its most efficient configuration. These
results establish CU-ICU as a scalable, low-overhead solution for delivering
accurate and interpretable clinical decision support in real-world ICU
environments.

</details>


### [22] [Lessons from the TREC Plain Language Adaptation of Biomedical Abstracts (PLABA) track](https://arxiv.org/abs/2507.14096)
*Brian Ondov,William Xia,Kush Attal,Ishita Unde,Jerry He,Hoa Dang,Ian Soboroff,Dina Demner-Fushman*

Main category: cs.CL

TL;DR: PLABA track评估了语言模型在将生物医学文献改写为通俗语言的能力，发现模型在事实准确性和完整性上接近人类水平，但在简洁性和自动评估工具上仍需改进。


<details>
  <summary>Details</summary>
Motivation: 生物医学文献对患者和护理人员难以理解，语言模型虽有潜力改写为通俗语言，但需严格评估以避免潜在危害。

Method: 通过PLABA竞赛（Task 1和Task 2）评估模型改写能力，结合专业参考和专家手动评估。

Result: 模型在事实准确性上表现优异，但简洁性不足；自动评估工具与人工评估相关性低。

Conclusion: 语言模型在生物医学文献改写中具有潜力，但需改进简洁性和自动评估工具。

Abstract: Objective: Recent advances in language models have shown potential to adapt
professional-facing biomedical literature to plain language, making it
accessible to patients and caregivers. However, their unpredictability,
combined with the high potential for harm in this domain, means rigorous
evaluation is necessary. Our goals with this track were to stimulate research
and to provide high-quality evaluation of the most promising systems.
  Methods: We hosted the Plain Language Adaptation of Biomedical Abstracts
(PLABA) track at the 2023 and 2024 Text Retrieval Conferences. Tasks included
complete, sentence-level, rewriting of abstracts (Task 1) as well as
identifying and replacing difficult terms (Task 2). For automatic evaluation of
Task 1, we developed a four-fold set of professionally-written references.
Submissions for both Tasks 1 and 2 were provided extensive manual evaluation
from biomedical experts.
  Results: Twelve teams spanning twelve countries participated in the track,
with models from multilayer perceptrons to large pretrained transformers. In
manual judgments of Task 1, top-performing models rivaled human levels of
factual accuracy and completeness, but not simplicity or brevity. Automatic,
reference-based metrics generally did not correlate well with manual judgments.
In Task 2, systems struggled with identifying difficult terms and classifying
how to replace them. When generating replacements, however, LLM-based systems
did well in manually judged accuracy, completeness, and simplicity, though not
in brevity.
  Conclusion: The PLABA track showed promise for using Large Language Models to
adapt biomedical literature for the general public, while also highlighting
their deficiencies and the need for improved automatic benchmarking tools.

</details>


### [23] [KiC: Keyword-inspired Cascade for Cost-Efficient Text Generation with LLMs](https://arxiv.org/abs/2507.13666)
*Woo-Chan Kim,Ji-Hoon Park,Seong-Whan Lee*

Main category: cs.CL

TL;DR: KiC框架通过语义对齐评估，在保持高准确率的同时显著降低LLM推理成本。


<details>
  <summary>Details</summary>
Motivation: 高成本API访问和现有级联方法在自由文本生成中的局限性。

Method: 提出Keyword-inspired Cascade (KiC)，通过评估语义对齐选择是否升级到更强模型。

Result: KiC在三个基准测试中达到GPT-4 97.53%的准确率，成本降低28.81%，并在特定任务中超越GPT-4。

Conclusion: KiC是一种高效且经济的自由文本生成解决方案。

Abstract: Large language models (LLMs) have demonstrated state-of-the-art performance
across a wide range of natural language processing tasks. However,
high-performing models are typically accessible only via APIs, incurring
substantial inference costs. Cascade methods address this by initially
employing a cheaper model and escalating to a stronger one only when necessary.
Nevertheless, existing cascade approaches struggle to select a reliable
representative response and assess the overall reliability of free-form
outputs, as they rely on exact text matching. To overcome these limitations, we
propose Keyword-inspired Cascade (KiC), a novel framework for cost-efficient
free-form text generation. KiC identifies the most representative answer among
multiple outputs from a weaker model and evaluates the semantic alignment of
other responses with it. Based on the degree of alignment, KiC determines
whether to accept the weaker model's output or escalate to a stronger model.
Experiments on three free-form text generation benchmarks show that KiC
achieves 97.53 percent of GPT-4's accuracy while reducing API costs by 28.81
percent on average, and even outperforms GPT-4 in a specific benchmark.

</details>


### [24] [LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for Multi-Turn Dialogues](https://arxiv.org/abs/2507.13681)
*Haoyang Li,Zhanchao Xu,Yiming Li,Xuejia Chen,Darian Li,Anxin Tian,Qingfa Xiao,Cheng Deng,Jun Wang,Qing Li,Lei Chen,Mingxuan Yuan*

Main category: cs.CL

TL;DR: LoopServe是一个用于多轮对话的自适应双阶段推理加速框架，通过动态稀疏化和渐进键值压缩优化大型语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有大型语言模型在长对话历史中面临的计算和内存挑战，提升效率和响应速度。

Method: 提出双阶段加速框架：在线稀疏化预填充阶段和渐进键值压缩解码阶段。

Result: 实验证明LoopServe在多种长上下文对话任务中优于现有基线，显著加速推理。

Conclusion: LoopServe通过自适应方法有效优化多轮对话中的模型性能，提供高效解决方案。

Abstract: Multi-turn dialogues are essential in many real-world applications of large
language models, such as chatbots and virtual assistants. As conversation
histories become longer, existing large language models face increasing
computational and memory challenges, which hinder their ability to provide
efficient and responsive interactions. Most current acceleration methods either
compress the context or optimize key value caching, but they often rely on
fixed or position-based heuristics that do not adapt well to the dynamic and
unpredictable patterns found in actual multi-turn conversations. In this paper,
we present LoopServe, an adaptive dual-phase inference acceleration framework
for large language models in multi-turn dialogues. LoopServe introduces two
main innovations. First, it performs online sparsification during the
prefilling phase by dynamically selecting the most important parts of the
attention matrix for each new input. Second, it uses progressive key value
compression during decoding by adaptively maintaining a relevant and efficient
cache based on the most recently generated output tokens. We also propose a
\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new
benchmark} with eleven multi-turn datasets that reflect realistic query
positions and conversational dependencies. Extensive experiments demonstrate
that LoopServe consistently achieves superior effectiveness compared to
existing baselines and significantly accelerates LLM inference across a wide
range of long-context dialogue tasks.

</details>


### [25] [The Judge Variable: Challenging Judge-Agnostic Legal Judgment Prediction](https://arxiv.org/abs/2507.13732)
*Guillaume Zambrano*

Main category: cs.CL

TL;DR: 研究通过机器学习预测法国上诉法院的儿童抚养权判决，发现法官个体决策模式显著影响结果，支持法律现实主义观点。


<details>
  <summary>Details</summary>
Motivation: 挑战法官中立假设，探讨个体法官决策模式对法律结果的影响。

Method: 使用18,937条判决数据，结合LLM和ML模型（RF、XGB、SVC），比较专家模型与通用模型的预测效果。

Result: 专家模型预测准确率更高（F1 92.85% vs 82.63%），显示法官个体模式稳定且不可转移。

Conclusion: 法官身份对法律结果有可测量影响，支持法律现实主义。

Abstract: This study examines the role of human judges in legal decision-making by
using machine learning to predict child physical custody outcomes in French
appellate courts. Building on the legal realism-formalism debate, we test
whether individual judges' decision-making patterns significantly influence
case outcomes, challenging the assumption that judges are neutral variables
that apply the law uniformly. To ensure compliance with French privacy laws, we
implement a strict pseudonymization process. Our analysis uses 18,937 living
arrangements rulings extracted from 10,306 cases. We compare models trained on
individual judges' past rulings (specialist models) with a judge-agnostic model
trained on aggregated data (generalist models). The prediction pipeline is a
hybrid approach combining large language models (LLMs) for structured feature
extraction and ML models for outcome prediction (RF, XGB and SVC). Our results
show that specialist models consistently achieve higher predictive accuracy
than the general model, with top-performing models reaching F1 scores as high
as 92.85%, compared to the generalist model's 82.63% trained on 20x to 100x
more samples. Specialist models capture stable individual patterns that are not
transferable to other judges. In-Domain and Cross-Domain validity tests provide
empirical support for legal realism, demonstrating that judicial identity plays
a measurable role in legal outcomes. All data and code used will be made
available.

</details>


### [26] [PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs](https://arxiv.org/abs/2507.13743)
*Maluna Menke,Thilo Hagendorff*

Main category: cs.CL

TL;DR: 论文探讨了如何通过参数高效微调技术（LoRA和软提示调优）减少大型语言模型（LLMs）对LGBTQIA+群体的偏见，发现LoRA能显著降低偏见分数。


<details>
  <summary>Details</summary>
Motivation: LLMs常因训练数据中的偏见而输出对LGBTQIA+群体不公的内容，亟需减少此类偏见。

Method: 使用WinoQueer基准评估LoRA和软提示调优两种技术，对比其在三种开源LLMs中的效果。

Result: LoRA（<0.1%额外参数）能将偏见分数降低50点，中立性从0%提升至36%；软提示调优效果有限。

Conclusion: LoRA能以极低计算成本显著提升公平性，建议推广社区参与的PEFT技术并扩展评估工具。

Abstract: Large Language Models (LLMs) frequently reproduce the gender- and
sexual-identity prejudices embedded in their training corpora, leading to
outputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of
great importance. To achieve this, we evaluate two parameter-efficient
fine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt
tuning - as lightweight alternatives to full-model fine-tuning for mitigating
such biases. Using the WinoQueer benchmark, we quantify bias in three
open-source LLMs and observe baseline bias scores reaching up to 98 (out of
100) across a range of queer identities defined by gender and/or sexual
orientation, where 50 would indicate neutrality. Fine-tuning with LoRA (< 0.1%
additional parameters) on a curated QueerNews corpus reduces those scores by up
to 50 points and raises neutrality from virtually 0% to as much as 36%.
Soft-prompt tuning (10 virtual tokens) delivers only marginal improvements.
These findings show that LoRA can deliver meaningful fairness gains with
minimal computation. We advocate broader adoption of community-informed PEFT,
the creation of larger queer-authored corpora, and richer evaluation suites
beyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.

</details>


### [27] [Innocence in the Crossfire: Roles of Skip Connections in Jailbreaking Visual Language Models](https://arxiv.org/abs/2507.13761)
*Palash Nandi,Maithili Joshi,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 研究发现视觉语言模型（VLMs）对提示设计敏感，三种关键因素可单独触发越狱行为，多模态环境下模型区分能力显著下降。


<details>
  <summary>Details</summary>
Motivation: 探究提示敏感性如何被利用生成不当内容，分析提示设计对VLMs生成不当内容的影响。

Method: 分析三种关键因素（详细视觉信息、对抗样本、积极开头短语）的影响，提出利用内部层跳连接的框架。

Result: 每种因素均可独立触发越狱，少量上下文示例即可导致不当输出；跳连接框架显著提高越狱成功率。

Conclusion: VLMs在多模态环境下存在复杂脆弱性，即使是看似无害的模因也可能引发有害内容。

Abstract: Language models are highly sensitive to prompt formulations - small changes
in input can drastically alter their output. This raises a critical question:
To what extent can prompt sensitivity be exploited to generate inapt content?
In this paper, we investigate how discrete components of prompt design
influence the generation of inappropriate content in Visual Language Models
(VLMs). Specifically, we analyze the impact of three key factors on successful
jailbreaks: (a) the inclusion of detailed visual information, (b) the presence
of adversarial examples, and (c) the use of positively framed beginning
phrases. Our findings reveal that while a VLM can reliably distinguish between
benign and harmful inputs in unimodal settings (text-only or image-only), this
ability significantly degrades in multimodal contexts. Each of the three
factors is independently capable of triggering a jailbreak, and we show that
even a small number of in-context examples (as few as three) can push the model
toward generating inappropriate outputs. Furthermore, we propose a framework
that utilizes a skip-connection between two internal layers of the VLM, which
substantially increases jailbreak success rates, even when using benign images.
Finally, we demonstrate that memes, often perceived as humorous or harmless,
can be as effective as toxic visuals in eliciting harmful content, underscoring
the subtle and complex vulnerabilities of VLMs.

</details>


### [28] [An Enhanced Model-based Approach for Short Text Clustering](https://arxiv.org/abs/2507.13793)
*Enhao Cheng,Shoujia Zhang,Jianhua Yin,Xuemeng Song,Tian Gan,Liqiang Nie*

Main category: cs.CL

TL;DR: 提出了一种改进的GSDMM+算法，用于短文本聚类，解决了稀疏性和高维度问题，并通过实验验证了其高效性和有效性。


<details>
  <summary>Details</summary>
Motivation: 短文本聚类在社交媒体中日益重要，但现有方法面临稀疏性、高维度和计算复杂度高的挑战。

Method: 提出了GSDMM+算法，通过减少初始化噪声、自适应调整词权重和策略性合并簇来优化性能。

Result: 实验表明，GSDMM+在效率和效果上优于经典和最新方法。

Conclusion: GSDMM+是一种高效的短文本聚类方法，适用于稀疏和高维数据。

Abstract: Short text clustering has become increasingly important with the popularity
of social media like Twitter, Google+, and Facebook. Existing methods can be
broadly categorized into two paradigms: topic model-based approaches and deep
representation learning-based approaches. This task is inherently challenging
due to the sparse, large-scale, and high-dimensional characteristics of the
short text data. Furthermore, the computational intensity required by
representation learning significantly increases the running time. To address
these issues, we propose a collapsed Gibbs Sampling algorithm for the Dirichlet
Multinomial Mixture model (GSDMM), which effectively handles the sparsity and
high dimensionality of short texts while identifying representative words for
each cluster. Based on several aspects of GSDMM that warrant further
refinement, we propose an improved approach, GSDMM+, designed to further
optimize its performance. GSDMM+ reduces initialization noise and adaptively
adjusts word weights based on entropy, achieving fine-grained clustering that
reveals more topic-related information. Additionally, strategic cluster merging
is employed to refine clustering granularity, better aligning the predicted
distribution with the true category distribution. We conduct extensive
experiments, comparing our methods with both classical and state-of-the-art
approaches. The experimental results demonstrate the efficiency and
effectiveness of our methods. The source code for our model is publicly
available at https://github.com/chehaoa/VEMC.

</details>


### [29] [The Expressions of Depression and Anxiety in Chinese Psycho-counseling: Usage of First-person Singular Pronoun and Negative Emotional Words](https://arxiv.org/abs/2507.13839)
*Lizhi Ma,Tong Zhao,Shuai Zhang,Nirui Song,Hongliang He,Anqi Li,Ran Feng,Huachuan Qiu,Jingsong Ma,Zhenzhong Lan*

Main category: cs.CL

TL;DR: 研究探讨了中文心理咨询中语言表达与抑郁、焦虑心理状态的关系，发现负面情绪词与心理状态严重程度显著正相关，但第一人称单数代词的使用频率与心理状态无关。


<details>
  <summary>Details</summary>
Motivation: 探索语言表达（负面情绪词和第一人称代词）与心理状态的关系，特别是在中国文化背景下与西方研究的差异。

Method: 基于735个在线心理咨询会话的语料库，使用LIWC软件量化语言模式，并通过混合效应模型分析。

Result: 负面情绪词与抑郁和焦虑严重程度显著正相关；第一人称单数代词使用频率与心理状态无显著关联。

Conclusion: 文化背景和心理咨询的互动特性影响语言使用，研究为中文人群的心理语言标记提供了新见解。

Abstract: This study explores the relationship between linguistic expressions and
psychological states of depression and anxiety within Chinese psycho-counseling
interactions, focusing specifically on the usage of first-person singular
pronouns and negative emotional words. Utilizing a corpus derived from 735
online counseling sessions, the analysis employed a general linear mixed-effect
model to assess linguistic patterns quantified by the Linguistic Inquiry and
Word Count (LIWC) software. Results indicate a significant positive correlation
between the frequency of negative emotional words and the severity of both
depressive and anxious states among clients. However, contrary to prior
findings predominantly derived from English-language contexts, the usage
frequency of first-person singular pronouns did not vary significantly with the
clients' psychological conditions. These outcomes are discussed within the
framework of cultural distinctions between collectivist Chinese contexts and
individualistic Western settings, as well as the interactive dynamics unique to
psycho-counseling conversations. The findings highlight the nuanced influence
of cultural and conversational contexts on language use in mental health
communications, providing insights into psycholinguistic markers relevant to
therapeutic practices in Chinese-speaking populations.

</details>


### [30] [Modeling Fair Play in Detective Stories with Language Models](https://arxiv.org/abs/2507.13841)
*Eitan Wagner,Renana Keydar,Omri Abend*

Main category: cs.CL

TL;DR: 论文提出了一个概率框架来定义侦探小说中的公平性（fair play），并设计相关指标。研究发现，LLM生成的故事虽然不可预测，但未能平衡惊喜与公平性，导致质量较差。


<details>
  <summary>Details</summary>
Motivation: 研究侦探小说中公平性的定义及其与故事连贯性和惊喜感的关系，以提升故事质量。

Method: 提出一个概率框架，定义公平性并设计指标，应用于LLM生成的侦探故事。

Result: LLM生成的故事缺乏惊喜与公平性的平衡，导致质量不佳。

Conclusion: 平衡惊喜与公平性是提升侦探小说质量的关键，LLM生成的故事需改进此方面。

Abstract: Effective storytelling relies on a delicate balance between meeting the
reader's prior expectations and introducing unexpected developments. In the
domain of detective fiction, this tension is known as fair play, which includes
the implicit agreement between the writer and the reader as to the range of
possible resolutions the mystery story may have. In this work, we present a
probabilistic framework for detective fiction that allows us to define desired
qualities. Using this framework, we formally define fair play and design
appropriate metrics for it. Stemming from these definitions is an inherent
tension between the coherence of the story, which measures how much it ``makes
sense'', and the surprise it induces. We validate the framework by applying it
to LLM-generated detective stories. This domain is appealing since we have an
abundance of data, we can sample from the distribution generating the story,
and the story-writing capabilities of LLMs are interesting in their own right.
Results show that while LLM-generated stories may be unpredictable, they
generally fail to balance the trade-off between surprise and fair play, which
greatly contributes to their poor quality.

</details>


### [31] [InTraVisTo: Inside Transformer Visualisation Tool](https://arxiv.org/abs/2507.13858)
*Nicolò Brunello,Davide Rigamonti,Andrea Sassella,Vincenzo Scotti,Mark James Carman*

Main category: cs.CL

TL;DR: 介绍了一种名为InTraVisTo的工具，用于可视化Transformer模型的内部计算过程和信息流，以帮助理解LLM的内部推理模式。


<details>
  <summary>Details</summary>
Motivation: LLM在生产环境中的应用仍具挑战性，因其行为不可预测且与期望输出存在差异，需工具来解析其内部计算。

Method: 开发了InTraVisTo工具，通过解码每层的token嵌入和Sankey图展示信息流，可视化Transformer模型的内部状态和计算过程。

Result: InTraVisTo提供了对LLM内部计算和推理过程的直观理解，揭示了模型的行为模式。

Conclusion: InTraVisTo有助于研究人员更深入地理解LLM的内部机制，为改进模型行为提供了工具支持。

Abstract: The reasoning capabilities of Large Language Models (LLMs) have increased
greatly over the last few years, as have their size and complexity.
Nonetheless, the use of LLMs in production remains challenging due to their
unpredictable nature and discrepancies that can exist between their desired
behavior and their actual model output. In this paper, we introduce a new tool,
InTraVisTo (Inside Transformer Visualisation Tool), designed to enable
researchers to investigate and trace the computational process that generates
each token in a Transformer-based LLM. InTraVisTo provides a visualization of
both the internal state of the Transformer model (by decoding token embeddings
at each layer of the model) and the information flow between the various
components across the different layers of the model (using a Sankey diagram).
With InTraVisTo, we aim to help researchers and practitioners better understand
the computations being performed within the Transformer model and thus to shed
some light on internal patterns and reasoning processes employed by LLMs.

</details>


### [32] [Label Unification for Cross-Dataset Generalization in Cybersecurity NER](https://arxiv.org/abs/2507.13870)
*Maciej Jalocha,Johan Hausted Schmidt,William Michelseen*

Main category: cs.CL

TL;DR: 研究探讨了网络安全NER领域标签标准化问题，通过粗粒度标签统一和跨数据集评估，发现模型在统一数据集上泛化能力差，并提出多头和基于图的迁移模型，但改进有限。


<details>
  <summary>Details</summary>
Motivation: 网络安全NER领域缺乏标准化标签，导致数据集难以结合，研究旨在提高数据资源的可用性。

Method: 进行粗粒度标签统一，使用BiLSTM模型进行跨数据集评估，并提出多头和基于图的迁移模型。

Result: 模型在统一数据集上泛化能力差，多头模型仅略有改进，基于图的迁移模型性能未显著提升。

Conclusion: 标签统一和现有模型改进效果有限，需进一步研究解决网络安全NER数据集差异问题。

Abstract: The field of cybersecurity NER lacks standardized labels, making it
challenging to combine datasets. We investigate label unification across four
cybersecurity datasets to increase data resource usability. We perform a
coarse-grained label unification and conduct pairwise cross-dataset evaluations
using BiLSTM models. Qualitative analysis of predictions reveals errors,
limitations, and dataset differences. To address unification limitations, we
propose alternative architectures including a multihead model and a graph-based
transfer model. Results show that models trained on unified datasets generalize
poorly across datasets. The multihead model with weight sharing provides only
marginal improvements over unified training, while our graph-based transfer
model built on BERT-base-NER shows no significant performance gains compared
BERT-base-NER.

</details>


### [33] [Optimizing ASR for Catalan-Spanish Code-Switching: A Comparative Analysis of Methodologies](https://arxiv.org/abs/2507.13875)
*Carlos Mena,Pol Serra,Jacobo Romero,Abir Messaoudi,Jose Giraldo,Carme Armentano-Oller,Rodolfo Zevallos,Ivan Meza,Javier Hernando*

Main category: cs.CL

TL;DR: 论文探讨了如何通过合成数据、拼接单语音频和利用真实语码转换数据来提升加泰罗尼亚语-西班牙语语码转换的自动语音识别性能。


<details>
  <summary>Details</summary>
Motivation: 语码转换（CS）在自动语音识别（ASR）中因训练数据稀缺和语言相似性而面临挑战，尤其是在多语言社会中。

Method: 采用三种策略：生成合成CS数据、拼接单语音频、利用带有语言标记的真实CS数据，并基于Whisper模型进行微调。

Result: 结合少量合成CS数据和主导语言标记的模型表现最佳。

Conclusion: 该方法有效提升了加泰罗尼亚语-西班牙语CS的ASR性能，模型已开源。

Abstract: Code-switching (CS), the alternating use of two or more languages, challenges
automatic speech recognition (ASR) due to scarce training data and linguistic
similarities. The lack of dedicated CS datasets limits ASR performance, as most
models rely on monolingual or mixed-language corpora that fail to reflect
real-world CS patterns. This issue is critical in multilingual societies where
CS occurs in informal and formal settings. A key example is Catalan-Spanish CS,
widely used in media and parliamentary speeches. In this work, we improve ASR
for Catalan-Spanish CS by exploring three strategies: (1) generating synthetic
CS data, (2) concatenating monolingual audio, and (3) leveraging real CS data
with language tokens. We extract CS data from Catalan speech corpora and
fine-tune OpenAI's Whisper models, making them available on Hugging Face.
Results show that combining a modest amount of synthetic CS data with the
dominant language token yields the best transcription performance.

</details>


### [34] [Using LLMs to identify features of personal and professional skills in an open-response situational judgment test](https://arxiv.org/abs/2507.13881)
*Cole Walsh,Rodica Ivan,Muhammad Zafar Iqbal,Colleen Robb*

Main category: cs.CL

TL;DR: 论文探讨了利用大型语言模型（LLMs）从情境判断测试（SJTs）中提取相关特征的新方法，以解决传统人工评分在规模化应用中的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着对学生个人和职业技能需求的增长，需要可扩展的系统来测量和评估这些技能。传统SJTs依赖人工评分，难以规模化。

Method: 提出了一种基于LLMs的方法，从SJT回答中提取相关特征，并以Casper SJT为例验证其有效性。

Result: 该方法为个人和职业技能的自动化评分奠定了基础。

Conclusion: 研究展示了LLMs在SJT评分中的潜力，为未来自动化评分系统的发展提供了方向。

Abstract: Academic programs are increasingly recognizing the importance of personal and
professional skills and their critical role alongside technical expertise in
preparing students for future success in diverse career paths. With this
growing demand comes the need for scalable systems to measure, evaluate, and
develop these skills. Situational Judgment Tests (SJTs) offer one potential
avenue for measuring these skills in a standardized and reliable way, but
open-response SJTs have traditionally relied on trained human raters for
evaluation, presenting operational challenges to delivering SJTs at scale. Past
attempts at developing NLP-based scoring systems for SJTs have fallen short due
to issues with construct validity of these systems. In this article, we explore
a novel approach to extracting construct-relevant features from SJT responses
using large language models (LLMs). We use the Casper SJT to demonstrate the
efficacy of this approach. This study sets the foundation for future
developments in automated scoring for personal and professional skills.

</details>


### [35] [Political Leaning and Politicalness Classification of Texts](https://arxiv.org/abs/2507.13913)
*Matous Volf,Jakub Simko*

Main category: cs.CL

TL;DR: 本文提出了一种基于Transformer模型的文本政治倾向和政治性自动分类方法，通过整合数据集和改进模型泛化能力来解决现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在分布外文本上表现不佳，且数据集和模型存在孤立性。

Method: 整合12个政治倾向分类数据集，扩展18个数据集以创建新的政治性分类数据集，并通过留一法和留出法进行基准测试。

Result: 新模型在泛化能力上表现优于现有方法。

Conclusion: 通过数据整合和模型优化，显著提升了政治文本分类的泛化性能。

Abstract: This paper addresses the challenge of automatically classifying text
according to political leaning and politicalness using transformer models. We
compose a comprehensive overview of existing datasets and models for these
tasks, finding that current approaches create siloed solutions that perform
poorly on out-of-distribution texts. To address this limitation, we compile a
diverse dataset by combining 12 datasets for political leaning classification
and creating a new dataset for politicalness by extending 18 existing datasets
with the appropriate label. Through extensive benchmarking with leave-one-in
and leave-one-out methodologies, we evaluate the performance of existing models
and train new ones with enhanced generalization capabilities.

</details>


### [36] [The Levers of Political Persuasion with Conversational AI](https://arxiv.org/abs/2507.13919)
*Kobi Hackenburg,Ben M. Tappin,Luke Hewitt,Ed Saunders,Sid Black,Hause Lin,Catherine Fist,Helen Margetts,David G. Rand,Christopher Summerfield*

Main category: cs.CL

TL;DR: 研究表明，当前和近期的AI说服力主要来自后训练和提示方法，而非模型规模或个人化，但这些方法会降低事实准确性。


<details>
  <summary>Details</summary>
Motivation: 评估对话AI对人类信念的潜在影响，特别是其在政治议题上的说服力和事实准确性。

Method: 通过三个大规模实验（N=76,977），测试19个LLM在707个政治议题上的说服力，并检查466,769个LLM生成声明的事实准确性。

Result: 后训练和提示方法显著提升说服力（分别达51%和27%），但会降低事实准确性。模型规模和个人化对说服力影响较小。

Conclusion: 当前AI的说服力主要依赖于后训练和提示方法，但这些方法可能以牺牲事实准确性为代价。

Abstract: There are widespread fears that conversational AI could soon exert
unprecedented influence over human beliefs. Here, in three large-scale
experiments (N=76,977), we deployed 19 LLMs-including some post-trained
explicitly for persuasion-to evaluate their persuasiveness on 707 political
issues. We then checked the factual accuracy of 466,769 resulting LLM claims.
Contrary to popular concerns, we show that the persuasive power of current and
near-future AI is likely to stem more from post-training and prompting
methods-which boosted persuasiveness by as much as 51% and 27%
respectively-than from personalization or increasing model scale. We further
show that these methods increased persuasion by exploiting LLMs' unique ability
to rapidly access and strategically deploy information and that, strikingly,
where they increased AI persuasiveness they also systematically decreased
factual accuracy.

</details>


### [37] [Marcel: A Lightweight and Open-Source Conversational Agent for University Student Support](https://arxiv.org/abs/2507.13937)
*Jan Trienes,Anastasiia Derzhanskaia,Roland Schwarzkopf,Markus Mühling,Jörg Schlötterer,Christin Seifert*

Main category: cs.CL

TL;DR: Marcel是一个轻量级开源对话代理，旨在帮助准学生解答入学相关问题，减轻大学工作人员负担。


<details>
  <summary>Details</summary>
Motivation: 支持准学生的入学咨询需求，同时减少大学工作人员的工作量。

Method: 采用检索增强生成技术，结合FAQ检索器优化检索质量，确保回答基于大学资源且可验证。

Result: 系统架构详细，技术评估显示其组件性能良好，并在实际部署中取得成效。

Conclusion: Marcel为资源有限的学术环境提供了易于部署的解决方案，有效支持入学咨询。

Abstract: We present Marcel, a lightweight and open-source conversational agent
designed to support prospective students with admission-related inquiries. The
system aims to provide fast and personalized responses, while reducing workload
of university staff. We employ retrieval-augmented generation to ground answers
in university resources and to provide users with verifiable, contextually
relevant information. To improve retrieval quality, we introduce an FAQ
retriever that maps user questions to knowledge-base entries, allowing
administrators to steer retrieval, and improving over standard dense/hybrid
retrieval strategies. The system is engineered for easy deployment in
resource-constrained academic settings. We detail the system architecture,
provide a technical evaluation of its components, and report insights from a
real-world deployment.

</details>


### [38] [Exploiting Primacy Effect To Improve Large Language Models](https://arxiv.org/abs/2507.13949)
*Bianca Raimondi,Maurizio Gabbrielli*

Main category: cs.CL

TL;DR: 研究发现微调后的LLMs在MCQA任务中存在首因效应偏差，通过语义相似度重新排序选项可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在MCQA任务中的首因效应偏差及其影响。

Method: 通过重新排序选项基于语义相似度，无需正确答案知识。

Result: 实验表明该方法显著提升MCQA任务性能。

Conclusion: 偏差既是挑战也是机会，为偏置感知模型设计提供见解。

Abstract: Large Language Models (LLMs) have become essential in many Natural Language
Processing (NLP) tasks, leveraging extensive pre-training and fine-tuning to
achieve high accuracy. However, like humans, LLMs exhibit biases, particularly
positional biases such as primacy and recency effects, which can influence the
accuracy of the answers. The primacy effect-where items presented first are
more likely to be remembered or selected-plays a key role in Multiple Choice
Question Answering (MCQA), where the order of answer options can affect
prediction outcomes. This study focuses on primacy bias in fine-tuned LLMs: We
first show that fine-tuning amplifies this bias, probably due to exposure to
human-like patterns. Hence, we strategically leverage this effect by reordering
response options based on semantic similarity to the query, without requiring
knowledge of the correct answer. Our experimental results show that this
approach significantly improves performance in MCQA. More generally, our
findings underscore the dual nature of biases as both challenges and
opportunities, offering insights for bias-aware model design and NLP
applications.

</details>


### [39] [Bottom-up Domain-specific Superintelligence: A Reliable Knowledge Graph is What We Need](https://arxiv.org/abs/2507.13966)
*Bhishma Dedhia,Yuval Kansal,Niraj K. Jha*

Main category: cs.CL

TL;DR: 该论文提出了一种基于知识图谱（KG）的任务生成方法，通过组合简单领域概念来训练语言模型，实现领域特定超级智能。在医学领域验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型在跨领域泛化中表现不足，缺乏深度领域专业知识。通过知识图谱的组合性结构，可以构建更复杂的领域概念。

Method: 利用知识图谱生成任务，训练语言模型（如QwQ-32B）以获取和组合领域概念，最终得到QwQ-Med-3模型。

Result: QwQ-Med-3在医学领域显著优于现有推理模型，并在ICD-Bench评测中表现出色。

Conclusion: 领域特定超级智能可能是实现通用人工智能（AGI）的有效途径。

Abstract: Language models traditionally used for cross-domain generalization have
recently demonstrated task-specific reasoning. However, their top-down training
approach on general corpora is insufficient for acquiring abstractions needed
for deep domain expertise. This may require a bottom-up approach that acquires
expertise by learning to compose simple domain concepts into more complex ones.
A knowledge graph (KG) provides this compositional structure, where domain
primitives are represented as head-relation-tail edges and their paths encode
higher-level concepts. We present a task generation pipeline that synthesizes
tasks directly from KG primitives, enabling models to acquire and compose them
for reasoning. We fine-tune language models on the resultant KG-grounded
curriculum to demonstrate domain-specific superintelligence. While broadly
applicable, we validate our approach in medicine, where reliable KGs exist.
Using a medical KG, we curate 24,000 reasoning tasks paired with thinking
traces derived from diverse medical primitives. We fine-tune the QwQ-32B model
on this curriculum to obtain QwQ-Med-3 that takes a step towards medical
superintelligence. We also introduce ICD-Bench, an evaluation suite to quantify
reasoning abilities across 15 medical domains. Our experiments demonstrate that
QwQ-Med-3 significantly outperforms state-of-the-art reasoning models on
ICD-Bench categories. Further analysis reveals that QwQ-Med-3 utilizes acquired
primitives to widen the performance gap on the hardest tasks of ICD-Bench.
Finally, evaluation on medical question-answer benchmarks shows that QwQ-Med-3
transfers acquired expertise to enhance the base model's performance. While the
industry's approach to artificial general intelligence (AGI) emphasizes broad
expertise, we envision a future in which AGI emerges from the composable
interaction of efficient domain-specific superintelligent agents.

</details>


### [40] [Open Automatic Speech Recognition Models for Classical and Modern Standard Arabic](https://arxiv.org/abs/2507.13977)
*Lilit Grigoryan,Nikolay Karpov,Enas Albasiri,Vitaly Lavrukhin,Boris Ginsburg*

Main category: cs.CL

TL;DR: 本文提出了一种通用的阿拉伯语语音和文本处理方法，并基于FastConformer架构训练了两个新模型：一个针对现代标准阿拉伯语（MSA），另一个首次统一处理MSA和古典阿拉伯语（CA）。MSA模型在相关数据集上达到SOTA性能，统一模型在CA带音标任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语自动语音识别（ASR）系统发展面临挑战，尤其是语言变体研究不足。本文旨在解决阿拉伯语的独特问题。

Method: 提出通用方法，基于FastConformer架构训练两个模型：MSA专用模型和首个MSA与CA统一模型。

Result: MSA模型在相关数据集上达到SOTA性能；统一模型在CA带音标任务上表现优异，同时保持MSA的强性能。

Conclusion: 通过开源模型和训练方法，促进可重复性，填补了阿拉伯语ASR领域的空白。

Abstract: Despite Arabic being one of the most widely spoken languages, the development
of Arabic Automatic Speech Recognition (ASR) systems faces significant
challenges due to the language's complexity, and only a limited number of
public Arabic ASR models exist. While much of the focus has been on Modern
Standard Arabic (MSA), there is considerably less attention given to the
variations within the language. This paper introduces a universal methodology
for Arabic speech and text processing designed to address unique challenges of
the language. Using this methodology, we train two novel models based on the
FastConformer architecture: one designed specifically for MSA and the other,
the first unified public model for both MSA and Classical Arabic (CA). The MSA
model sets a new benchmark with state-of-the-art (SOTA) performance on related
datasets, while the unified model achieves SOTA accuracy with diacritics for CA
while maintaining strong performance for MSA. To promote reproducibility, we
open-source the models and their training recipes.

</details>


### [41] [Efficient Temporal Tokenization for Mobility Prediction with Large Language Models](https://arxiv.org/abs/2507.14017)
*Haoyu He,Haozheng Luo,Yan Chen,Qi R. Wang*

Main category: cs.CL

TL;DR: RHYTHM框架利用LLM作为时空预测器和轨迹推理器，通过分层注意力编码轨迹，显著提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法在捕捉人类移动轨迹时空依赖时的计算复杂性和效率问题。

Method: 将轨迹分段为每日令牌，利用分层注意力和预计算提示嵌入增强LLM的推理能力，同时冻结LLM主干以减少计算开销。

Result: 在三个真实数据集上，准确率提升2.4%，周末表现提升5.0%，训练时间减少24.6%。

Conclusion: RHYTHM通过高效的分层令牌化和冻结LLM，显著提升了时空预测的性能和效率。

Abstract: We introduce RHYTHM (Reasoning with Hierarchical Temporal Tokenization for
Human Mobility), a framework that leverages large language models (LLMs) as
spatio-temporal predictors and trajectory reasoners. RHYTHM partitions
trajectories into daily segments encoded as discrete tokens with hierarchical
attention, capturing both daily and weekly dependencies while substantially
reducing the sequence length. Token representations are enriched with
pre-computed prompt embeddings via a frozen LLM, enhancing the model's ability
to capture interdependencies without extensive computational overhead. By
freezing the LLM backbone, RHYTHM achieves significant computational
efficiency. Evaluation on three real-world datasets demonstrates a 2.4%
improvement in accuracy, 5.0% increase on weekends, and 24.6% reduction in
training time compared to state-of-the-art methods.

</details>


### [42] [CPC-CMS: Cognitive Pairwise Comparison Classification Model Selection Framework for Document-level Sentiment Analysis](https://arxiv.org/abs/2507.14022)
*Jianfei Li,Kevin Kam Fung Yuen*

Main category: cs.CL

TL;DR: 提出了CPC-CMS框架用于文档级情感分析，通过专家知识计算评价标准权重，选择最佳分类模型。


<details>
  <summary>Details</summary>
Motivation: 解决文档级情感分析中如何选择最佳分类模型的问题。

Method: 基于专家知识计算权重，构建加权决策矩阵，比较多种基线模型（如Naive Bayes、LSTM等）。

Result: ALBERT在三个数据集上表现最佳（不考虑时间因素）；若考虑时间消耗，无单一模型始终最优。

Conclusion: CPC-CMS可扩展至其他分类应用领域。

Abstract: This study proposes the Cognitive Pairwise Comparison Classification Model
Selection (CPC-CMS) framework for document-level sentiment analysis. The CPC,
based on expert knowledge judgment, is used to calculate the weights of
evaluation criteria, including accuracy, precision, recall, F1-score,
specificity, Matthews Correlation Coefficient (MCC), Cohen's Kappa (Kappa), and
efficiency. Naive Bayes, Linear Support Vector Classification (LSVC), Random
Forest, Logistic Regression, Extreme Gradient Boosting (XGBoost), Long
Short-Term Memory (LSTM), and A Lite Bidirectional Encoder Representations from
Transformers (ALBERT) are chosen as classification baseline models. A weighted
decision matrix consisting of classification evaluation scores with respect to
criteria weights, is formed to select the best classification model for a
classification problem. Three open datasets of social media are used to
demonstrate the feasibility of the proposed CPC-CMS. Based on our simulation,
for evaluation results excluding the time factor, ALBERT is the best for the
three datasets; if time consumption is included, no single model always
performs better than the other models. The CPC-CMS can be applied to the other
classification applications in different areas.

</details>


### [43] [Evaluating the Effectiveness of Cost-Efficient Large Language Models in Benchmark Biomedical Tasks](https://arxiv.org/abs/2507.14045)
*Israt Jahan,Md Tahmid Rahman Laskar,Chun Peng,Jimmy Huang*

Main category: cs.CL

TL;DR: 该论文评估了多种成本效益高的大型语言模型（LLMs）在生物医学任务中的表现，发现不同模型在不同任务中表现各异，开源模型在某些任务中甚至优于闭源模型。


<details>
  <summary>Details</summary>
Motivation: 研究动机是评估不同LLMs在生物医学任务中的表现，为特定应用选择最优模型提供依据。

Method: 方法包括对闭源和开源LLMs在文本分类、生成、问答及多模态图像处理等任务上的全面评估。

Result: 实验结果表明，没有单一LLM在所有任务中表现最佳，开源模型在某些任务中表现优于闭源模型。

Conclusion: 结论是应根据具体任务需求选择模型，开源模型在速度、隐私等方面具有额外优势。

Abstract: This paper presents a comprehensive evaluation of cost-efficient Large
Language Models (LLMs) for diverse biomedical tasks spanning both text and
image modalities. We evaluated a range of closed-source and open-source LLMs on
tasks such as biomedical text classification and generation, question
answering, and multimodal image processing. Our experimental findings indicate
that there is no single LLM that can consistently outperform others across all
tasks. Instead, different LLMs excel in different tasks. While some
closed-source LLMs demonstrate strong performance on specific tasks, their
open-source counterparts achieve comparable results (sometimes even better),
with additional benefits like faster inference and enhanced privacy. Our
experimental results offer valuable insights for selecting models that are
optimally suited for specific biomedical applications.

</details>


### [44] [Collaborative Rational Speech Act: Pragmatic Reasoning for Multi-Turn Dialog](https://arxiv.org/abs/2507.14063)
*Lautaro Estienne,Gabriel Ben Zenou,Nona Naderi,Jackie Cheung,Pablo Piantanida*

Main category: cs.CL

TL;DR: 论文提出了一种协作理性言语行为（CRSA）框架，扩展了RSA模型，用于多轮对话场景，优化信息增益函数，并在实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: AI系统在协作角色中需要推理共享目标和信念，而现有RSA扩展在多轮协作场景中面临挑战。

Method: 提出CRSA框架，基于信息论扩展RSA，优化增益函数，适用于多轮对话。

Result: 在指称游戏和医患对话实验中，CRSA表现出更一致、可解释和协作的行为。

Conclusion: CRSA为更实用和社交意识的语言代理提供了新途径。

Abstract: As AI systems take on collaborative roles, they must reason about shared
goals and beliefs-not just generate fluent language. The Rational Speech Act
(RSA) framework offers a principled approach to pragmatic reasoning, but
existing extensions face challenges in scaling to multi-turn, collaborative
scenarios. In this paper, we introduce Collaborative Rational Speech Act
(CRSA), an information-theoretic (IT) extension of RSA that models multi-turn
dialog by optimizing a gain function adapted from rate-distortion theory. This
gain is an extension of the gain model that is maximized in the original RSA
model but takes into account the scenario in which both agents in a
conversation have private information and produce utterances conditioned on the
dialog. We demonstrate the effectiveness of CRSA on referential games and
template-based doctor-patient dialogs in the medical domain. Empirical results
show that CRSA yields more consistent, interpretable, and collaborative
behavior than existing baselines-paving the way for more pragmatic and socially
aware language agents.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [45] [GraphTrafficGPT: Enhancing Traffic Management Through Graph-Based AI Agent Coordination](https://arxiv.org/abs/2507.13511)
*Nabil Abdelaziz Ferhat Taleb,Abdolazim Rezaei,Raj Atulkumar Patel,Mehdi Sookhak*

Main category: cs.AI

TL;DR: GraphTrafficGPT是一种基于图的架构，通过并行执行和动态资源分配优化LLM驱动的交通管理任务，显著降低令牌消耗和响应延迟。


<details>
  <summary>Details</summary>
Motivation: 现有链式系统（如TrafficGPT）存在顺序执行、高令牌消耗和扩展性差的问题，无法满足复杂交通管理需求。

Method: 提出GraphTrafficGPT，将任务及其依赖关系表示为有向图中的节点和边，通过Brain Agent分解查询并协调多个专业代理。

Result: 实验显示，GraphTrafficGPT令牌消耗减少50.2%，响应延迟降低19.0%，多查询执行效率提升23.0%。

Conclusion: GraphTrafficGPT显著提升了LLM在交通管理中的效率和扩展性。

Abstract: Large Language Models (LLMs) offer significant promise for intelligent
traffic management; however, current chain-based systems like TrafficGPT are
hindered by sequential task execution, high token usage, and poor scalability,
making them inefficient for complex, real-world scenarios. To address these
limitations, we propose GraphTrafficGPT, a novel graph-based architecture,
which fundamentally redesigns the task coordination process for LLM-driven
traffic applications. GraphTrafficGPT represents tasks and their dependencies
as nodes and edges in a directed graph, enabling efficient parallel execution
and dynamic resource allocation. The main idea behind the proposed model is a
Brain Agent that decomposes user queries, constructs optimized dependency
graphs, and coordinates a network of specialized agents for data retrieval,
analysis, visualization, and simulation. By introducing advanced context-aware
token management and supporting concurrent multi-query processing, the proposed
architecture handles interdependent tasks typical of modern urban mobility
environments. Experimental results demonstrate that GraphTrafficGPT reduces
token consumption by 50.2% and average response latency by 19.0% compared to
TrafficGPT, while supporting simultaneous multi-query execution with up to
23.0% improvement in efficiency.

</details>


### [46] [PrefPalette: Personalized Preference Modeling with Latent Attributes](https://arxiv.org/abs/2507.13541)
*Shuyue Stella Li,Melanie Sclar,Hunter Lang,Ansong Ni,Jacqueline He,Puxin Xu,Andrew Cohen,Chan Young Park,Yulia Tsvetkov,Asli Celikyilmaz*

Main category: cs.AI

TL;DR: PrefPalette通过分解偏好为属性维度，并基于社区价值观进行预测，显著优于GPT-4o，同时提供可解释的社区偏好分析。


<details>
  <summary>Details</summary>
Motivation: 当前偏好模型将人类判断视为黑箱，缺乏对偏好背后原因的理解。PrefPalette旨在通过多属性决策原则，提供透明且个性化的偏好预测。

Method: PrefPalette采用两步方法：1) 生成合成数据以隔离属性效应；2) 基于注意力的偏好建模，动态学习不同社区对属性的权重分配。

Result: 在Reddit的45个社区中，PrefPalette的平均预测准确率比GPT-4o高46.6%，并揭示了社区特定的偏好模式。

Conclusion: PrefPalette不仅提升了预测性能，还提供了可解释的偏好分析，为更可信的个性化应用奠定了基础。

Abstract: Personalizing AI systems requires understanding not just what users prefer,
but the reasons that underlie those preferences - yet current preference models
typically treat human judgment as a black box. We introduce PrefPalette, a
framework that decomposes preferences into attribute dimensions and tailors its
preference prediction to distinct social community values in a
human-interpretable manner. PrefPalette operationalizes a cognitive science
principle known as multi-attribute decision making in two ways: (1) a scalable
counterfactual attribute synthesis step that involves generating synthetic
training data to isolate for individual attribute effects (e.g., formality,
humor, cultural values), and (2) attention-based preference modeling that
learns how different social communities dynamically weight these attributes.
This approach moves beyond aggregate preference modeling to capture the diverse
evaluation frameworks that drive human judgment. When evaluated on 45 social
communities from the online platform Reddit, PrefPalette outperforms GPT-4o by
46.6% in average prediction accuracy. Beyond raw predictive improvements,
PrefPalette also shed light on intuitive, community-specific profiles:
scholarly communities prioritize verbosity and stimulation, conflict-oriented
communities value sarcasm and directness, and support-based communities
emphasize empathy. By modeling the attribute-mediated structure of human
judgment, PrefPalette delivers both superior preference modeling and
transparent, interpretable insights, and serves as a first step toward more
trustworthy, value-aware personalized applications.

</details>


### [47] [GOFAI meets Generative AI: Development of Expert Systems by means of Large Language Models](https://arxiv.org/abs/2507.13550)
*Eduardo C. Garrido-Merchán,Cristina Puente*

Main category: cs.AI

TL;DR: 本文提出了一种结合大语言模型（LLMs）和符号系统的新方法，以开发可控、透明的专家系统，解决LLMs的幻觉和不可靠问题。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在生成知识时存在幻觉和不可验证事实的问题，限制了其在敏感领域的应用。本文旨在通过结合LLMs和符号系统，开发可靠、可解释的专家系统。

Method: 通过限制领域并使用结构化提示提取方法，将知识转化为Prolog符号表示，并由人类专家验证和修正。

Result: 实验表明，该方法在事实准确性和语义连贯性上表现优异，结合了LLMs的召回能力和符号系统的精确性。

Conclusion: 该方法为敏感领域提供了可靠、可扩展的AI解决方案，奠定了可信赖AI应用的基础。

Abstract: The development of large language models (LLMs) has successfully transformed
knowledge-based systems such as open domain question nswering, which can
automatically produce vast amounts of seemingly coherent information. Yet,
those models have several disadvantages like hallucinations or confident
generation of incorrect or unverifiable facts. In this paper, we introduce a
new approach to the development of expert systems using LLMs in a controlled
and transparent way. By limiting the domain and employing a well-structured
prompt-based extraction approach, we produce a symbolic representation of
knowledge in Prolog, which can be validated and corrected by human experts.
This approach also guarantees interpretability, scalability and reliability of
the developed expert systems. Via quantitative and qualitative experiments with
Claude Sonnet 3.7 and GPT-4.1, we show strong adherence to facts and semantic
coherence on our generated knowledge bases. We present a transparent hybrid
solution that combines the recall capacity of LLMs with the precision of
symbolic systems, thereby laying the foundation for dependable AI applications
in sensitive domains.

</details>


### [48] [Why Isn't Relational Learning Taking Over the World?](https://arxiv.org/abs/2507.13558)
*David Poole*

Main category: cs.AI

TL;DR: 论文探讨了当前AI主要建模像素和文字，而忽略了实体及其关系的重要性，并分析了关系学习未普及的原因及改进方向。


<details>
  <summary>Details</summary>
Motivation: 当前AI主要关注像素和文字建模，但世界由实体及其关系构成，关系学习应成为核心。

Method: 分析关系学习未普及的原因，并提出改进建议。

Result: 关系学习仅在有限场景中应用，需进一步推广。

Conclusion: 关系学习应成为AI的核心方向，需更多研究和实践支持。

Abstract: AI seems to be taking over the world with systems that model pixels, words,
and phonemes. The world is arguably made up, not of pixels, words, and phonemes
but of entities (objects, things, including events) with properties and
relations among them. Surely we should model these, not the perception or
description of them. You might suspect that concentrating on modeling words and
pixels is because all of the (valuable) data in the world is in terms of text
and images. If you look into almost any company you will find their most
valuable data is in spreadsheets, databases and other relational formats. These
are not the form that are studied in introductory machine learning, but are
full of product numbers, student numbers, transaction numbers and other
identifiers that can't be interpreted naively as numbers. The field that
studies this sort of data has various names including relational learning,
statistical relational AI, and many others. This paper explains why relational
learning is not taking over the world -- except in a few cases with restricted
relations -- and what needs to be done to bring it to it's rightful prominence.

</details>


### [49] [BifrostRAG: Bridging Dual Knowledge Graphs for Multi-Hop Question Answering in Construction Safety](https://arxiv.org/abs/2507.13625)
*Yuxin Zhang,Xi Wang,Mo Hu,Zhenyu Zhang*

Main category: cs.AI

TL;DR: BifrostRAG是一种双图RAG系统，通过实体网络图和文档导航图结合语言关系和文档结构，显著提升多跳问题回答的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决安全法规文本的语言和结构复杂性对自动化合规检查的阻碍，特别是多跳查询的挑战。

Method: 提出BifrostRAG，结合实体网络图和文档导航图，实现图遍历与向量语义搜索的混合检索机制。

Result: 在多跳问题数据集上，BifrostRAG达到92.8%精确率、85.5%召回率和87.3% F1分数，显著优于基线方法。

Conclusion: BifrostRAG为复杂技术文档的知识检索提供了可迁移的解决方案，适用于知识密集型工程领域。

Abstract: Information retrieval and question answering from safety regulations are
essential for automated construction compliance checking but are hindered by
the linguistic and structural complexity of regulatory text. Many
compliance-related queries are multi-hop, requiring synthesis of information
across interlinked clauses. This poses a challenge for traditional
retrieval-augmented generation (RAG) systems. To overcome this, we introduce
BifrostRAG: a dual-graph RAG-integrated system that explicitly models both
linguistic relationships (via an Entity Network Graph) and document structure
(via a Document Navigator Graph). This architecture powers a hybrid retrieval
mechanism that combines graph traversal with vector-based semantic search,
enabling large language models to reason over both the meaning and the
structure of the text. Evaluation on a multi-hop question dataset shows that
BifrostRAG achieves 92.8 percent precision, 85.5 percent recall, and an F1
score of 87.3 percent. These results significantly outperform vector-only and
graph-only RAG baselines that represent current leading approaches. Error
analysis further highlights the comparative advantages of our hybrid method
over single-modality RAGs. These findings establish BifrostRAG as a robust
knowledge engine for LLM-driven compliance checking. Its dual-graph, hybrid
retrieval mechanism offers a transferable blueprint for navigating complex
technical documents across knowledge-intensive engineering domains.

</details>


### [50] [Buggy rule diagnosis for combined steps through final answer evaluation in stepwise tasks](https://arxiv.org/abs/2507.13651)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 智能辅导系统通过最终答案诊断学生错误，缓解组合爆炸问题，验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决学生在分步任务中组合步骤导致的错误诊断困难问题。

Method: 利用最终答案自动完成并诊断任务，设计基于最终答案的错误规则诊断服务。

Result: 在二次方程数据集上，该方法诊断了29.4%的步骤，与教师诊断结果97%一致。

Conclusion: 基于最终答案的诊断方法可行，值得进一步探索。

Abstract: Many intelligent tutoring systems can support a student in solving a stepwise
task. When a student combines several steps in one step, the number of possible
paths connecting consecutive inputs may be very large. This combinatorial
explosion makes error diagnosis hard. Using a final answer to diagnose a
combination of steps can mitigate the combinatorial explosion, because there
are generally fewer possible (erroneous) final answers than (erroneous)
solution paths. An intermediate input for a task can be diagnosed by
automatically completing it according to the task solution strategy and
diagnosing this solution. This study explores the potential of automated error
diagnosis based on a final answer. We investigate the design of a service that
provides a buggy rule diagnosis when a student combines several steps. To
validate the approach, we apply the service to an existing dataset (n=1939) of
unique student steps when solving quadratic equations, which could not be
diagnosed by a buggy rule service that tries to connect consecutive inputs with
a single rule. Results show that final answer evaluation can diagnose 29,4% of
these steps. Moreover, a comparison of the generated diagnoses with teacher
diagnoses on a subset (n=115) shows that the diagnoses align in 97% of the
cases. These results can be considered a basis for further exploration of the
approach.

</details>


### [51] [Combining model tracing and constraint-based modeling for multistep strategy diagnoses](https://arxiv.org/abs/2507.13652)
*Gerben van der Hoek,Johan Jeuring,Rogier Bos*

Main category: cs.AI

TL;DR: 提出了一种结合模型追踪和约束建模的方法，用于诊断学生在多步任务中的输入，并在二次方程求解数据集上验证了其与教师诊断的一致性。


<details>
  <summary>Details</summary>
Motivation: 解决模型追踪和约束建模在诊断学生输入时的局限性，尤其是当学生将多个步骤合并为一步时。

Method: 通过将约束定义为学生输入与策略步骤的共同属性，设计并评估了一个多步策略诊断系统。

Result: 系统诊断与教师编码在140个学生步骤中完全一致。

Conclusion: 该方法有效结合了两种范式，能够准确诊断学生在多步任务中的输入偏差。

Abstract: Model tracing and constraint-based modeling are two approaches to diagnose
student input in stepwise tasks. Model tracing supports identifying consecutive
problem-solving steps taken by a student, whereas constraint-based modeling
supports student input diagnosis even when several steps are combined into one
step. We propose an approach that merges both paradigms. By defining
constraints as properties that a student input has in common with a step of a
strategy, it is possible to provide a diagnosis when a student deviates from a
strategy even when the student combines several steps. In this study we explore
the design of a system for multistep strategy diagnoses, and evaluate these
diagnoses. As a proof of concept, we generate diagnoses for an existing dataset
containing steps students take when solving quadratic equations (n=2136). To
compare with human diagnoses, two teachers coded a random sample of deviations
(n=70) and applications of the strategy (n=70). Results show that that the
system diagnosis aligned with the teacher coding in all of the 140 student
steps.

</details>


### [52] [DailyLLM: Context-Aware Activity Log Generation Using Multi-Modal Sensors and LLMs](https://arxiv.org/abs/2507.13737)
*Ye Tian,Xiaoyuan Ren,Zihao Wang,Onat Gungor,Xiaofan Yu,Tajana Rosing*

Main category: cs.AI

TL;DR: DailyLLM是一个基于轻量级LLM的框架，通过整合位置、运动、环境和生理四个维度的上下文信息，显著提升了活动日志生成的准确性、效率和语义丰富性。


<details>
  <summary>Details</summary>
Motivation: 现有活动日志生成方法在准确性、效率和语义丰富性方面存在不足，需要一种更高效且全面的解决方案。

Method: DailyLLM采用轻量级LLM框架，结合结构化提示和高效特征提取，实现高级活动理解。

Result: 实验表明，DailyLLM在BERTScore精度上比70B参数的SOTA基线提升17%，推理速度快10倍，且可在个人电脑和Raspberry Pi上高效部署。

Conclusion: DailyLLM为活动日志生成提供了一种高效、轻量且全面的解决方案，显著优于现有方法。

Abstract: Rich and context-aware activity logs facilitate user behavior analysis and
health monitoring, making them a key research focus in ubiquitous computing.
The remarkable semantic understanding and generation capabilities of Large
Language Models (LLMs) have recently created new opportunities for activity log
generation. However, existing methods continue to exhibit notable limitations
in terms of accuracy, efficiency, and semantic richness. To address these
challenges, we propose DailyLLM. To the best of our knowledge, this is the
first log generation and summarization system that comprehensively integrates
contextual activity information across four dimensions: location, motion,
environment, and physiology, using only sensors commonly available on
smartphones and smartwatches. To achieve this, DailyLLM introduces a
lightweight LLM-based framework that integrates structured prompting with
efficient feature extraction to enable high-level activity understanding.
Extensive experiments demonstrate that DailyLLM outperforms state-of-the-art
(SOTA) log generation methods and can be efficiently deployed on personal
computers and Raspberry Pi. Utilizing only a 1.5B-parameter LLM model, DailyLLM
achieves a 17% improvement in log generation BERTScore precision compared to
the 70B-parameter SOTA baseline, while delivering nearly 10x faster inference
speed.

</details>


### [53] [OntView: What you See is What you Meant](https://arxiv.org/abs/2507.13759)
*Carlos Bobed,Carlota Quintana,Eduardo Mena,Jorge Bobed,Fernando Bobillo*

Main category: cs.AI

TL;DR: OntView是一个直观的本体可视化工具，通过用户友好界面展示本体概念及其形式定义，支持GCI可视化并提供简化视图功能。


<details>
  <summary>Details</summary>
Motivation: 现有本体可视化工具无法有效展示大型本体结构，限制了用户对依赖关系和属性的理解。

Method: OntView基于DL推理器，采用“所见即所意”范式，支持GCI可视化，并提供三种简化视图方法。

Result: OntView成功实现了直观的本体可视化，并通过开源方式发布。

Conclusion: OntView填补了现有工具在GCI可视化和简化视图方面的空白，提升了本体理解的效率。

Abstract: In the field of knowledge management and computer science, ontologies provide
a structured framework for modeling domain-specific knowledge by defining
concepts and their relationships. However, the lack of tools that provide
effective visualization is still a significant challenge. While numerous
ontology editors and viewers exist, most of them fail to graphically represent
ontology structures in a meaningful and non-overwhelming way, limiting users'
ability to comprehend dependencies and properties within large ontological
frameworks.
  In this paper, we present OntView, an ontology viewer that is designed to
provide users with an intuitive visual representation of ontology concepts and
their formal definitions through a user-friendly interface. Building on the use
of a DL reasoner, OntView follows a "What you see is what you meant" paradigm,
showing the actual inferred knowledge. One key aspect for this is its ability
to visualize General Concept Inclusions (GCI), a feature absent in existing
visualization tools. Moreover, to avoid a possible information overload,
OntView also offers different ways to show a simplified view of the ontology
by: 1) creating ontology summaries by assessing the importance of the concepts
(according to different available algorithms), 2) focusing the visualization on
the existing TBox elements between two given classes and 3) allowing to
hide/show different branches in a dynamic way without losing the semantics.
OntView has been released with an open-source license for the whole community.

</details>


### [54] [From Extraction to Synthesis: Entangled Heuristics for Agent-Augmented Strategic Reasoning](https://arxiv.org/abs/2507.13768)
*Renato Ghisellini,Remo Pareschi,Marco Pedroni,Giovanni Battista Raggi*

Main category: cs.AI

TL;DR: 提出了一种结合启发式提取、语义激活和组合合成的混合架构，用于增强代理的战略推理能力，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统决策引擎通常选择最佳规则，而本文旨在通过语义交互建模和修辞框架，将冲突的启发式融合为连贯且上下文敏感的叙述。

Method: 结合启发式提取、语义激活和组合合成，利用量子认知研究的语义相互依赖性激活和组合多个启发式。

Result: 通过Meta vs. FTC案例研究展示了框架的有效性，并通过语义指标进行了初步验证。

Conclusion: 讨论了局限性（如动态干扰调整）和未来扩展方向。

Abstract: We present a hybrid architecture for agent-augmented strategic reasoning,
combining heuristic extraction, semantic activation, and compositional
synthesis. Drawing on sources ranging from classical military theory to
contemporary corporate strategy, our model activates and composes multiple
heuristics through a process of semantic interdependence inspired by research
in quantum cognition. Unlike traditional decision engines that select the best
rule, our system fuses conflicting heuristics into coherent and
context-sensitive narratives, guided by semantic interaction modeling and
rhetorical framing. We demonstrate the framework via a Meta vs. FTC case study,
with preliminary validation through semantic metrics. Limitations and
extensions (e.g., dynamic interference tuning) are discussed.

</details>


### [55] [When Speed meets Accuracy: an Efficient and Effective Graph Model for Temporal Link Prediction](https://arxiv.org/abs/2507.13825)
*Haoyang Li,Yuming Xu,Yiming Li,Hanmo Liu,Darian Li,Chen Jason Zhang,Lei Chen,Qing Li*

Main category: cs.AI

TL;DR: EAGLE是一个轻量级框架，通过结合短期时间邻近性和长期全局结构模式，解决了动态图中时间链路预测的效率和可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 现有T-GNNs在建模时间和结构依赖时计算开销高，导致效率和可扩展性问题。

Method: EAGLE包含时间感知模块和结构感知模块，通过自适应权重机制动态平衡两者贡献，避免复杂计算。

Result: 在七个真实世界动态图上，EAGLE在性能和效率上均优于现有T-GNNs，速度提升50倍以上。

Conclusion: EAGLE通过轻量设计显著提升了时间链路预测的效率和效果。

Abstract: Temporal link prediction in dynamic graphs is a critical task with
applications in diverse domains such as social networks, recommendation
systems, and e-commerce platforms. While existing Temporal Graph Neural
Networks (T-GNNs) have achieved notable success by leveraging complex
architectures to model temporal and structural dependencies, they often suffer
from scalability and efficiency challenges due to high computational overhead.
In this paper, we propose EAGLE, a lightweight framework that integrates
short-term temporal recency and long-term global structural patterns. EAGLE
consists of a time-aware module that aggregates information from a node's most
recent neighbors to reflect its immediate preferences, and a structure-aware
module that leverages temporal personalized PageRank to capture the influence
of globally important nodes. To balance these attributes, EAGLE employs an
adaptive weighting mechanism to dynamically adjust their contributions based on
data characteristics. Also, EAGLE eliminates the need for complex multi-hop
message passing or memory-intensive mechanisms, enabling significant
improvements in efficiency. Extensive experiments on seven real-world temporal
graphs demonstrate that EAGLE consistently achieves superior performance
against state-of-the-art T-GNNs in both effectiveness and efficiency,
delivering more than a 50x speedup over effective transformer-based T-GNNs.

</details>


### [56] [Causal Knowledge Transfer for Multi-Agent Reinforcement Learning in Dynamic Environments](https://arxiv.org/abs/2507.13846)
*Kathrin Korte,Christian Medeiros Adriano,Sona Ghahremani,Holger Giese*

Main category: cs.AI

TL;DR: 本文提出了一种基于因果知识转移的多智能体强化学习框架，用于非平稳环境中智能体的知识共享与适应。


<details>
  <summary>Details</summary>
Motivation: 传统知识转移方法在多智能体强化学习中难以泛化，且智能体需要昂贵的重新训练来适应变化。本文旨在解决这一问题。

Method: 通过因果干预建模碰撞恢复策略，形成可转移的宏动作，实现零样本知识共享。

Result: 实验表明，该方法能使异构目标智能体在新环境中适应性能提升约50%，且效果受环境复杂度和目标异质性影响。

Conclusion: 因果知识转移框架为多智能体在动态环境中的协作提供了有效解决方案。

Abstract: [Context] Multi-agent reinforcement learning (MARL) has achieved notable
success in environments where agents must learn coordinated behaviors. However,
transferring knowledge across agents remains challenging in non-stationary
environments with changing goals. [Problem] Traditional knowledge transfer
methods in MARL struggle to generalize, and agents often require costly
retraining to adapt. [Approach] This paper introduces a causal knowledge
transfer framework that enables RL agents to learn and share compact causal
representations of paths within a non-stationary environment. As the
environment changes (new obstacles), agents' collisions require adaptive
recovery strategies. We model each collision as a causal intervention
instantiated as a sequence of recovery actions (a macro) whose effect
corresponds to a causal knowledge of how to circumvent the obstacle while
increasing the chances of achieving the agent's goal (maximizing cumulative
reward). This recovery action macro is transferred online from a second agent
and is applied in a zero-shot fashion, i.e., without retraining, just by
querying a lookup model with local context information (collisions). [Results]
Our findings reveal two key insights: (1) agents with heterogeneous goals were
able to bridge about half of the gap between random exploration and a fully
retrained policy when adapting to new environments, and (2) the impact of
causal knowledge transfer depends on the interplay between environment
complexity and agents' heterogeneous goals.

</details>


### [57] [Large Language Models as Innovators: A Framework to Leverage Latent Space Exploration for Novelty Discovery](https://arxiv.org/abs/2507.13874)
*Mateusz Bystroński,Mikołaj Hołysz,Grzegorz Piotrowski,Nitesh V. Chawla,Tomasz Kajdanowicz*

Main category: cs.AI

TL;DR: 提出了一种模型无关的潜在空间创意框架，通过导航连续嵌入空间实现可控、可扩展的创意生成。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型（LLMs）在生成新颖且相关内容时的局限性，避免依赖领域特定启发式或结构化提示。

Method: 提出模型无关的潜在空间创意框架，无需手工规则，适应不同领域和任务。

Result: 初步结果展示了其作为通用人机协作创意工具的潜力。

Conclusion: 该框架为可控创意生成提供了灵活且可扩展的解决方案。

Abstract: Innovative idea generation remains a core challenge in AI, as large language
models (LLMs) often struggle to produce outputs that are both novel and
relevant. Despite their fluency, LLMs tend to replicate patterns seen during
training, limiting their ability to diverge creatively without extensive prompt
engineering. Prior work has addressed this through domain-specific heuristics
and structured prompting pipelines, but such solutions are brittle and
difficult to generalize. In this paper, we propose a model-agnostic
latent-space ideation framework that enables controlled, scalable creativity by
navigating the continuous embedding space of ideas. Unlike prior methods, our
framework requires no handcrafted rules and adapts easily to different domains,
input formats, and creative tasks. This paper introduces an early-stage
prototype of our method, outlining the conceptual framework and preliminary
results highlighting its potential as a general-purpose co-ideator for human-AI
collaboration.

</details>


### [58] [Cross-modal Causal Intervention for Alzheimer's Disease Prediction](https://arxiv.org/abs/2507.13956)
*Yutao Jin,Haowen Xiao,Jielei Chu,Fengmao Lv,Yuxiao Li,Tianrui Li*

Main category: cs.AI

TL;DR: 提出了一种名为ADPC的视觉-语言因果干预框架，用于阿尔茨海默病的早期诊断，通过结合MRI、fMRI和LLM生成的文本数据，有效消除混杂因素，实现高精度分类。


<details>
  <summary>Details</summary>
Motivation: 早期诊断阿尔茨海默病（AD）对延缓痴呆进展至关重要，但现有方法因数据选择偏差和变量复杂关系面临挑战。

Method: ADPC框架结合MRI、fMRI和LLM生成的文本数据，通过因果干预消除混杂因素，分类认知正常（CN）、轻度认知障碍（MCI）和AD。

Result: 实验显示ADPC在CN/MCI/AD分类中表现优异，达到SOTA指标。

Conclusion: 研究展示了因果推理与多模态学习结合在神经疾病诊断中的潜力。

Abstract: Mild Cognitive Impairment (MCI) serves as a prodromal stage of Alzheimer's
Disease (AD), where early identification and intervention can effectively slow
the progression to dementia. However, diagnosing AD remains a significant
challenge in neurology due to the confounders caused mainly by the selection
bias of multimodal data and the complex relationships between variables. To
address these issues, we propose a novel visual-language causal intervention
framework named Alzheimer's Disease Prediction with Cross-modal Causal
Intervention (ADPC) for diagnostic assistance. Our ADPC employs large language
model (LLM) to summarize clinical data under strict templates, maintaining
structured text outputs even with incomplete or unevenly distributed datasets.
The ADPC model utilizes Magnetic Resonance Imaging (MRI), functional MRI (fMRI)
images and textual data generated by LLM to classify participants into
Cognitively Normal (CN), MCI, and AD categories. Because of the presence of
confounders, such as neuroimaging artifacts and age-related biomarkers,
non-causal models are likely to capture spurious input-output correlations,
generating less reliable results. Our framework implicitly eliminates
confounders through causal intervention. Experimental results demonstrate the
outstanding performance of our method in distinguishing CN/MCI/AD cases,
achieving state-of-the-art (SOTA) metrics across most evaluation metrics. The
study showcases the potential of integrating causal reasoning with multi-modal
learning for neurological disease diagnosis.

</details>


### [59] [Towards Constraint Temporal Answer Set Programming](https://arxiv.org/abs/2507.13958)
*Pedro Cabalar,Martín Diéguez,François Olivier,Torsten Schaub,Igor Stéphan*

Main category: cs.AI

TL;DR: 提出了一种新颖的时态和约束扩展方法，用于增强ASP在动态系统中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决逻辑方法（如ASP）在细粒度时态和数值推理中的挑战。

Method: 结合线性时间逻辑和约束逻辑，扩展Here-and-There逻辑及其非单调平衡扩展。

Result: 实现了非单调时态推理与约束的直接集成，为复杂动态系统提供高分辨率框架。

Conclusion: 为ASP范式下处理高分辨率动态系统奠定了逻辑基础。

Abstract: Reasoning about dynamic systems with a fine-grained temporal and numeric
resolution presents significant challenges for logic-based approaches like
Answer Set Programming (ASP). To address this, we introduce and elaborate upon
a novel temporal and constraint-based extension of the logic of Here-and-There
and its nonmonotonic equilibrium extension, representing, to the best of our
knowledge, the first approach to nonmonotonic temporal reasoning with
constraints specifically tailored for ASP. This expressive system is achieved
by a synergistic combination of two foundational ASP extensions: the
linear-time logic of Here-and-There, providing robust nonmonotonic temporal
reasoning capabilities, and the logic of Here-and-There with constraints,
enabling the direct integration and manipulation of numeric constraints, among
others. This work establishes the foundational logical framework for tackling
complex dynamic systems with high resolution within the ASP paradigm.

</details>


### [60] [Automated Interpretation of Non-Destructive Evaluation Contour Maps Using Large Language Models for Bridge Condition Assessment](https://arxiv.org/abs/2507.14107)
*Viraj Nishesh Darji,Callie C. Liao,Duoduo Liao*

Main category: cs.AI

TL;DR: 该研究探讨了利用大型语言模型（LLMs）自动解释非破坏性评估（NDE）轮廓图，以提高桥梁维护效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 桥梁维护和安全至关重要，但NDE数据分析耗时且依赖专家知识，LLMs提供了自动化分析的潜力。

Method: 研究设计了特定提示，用多种LLMs解释NDE轮廓图，评估其描述质量、缺陷识别、建议提供和准确性。

Result: 九种模型中四种表现更优，ChatGPT-4和Claude 3.5 Sonnet生成的总结更有效，显著提升效率和准确性。

Conclusion: LLMs可显著改善桥梁维护决策，为基础设施管理提供创新解决方案。

Abstract: Bridge maintenance and safety are essential for transportation authorities,
and Non-Destructive Evaluation (NDE) techniques are critical to assessing
structural integrity. However, interpreting NDE data can be time-consuming and
requires expertise, potentially delaying decision-making. Recent advancements
in Large Language Models (LLMs) offer new ways to automate and improve this
analysis. This pilot study introduces a holistic assessment of LLM capabilities
for interpreting NDE contour maps and demonstrates the effectiveness of LLMs in
providing detailed bridge condition analyses. It establishes a framework for
integrating LLMs into bridge inspection workflows, indicating that LLM-assisted
analysis can enhance efficiency without compromising accuracy. In this study,
several LLMs are explored with prompts specifically designed to enhance the
quality of image descriptions, which are applied to interpret five different
NDE contour maps obtained through technologies for assessing bridge conditions.
Each LLM model is evaluated based on its ability to produce detailed
descriptions, identify defects, provide actionable recommendations, and
demonstrate overall accuracy. The research indicates that four of the nine
models provide better image descriptions, effectively covering a wide range of
topics related to the bridge's condition. The outputs from these four models
are summarized using five different LLMs to form a comprehensive overview of
the bridge. Notably, LLMs ChatGPT-4 and Claude 3.5 Sonnet generate more
effective summaries. The findings suggest that LLMs have the potential to
significantly improve efficiency and accuracy. This pilot study presents an
innovative approach that leverages LLMs for image captioning in parallel and
summarization, enabling faster decision-making in bridge maintenance and
enhancing infrastructure management and safety assessments.

</details>


### [61] [KROMA: Ontology Matching with Knowledge Retrieval and Large Language Models](https://arxiv.org/abs/2507.14032)
*Lam Nguyen,Erika Barcelos,Roger French,Yinghui Wu*

Main category: cs.AI

TL;DR: KROMA是一个利用大型语言模型（LLMs）和检索增强生成（RAG）的动态本体匹配框架，通过优化技术和轻量级本体细化显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有本体匹配系统依赖手工规则或专用模型，适应性有限，KROMA旨在通过动态语义上下文增强解决这一问题。

Method: 结合双相似度概念匹配和轻量级本体细化，减少LLMs调用开销，并通过RAG管道动态丰富语义上下文。

Result: 在多个基准数据集上，KROMA显著优于传统系统和前沿LLM方法，同时保持通信开销可控。

Conclusion: KROMA证明了知识检索、提示增强和本体细化等技术在大规模本体匹配中的可行性和优势。

Abstract: Ontology Matching (OM) is a cornerstone task of semantic interoperability,
yet existing systems often rely on handcrafted rules or specialized models with
limited adaptability. We present KROMA, a novel OM framework that harnesses
Large Language Models (LLMs) within a Retrieval-Augmented Generation (RAG)
pipeline to dynamically enrich the semantic context of OM tasks with
structural, lexical, and definitional knowledge. To optimize both performance
and efficiency, KROMA integrates a bisimilarity-based concept matching and a
lightweight ontology refinement step, which prune candidate concepts and
substantially reduce the communication overhead from invoking LLMs. Through
experiments on multiple benchmark datasets, we show that integrating knowledge
retrieval with context-augmented LLMs significantly enhances ontology matching,
outperforming both classic OM systems and cutting-edge LLM-based approaches
while keeping communication overhead comparable. Our study highlights the
feasibility and benefit of the proposed optimization techniques (targeted
knowledge retrieval, prompt enrichment, and ontology refinement) for ontology
matching at scale.

</details>


### [62] [Glucose-ML: A collection of longitudinal diabetes datasets for development of robust AI solutions](https://arxiv.org/abs/2507.14077)
*Temiloluwa Prioleau,Baiying Lu,Yanjun Cui*

Main category: cs.AI

TL;DR: Glucose-ML是一个包含10个公开糖尿病数据集的集合，旨在促进透明、可重复和稳健的AI解决方案开发。


<details>
  <summary>Details</summary>
Motivation: 解决高质量糖尿病数据集获取困难的问题，加速AI在糖尿病管理中的应用。

Method: 收集并分析10个公开数据集，提供比较分析和血糖预测案例研究。

Result: 同一算法在不同数据集上表现差异显著，为开发稳健AI提供了基准和建议。

Conclusion: Glucose-ML为糖尿病AI研究提供了丰富资源和实用指南。

Abstract: Artificial intelligence (AI) algorithms are a critical part of
state-of-the-art digital health technology for diabetes management. Yet, access
to large high-quality datasets is creating barriers that impede development of
robust AI solutions. To accelerate development of transparent, reproducible,
and robust AI solutions, we present Glucose-ML, a collection of 10 publicly
available diabetes datasets, released within the last 7 years (i.e., 2018 -
2025). The Glucose-ML collection comprises over 300,000 days of continuous
glucose monitor (CGM) data with a total of 38 million glucose samples collected
from 2500+ people across 4 countries. Participants include persons living with
type 1 diabetes, type 2 diabetes, prediabetes, and no diabetes. To support
researchers and innovators with using this rich collection of diabetes
datasets, we present a comparative analysis to guide algorithm developers with
data selection. Additionally, we conduct a case study for the task of blood
glucose prediction - one of the most common AI tasks within the field. Through
this case study, we provide a benchmark for short-term blood glucose prediction
across all 10 publicly available diabetes datasets within the Glucose-ML
collection. We show that the same algorithm can have significantly different
prediction results when developed/evaluated with different datasets. Findings
from this study are then used to inform recommendations for developing robust
AI solutions within the diabetes or broader health domain. We provide direct
links to each longitudinal diabetes dataset in the Glucose-ML collection and
openly provide our code.

</details>


### [63] [Generative AI-Driven High-Fidelity Human Motion Simulation](https://arxiv.org/abs/2507.14097)
*Hari Iyer,Neel Macwan,Atharva Jitendra Hude,Heejin Jeong,Shenghan Guo*

Main category: cs.AI

TL;DR: G-AI-HMS结合文本到文本和文本到动作模型，提升工业任务中人类运动模拟的保真度，并通过计算机视觉验证其准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的人类运动模拟方法运动保真度低，无法满足工业任务中对行为、安全和生产力评估的需求。

Method: 整合文本到文本和文本到动作模型，利用MotionGPT训练词汇对齐的大语言模型，并通过计算机视觉和姿态估计算法验证AI生成动作的准确性。

Result: 在八项任务中，AI增强的动作在大多数场景中表现出更低的误差，尤其在空间准确性、姿态对齐和时间相似性方面优于人工描述。

Conclusion: G-AI-HMS显著减少了关节误差和时间错位，同时保持了姿态准确性，为工业任务提供了更高质量的运动模拟。

Abstract: Human motion simulation (HMS) supports cost-effective evaluation of worker
behavior, safety, and productivity in industrial tasks. However, existing
methods often suffer from low motion fidelity. This study introduces
Generative-AI-Enabled HMS (G-AI-HMS), which integrates text-to-text and
text-to-motion models to enhance simulation quality for physical tasks.
G-AI-HMS tackles two key challenges: (1) translating task descriptions into
motion-aware language using Large Language Models aligned with MotionGPT's
training vocabulary, and (2) validating AI-enhanced motions against real human
movements using computer vision. Posture estimation algorithms are applied to
real-time videos to extract joint landmarks, and motion similarity metrics are
used to compare them with AI-enhanced sequences. In a case study involving
eight tasks, the AI-enhanced motions showed lower error than human created
descriptions in most scenarios, performing better in six tasks based on spatial
accuracy, four tasks based on alignment after pose normalization, and seven
tasks based on overall temporal similarity. Statistical analysis showed that
AI-enhanced prompts significantly (p $<$ 0.0001) reduced joint error and
temporal misalignment while retaining comparable posture accuracy.

</details>


### [64] [CUDA-L1: Improving CUDA Optimization via Contrastive Reinforcement Learning](https://arxiv.org/abs/2507.14111)
*Xiaoya Li,Xiaofei Sun,Albert Wang,Jiwei Li,Chris Shum*

Main category: cs.AI

TL;DR: CUDA-L1是一种基于强化学习的自动化CUDA优化框架，显著提升CUDA内核性能，并展现出跨GPU架构的优异可移植性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，GPU计算资源需求激增，亟需自动化CUDA优化策略。现有模型在CUDA优化任务上成功率低，CUDA-L1旨在填补这一空白。

Method: 采用强化学习框架，通过速度提升奖励信号训练模型，无需人工干预或领域知识。

Result: 在NVIDIA A100上平均加速17.7倍，峰值达449倍，并在多种GPU架构上表现优异。

Conclusion: CUDA-L1证明强化学习可有效优化CUDA性能，为自动化GPU优化开辟新途径。

Abstract: The exponential growth in demand for GPU computing resources, driven by the
rapid advancement of Large Language Models, has created an urgent need for
automated CUDA optimization strategies. While recent advances in LLMs show
promise for code generation, current SOTA models (e.g. R1, o1) achieve low
success rates in improving CUDA speed. In this paper, we introduce CUDA-L1, an
automated reinforcement learning framework for CUDA optimization.
  CUDA-L1 achieves performance improvements on the CUDA optimization task:
trained on NVIDIA A100, it delivers an average speedup of x17.7 across all 250
CUDA kernels of KernelBench, with peak speedups reaching x449. Furthermore, the
model also demonstrates excellent portability across GPU architectures,
achieving average speedups of x17.8 on H100, x19.0 on RTX 3090, x16.5 on L40,
x14.7 on H800, and x13.9 on H20 despite being optimized specifically for A100.
Beyond these benchmark results, CUDA-L1 demonstrates several remarkable
properties: 1) Discovers a variety of CUDA optimization techniques and learns
to combine them strategically to achieve optimal performance; 2) Uncovers
fundamental principles of CUDA optimization; 3) Identifies non-obvious
performance bottlenecks and rejects seemingly beneficial optimizations that
harm performance.
  The capabilities of CUDA-L1 demonstrate that reinforcement learning can
transform an initially poor-performing LLM into an effective CUDA optimizer
through speedup-based reward signals alone, without human expertise or domain
knowledge. More importantly, the trained RL model extend the acquired reasoning
abilities to new kernels. This paradigm opens possibilities for automated
optimization of CUDA operations, and holds promise to substantially promote GPU
efficiency and alleviate the rising pressure on GPU computing resources.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [65] [DyG-RAG: Dynamic Graph Retrieval-Augmented Generation with Event-Centric Reasoning](https://arxiv.org/abs/2507.13396)
*Qingyun Sun,Jiaqi Yuan,Shan He,Xiao Guan,Haonan Yuan,Xingcheng Fu,Jianxin Li,Philip S. Yu*

Main category: cs.IR

TL;DR: DyG-RAG是一种新颖的动态图检索增强生成框架，专注于解决时间推理问题，通过动态事件单元和事件图实现时间感知的检索和生成。


<details>
  <summary>Details</summary>
Motivation: 现有图RAG方法难以处理时间推理问题，因为它们无法建模现实世界事件的结构和顺序变化。

Method: DyG-RAG提出动态事件单元（DEUs）编码语义和时间信息，构建事件图以捕捉时间与因果依赖，并引入时间感知检索和生成策略。

Result: 实验表明，DyG-RAG显著提升了时间敏感问题的准确性和召回率。

Conclusion: DyG-RAG为时间感知生成提供了更可靠的解决方案，适用于复杂时间敏感查询。

Abstract: Graph Retrieval-Augmented Generation has emerged as a powerful paradigm for
grounding large language models with external structured knowledge. However,
existing Graph RAG methods struggle with temporal reasoning, due to their
inability to model the evolving structure and order of real-world events. In
this work, we introduce DyG-RAG, a novel event-centric dynamic graph
retrieval-augmented generation framework designed to capture and reason over
temporal knowledge embedded in unstructured text. To eliminate temporal
ambiguity in traditional retrieval units, DyG-RAG proposes Dynamic Event Units
(DEUs) that explicitly encode both semantic content and precise temporal
anchors, enabling accurate and interpretable time-aware retrieval. To capture
temporal and causal dependencies across events, DyG-RAG constructs an event
graph by linking DEUs that share entities and occur close in time, supporting
efficient and meaningful multi-hop reasoning. To ensure temporally consistent
generation, DyG-RAG introduces an event timeline retrieval pipeline that
retrieves event sequences via time-aware traversal, and proposes a Time
Chain-of-Thought strategy for temporally grounded answer generation. This
unified pipeline enables DyG-RAG to retrieve coherent, temporally ordered event
sequences and to answer complex, time-sensitive queries that standard RAG
systems cannot resolve. Extensive experiments on temporal QA benchmarks
demonstrate that DyG-RAG significantly improves the accuracy and recall of
three typical types of temporal reasoning questions, paving the way for more
faithful and temporal-aware generation. DyG-RAG is available at
https://github.com/RingBDStack/DyG-RAG.

</details>


### [66] [Revisiting Prompt Engineering: A Comprehensive Evaluation for LLM-based Personalized Recommendation](https://arxiv.org/abs/2507.13525)
*Genki Kusano,Kosuke Akimoto,Kunihiro Takeoka*

Main category: cs.IR

TL;DR: 论文研究了在单用户设置下，如何通过提示工程优化LLM在推荐任务中的表现，比较了23种提示类型和12种LLM，发现简单提示对高性能LLM更有效。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在推荐任务中的潜力，特别是在隐私敏感或数据有限的应用场景中，提示工程的重要性。

Method: 在8个公共数据集上比较23种提示类型和12种LLM，使用统计测试和线性混合效应模型评估准确性和推理成本。

Result: 发现三种提示类型对低成本LLM特别有效，而高性能LLM更适合简单提示。

Conclusion: 提供了根据准确性和成本需求选择提示和LLM的实用建议。

Abstract: Large language models (LLMs) can perform recommendation tasks by taking
prompts written in natural language as input. Compared to traditional methods
such as collaborative filtering, LLM-based recommendation offers advantages in
handling cold-start, cross-domain, and zero-shot scenarios, as well as
supporting flexible input formats and generating explanations of user behavior.
In this paper, we focus on a single-user setting, where no information from
other users is used. This setting is practical for privacy-sensitive or
data-limited applications. In such cases, prompt engineering becomes especially
important for controlling the output generated by the LLM. We conduct a
large-scale comparison of 23 prompt types across 8 public datasets and 12 LLMs.
We use statistical tests and linear mixed-effects models to evaluate both
accuracy and inference cost. Our results show that for cost-efficient LLMs,
three types of prompts are especially effective: those that rephrase
instructions, consider background knowledge, and make the reasoning process
easier to follow. For high-performance LLMs, simple prompts often outperform
more complex ones while reducing cost. In contrast, commonly used prompting
styles in natural language processing, such as step-by-step reasoning, or the
use of reasoning models often lead to lower accuracy. Based on these findings,
we provide practical suggestions for selecting prompts and LLMs depending on
the required balance between accuracy and cost.

</details>


### [67] [IP2: Entity-Guided Interest Probing for Personalized News Recommendation](https://arxiv.org/abs/2507.13622)
*Youlin Wu,Yuanyuan Sun,Xiaokun Zhang,Haoxi Zhan,Bo Xu,Liang Yang,Hongfei Lin*

Main category: cs.IR

TL;DR: 论文提出了一种名为IP2的新方法，通过分析新闻实体在扫描和标题阅读阶段的作用，改进了新闻推荐系统。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了新闻实体在推荐中的独特作用，而行为科学研究表明实体在阅读过程中起关键作用。

Method: IP2方法结合了Transformer-based实体编码器和双塔用户编码器，分别处理新闻内和新闻间的实体兴趣。

Result: 在两个真实数据集上的实验表明，IP2在新闻推荐中达到了最先进的性能。

Conclusion: IP2通过有效利用实体信息，显著提升了新闻推荐的准确性和用户体验。

Abstract: News recommender systems aim to provide personalized news reading experiences
for users based on their reading history. Behavioral science studies suggest
that screen-based news reading contains three successive steps: scanning, title
reading, and then clicking. Adhering to these steps, we find that intra-news
entity interest dominates the scanning stage, while the inter-news entity
interest guides title reading and influences click decisions. Unfortunately,
current methods overlook the unique utility of entities in news recommendation.
To this end, we propose a novel method called IP2 to probe entity-guided
reading interest at both intra- and inter-news levels. At the intra-news level,
a Transformer-based entity encoder is devised to aggregate mentioned entities
in the news title into one signature entity. Then, a signature entity-title
contrastive pre-training is adopted to initialize entities with proper meanings
using the news story context, which in the meantime facilitates us to probe for
intra-news entity interest. As for the inter-news level, a dual tower user
encoder is presented to capture inter-news reading interest from both the title
meaning and entity sides. In addition to highlighting the contribution of
inter-news entity guidance, a cross-tower attention link is adopted to
calibrate title reading interest using inter-news entity interest, thus further
aligning with real-world behavior. Extensive experiments on two real-world
datasets demonstrate that our IP2 achieves state-of-the-art performance in news
recommendation.

</details>


### [68] [Point of Interest Recommendation: Pitfalls and Viable Solutions](https://arxiv.org/abs/2507.13725)
*Alejandro Bellogín,Linus W. Dietz,Francesco Ricci,Pablo Sánchez*

Main category: cs.IR

TL;DR: 本文对兴趣点（POI）推荐研究的现状进行了批判性评估，指出了数据集、算法和评估方法中的关键问题，并提出了未来研究的方向。


<details>
  <summary>Details</summary>
Motivation: POI推荐对提升游客体验至关重要，但现有研究存在诸多未解决的问题，阻碍了实际应用。

Method: 通过分析当前POI推荐研究的三个主要维度（数据集、算法、评估方法），识别关键问题并提出改进方向。

Result: 指出了数据集标准化不足、模型设计假设缺陷和用户行为偏差处理不当等问题。

Conclusion: 提出了多利益相关者设计、上下文感知、数据收集、可信度、新颖交互和真实评估等未来研究方向。

Abstract: Point of interest (POI) recommendation can play a pivotal role in enriching
tourists' experiences by suggesting context-dependent and preference-matching
locations and activities, such as restaurants, landmarks, itineraries, and
cultural attractions. Unlike some more common recommendation domains (e.g.,
music and video), POI recommendation is inherently high-stakes: users invest
significant time, money, and effort to search, choose, and consume these
suggested POIs. Despite the numerous research works in the area, several
fundamental issues remain unresolved, hindering the real-world applicability of
the proposed approaches. In this paper, we discuss the current status of the
POI recommendation problem and the main challenges we have identified. The
first contribution of this paper is a critical assessment of the current state
of POI recommendation research and the identification of key shortcomings
across three main dimensions: datasets, algorithms, and evaluation
methodologies. We highlight persistent issues such as the lack of standardized
benchmark datasets, flawed assumptions in the problem definition and model
design, and inadequate treatment of biases in the user behavior and system
performance. The second contribution is a structured research agenda that,
starting from the identified issues, introduces important directions for future
work related to multistakeholder design, context awareness, data collection,
trustworthiness, novel interactions, and real-world evaluation.

</details>


### [69] [RAG-based Architectures for Drug Side Effect Retrieval in LLMs](https://arxiv.org/abs/2507.13822)
*Shad Nygren,Pinar Avci,Andre Daniels,Reza Rassol,Afshin Beheshti,Diego Galeano*

Main category: cs.IR

TL;DR: 论文提出两种架构（RAG和GraphRAG），将药物副作用知识整合到Llama 3 8B语言模型中，显著提升了药物副作用检索的准确性。


<details>
  <summary>Details</summary>
Motivation: 药物副作用是全球健康问题，现有大型语言模型（LLMs）在专业领域（如药物警戒）中存在可靠性不足的问题。

Method: 提出Retrieval-Augmented Generation (RAG)和GraphRAG两种架构，整合药物副作用知识到Llama 3 8B模型中。

Result: 在19,520个药物副作用关联数据上评估，GraphRAG实现了近乎完美的检索准确性。

Conclusion: GraphRAG为药物警戒提供了一种高精度、可扩展的解决方案，是LLMs在关键领域应用的重要进展。

Abstract: Drug side effects are a major global health concern, necessitating advanced
methods for their accurate detection and analysis. While Large Language Models
(LLMs) offer promising conversational interfaces, their inherent limitations,
including reliance on black-box training data, susceptibility to
hallucinations, and lack of domain-specific knowledge, hinder their reliability
in specialized fields like pharmacovigilance. To address this gap, we propose
two architectures: Retrieval-Augmented Generation (RAG) and GraphRAG, which
integrate comprehensive drug side effect knowledge into a Llama 3 8B language
model. Through extensive evaluations on 19,520 drug side effect associations
(covering 976 drugs and 3,851 side effect terms), our results demonstrate that
GraphRAG achieves near-perfect accuracy in drug side effect retrieval. This
framework offers a highly accurate and scalable solution, signifying a
significant advancement in leveraging LLMs for critical pharmacovigilance
applications.

</details>


### [70] [SPARQL Query Generation with LLMs: Measuring the Impact of Training Data Memorization and Knowledge Injection](https://arxiv.org/abs/2507.13859)
*Aleksandr Gashkov,Aleksandr Perevalov,Maria Eltsova,Andreas Both*

Main category: cs.IR

TL;DR: 论文提出了一种评估大型语言模型（LLM）在生成SPARQL查询时质量的新方法，通过不同条件下的测试（零样本、知识注入和匿名知识注入），以衡量训练数据对问答质量的影响。


<details>
  <summary>Details</summary>
Motivation: 由于LLM在问答系统中的潜力巨大，但其训练数据可能包含基准测试或知识图谱，导致结果偏差。因此，需要一种方法来评估LLM的实际能力，避免因数据记忆而误导性能评估。

Method: 通过三种条件（零样本、知识注入和匿名知识注入）生成SPARQL查询，分析LLM在不同情况下的表现，以评估其真实能力。

Result: 该方法可移植性强、鲁棒性高，适用于任何知识图谱或LLM，能够生成一致的评估结果。

Conclusion: 该方法首次量化了训练数据对LLM问答质量的影响，为评估LLM的实际能力提供了可靠工具，有助于避免因数据记忆导致的性能误判。

Abstract: Nowadays, the importance of software with natural-language user interfaces
cannot be underestimated. In particular, in Question Answering (QA) systems,
generating a SPARQL query for a given natural-language question (often named
Query Building) from the information retrieved from the same question is the
central task of QA systems working over Knowledge Graphs (KGQA). Due to the
rise of Large Language Models (LLMs), they are considered a well-suited method
to increase the quality of the question-answering functionality, as there is
still a lot of room for improvement, aiming for enhanced quality and
trustworthiness. However, LLMs are trained on web data, where researchers have
no control over whether the benchmark or the knowledge graph was already
included in the training data. In this paper, we introduce a novel method that
evaluates the quality of LLMs by generating a SPARQL query from a
natural-language question under various conditions: (1) zero-shot SPARQL
generation, (2) with knowledge injection, and (3) with "anonymized" knowledge
injection. This enables us, for the first time, to estimate the influence of
the training data on the QA quality improved by LLMs. Ultimately, this will
help to identify how portable a method is or whether good results might mostly
be achieved because a benchmark was already included in the training data (cf.
LLM memorization). The developed method is portable, robust, and supports any
knowledge graph; therefore, it could be easily applied to any KGQA or LLM,
s.t., generating consistent insights into the actual LLM capabilities is
possible.

</details>


### [71] [PARK: Personalized academic retrieval with knowledge-graphs](https://arxiv.org/abs/2507.13910)
*Pranav Kasela,Gabriella Pasi,Raffaele Perego*

Main category: cs.IR

TL;DR: 论文提出了一种结合神经语言模型和知识图谱的两步方法，用于个性化学术搜索，显著提升了检索效果。


<details>
  <summary>Details</summary>
Motivation: 现有个性化学术搜索模型难以全面捕捉用户的学术兴趣，尤其是未能充分利用引文图的信息。

Method: 1. 训练神经语言模型用于检索；2. 将学术图转换为知识图谱，并通过翻译嵌入技术将其与语言模型嵌入共享语义空间。

Result: 在四个学术搜索领域中，该方法在三个领域优于传统图基和个性化模型，MAP@100提升高达10%。

Conclusion: 基于知识图谱的用户模型能有效提升检索效果，展示了其在学术搜索中的潜力。

Abstract: Academic Search is a search task aimed to manage and retrieve scientific
documents like journal articles and conference papers. Personalization in this
context meets individual researchers' needs by leveraging, through user
profiles, the user related information (e.g. documents authored by a
researcher), to improve search effectiveness and to reduce the information
overload. While citation graphs are a valuable means to support the outcome of
recommender systems, their use in personalized academic search (with, e.g.
nodes as papers and edges as citations) is still under-explored.
  Existing personalized models for academic search often struggle to fully
capture users' academic interests. To address this, we propose a two-step
approach: first, training a neural language model for retrieval, then
converting the academic graph into a knowledge graph and embedding it into a
shared semantic space with the language model using translational embedding
techniques. This allows user models to capture both explicit relationships and
hidden structures in citation graphs and paper content. We evaluate our
approach in four academic search domains, outperforming traditional graph-based
and personalized models in three out of four, with up to a 10\% improvement in
MAP@100 over the second-best model. This highlights the potential of knowledge
graph-based user models to enhance retrieval effectiveness.

</details>


### [72] [DUALRec: A Hybrid Sequential and Language Model Framework for Context-Aware Movie Recommendation](https://arxiv.org/abs/2507.13957)
*Yitong Li,Raoul Grasman*

Main category: cs.IR

TL;DR: DUALRec结合LSTM的时间建模能力和LLM的语义推理能力，提出了一种动态用户感知推荐系统，显著优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统难以捕捉动态和上下文丰富的用户偏好，传统方法在时间模式和语义理解上存在不足。

Method: 提出DUALRec模型，结合LSTM的时间动态建模和微调LLM的语义推理能力。

Result: 在MovieLens-1M数据集上，DUALRec在HR@k、NDCG@k和类型相似性指标上优于基线模型。

Conclusion: DUALRec填补了时间序列建模与语义推理之间的空白，为智能推荐系统提供了新方向。

Abstract: The modern recommender systems are facing an increasing challenge of
modelling and predicting the dynamic and context-rich user preferences.
Traditional collaborative filtering and content-based methods often struggle to
capture the temporal patternings and evolving user intentions. While Large
Language Models (LLMs) have gained gradual attention in recent years, by their
strong semantic understanding and reasoning abilities, they are not inherently
designed to model chronologically evolving user preference and intentions. On
the other hand, for sequential models like LSTM (Long-Short-Term-Memory) which
is good at capturing the temporal dynamics of user behaviour and evolving user
preference over time, but still lacks a rich semantic understanding for
comprehensive recommendation generation. In this study, we propose DUALRec
(Dynamic User-Aware Language-based Recommender), a novel recommender that
leverages the complementary strength of both models, which combines the
temporal modelling abilities of LSTM networks with semantic reasoning power of
the fine-tuned Large Language Models. The LSTM component will capture users
evolving preference through their viewing history, while the fine-tuned LLM
variants will leverage these temporal user insights to generate next movies
that users might enjoy. Experimental results on MovieLens-1M dataset shows that
the DUALRec model outperforms a wide range of baseline models, with
comprehensive evaluation matrices of Hit Rate (HR@k), Normalized Discounted
Cumulative Gain (NDCG@k), and genre similarity metrics. This research proposes
a novel architecture that bridges the gap between temporal sequence modeling
and semantic reasoning, and offers a promising direction for developing more
intelligent and context-aware recommenders.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [73] [Physical models realizing the transformer architecture of large language models](https://arxiv.org/abs/2507.13354)
*Zeqian Chen*

Main category: cs.LG

TL;DR: 论文从物理角度分析Transformer架构，提出基于开放量子系统的物理模型，解释其工作原理。


<details>
  <summary>Details</summary>
Motivation: 现有理论对Transformer架构的理解存在空白，尤其是其物理工作原理。

Method: 从现代芯片的物理视角，在Fock空间构建基于开放量子系统的物理模型。

Result: 提出了支持Transformer架构的物理模型，为大型语言模型提供理论基础。

Conclusion: 物理模型为Transformer架构提供了新的理论支持，填补了现有理解的空白。

Abstract: The introduction of the transformer architecture in 2017 (cf.\cite{VSP2017})
marked the most striking advancement in natural language processing. The
transformer is a model architecture relying entirely on an attention mechanism
to draw global dependencies between input and output. However, we believe there
is a gap in our theoretical understanding of what the transformer is, and why
it works physically. In this paper, from a physical perspective on modern
chips, we construct physical models in the Fock space over the Hilbert space of
tokens realizing large language models based on a transformer architecture as
open quantum systems. Our physical models underlie the transformer architecture
for large language models.

</details>


### [74] [Whose View of Safety? A Deep DIVE Dataset for Pluralistic Alignment of Text-to-Image Models](https://arxiv.org/abs/2507.13383)
*Charvi Rastogi,Tian Huey Teh,Pushkar Mishra,Roma Patel,Ding Wang,Mark Díaz,Alicia Parrish,Aida Mostafazadeh Davani,Zoe Ashwood,Michela Paganini,Vinodkumar Prabhakaran,Verena Rieser,Lora Aroyo*

Main category: cs.LG

TL;DR: 论文提出了一种多元对齐方法，通过引入DIVE数据集和实证研究，改进文本到图像（T2I）模型对多样化人类价值观的理解和响应。


<details>
  <summary>Details</summary>
Motivation: 当前T2I模型未能充分考虑多样化的人类经验，导致系统与人类价值观不一致。作者主张通过多元对齐方法解决这一问题。

Method: 1. 创建DIVE数据集，用于多元对齐评估；2. 实证研究人口统计学因素对观点多样性的影响；3. 探讨对齐T2I模型的策略。

Result: 证实人口统计学因素是多样化观点的重要代理，揭示了与传统评估不同的危害感知差异。

Conclusion: 研究为构建更公平和对齐的T2I系统提供了基础工具。

Abstract: Current text-to-image (T2I) models often fail to account for diverse human
experiences, leading to misaligned systems. We advocate for pluralistic
alignment, where an AI understands and is steerable towards diverse, and often
conflicting, human values. Our work provides three core contributions to
achieve this in T2I models. First, we introduce a novel dataset for Diverse
Intersectional Visual Evaluation (DIVE) -- the first multimodal dataset for
pluralistic alignment. It enable deep alignment to diverse safety perspectives
through a large pool of demographically intersectional human raters who
provided extensive feedback across 1000 prompts, with high replication,
capturing nuanced safety perceptions. Second, we empirically confirm
demographics as a crucial proxy for diverse viewpoints in this domain,
revealing significant, context-dependent differences in harm perception that
diverge from conventional evaluations. Finally, we discuss implications for
building aligned T2I models, including efficient data collection strategies,
LLM judgment capabilities, and model steerability towards diverse perspectives.
This research offers foundational tools for more equitable and aligned T2I
systems. Content Warning: The paper includes sensitive content that may be
harmful.

</details>


### [75] [Improving KAN with CDF normalization to quantiles](https://arxiv.org/abs/2507.13393)
*Jakub Strawa,Jarek Duda*

Main category: cs.LG

TL;DR: 论文探讨了在机器学习中使用CDF归一化的优势，并通过KANs展示了其优于传统归一化方法的效果。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习中常用的归一化方法（如均值标准差归一化或固定范围缩放）在金融领域的copula理论中并不常见，而CDF归一化能减少过拟合并提供更简单的表示。

Method: 通过将数据转换为CDF(x)实现归一化，应用于Kolmogorov-Arnold Networks (KANs)，并与Legendre-KAN进行比较。

Result: CDF归一化显著提升了KANs的预测性能，同时神经元权重可解释为混合矩，支持概率分布传播和方向调整。

Conclusion: CDF归一化在机器学习中具有潜力，尤其在KANs等模型中表现优异，值得进一步研究。

Abstract: Data normalization is crucial in machine learning, usually performed by
subtracting the mean and dividing by standard deviation, or by rescaling to a
fixed range. In copula theory, popular in finance, there is used normalization
to approximately quantiles by transforming x to CDF(x) with estimated CDF
(cumulative distribution function) to nearly uniform distribution in [0,1],
allowing for simpler representations which are less likely to overfit. It seems
nearly unknown in machine learning, therefore, we would like to present some
its advantages on example of recently popular Kolmogorov-Arnold Networks
(KANs), improving predictions from Legendre-KAN by just switching rescaling to
CDF normalization. Additionally, in HCR interpretation, weights of such neurons
are mixed moments providing local joint distribution models, allow to propagate
also probability distributions, and change propagation direction.

</details>


### [76] [Selective Embedding for Deep Learning](https://arxiv.org/abs/2507.13399)
*Mert Sehri,Zehui Hua,Francisco de Assis Boldt,Patrick Dumond*

Main category: cs.LG

TL;DR: 提出了一种名为选择性嵌入的新数据加载策略，通过交替多源数据的短片段来提升深度学习模型的泛化能力和计算效率。


<details>
  <summary>Details</summary>
Motivation: 深度学习对输入数据敏感，性能在非稳态条件和跨域数据中下降，传统数据加载策略泛化能力有限或计算成本高。

Method: 选择性嵌入策略，交替多源数据的短片段于单一输入通道，模仿人类信息处理以减少过拟合。

Result: 在六个时域数据集上验证，分类准确率高且训练时间显著减少，适用于多源数据的复杂系统。

Conclusion: 选择性嵌入为医疗、重工业、海洋、铁路和农业等领域提供了可扩展且资源高效的解决方案。

Abstract: Deep learning has revolutionized many industries by enabling models to
automatically learn complex patterns from raw data, reducing dependence on
manual feature engineering. However, deep learning algorithms are sensitive to
input data, and performance often deteriorates under nonstationary conditions
and across dissimilar domains, especially when using time-domain data.
Conventional single-channel or parallel multi-source data loading strategies
either limit generalization or increase computational costs. This study
introduces selective embedding, a novel data loading strategy, which alternates
short segments of data from multiple sources within a single input channel.
Drawing inspiration from cognitive psychology, selective embedding mimics
human-like information processing to reduce model overfitting, enhance
generalization, and improve computational efficiency. Validation is conducted
using six time-domain datasets, demonstrating that the proposed method
consistently achieves high classification accuracy across various deep learning
architectures while significantly reducing training times. The approach proves
particularly effective for complex systems with multiple data sources, offering
a scalable and resource-efficient solution for real-world applications in
healthcare, heavy machinery, marine, railway, and agriculture, where robustness
and adaptability are critical.

</details>


### [77] [LightAutoDS-Tab: Multi-AutoML Agentic System for Tabular Data](https://arxiv.org/abs/2507.13413)
*Aleksey Lapin,Igor Hromov,Stanislav Chumakov,Mile Mitrovic,Dmitry Simakov,Nikolay O. Nikitin,Andrey V. Savchenko*

Main category: cs.LG

TL;DR: LightAutoDS-Tab是一个结合LLM代码生成和多种AutoML工具的多代理系统，用于表格数据任务，提升了灵活性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决AutoML在依赖特定工具时效率受限的问题。

Method: 结合LLM代码生成和多种AutoML工具，设计多代理系统。

Result: 在多个Kaggle数据科学任务中优于现有开源解决方案。

Conclusion: LightAutoDS-Tab为表格数据任务提供了更高效、灵活的解决方案。

Abstract: AutoML has advanced in handling complex tasks using the integration of LLMs,
yet its efficiency remains limited by dependence on specific underlying tools.
In this paper, we introduce LightAutoDS-Tab, a multi-AutoML agentic system for
tasks with tabular data, which combines an LLM-based code generation with
several AutoML tools. Our approach improves the flexibility and robustness of
pipeline design, outperforming state-of-the-art open-source solutions on
several data science tasks from Kaggle. The code of LightAutoDS-Tab is
available in the open repository https://github.com/sb-ai-lab/LADS

</details>


### [78] [Gauge Flow Models](https://arxiv.org/abs/2507.13414)
*Alexander Strunk,Roland Assam*

Main category: cs.LG

TL;DR: Gauge Flow Models是一种新型生成流模型，通过引入可学习的Gauge Field提升性能，实验证明其在高斯混合模型上表现优于传统流模型。


<details>
  <summary>Details</summary>
Motivation: 传统生成流模型在性能上存在局限，需要更高效的模型结构。

Method: 在流常微分方程中引入可学习的Gauge Field，构建Gauge Flow Models。

Result: 在高斯混合模型实验中，Gauge Flow Models表现显著优于传统流模型。

Conclusion: Gauge Flow Models具有潜力在更广泛的生成任务中提升性能。

Abstract: This paper introduces Gauge Flow Models, a novel class of Generative Flow
Models. These models incorporate a learnable Gauge Field within the Flow
Ordinary Differential Equation (ODE). A comprehensive mathematical framework
for these models, detailing their construction and properties, is provided.
Experiments using Flow Matching on Gaussian Mixture Models demonstrate that
Gauge Flow Models yields significantly better performance than traditional Flow
Models of comparable or even larger size. Additionally, unpublished research
indicates a potential for enhanced performance across a broader range of
generative tasks.

</details>


### [79] [Single- to multi-fidelity history-dependent learning with uncertainty quantification and disentanglement: application to data-driven constitutive modeling](https://arxiv.org/abs/2507.13416)
*Jiaxiang Yi,Bernardo P. Ferreira,Miguel A. Bessa*

Main category: cs.LG

TL;DR: 该论文提出了一种层次化的多保真度数据驱动学习方法，能够量化认知不确定性并分离数据噪声，适用于从简单单保真度神经网络到复杂的多保真度贝叶斯循环神经网络。


<details>
  <summary>Details</summary>
Motivation: 解决多保真度数据驱动学习中的认知不确定性和数据噪声分离问题，提升模型在不确定性设计和分析中的适用性。

Method: 提出了一种层次化的多保真度学习方法，包括单保真度确定性神经网络和多保真度方差估计贝叶斯循环神经网络。

Result: 方法能够准确预测响应、量化模型误差，并在存在噪声时发现噪声分布。

Conclusion: 该方法为科学和工程领域中的不确定性设计和分析提供了新的可能性。

Abstract: Data-driven learning is generalized to consider history-dependent
multi-fidelity data, while quantifying epistemic uncertainty and disentangling
it from data noise (aleatoric uncertainty). This generalization is hierarchical
and adapts to different learning scenarios: from training the simplest
single-fidelity deterministic neural networks up to the proposed multi-fidelity
variance estimation Bayesian recurrent neural networks. The versatility and
generality of the proposed methodology are demonstrated by applying it to
different data-driven constitutive modeling scenarios that include multiple
fidelities with and without aleatoric uncertainty (noise). The method
accurately predicts the response and quantifies model error while also
discovering the noise distribution (when present). This opens opportunities for
future real-world applications in diverse scientific and engineering domains;
especially, the most challenging cases involving design and analysis under
uncertainty.

</details>


### [80] [Soft-ECM: An extension of Evidential C-Means for complex data](https://arxiv.org/abs/2507.13417)
*Armel Soubeiga,Thomas Guyet,Violaine Antoine*

Main category: cs.LG

TL;DR: 提出了一种基于信念函数的聚类算法Soft-ECM，适用于复杂数据（如混合数据和时间序列），克服了现有方法在非欧几里得空间中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有基于信念函数的聚类算法无法处理复杂数据（如混合数据或时间序列），因为它们依赖于欧几里得空间的性质。

Method: 重新定义了Evidential C-Means (ECM)问题，提出Soft-ECM算法，仅需半度量即可定位模糊簇的质心。

Result: Soft-ECM在数值数据上与传统模糊聚类方法效果相当，并能有效处理混合数据和时间序列数据。

Conclusion: Soft-ECM为复杂数据的聚类提供了一种有效且灵活的方法，尤其适用于非欧几里得空间的数据。

Abstract: Clustering based on belief functions has been gaining increasing attention in
the machine learning community due to its ability to effectively represent
uncertainty and/or imprecision. However, none of the existing algorithms can be
applied to complex data, such as mixed data (numerical and categorical) or
non-tabular data like time series. Indeed, these types of data are, in general,
not represented in a Euclidean space and the aforementioned algorithms make use
of the properties of such spaces, in particular for the construction of
barycenters. In this paper, we reformulate the Evidential C-Means (ECM) problem
for clustering complex data. We propose a new algorithm, Soft-ECM, which
consistently positions the centroids of imprecise clusters requiring only a
semi-metric. Our experiments show that Soft-ECM present results comparable to
conventional fuzzy clustering approaches on numerical data, and we demonstrate
its ability to handle mixed data and its benefits when combining fuzzy
clustering with semi-metrics such as DTW for time series data.

</details>


### [81] [Air Traffic Controller Task Demand via Graph Neural Networks: An Interpretable Approach to Airspace Complexity](https://arxiv.org/abs/2507.13423)
*Edward Henderson,Dewi Gould,Richard Everson,George De Ath,Nick Pepper*

Main category: cs.LG

TL;DR: 提出了一种基于图神经网络（GNN）的可解释框架，用于实时评估空中交通管制员（ATCO）的任务需求，优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 现有复杂性指标难以捕捉空中交通管制任务中的细微操作驱动因素，需要一种更可靠的方法。

Method: 使用注意力机制的GNN模型，通过静态交通场景中的交互预测即将发布的指令数量，并通过系统消融飞机来获得任务需求评分。

Result: 模型显著优于启发式方法，并能更可靠地评估场景复杂性，还能将任务需求归因于特定飞机。

Conclusion: 该框架为控制器培训和空域重新设计提供了新的复杂性分析工具。

Abstract: Real-time assessment of near-term Air Traffic Controller (ATCO) task demand
is a critical challenge in an increasingly crowded airspace, as existing
complexity metrics often fail to capture nuanced operational drivers beyond
simple aircraft counts. This work introduces an interpretable Graph Neural
Network (GNN) framework to address this gap. Our attention-based model predicts
the number of upcoming clearances, the instructions issued to aircraft by
ATCOs, from interactions within static traffic scenarios. Crucially, we derive
an interpretable, per-aircraft task demand score by systematically ablating
aircraft and measuring the impact on the model's predictions. Our framework
significantly outperforms an ATCO-inspired heuristic and is a more reliable
estimator of scenario complexity than established baselines. The resulting tool
can attribute task demand to specific aircraft, offering a new way to analyse
and understand the drivers of complexity for applications in controller
training and airspace redesign.

</details>


### [82] [Off-Policy Evaluation and Learning for Matching Markets](https://arxiv.org/abs/2507.13608)
*Yudai Hayashi,Shuhei Goda,Yuta Saito*

Main category: cs.LG

TL;DR: 论文提出了两种新的离线策略评估（OPE）方法DiPS和DPR，专门用于匹配市场，解决了传统OPE方法在双向用户交互中的方差和奖励稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 匹配市场中的双向用户交互和大规模特性导致传统OPE方法不可靠，需要更有效的评估方法。

Method: 结合了直接方法（DM）、逆倾向得分（IPS）和双重稳健（DR）估计器，并引入中间标签（如初始参与信号）以优化偏差-方差控制。

Result: 理论分析和实验表明，DiPS和DPR在偏差和方差控制上优于传统方法，并能扩展到离线策略学习。

Conclusion: 提出的方法在合成数据和真实招聘平台数据上表现优异，为匹配市场的推荐策略评估和学习提供了有效工具。

Abstract: Matching users based on mutual preferences is a fundamental aspect of
services driven by reciprocal recommendations, such as job search and dating
applications. Although A/B tests remain the gold standard for evaluating new
policies in recommender systems for matching markets, it is costly and
impractical for frequent policy updates. Off-Policy Evaluation (OPE) thus plays
a crucial role by enabling the evaluation of recommendation policies using only
offline logged data naturally collected on the platform. However, unlike
conventional recommendation settings, the large scale and bidirectional nature
of user interactions in matching platforms introduce variance issues and
exacerbate reward sparsity, making standard OPE methods unreliable. To address
these challenges and facilitate effective offline evaluation, we propose novel
OPE estimators, \textit{DiPS} and \textit{DPR}, specifically designed for
matching markets. Our methods combine elements of the Direct Method (DM),
Inverse Propensity Score (IPS), and Doubly Robust (DR) estimators while
incorporating intermediate labels, such as initial engagement signals, to
achieve better bias-variance control in matching markets. Theoretically, we
derive the bias and variance of the proposed estimators and demonstrate their
advantages over conventional methods. Furthermore, we show that these
estimators can be seamlessly extended to offline policy learning methods for
improving recommendation policies for making more matches. We empirically
evaluate our methods through experiments on both synthetic data and A/B testing
logs from a real job-matching platform. The empirical results highlight the
superiority of our approach over existing methods in off-policy evaluation and
learning tasks for a variety of configurations.

</details>


### [83] [Improving Out-of-distribution Human Activity Recognition via IMU-Video Cross-modal Representation Learning](https://arxiv.org/abs/2507.13482)
*Seyyed Saeid Cheshmi,Buyao Lyu,Thomas Lisko,Rajesh Rajamani,Robert A. McGovern,Yogatheesan Varatharajah*

Main category: cs.LG

TL;DR: 提出了一种跨模态自监督预训练方法，用于从大规模未标记的IMU-视频数据中学习表示，提高了在分布外（OOD）IMU数据集上的HAR任务泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有HAR方法依赖特定应用标签且泛化能力不足的问题，特别是在不同环境或人群中的数据。

Method: 采用跨模态自监督预训练方法，利用IMU-视频数据学习通用表示。

Result: 在零样本和少样本评估中，该方法优于现有IMU-视频预训练和仅IMU预训练方法。

Conclusion: 跨模态预训练是学习动态数据模态（如IMU信号）通用表示的有效工具。

Abstract: Human Activity Recognition (HAR) based on wearable inertial sensors plays a
critical role in remote health monitoring. In patients with movement disorders,
the ability to detect abnormal patient movements in their home environments can
enable continuous optimization of treatments and help alert caretakers as
needed. Machine learning approaches have been proposed for HAR tasks using
Inertial Measurement Unit (IMU) data; however, most rely on
application-specific labels and lack generalizability to data collected in
different environments or populations. To address this limitation, we propose a
new cross-modal self-supervised pretraining approach to learn representations
from large-sale unlabeled IMU-video data and demonstrate improved
generalizability in HAR tasks on out of distribution (OOD) IMU datasets,
including a dataset collected from patients with Parkinson's disease.
Specifically, our results indicate that the proposed cross-modal pretraining
approach outperforms the current state-of-the-art IMU-video pretraining
approach and IMU-only pretraining under zero-shot and few-shot evaluations.
Broadly, our study provides evidence that in highly dynamic data modalities,
such as IMU signals, cross-modal pretraining may be a useful tool to learn
generalizable data representations. Our software is available at
https://github.com/scheshmi/IMU-Video-OOD-HAR.

</details>


### [84] [Model-free Reinforcement Learning for Model-based Control: Towards Safe, Interpretable and Sample-efficient Agents](https://arxiv.org/abs/2507.13491)
*Thomas Banker,Ali Mesbah*

Main category: cs.LG

TL;DR: 论文提出基于模型的智能体作为模型无关强化学习的替代方案，结合系统动力学、成本和约束模型，实现安全、高效和可解释的决策学习。


<details>
  <summary>Details</summary>
Motivation: 模型无关强化学习（RL）虽然能通过系统交互直接优化决策，但依赖深度神经网络导致样本效率低、学习不安全且解释性差。因此，研究基于模型的智能体以解决这些问题。

Method: 利用系统动力学、成本和约束的模型进行策略近似，结合贝叶斯优化、策略搜索RL和离线学习等方法。

Result: 基于模型的智能体（如模型预测控制）能编码先验知识，提升学习效率和安全性，同时通过模型无关RL弥补模型失配问题。

Conclusion: 结合模型无关RL和基于模型的智能体，有望实现样本高效、安全且可解释的决策学习。

Abstract: Training sophisticated agents for optimal decision-making under uncertainty
has been key to the rapid development of modern autonomous systems across
fields. Notably, model-free reinforcement learning (RL) has enabled
decision-making agents to improve their performance directly through system
interactions, with minimal prior knowledge about the system. Yet, model-free RL
has generally relied on agents equipped with deep neural network function
approximators, appealing to the networks' expressivity to capture the agent's
policy and value function for complex systems. However, neural networks amplify
the issues of sample inefficiency, unsafe learning, and limited
interpretability in model-free RL. To this end, this work introduces
model-based agents as a compelling alternative for control policy
approximation, leveraging adaptable models of system dynamics, cost, and
constraints for safe policy learning. These models can encode prior system
knowledge to inform, constrain, and aid in explaining the agent's decisions,
while deficiencies due to model mismatch can be remedied with model-free RL. We
outline the benefits and challenges of learning model-based agents --
exemplified by model predictive control -- and detail the primary learning
approaches: Bayesian optimization, policy search RL, and offline strategies,
along with their respective strengths. While model-free RL has long been
established, its interplay with model-based agents remains largely unexplored,
motivating our perspective on their combined potentials for sample-efficient
learning of safe and interpretable decision-making agents.

</details>


### [85] [Fake or Real: The Impostor Hunt in Texts for Space Operations](https://arxiv.org/abs/2507.13508)
*Agata Kaczmarek,Dawid Płudowski,Piotr Wilczyński,Przemysław Biecek,Krzysztof Kotowski,Ramez Shendy,Jakub Nalepa,Artur Janicki,Evridiki Ntagiou*

Main category: cs.LG

TL;DR: Kaggle竞赛“Fake or Real”旨在解决LLM输出被恶意篡改的问题，要求参与者开发新技术或调整现有方法。


<details>
  <summary>Details</summary>
Motivation: 竞赛基于欧洲航天局资助的项目，针对AI安全威胁（数据投毒和过度依赖LLM），填补了该领域的研究空白。

Method: 参与者需区分正常LLM输出与恶意篡改的输出，开发或调整技术以解决这一问题。

Result: 竞赛结果未提及，但目标是推动新技术或方法的出现。

Conclusion: 该竞赛为LLM安全问题提供了实践平台，鼓励创新解决方案。

Abstract: The "Fake or Real" competition hosted on Kaggle
(\href{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt}{https://www.kaggle.com/competitions/fake-or-real-the-impostor-hunt})
is the second part of a series of follow-up competitions and hackathons related
to the "Assurance for Space Domain AI Applications" project funded by the
European Space Agency
(\href{https://assurance-ai.space-codev.org/}{https://assurance-ai.space-codev.org/}).
The competition idea is based on two real-life AI security threats identified
within the project -- data poisoning and overreliance in Large Language Models.
The task is to distinguish between the proper output from LLM and the output
generated under malicious modification of the LLM. As this problem was not
extensively researched, participants are required to develop new techniques to
address this issue or adjust already existing ones to this problem's statement.

</details>


### [86] [Provable Low-Frequency Bias of In-Context Learning of Representations](https://arxiv.org/abs/2507.13540)
*Yongyi Yang,Hidenori Tanaka,Wei Hu*

Main category: cs.LG

TL;DR: 论文提出了一个双收敛框架，解释了LLMs如何通过上下文学习（ICL）从输入序列中学习新行为，而无需参数更新。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何通过ICL从输入序列中学习新行为的机制，填补了现有研究的空白。

Method: 引入双收敛框架，分析隐藏表示在上下文和层间的收敛过程，证明其偏向平滑（低频）表示。

Result: 理论解释了ICL的多个经验观察，如表示几何的全局结构与局部扭曲，以及能量衰减现象。

Conclusion: 研究为ICL的机制提供了新见解，并为其在更广泛数据分布和设置中的研究奠定了理论基础。

Abstract: In-context learning (ICL) enables large language models (LLMs) to acquire new
behaviors from the input sequence alone without any parameter updates. Recent
studies have shown that ICL can surpass the original meaning learned in
pretraining stage through internalizing the structure the data-generating
process (DGP) of the prompt into the hidden representations. However, the
mechanisms by which LLMs achieve this ability is left open. In this paper, we
present the first rigorous explanation of such phenomena by introducing a
unified framework of double convergence, where hidden representations converge
both over context and across layers. This double convergence process leads to
an implicit bias towards smooth (low-frequency) representations, which we prove
analytically and verify empirically. Our theory explains several open empirical
observations, including why learned representations exhibit globally structured
but locally distorted geometry, and why their total energy decays without
vanishing. Moreover, our theory predicts that ICL has an intrinsic robustness
towards high-frequency noise, which we empirically confirm. These results
provide new insights into the underlying mechanisms of ICL, and a theoretical
foundation to study it that hopefully extends to more general data
distributions and settings.

</details>


### [87] [Acoustic Index: A Novel AI-Driven Parameter for Cardiac Disease Risk Stratification Using Echocardiography](https://arxiv.org/abs/2507.13542)
*Beka Begiashvili,Carlos J. Fernandez-Candel,Matías Pérez Paredes*

Main category: cs.LG

TL;DR: 提出了一种名为Acoustic Index的新型AI衍生超声心动图参数，用于量化心脏功能障碍，结合了EDMD和混合神经网络，表现优于传统参数。


<details>
  <summary>Details</summary>
Motivation: 传统超声心动图参数（如EF和GLS）在早期检测心脏功能障碍时存在局限性，需要一种可重复、可解释且操作者独立的参数。

Method: 结合EDMD和混合神经网络，提取超声心动图序列的时空动态特征，并通过注意力机制和流形学习融合临床数据，生成0-1的连续评分。

Result: 在736名患者的队列中，Acoustic Index的AUC为0.89，交叉验证显示敏感性和特异性均超过0.8。

Conclusion: Acoustic Index是一种物理信息驱动的可解释AI生物标志物，有望成为早期检测和监测的工具，未来需进一步验证和优化。

Abstract: Traditional echocardiographic parameters such as ejection fraction (EF) and
global longitudinal strain (GLS) have limitations in the early detection of
cardiac dysfunction. EF often remains normal despite underlying pathology, and
GLS is influenced by load conditions and vendor variability. There is a growing
need for reproducible, interpretable, and operator-independent parameters that
capture subtle and global cardiac functional alterations.
  We introduce the Acoustic Index, a novel AI-derived echocardiographic
parameter designed to quantify cardiac dysfunction from standard ultrasound
views. The model combines Extended Dynamic Mode Decomposition (EDMD) based on
Koopman operator theory with a hybrid neural network that incorporates clinical
metadata. Spatiotemporal dynamics are extracted from echocardiographic
sequences to identify coherent motion patterns. These are weighted via
attention mechanisms and fused with clinical data using manifold learning,
resulting in a continuous score from 0 (low risk) to 1 (high risk).
  In a prospective cohort of 736 patients, encompassing various cardiac
pathologies and normal controls, the Acoustic Index achieved an area under the
curve (AUC) of 0.89 in an independent test set. Cross-validation across five
folds confirmed the robustness of the model, showing that both sensitivity and
specificity exceeded 0.8 when evaluated on independent data. Threshold-based
analysis demonstrated stable trade-offs between sensitivity and specificity,
with optimal discrimination near this threshold.
  The Acoustic Index represents a physics-informed, interpretable AI biomarker
for cardiac function. It shows promise as a scalable, vendor-independent tool
for early detection, triage, and longitudinal monitoring. Future directions
include external validation, longitudinal studies, and adaptation to
disease-specific classifiers.

</details>


### [88] [Time Series Forecastability Measures](https://arxiv.org/abs/2507.13556)
*Rui Wang,Steven Klee,Alexis Roos*

Main category: cs.LG

TL;DR: 本文提出两种指标（谱可预测性分数和最大李雅普诺夫指数）来量化时间序列的预测性，这些指标在模型开发前评估数据的固有特性，帮助优化预测策略。


<details>
  <summary>Details</summary>
Motivation: 传统模型评估指标在模型开发后才评估预测性能，而本文旨在通过提前量化时间序列的固有预测性，帮助实践者在模型训练前优化资源分配和策略。

Method: 使用谱可预测性分数评估时间序列频率成分的强度和规律性，同时利用李雅普诺夫指数量化数据生成系统的混沌和稳定性。

Result: 在合成和真实世界时间序列（M5预测竞赛数据集）上的实验表明，这两种指标能准确反映时间序列的固有预测性，并与实际模型预测性能高度相关。

Conclusion: 通过提前了解时间序列的预测性，实践者可针对高预测性产品优化资源，同时对低预测性产品设定合理预期或寻求替代策略。

Abstract: This paper proposes using two metrics to quantify the forecastability of time
series prior to model development: the spectral predictability score and the
largest Lyapunov exponent. Unlike traditional model evaluation metrics, these
measures assess the inherent forecastability characteristics of the data before
any forecast attempts. The spectral predictability score evaluates the strength
and regularity of frequency components in the time series, whereas the Lyapunov
exponents quantify the chaos and stability of the system generating the data.
We evaluated the effectiveness of these metrics on both synthetic and
real-world time series from the M5 forecast competition dataset. Our results
demonstrate that these two metrics can correctly reflect the inherent
forecastability of a time series and have a strong correlation with the actual
forecast performance of various models. By understanding the inherent
forecastability of time series before model training, practitioners can focus
their planning efforts on products and supply chain levels that are more
forecastable, while setting appropriate expectations or seeking alternative
strategies for products with limited forecastability.

</details>


### [89] [Change of Thought: Adaptive Test-Time Computation](https://arxiv.org/abs/2507.13569)
*Mrinal Mathur,Mike Doan,Barak Pearlmutter,Sergey Plis*

Main category: cs.LG

TL;DR: SELF-Transformer通过内部迭代更新注意力权重，提升编码器Transformer的表达能力，无需依赖自回归解码，在测试时根据输入难度动态调整计算，实现了20%的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在固定深度下表达能力有限，而自回归方法依赖外部化中间状态。SELF-Transformer旨在通过内部迭代提升表达能力，同时保持编码器架构的简洁性。

Method: 引入SELF-Transformer，通过迭代更新注意力权重至固定点，动态调整测试时计算，避免依赖自回归解码。

Result: 在编码器基准测试中，SELF-Transformer实现了高达20%的准确率提升，且不增加参数量。

Conclusion: SELF-Transformer通过内部迭代恢复了迭代推理的表达能力，同时保持了编码器架构的简洁性，为测试时自适应对齐提供了显著优势。

Abstract: Transformers evaluated in a single, fixed-depth pass are provably limited in
expressive power to the constant-depth circuit class TC0. Running a Transformer
autoregressively removes that ceiling -- first in next-token prediction and,
more recently, in chain-of-thought reasoning. Both regimes rely on feedback
loops that decode internal states into tokens only to re-encode them in
subsequent steps. While this "thinking aloud" mirrors human reasoning,
biological brains iterate without externalising intermediate states as
language. To boost the expressive power of encoder Transformers without
resorting to token-level autoregression, we introduce the SELF-Transformer: an
encoder layer that iteratively refines its own attention weights to a fixed
point. Instead of producing -- in one pass -- the alignment matrix that remixes
the input sequence, the SELF-Transformer iteratively updates that matrix
internally, scaling test-time computation with input difficulty. This
adaptivity yields up to 20\% accuracy gains on encoder-style benchmarks without
increasing parameter count, demonstrating that input-adaptive alignment at test
time offers substantial benefits for only a modest extra compute budget.
Self-Transformers thus recover much of the expressive power of iterative
reasoning while preserving the simplicity of pure encoder architectures.

</details>


### [90] [Apple Intelligence Foundation Language Models: Tech Report 2025](https://arxiv.org/abs/2507.13575)
*Hanzhi Zhou,Erik Hornberger,Pengsheng Guo,Xiyou Zhou,Saiwen Wang,Xin Wang,Yifei He,Xuankai Chang,Rene Rauch,Louis D'hauwe,John Peebles,Alec Doane,Kohen Chia,Jenna Thibodeau,Zi-Yi Dou,Yuanyang Zhang,Ruoming Pang,Reed Li,Zhifeng Chen,Jeremy Warner,Zhaoyang Xu,Sophy Lee,David Mizrahi,Ramsey Tantawi,Chris Chaney,Kelsey Peterson,Jun Qin,Alex Dombrowski,Mira Chiang,Aiswarya Raghavan,Gerard Casamayor,Qibin Chen,Aonan Zhang,Nathalie Tran,Jianyu Wang,Hang Su,Thomas Voice,Alessandro Pappalardo,Brycen Wershing,Prasanth Yadla,Rui Li,Priyal Chhatrapati,Ismael Fernandez,Yusuf Goren,Xin Zheng,Forrest Huang,Tao Lei,Eray Yildiz,Alper Kokmen,Gokul Santhanam,Areeba Kamal,Kaan Elgin,Dian Ang Yap,Jeremy Liu,Peter Gray,Howard Xing,Kieran Liu,Matteo Ronchi,Moritz Schwarzer-Becker,Yun Zhu,Mandana Saebi,Jeremy Snow,David Griffiths,Guillaume Tartavel,Erin Feldman,Simon Lehnerer,Fernando Bermúdez-Medina,Hans Han,Joe Zhou,Xiaoyi Ren,Sujeeth Reddy,Zirui Wang,Tom Gunter,Albert Antony,Yuanzhi Li,John Dennison,Tony Sun,Yena Han,Yi Qin,Sam Davarnia,Jeffrey Bigham,Wayne Shan,Hannah Gillis Coleman,Guillaume Klein,Peng Liu,Muyang Yu,Jack Cackler,Yuan Gao,Crystal Xiao,Binazir Karimzadeh,Zhengdong Zhang,Felix Bai,Albin Madappally Jose,Feng Nan,Nazir Kamaldin,Dong Yin,Hans Hao,Yanchao Sun,Yi Hua,Charles Maalouf,Alex Guillen Garcia,Guoli Yin,Lezhi Li,Mohana Prasad Sathya Moorthy,Hongbin Gao,Jay Tang,Joanna Arreaza-Taylor,Faye Lao,Carina Peng,Josh Shaffer,Dan Masi,Sushma Rao,Tommi Vehvilainen,Senyu Tong,Dongcai Shen,Yang Zhao,Chris Bartels,Peter Fu,Qingqing Cao,Christopher Neubauer,Ethan Li,Mingfei Gao,Rebecca Callahan,Richard Wei,Patrick Dong,Alex Braunstein,Sachin Ravi,Adolfo Lopez Mendez,Kaiwei Huang,Kun Duan,Haoshuo Huang,Rui Qian,Stefano Ligas,Jordan Huffaker,Dongxu Li,Bailin Wang,Nanzhu Wang,Anuva Agarwal,Tait Madsen,Josh Newnham,Abhishek Sharma,Zhile Ren,Deepak Gopinath,Erik Daxberger,Saptarshi Guha,Oron Levy,Jing Lu,Nan Dun,Marc Kirchner,Yinfei Yang,Manjot Bilkhu,Dave Nelson,Anthony Spalvieri-Kruse,Juan Lao Tebar,Yang Xu,Phani Mutyala,Gabriel Jacoby-Cooper,Yingbo Wang,Karla Vega,Vishaal Mahtani,Darren Botten,Eric Wang,Hanli Li,Matthias Paulik,Haoran Yan,Navid Shiee,Yihao Qian,Bugu Wu,Qi Zhu,Ob Adaranijo,Bhuwan Dhingra,Zhe Gan,Nicholas Seidl,Grace Duanmu,Rong Situ,Yiping Ma,Yin Xia,David Riazati,Vasileios Saveris,Anh Nguyen,Michael,Lee,Patrick Sonnenberg,Chinguun Erdenebileg,Yanghao Li,Vivian Ma,James Chou,Isha Garg,Mark Lee,Keen You,Yuhong Li,Ransen Niu,Nandhitha Raghuram,Pulkit Agrawal,Henry Mason,Sumeet Singh,Keyu He,Hong-You Chen,Lucas Guibert,Shiyu Li,Varsha Paidi,Narendran Raghavan,Mingze Xu,Yuli Yang,Sergiu Sima,Irina Belousova,Sprite Chu,Afshin Dehghan,Philipp Dufter,David Haldimann,Zhen Yang,Margit Bowler,Chang Liu,Ying-Chang Cheng,Vivek Rathod,Syd Evans,Wilson Tsao,Dustin Withers,Haitian Sun,Biyao Wang,Peter Grasch,Walker Cheng,Yihao Feng,Vivek Kumar,Frank Chu,Victoria MönchJuan Haladjian,Doug Kang,Jiarui Lu,Ciro Sannino,Max Lam,Floris Weers,Bowen Pan,Kenneth Jung,Dhaval Doshi,Fangping Shi,Olli Saarikivi,Alp Aygar,Josh Elman,Cheng Leong,Eshan Verma,Matthew Lei,Jeff Nichols,Jiulong Shan,Donald Zhang,Lawrence Zhou,Stephen Murphy,Xianzhi Du,Chang Lan,Ankur Jain,Elmira Amirloo,Marcin Eichner,Naomy Sabo,Anupama Mann Anupama,David Qiu,Zhao Meng,Michael FitzMaurice,Peng Zhang,Simon Yeung,Chen Chen,Marco Zuliani,Andrew Hansen,Yang Lu,Brent Ramerth,Ziyi Zhong,Parsa Mazaheri,Matthew Hopkins,Mengyu Li,Simon Wang,David Chen,Farzin Rasteh,Chong Wang,Josh Gardner,Asaf Liberman,Haoxuan You,Andrew Walkingshaw,Xingyu Zhou,Jinhao Lei,Yan Meng,Quentin Keunebroek,Sam Wiseman,Anders Boesen Lindbo Larsen,Yi Zhang,Zaid Ahmed,Haiming Gang,Aaron Franklin,Kelvin Zou,Guillaume Seguin,Jonathan Janke,Rachel Burger,Co Giang,Cheng Shen,Jen Liu,Sanskruti Shah,Xiang Kong,Yiran Fei,TJ Collins,Chen Zhang,Zhiyun Lu,Michael Booker,Qin Ba,Yasutaka Tanaka,Andres Romero Mier Y Teran,Federico Scozzafava,Regan Poston,Jane Li,Eduardo Jimenez,Bas Straathof,Karanjeet Singh,Lindsay Hislop,Rajat Arora,Deepa Seshadri,Boyue Li,Colorado Reed,Zhen Li,TJ Lu,Yi Wang,Kaelen Haag,Nicholas Lusskin,Raunak Sinha,Rahul Nair,Eldon Schoop,Mary Beth Kery,Mehrdad Farajtbar,Brenda Yang,George Horrell,Shiwen Zhao,Dhruti Shah,Cha Chen,Bowen Zhang,Chang Gao,Devi Krishna,Jennifer Mallalieu,Javier Movellan,Di Feng,Emily Zhang,Sam Xu,Junting Pan,Dominik Moritz,Suma Jayaram,Kevin Smith,Dongseong Hwang,Daniel Parilla,Jiaming Hu,You-Cyuan Jhang,Emad Soroush,Fred Hohman,Nan Du,Emma Wang,Sam Dodge,Pragnya Sridhar,Joris Pelemans,Wei Fang,Nina Wenzel,Joseph Yitan Cheng,Hadas Kotek,Chung-Cheng Chiu,Meng Cao,Haijing Fu,Ruixuan Hou,Ke Ye,Diane Zhu,Nikhil Bhendawade,Joseph Astrauskas,Jian Liu,Sai Aitharaju,Wentao Wu,Artsiom Peshko,Hyunjik Kim,Nilesh Shahdadpuri,Andy De Wang,Qi Shan,Piotr Maj,Raul Rea Menacho,Justin Lazarow,Eric Liang Yang,Arsalan Farooq,Donghan Yu,David Güera,Minsik Cho,Kavya Nerella,Yongqiang Wang,Tao Jia,John Park,Jeff Lai,Haotian Zhang,Futang Peng,Daniele Molinari,Aparna Rajamani,Tyler Johnson,Lauren Gardiner,Chao Jia,Violet Yao,Wojciech Kryscinski,Xiujun Li,Shang-Chen Wu*

Main category: cs.LG

TL;DR: 苹果推出两款多语言、多模态基础语言模型，分别用于设备端和服务器端，支持多语言和图像理解，性能优于同类开源模型。


<details>
  <summary>Details</summary>
Motivation: 为苹果设备和服务提供智能功能，同时兼顾性能、隐私和负责任的人工智能开发。

Method: 设备端模型采用3B参数和KV缓存共享等技术；服务器端模型基于PT-MoE架构，结合并行计算和稀疏计算。两者均通过大规模数据训练和优化。

Result: 模型在多语言和图像理解任务中表现优异，性能超越同类开源模型。

Conclusion: 苹果通过技术创新和负责任的人工智能实践，推出了高性能且隐私保护的智能模型。

Abstract: We introduce two multilingual, multimodal foundation language models that
power Apple Intelligence features across Apple devices and services: i a
3B-parameter on-device model optimized for Apple silicon through architectural
innovations such as KV-cache sharing and 2-bit quantization-aware training; and
ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts
PT-MoE transformer that combines track parallelism, mixture-of-experts sparse
computation, and interleaved global-local attention to deliver high quality
with competitive cost on Apple's Private Cloud Compute platform. Both models
are trained on large-scale multilingual and multimodal datasets sourced via
responsible web crawling, licensed corpora, and high-quality synthetic data,
then further refined with supervised fine-tuning and reinforcement learning on
a new asynchronous platform. The resulting models support several additional
languages while understanding images and executing tool calls. In public
benchmarks and human evaluations, both the server model and the on-device model
match or surpass comparably sized open baselines.
  A new Swift-centric Foundation Models framework exposes guided generation,
constrained tool calling, and LoRA adapter fine-tuning, allowing developers to
integrate these capabilities with a few lines of code. The latest advancements
in Apple Intelligence models are grounded in our Responsible AI approach with
safeguards like content filtering and locale-specific evaluation, as well as
our commitment to protecting our users' privacy with innovations like Private
Cloud Compute.

</details>


### [91] [Learning Pluralistic User Preferences through Reinforcement Learning Fine-tuned Summaries](https://arxiv.org/abs/2507.13579)
*Hyunji Nam,Yanming Wan,Mickel Liu,Jianxun Lian,Natasha Jaques*

Main category: cs.LG

TL;DR: PLUS框架通过生成用户偏好摘要，实现个性化LLM响应，优于传统RLHF和上下文学习。


<details>
  <summary>Details</summary>
Motivation: 传统RLHF无法区分用户差异，需个性化响应以适应不同用户偏好。

Method: 提出PLUS框架，通过强化学习生成用户偏好摘要并更新奖励模型。

Result: PLUS能有效捕捉用户偏好，适应新用户和多样话题，并可零样本迁移至GPT-4。

Conclusion: PLUS提供透明、可解释的用户摘要，增强LLM对齐的用户控制。

Abstract: As everyday use cases of large language model (LLM) AI assistants have
expanded, it is becoming increasingly important to personalize responses to
align to different users' preferences and goals. While reinforcement learning
from human feedback (RLHF) is effective at improving LLMs to be generally more
helpful and fluent, it does not account for variability across users, as it
models the entire user population with a single reward model. We present a
novel framework, Preference Learning Using Summarization (PLUS), that learns
text-based summaries of each user's preferences, characteristics, and past
conversations. These summaries condition the reward model, enabling it to make
personalized predictions about the types of responses valued by each user. We
train the user-summarization model with reinforcement learning, and update the
reward model simultaneously, creating an online co-adaptation loop. We show
that in contrast with prior personalized RLHF techniques or with in-context
learning of user information, summaries produced by PLUS capture meaningful
aspects of a user's preferences. Across different pluralistic user datasets, we
show that our method is robust to new users and diverse conversation topics.
Additionally, we demonstrate that the textual summaries generated about users
can be transferred for zero-shot personalization of stronger, proprietary
models like GPT-4. The resulting user summaries are not only concise and
portable, they are easy for users to interpret and modify, allowing for more
transparency and user control in LLM alignment.

</details>


### [92] [Tri-Learn Graph Fusion Network for Attributed Graph Clustering](https://arxiv.org/abs/2507.13620)
*Binxiong Li,Yuefei Wang,Xu Xiang,Xue Li,Binyu Zhao,Heyang Gao,Qinyu Zhao,Xi Yu*

Main category: cs.LG

TL;DR: 提出了一种名为Tri-GFN的新型深度聚类框架，结合GCN、AE和Graph Transformer，通过三重学习机制和特征融合策略提升图数据聚类质量。


<details>
  <summary>Details</summary>
Motivation: 解决GCN在处理大规模复杂图数据时的过平滑和过压缩问题，以及Graph Transformer在异构图数据上的性能限制。

Method: 结合GCN、AE和Graph Transformer模块，通过三重学习机制和特征融合增强策略，最大化利用节点属性和拓扑结构。

Result: 在ACM、Reuters和USPS数据集上分别提升0.87%、14.14%和7.58%的准确率，特别适用于新闻分类和主题检索。

Conclusion: Tri-GFN通过多模块融合和学习机制显著提升了图聚类的性能，具有广泛的应用潜力。

Abstract: In recent years, models based on Graph Convolutional Networks (GCN) have made
significant strides in the field of graph data analysis. However, challenges
such as over-smoothing and over-compression remain when handling large-scale
and complex graph datasets, leading to a decline in clustering quality.
Although the Graph Transformer architecture has mitigated some of these issues,
its performance is still limited when processing heterogeneous graph data. To
address these challenges, this study proposes a novel deep clustering framework
that comprising GCN, Autoencoder (AE), and Graph Transformer, termed the
Tri-Learn Graph Fusion Network (Tri-GFN). This framework enhances the
differentiation and consistency of global and local information through a
unique tri-learning mechanism and feature fusion enhancement strategy. The
framework integrates GCN, AE, and Graph Transformer modules. These components
are meticulously fused by a triple-channel enhancement module, which maximizes
the use of both node attributes and topological structures, ensuring robust
clustering representation. The tri-learning mechanism allows mutual learning
among these modules, while the feature fusion strategy enables the model to
capture complex relationships, yielding highly discriminative representations
for graph clustering. It surpasses many state-of-the-art methods, achieving an
accuracy improvement of approximately 0.87% on the ACM dataset, 14.14 % on the
Reuters dataset, and 7.58 % on the USPS dataset. Due to its outstanding
performance on the Reuters dataset, Tri-GFN can be applied to automatic news
classification, topic retrieval, and related fields.

</details>


### [93] [FedSkipTwin: Digital-Twin-Guided Client Skipping for Communication-Efficient Federated Learning](https://arxiv.org/abs/2507.13624)
*Daniel Commey,Kamel Abbad,Garth V. Crosby,Lyes Khoukhi*

Main category: cs.LG

TL;DR: FedSkipTwin是一种基于服务器端数字孪生的客户端跳过算法，通过预测梯度更新的幅度和认知不确定性来减少通信开销，在非IID数据分布下显著节省带宽并提升模型精度。


<details>
  <summary>Details</summary>
Motivation: 联邦学习中的通信开销是主要瓶颈，尤其是带宽受限的移动和物联网设备。

Method: 使用轻量级LSTM数字孪生预测客户端的梯度更新幅度和认知不确定性，仅在预测值超过阈值时请求通信，否则跳过轮次。

Result: 在UCI-HAR和MNIST数据集上，FedSkipTwin减少了12-15.5%的总通信量，同时模型精度提升0.5个百分点。

Conclusion: 预测引导的跳过策略是带宽受限边缘环境中资源感知联邦学习的实用有效方法。

Abstract: Communication overhead remains a primary bottleneck in federated learning
(FL), particularly for applications involving mobile and IoT devices with
constrained bandwidth. This work introduces FedSkipTwin, a novel
client-skipping algorithm driven by lightweight, server-side digital twins.
Each twin, implemented as a simple LSTM, observes a client's historical
sequence of gradient norms to forecast both the magnitude and the epistemic
uncertainty of its next update. The server leverages these predictions,
requesting communication only when either value exceeds a predefined threshold;
otherwise, it instructs the client to skip the round, thereby saving bandwidth.
Experiments are conducted on the UCI-HAR and MNIST datasets with 10 clients
under a non-IID data distribution. The results demonstrate that FedSkipTwin
reduces total communication by 12-15.5% across 20 rounds while simultaneously
improving final model accuracy by up to 0.5 percentage points compared to the
standard FedAvg algorithm. These findings establish that prediction-guided
skipping is a practical and effective strategy for resource-aware FL in
bandwidth-constrained edge environments.

</details>


### [94] [A Comprehensive Review of Transformer-based language models for Protein Sequence Analysis and Design](https://arxiv.org/abs/2507.13646)
*Nimisha Ghosh,Daniele Santoni,Debaleena Nawn,Eleonora Ottaviani,Giovanni Felici*

Main category: cs.LG

TL;DR: 本文综述了基于Transformer的语言模型在蛋白质序列分析与设计中的最新进展，包括基因本体、蛋白质功能与结构识别、新蛋白质生成及蛋白质结合等应用，并探讨了现有研究的优缺点及未来发展方向。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在NLP领域的成功启发了其在生物信息学中的应用，本文旨在总结和评估这些模型在蛋白质研究中的最新进展。

Method: 通过综述和分析大量相关研究，探讨了Transformer模型在蛋白质序列分析中的多种应用。

Result: 文章总结了现有研究的优缺点，为读者提供了全面的视角，并指出了当前研究的不足。

Conclusion: 本文为相关领域的研究者提供了当前研究现状的概览，并提出了未来发展的潜在方向。

Abstract: The impact of Transformer-based language models has been unprecedented in
Natural Language Processing (NLP). The success of such models has also led to
their adoption in other fields including bioinformatics. Taking this into
account, this paper discusses recent advances in Transformer-based models for
protein sequence analysis and design. In this review, we have discussed and
analysed a significant number of works pertaining to such applications. These
applications encompass gene ontology, functional and structural protein
identification, generation of de novo proteins and binding of proteins. We
attempt to shed light on the strength and weaknesses of the discussed works to
provide a comprehensive insight to readers. Finally, we highlight shortcomings
in existing research and explore potential avenues for future developments. We
believe that this review will help researchers working in this field to have an
overall idea of the state of the art in this field, and to orient their future
studies.

</details>


### [95] [Generalist Bimanual Manipulation via Foundation Video Diffusion Models](https://arxiv.org/abs/2507.12898)
*Yao Feng,Hengkai Tan,Xinyi Mao,Guodong Liu,Shuhe Huang,Chendong Xiang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: VIDAR是一个两阶段框架，通过扩散模型预训练和掩码逆向动力学模型解决双手机器人操作中的数据稀缺和异构性问题，仅需少量演示即可泛化到新任务和背景。


<details>
  <summary>Details</summary>
Motivation: 双手机器人操作中存在数据稀缺和异构性问题，限制了其扩展能力。

Method: VIDAR结合大规模视频扩散预训练和掩码逆向动力学模型，利用多视角视频数据和无像素标签的动作预测。

Result: 仅需20分钟人类演示（1%数据需求），VIDAR在新任务和背景中表现优于现有方法。

Conclusion: 视频基础模型与掩码动作预测结合，有望实现可扩展和泛化的机器人操作。

Abstract: Bimanual robotic manipulation, which involves the coordinated control of two
robotic arms, is foundational for solving challenging tasks. Despite recent
progress in general-purpose manipulation, data scarcity and embodiment
heterogeneity remain serious obstacles to further scaling up in bimanual
settings. In this paper, we introduce VIdeo Diffusion for Action Reasoning
(VIDAR), a two-stage framework that leverages large-scale, diffusion-based
video pre-training and a novel masked inverse dynamics model for action
prediction. We pre-train the video diffusion model on 750K multi-view videos
from three real-world bimanual robot platforms, utilizing a unified observation
space that encodes robot, camera, task, and scene contexts. Our masked inverse
dynamics model learns masks to extract action-relevant information from
generated trajectories without requiring pixel-level labels, and the masks can
effectively generalize to unseen backgrounds. Our experiments demonstrate that
with only 20 minutes of human demonstrations on an unseen robot platform (only
1% of typical data requirements), VIDAR generalizes to unseen tasks and
backgrounds with strong semantic understanding, surpassing state-of-the-art
methods. Our findings highlight the potential of video foundation models,
coupled with masked action prediction, to enable scalable and generalizable
robotic manipulation in diverse real-world settings.

</details>


### [96] [Kolmogorov-Arnold Networks-based GRU and LSTM for Loan Default Early Prediction](https://arxiv.org/abs/2507.13685)
*Yue Yang,Zihan Su,Ying Zhang,Chang Chuan Goh,Yuxiang Lin,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: 论文提出GRU-KAN和LSTM-KAN架构，显著提升贷款违约早期预测能力，准确率分别达92%（3个月前）和88%（8个月前）。


<details>
  <summary>Details</summary>
Motivation: 解决现有贷款违约预测模型在早期预测中的准确率不足和依赖特定时间框架的问题。

Method: 结合Kolmogorov-Arnold Networks (KAN)与GRU和LSTM网络，提出GRU-KAN和LSTM-KAN模型。

Result: 新模型在多种测试条件下显著优于基线模型，3个月前预测准确率达92%，8个月前达88%。

Conclusion: GRU-KAN和LSTM-KAN架构有效提升了贷款违约的早期预测能力，具有实际应用价值。

Abstract: This study addresses a critical challenge in time series anomaly detection:
enhancing the predictive capability of loan default models more than three
months in advance to enable early identification of default events, helping
financial institutions implement preventive measures before risk events
materialize. Existing methods have significant drawbacks, such as their lack of
accuracy in early predictions and their dependence on training and testing
within the same year and specific time frames. These issues limit their
practical use, particularly with out-of-time data. To address these, the study
introduces two innovative architectures, GRU-KAN and LSTM-KAN, which merge
Kolmogorov-Arnold Networks (KAN) with Gated Recurrent Units (GRU) and Long
Short-Term Memory (LSTM) networks. The proposed models were evaluated against
the baseline models (LSTM, GRU, LSTM-Attention, and LSTM-Transformer) in terms
of accuracy, precision, recall, F1 and AUC in different lengths of feature
window, sample sizes, and early prediction intervals. The results demonstrate
that the proposed model achieves a prediction accuracy of over 92% three months
in advance and over 88% eight months in advance, significantly outperforming
existing baselines.

</details>


### [97] [Binarizing Physics-Inspired GNNs for Combinatorial Optimization](https://arxiv.org/abs/2507.13703)
*Martin Krutský,Gustav Šír,Vyacheslav Kungurtsev,Georgios Korpas*

Main category: cs.LG

TL;DR: PI-GNNs在组合优化问题中表现良好，但随着问题图密度增加，性能下降。研究发现训练动态中存在相变，并提出基于模糊逻辑和二值化神经网络的改进方法，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 研究PI-GNNs在组合优化问题中的性能表现，尤其是随着问题图密度增加时性能下降的原因。

Method: 通过分析训练动态中的相变现象，提出基于模糊逻辑和二值化神经网络的改进策略。

Result: 实验证明，提出的方法显著提升了PI-GNNs在高密度问题中的性能。

Conclusion: 改进后的PI-GNNs能够更好地处理高密度组合优化问题，填补了松弛模型输出与二值解之间的差距。

Abstract: Physics-inspired graph neural networks (PI-GNNs) have been utilized as an
efficient unsupervised framework for relaxing combinatorial optimization
problems encoded through a specific graph structure and loss, reflecting
dependencies between the problem's variables. While the framework has yielded
promising results in various combinatorial problems, we show that the
performance of PI-GNNs systematically plummets with an increasing density of
the combinatorial problem graphs. Our analysis reveals an interesting phase
transition in the PI-GNNs' training dynamics, associated with degenerate
solutions for the denser problems, highlighting a discrepancy between the
relaxed, real-valued model outputs and the binary-valued problem solutions. To
address the discrepancy, we propose principled alternatives to the naive
strategy used in PI-GNNs by building on insights from fuzzy logic and binarized
neural networks. Our experiments demonstrate that the portfolio of proposed
methods significantly improves the performance of PI-GNNs in increasingly dense
settings.

</details>


### [98] [Bayesian Optimization for Molecules Should Be Pareto-Aware](https://arxiv.org/abs/2507.13704)
*Anabel Yong,Austin Tripp,Layla Hosseini-Gerami,Brooks Paige*

Main category: cs.LG

TL;DR: 多目标贝叶斯优化（MOBO）在分子设计中优于标量化方法，尤其在低数据情况下。


<details>
  <summary>Details</summary>
Motivation: 探索MOBO在分子设计中的实际优势，尤其是与标量化方法相比。

Method: 使用基于Pareto的MOBO策略（EHVI）与固定权重标量化基线（EI）进行对比实验。

Result: EHVI在Pareto前沿覆盖、收敛速度和化学多样性上均优于EI。

Conclusion: Pareto感知的获取策略在分子优化中具有实际优势，尤其在预算有限和权衡复杂时。

Abstract: Multi-objective Bayesian optimization (MOBO) provides a principled framework
for navigating trade-offs in molecular design. However, its empirical
advantages over scalarized alternatives remain underexplored. We benchmark a
simple Pareto-based MOBO strategy -- Expected Hypervolume Improvement (EHVI) --
against a simple fixed-weight scalarized baseline using Expected Improvement
(EI), under a tightly controlled setup with identical Gaussian Process
surrogates and molecular representations. Across three molecular optimization
tasks, EHVI consistently outperforms scalarized EI in terms of Pareto front
coverage, convergence speed, and chemical diversity. While scalarization
encompasses flexible variants -- including random or adaptive schemes -- our
results show that even strong deterministic instantiations can underperform in
low-data regimes. These findings offer concrete evidence for the practical
advantages of Pareto-aware acquisition in de novo molecular optimization,
especially when evaluation budgets are limited and trade-offs are nontrivial.

</details>


### [99] [Learning Deformable Body Interactions With Adaptive Spatial Tokenization](https://arxiv.org/abs/2507.13707)
*Hao Wang,Yu Liu,Daniel Biggs,Haoru Wang,Jiandong Yu,Ping Huang*

Main category: cs.LG

TL;DR: 提出了一种自适应空间标记化（AST）方法，通过网格划分和注意力机制，高效模拟可变形体交互，解决了现有方法的可扩展性问题。


<details>
  <summary>Details</summary>
Motivation: 模拟可变形体交互在材料科学、机械设计和机器人学中至关重要，但现有基于图神经网络的方法在建模大规模网格时存在计算瓶颈。

Method: 将模拟空间划分为网格单元，将非结构化网格映射到结构化网格上，利用交叉注意力模块生成紧凑的固定长度嵌入，并通过自注意力模块预测下一状态。

Result: 实验表明，该方法在可变形体交互建模中显著优于现有技术，尤其适用于超过10万个节点的大规模模拟。

Conclusion: AST方法通过结合标记化效率和注意力机制的表达能力，实现了准确且可扩展的模拟，同时贡献了一个新的大规模数据集。

Abstract: Simulating interactions between deformable bodies is vital in fields like
material science, mechanical design, and robotics. While learning-based methods
with Graph Neural Networks (GNNs) are effective at solving complex physical
systems, they encounter scalability issues when modeling deformable body
interactions. To model interactions between objects, pairwise global edges have
to be created dynamically, which is computationally intensive and impractical
for large-scale meshes. To overcome these challenges, drawing on insights from
geometric representations, we propose an Adaptive Spatial Tokenization (AST)
method for efficient representation of physical states. By dividing the
simulation space into a grid of cells and mapping unstructured meshes onto this
structured grid, our approach naturally groups adjacent mesh nodes. We then
apply a cross-attention module to map the sparse cells into a compact,
fixed-length embedding, serving as tokens for the entire physical state.
Self-attention modules are employed to predict the next state over these tokens
in latent space. This framework leverages the efficiency of tokenization and
the expressive power of attention mechanisms to achieve accurate and scalable
simulation results. Extensive experiments demonstrate that our method
significantly outperforms state-of-the-art approaches in modeling deformable
body interactions. Notably, it remains effective on large-scale simulations
with meshes exceeding 100,000 nodes, where existing methods are hindered by
computational limitations. Additionally, we contribute a novel large-scale
dataset encompassing a wide range of deformable body interactions to support
future research in this area.

</details>


### [100] [Benchmarking of EEG Analysis Techniques for Parkinson's Disease Diagnosis: A Comparison between Traditional ML Methods and Foundation DL Methods](https://arxiv.org/abs/2507.13716)
*Danilo Avola,Andrea Bernardini,Giancarlo Crocetti,Andrea Ladogana,Mario Lezoche,Maurizio Mancini,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 该研究系统评估了传统机器学习和深度学习模型在帕金森病（PD）分类中的表现，旨在为开发有效的学习系统奠定基础。


<details>
  <summary>Details</summary>
Motivation: 帕金森病（PD）是一种进行性神经退行性疾病，早期诊断对临床干预至关重要。脑电图（EEG）是一种非侵入性且经济高效的检测手段，但开发可靠的自动化诊断模型仍具挑战性。

Method: 研究使用公开的oddball任务数据集，统一了七步预处理流程，并应用一致的交叉验证和评估标准，比较了传统机器学习和深度学习模型的性能。

Result: 结果表明，CNN-LSTM等深度学习模型表现最佳，强调了捕捉长期时间依赖性的重要性；同时，XGBoost等传统分类器也表现出色。

Conclusion: 研究为未来开发更复杂或专门化的架构提供了参考框架，强调了建立可靠基线结果对科学严谨性和可重复性的重要性。

Abstract: Parkinson's Disease PD is a progressive neurodegenerative disorder that
affects motor and cognitive functions with early diagnosis being critical for
effective clinical intervention Electroencephalography EEG offers a noninvasive
and costeffective means of detecting PDrelated neural alterations yet the
development of reliable automated diagnostic models remains a challenge In this
study we conduct a systematic benchmark of traditional machine learning ML and
deep learning DL models for classifying PD using a publicly available oddball
task dataset Our aim is to lay the groundwork for developing an effective
learning system and to determine which approach produces the best results We
implement a unified sevenstep preprocessing pipeline and apply consistent
subjectwise crossvalidation and evaluation criteria to ensure comparability
across models Our results demonstrate that while baseline deep learning
architectures particularly CNNLSTM models achieve the best performance compared
to other deep learning architectures underlining the importance of capturing
longrange temporal dependencies several traditional classifiers such as XGBoost
also offer strong predictive accuracy and calibrated decision boundaries By
rigorously comparing these baselines our work provides a solid reference
framework for future studies aiming to develop and evaluate more complex or
specialized architectures Establishing a reliable set of baseline results is
essential to contextualize improvements introduced by novel methods ensuring
scientific rigor and reproducibility in the evolving field of EEGbased
neurodiagnostics

</details>


### [101] [Bi-GRU Based Deception Detection using EEG Signals](https://arxiv.org/abs/2507.13718)
*Danilo Avola,Muhammad Yasir Bilal,Emad Emam,Cristina Lakasz,Daniele Pannone,Amedeo Ranaldi*

Main category: cs.LG

TL;DR: 该研究提出了一种基于双向门控循环单元（Bi-GRU）的深度学习模型，用于通过脑电图（EEG）信号检测欺骗行为，测试准确率达到97%。


<details>
  <summary>Details</summary>
Motivation: 欺骗检测在安全、心理学和法医学等领域具有重要意义，但传统方法存在局限性。本研究旨在利用EEG信号和深度学习技术，提高欺骗检测的准确性和实用性。

Method: 研究使用Bag-of-Lies数据集中的EEG信号，训练了一个Bi-GRU神经网络，进行欺骗与真实行为的二元分类。

Result: 模型在测试中达到了97%的准确率，并且在精确率、召回率和F1分数上表现优异。

Conclusion: 结果表明，双向时序建模在基于EEG的欺骗检测中非常有效，具有实时应用的潜力，并值得进一步探索更先进的神经网络架构。

Abstract: Deception detection is a significant challenge in fields such as security,
psychology, and forensics. This study presents a deep learning approach for
classifying deceptive and truthful behavior using ElectroEncephaloGram (EEG)
signals from the Bag-of-Lies dataset, a multimodal corpus designed for
naturalistic, casual deception scenarios. A Bidirectional Gated Recurrent Unit
(Bi-GRU) neural network was trained to perform binary classification of EEG
samples. The model achieved a test accuracy of 97\%, along with high precision,
recall, and F1-scores across both classes. These results demonstrate the
effectiveness of using bidirectional temporal modeling for EEG-based deception
detection and suggest potential for real-time applications and future
exploration of advanced neural architectures.

</details>


### [102] [Graph-Structured Data Analysis of Component Failure in Autonomous Cargo Ships Based on Feature Fusion](https://arxiv.org/abs/2507.13721)
*Zizhao Zhang,Tianxiang Zhao,Yu Sun,Liping Sun,Jichuan Kang*

Main category: cs.LG

TL;DR: 本文提出了一种混合特征融合框架，用于构建自主货船故障模式的图结构数据集，通过改进的布谷鸟搜索算法（HN-CSA）显著提高了文献检索效率，并在故障分类和预测中取得了高精度。


<details>
  <summary>Details</summary>
Motivation: 解决自主货船中组件故障引发的级联反应和应急决策中的不确定性。

Method: 采用改进的布谷鸟搜索算法（HN-CSA）提高检索效率，构建分层特征融合框架，使用Word2Vec、BERT-KPCA和Sentence-BERT处理特征和语义关联。

Result: 数据集覆盖12个系统、1,262种故障模式和6,150条传播路径，GATE-GNN模型分类准确率为0.735，预测F1分数达0.93。

Conclusion: 该研究为自主货船的故障分析、风险评估和智能决策系统提供了可靠支持。

Abstract: To address the challenges posed by cascading reactions caused by component
failures in autonomous cargo ships (ACS) and the uncertainties in emergency
decision-making, this paper proposes a novel hybrid feature fusion framework
for constructing a graph-structured dataset of failure modes. By employing an
improved cuckoo search algorithm (HN-CSA), the literature retrieval efficiency
is significantly enhanced, achieving improvements of 7.1% and 3.4% compared to
the NSGA-II and CSA search algorithms, respectively. A hierarchical feature
fusion framework is constructed, using Word2Vec encoding to encode
subsystem/component features, BERT-KPCA to process failure modes/reasons, and
Sentence-BERT to quantify the semantic association between failure impact and
emergency decision-making. The dataset covers 12 systems, 1,262 failure modes,
and 6,150 propagation paths. Validation results show that the GATE-GNN model
achieves a classification accuracy of 0.735, comparable to existing benchmarks.
Additionally, a silhouette coefficient of 0.641 indicates that the features are
highly distinguishable. In the label prediction results, the Shore-based
Meteorological Service System achieved an F1 score of 0.93, demonstrating high
prediction accuracy. This paper not only provides a solid foundation for
failure analysis in autonomous cargo ships but also offers reliable support for
fault diagnosis, risk assessment, and intelligent decision-making systems. The
link to the dataset is
https://github.com/wojiufukele/Graph-Structured-about-CSA.

</details>


### [103] [Adversarial Training Improves Generalization Under Distribution Shifts in Bioacoustics](https://arxiv.org/abs/2507.13727)
*René Heinrich,Lukas Rauch,Bernhard Sick,Christoph Scholz*

Main category: cs.LG

TL;DR: 研究探讨了对抗训练在音频分类中如何提升泛化性能和对抗鲁棒性，特别是在数据分布显著变化的情况下。


<details>
  <summary>Details</summary>
Motivation: 对抗训练在增强模型对抗攻击鲁棒性方面有潜力，但其在音频分类中面对数据分布变化时的泛化能力尚未充分研究。

Method: 研究采用两种对抗训练策略（输出空间攻击和嵌入空间攻击），并在两种模型架构（ConvNeXt和AudioProtoPNet）上进行评估，使用鸟类声音分类基准测试。

Result: 对抗训练（尤其是输出空间攻击）显著提升了干净测试数据的性能（平均提升10.5%）并增强了模型的对抗鲁棒性。

Conclusion: 对抗训练在音频分类中不仅能提升对抗鲁棒性，还能应对数据分布变化，具有广泛应用潜力。

Abstract: Adversarial training is a promising strategy for enhancing model robustness
against adversarial attacks. However, its impact on generalization under
substantial data distribution shifts in audio classification remains largely
unexplored. To address this gap, this work investigates how different
adversarial training strategies improve generalization performance and
adversarial robustness in audio classification. The study focuses on two model
architectures: a conventional convolutional neural network (ConvNeXt) and an
inherently interpretable prototype-based model (AudioProtoPNet). The approach
is evaluated using a challenging bird sound classification benchmark. This
benchmark is characterized by pronounced distribution shifts between training
and test data due to varying environmental conditions and recording methods, a
common real-world challenge. The investigation explores two adversarial
training strategies: one based on output-space attacks that maximize the
classification loss function, and another based on embedding-space attacks
designed to maximize embedding dissimilarity. These attack types are also used
for robustness evaluation. Additionally, for AudioProtoPNet, the study assesses
the stability of its learned prototypes under targeted embedding-space attacks.
Results show that adversarial training, particularly using output-space
attacks, improves clean test data performance by an average of 10.5% relative
and simultaneously strengthens the adversarial robustness of the models. These
findings, although derived from the bird sound domain, suggest that adversarial
training holds potential to enhance robustness against both strong distribution
shifts and adversarial attacks in challenging audio classification settings.

</details>


### [104] [An End-to-End DNN Inference Framework for the SpiNNaker2 Neuromorphic MPSoC](https://arxiv.org/abs/2507.13736)
*Matthias Jobst,Tim Langer,Chen Liu,Mehmet Alici,Hector A. Gonzalez,Christian Mayr*

Main category: cs.LG

TL;DR: 提出了一种基于多层级DNN调度框架的扩展OctopuScheduler，支持从PyTorch模型到SpiNNaker2芯片推理的端到端流程。


<details>
  <summary>Details</summary>
Motivation: 解决在神经形态平台SpiNNaker2上执行大型复杂DNN（如Transformer）的边缘计算需求。

Method: 结合量化和降级步骤的前端，扩展OctopuScheduler为多层级DNN调度框架。

Result: 实现了在SpiNNaker2芯片上高效执行大型复杂DNN的能力。

Conclusion: 该框架为神经形态平台上的边缘计算提供了可行的解决方案。

Abstract: This work presents a multi-layer DNN scheduling framework as an extension of
OctopuScheduler, providing an end-to-end flow from PyTorch models to inference
on a single SpiNNaker2 chip. Together with a front-end comprised of
quantization and lowering steps, the proposed framework enables the edge-based
execution of large and complex DNNs up to transformer scale using the
neuromorphic platform SpiNNaker2.

</details>


### [105] [SamGoG: A Sampling-Based Graph-of-Graphs Framework for Imbalanced Graph Classification](https://arxiv.org/abs/2507.13741)
*Shangyou Wang,Zezhong Ding,Xike Xie*

Main category: cs.LG

TL;DR: SamGoG框架通过采样机制解决图神经网络中的类别和大小不平衡问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现实图中的类别和大小不平衡会偏置学习过程并降低模型性能，现有方法通常只解决一种不平衡或计算成本高。

Method: 提出SamGoG框架，通过重要性采样构建多个图之图（GoG），并依次训练，结合可学习相似性和自适应GoG节点度提升边同质性。

Result: 在基准数据集上，SamGoG实现了15.66%的准确率提升和6.7倍的训练加速。

Conclusion: SamGoG能有效解决图分类任务中的双重不平衡问题，且兼容多种下游GNN。

Abstract: Graph Neural Networks (GNNs) have shown remarkable success in graph
classification tasks by capturing both structural and feature-based
representations. However, real-world graphs often exhibit two critical forms of
imbalance: class imbalance and graph size imbalance. These imbalances can bias
the learning process and degrade model performance. Existing methods typically
address only one type of imbalance or incur high computational costs. In this
work, we propose SamGoG, a sampling-based Graph-of-Graphs (GoG) learning
framework that effectively mitigates both class and graph size imbalance.
SamGoG constructs multiple GoGs through an efficient importance-based sampling
mechanism and trains on them sequentially. This sampling mechanism incorporates
the learnable pairwise similarity and adaptive GoG node degree to enhance edge
homophily, thus improving downstream model quality. SamGoG can seamlessly
integrate with various downstream GNNs, enabling their efficient adaptation for
graph classification tasks. Extensive experiments on benchmark datasets
demonstrate that SamGoG achieves state-of-the-art performance with up to a
15.66% accuracy improvement with 6.7$\times$ training acceleration.

</details>


### [106] [Search-Optimized Quantization in Biomedical Ontology Alignment](https://arxiv.org/abs/2507.13742)
*Oussama Bouaggad,Natalia Grabar*

Main category: cs.LG

TL;DR: 本文提出了一种基于监督学习的变压器模型方法，用于生物医学词汇与UMLS的语义对齐，并通过优化技术显著提升了推理速度和内存效率。


<details>
  <summary>Details</summary>
Motivation: 解决AI模型在边缘设备或资源受限环境中的部署挑战，如能耗、内存和延迟问题。

Method: 采用基于余弦语义相似度的变压器模型，结合Microsoft Olive、ONNX Runtime、Intel Neural Compressor和IPEX进行动态量化和优化。

Result: 在DEFT 2020任务中取得新SOTA，推理速度提升20倍，内存使用减少约70%。

Conclusion: 提出的优化方法在保持性能的同时显著提升了效率，适用于资源受限环境。

Abstract: In the fast-moving world of AI, as organizations and researchers develop more
advanced models, they face challenges due to their sheer size and computational
demands. Deploying such models on edge devices or in resource-constrained
environments adds further challenges related to energy consumption, memory
usage and latency. To address these challenges, emerging trends are shaping the
future of efficient model optimization techniques. From this premise, by
employing supervised state-of-the-art transformer-based models, this research
introduces a systematic method for ontology alignment, grounded in cosine-based
semantic similarity between a biomedical layman vocabulary and the Unified
Medical Language System (UMLS) Metathesaurus. It leverages Microsoft Olive to
search for target optimizations among different Execution Providers (EPs) using
the ONNX Runtime backend, followed by an assembled process of dynamic
quantization employing Intel Neural Compressor and IPEX (Intel Extension for
PyTorch). Through our optimization process, we conduct extensive assessments on
the two tasks from the DEFT 2020 Evaluation Campaign, achieving a new
state-of-the-art in both. We retain performance metrics intact, while attaining
an average inference speed-up of 20x and reducing memory usage by approximately
70%.

</details>


### [107] [MolPIF: A Parameter Interpolation Flow Model for Molecule Generation](https://arxiv.org/abs/2507.13762)
*Yaowei Jin,Junjie Wang,Wenkai Xiang,Duanhua Cao,Dan Teng,Zhehuan Fan,Jiacheng Xiong,Xia Sheng,Chuanlong Zeng,Mingyue Zheng,Qian Shi*

Main category: cs.LG

TL;DR: 本文提出了一种新的参数插值流模型（PIF），用于分子生成，解决了贝叶斯流网络（BFNs）在灵活性和适应性上的不足，并在药物设计中展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯流网络（BFNs）在分子生成中表现优异，但其基于贝叶斯推断的策略限制了分布变换路径的灵活性，难以适应多样化的数据分布和任务需求。

Method: 提出参数插值流模型（PIF），并详细阐述了其理论基础、训练和推理过程。进一步开发了MolPIF用于基于结构的药物设计。

Result: MolPIF在多种指标上优于基线模型，验证了参数空间生成建模范式的有效性。

Conclusion: PIF为分子生成提供了新的设计视角，展示了参数空间模型的潜力。

Abstract: Advances in deep learning for molecular generation show promise in
accelerating drug discovery. Bayesian Flow Networks (BFNs) have recently shown
impressive performance across diverse chemical tasks, with their success often
ascribed to the paradigm of modeling in a low-variance parameter space.
However, the Bayesian inference-based strategy imposes limitations on designing
more flexible distribution transformation pathways, making it challenging to
adapt to diverse data distributions and varied task requirements. Furthermore,
the potential for simpler, more efficient parameter-space-based models is
unexplored. To address this, we propose a novel Parameter Interpolation Flow
model (named PIF) with detailed theoretical foundation, training, and inference
procedures. We then develop MolPIF for structure-based drug design,
demonstrating its superior performance across diverse metrics compared to
baselines. This work validates the effectiveness of parameter-space-based
generative modeling paradigm for molecules and offers new perspectives for
model design.

</details>


### [108] [Dual-Center Graph Clustering with Neighbor Distribution](https://arxiv.org/abs/2507.13765)
*Enhao Cheng,Shoujia Zhang,Jianhua Yin,Li Jin,Liqiang Nie*

Main category: cs.LG

TL;DR: 提出了一种基于邻居分布属性的双中心图聚类方法（DCGC），通过邻居分布作为监督信号提升对比学习效果，并引入双中心优化。


<details>
  <summary>Details</summary>
Motivation: 现有目标导向聚类方法仅利用特征构建单目标分布，导致指导不完整且不可靠。

Method: 利用邻居分布作为监督信号挖掘对比学习中的难负样本，并引入邻居分布中心与特征中心共同构建双目标分布进行优化。

Result: 实验证明该方法性能优越且有效。

Conclusion: DCGC通过邻居分布和双中心优化，显著提升了图聚类的效果和可靠性。

Abstract: Graph clustering is crucial for unraveling intricate data structures, yet it
presents significant challenges due to its unsupervised nature. Recently,
goal-directed clustering techniques have yielded impressive results, with
contrastive learning methods leveraging pseudo-label garnering considerable
attention. Nonetheless, pseudo-label as a supervision signal is unreliable and
existing goal-directed approaches utilize only features to construct a
single-target distribution for single-center optimization, which lead to
incomplete and less dependable guidance. In our work, we propose a novel
Dual-Center Graph Clustering (DCGC) approach based on neighbor distribution
properties, which includes representation learning with neighbor distribution
and dual-center optimization. Specifically, we utilize neighbor distribution as
a supervision signal to mine hard negative samples in contrastive learning,
which is reliable and enhances the effectiveness of representation learning.
Furthermore, neighbor distribution center is introduced alongside feature
center to jointly construct a dual-target distribution for dual-center
optimization. Extensive experiments and analysis demonstrate superior
performance and effectiveness of our proposed method.

</details>


### [109] [On-the-Fly Fine-Tuning of Foundational Neural Network Potentials: A Bayesian Neural Network Approach](https://arxiv.org/abs/2507.13805)
*Tim Rensmeyer,Denis Kramer,Oliver Niggemann*

Main category: cs.LG

TL;DR: 论文提出了一种基于贝叶斯神经网络的方法，用于微调预训练的基础模型，并通过实时学习自动优化模型，同时检测和采样稀有事件。


<details>
  <summary>Details</summary>
Motivation: 由于从头计算原子间力的计算复杂性，机器学习力场的研究变得活跃，但生成足够多样化的训练数据集仍具有挑战性，尤其是在稀有事件或大配置空间系统中。

Method: 采用贝叶斯神经网络方法微调预训练的基础模型，结合实时学习工作流，自动优化模型并量化不确定性。

Result: 该方法能够在保持预设精度的同时自动微调模型，并有效检测和采样稀有事件（如过渡态）。

Conclusion: 提出的方法解决了基础模型微调中的不确定性量化问题，为稀有事件建模提供了高效解决方案。

Abstract: Due to the computational complexity of evaluating interatomic forces from
first principles, the creation of interatomic machine learning force fields has
become a highly active field of research. However, the generation of training
datasets of sufficient size and sample diversity itself comes with a
computational burden that can make this approach impractical for modeling rare
events or systems with a large configuration space. Fine-tuning foundation
models that have been pre-trained on large-scale material or molecular
databases offers a promising opportunity to reduce the amount of training data
necessary to reach a desired level of accuracy. However, even if this approach
requires less training data overall, creating a suitable training dataset can
still be a very challenging problem, especially for systems with rare events
and for end-users who don't have an extensive background in machine learning.
In on-the-fly learning, the creation of a training dataset can be largely
automated by using model uncertainty during the simulation to decide if the
model is accurate enough or if a structure should be recalculated with
classical methods and used to update the model. A key challenge for applying
this form of active learning to the fine-tuning of foundation models is how to
assess the uncertainty of those models during the fine-tuning process, even
though most foundation models lack any form of uncertainty quantification. In
this paper, we overcome this challenge by introducing a fine-tuning approach
based on Bayesian neural network methods and a subsequent on-the-fly workflow
that automatically fine-tunes the model while maintaining a pre-specified
accuracy and can detect rare events such as transition states and sample them
at an increased rate relative to their occurrence.

</details>


### [110] [Scalable Submodular Policy Optimization via Pruned Submodularity Graph](https://arxiv.org/abs/2507.13834)
*Aditi Anand,Suman Banerjee,Dildar Ali*

Main category: cs.LG

TL;DR: 本文研究了一种强化学习问题变体，其中奖励函数是次模的，提出了一种基于修剪次模性图的方法，以在可行计算时间内提供近似解。


<details>
  <summary>Details</summary>
Motivation: 现实中的许多问题（如路径规划、覆盖控制等）的奖励函数遵循递减回报，可以建模为次模函数，而传统强化学习假设奖励函数为加性。

Method: 提出了一种基于修剪次模性图的方法，分析了其时间和空间需求，并提供了性能保证。

Result: 在基准环境中实验，结果表明所提方法获得的策略比基线方法产生更多奖励。

Conclusion: 该方法在次模奖励函数的强化学习问题中表现优于基线方法，具有实际应用潜力。

Abstract: In Reinforcement Learning (abbreviated as RL), an agent interacts with the
environment via a set of possible actions, and a reward is generated from some
unknown distribution. The task here is to find an optimal set of actions such
that the reward after a certain time step gets maximized. In a traditional
setup, the reward function in an RL Problem is considered additive. However, in
reality, there exist many problems, including path planning, coverage control,
etc., the reward function follows the diminishing return, which can be modeled
as a submodular function. In this paper, we study a variant of the RL Problem
where the reward function is submodular, and our objective is to find an
optimal policy such that this reward function gets maximized. We have proposed
a pruned submodularity graph-based approach that provides a provably
approximate solution in a feasible computation time. The proposed approach has
been analyzed to understand its time and space requirements as well as a
performance guarantee. We have experimented with a benchmark agent-environment
setup, which has been used for similar previous studies, and the results are
reported. From the results, we observe that the policy obtained by our proposed
approach leads to more reward than the baseline methods.

</details>


### [111] [Self-supervised learning on gene expression data](https://arxiv.org/abs/2507.13912)
*Kevin Dradjat,Massinissa Hamidi,Pierre Bartet,Blaise Hanczar*

Main category: cs.LG

TL;DR: 研究探讨了自监督学习在基因表达数据表型预测中的应用，证明其优于传统监督学习，并减少对标注数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 基因表达数据标注成本高，传统监督学习依赖大量标注数据，自监督学习能直接从无标签数据中提取信息。

Method: 选择了三种自监督学习方法，评估其在基因表达数据中的表现，并用于下游预测任务。

Result: 自监督学习方法能有效捕捉复杂信息，提高表型预测准确性，优于传统监督模型。

Conclusion: 自监督学习在基因表达数据分析中具有潜力，未来研究可进一步优化其应用。

Abstract: Predicting phenotypes from gene expression data is a crucial task in
biomedical research, enabling insights into disease mechanisms, drug responses,
and personalized medicine. Traditional machine learning and deep learning rely
on supervised learning, which requires large quantities of labeled data that
are costly and time-consuming to obtain in the case of gene expression data.
Self-supervised learning has recently emerged as a promising approach to
overcome these limitations by extracting information directly from the
structure of unlabeled data. In this study, we investigate the application of
state-of-the-art self-supervised learning methods to bulk gene expression data
for phenotype prediction. We selected three self-supervised methods, based on
different approaches, to assess their ability to exploit the inherent structure
of the data and to generate qualitative representations which can be used for
downstream predictive tasks. By using several publicly available gene
expression datasets, we demonstrate how the selected methods can effectively
capture complex information and improve phenotype prediction accuracy. The
results obtained show that self-supervised learning methods can outperform
traditional supervised models besides offering significant advantage by
reducing the dependency on annotated data. We provide a comprehensive analysis
of the performance of each method by highlighting their strengths and
limitations. We also provide recommendations for using these methods depending
on the case under study. Finally, we outline future research directions to
enhance the application of self-supervised learning in the field of gene
expression data analysis. This study is the first work that deals with bulk
RNA-Seq data and self-supervised learning.

</details>


### [112] [Reframing attention as a reinforcement learning problem for causal discovery](https://arxiv.org/abs/2507.13920)
*Turan Orujlu,Christian Gumbsch,Martin V. Butz,Charley M Wu*

Main category: cs.LG

TL;DR: 论文提出了一种新的因果过程框架（Causal Process framework）及其实现（Causal Process Model），用于表示动态因果结构假设，并在强化学习（RL）环境中重新定义了Transformer的注意力机制，以从视觉观察中推断可解释的因果过程。


<details>
  <summary>Details</summary>
Motivation: 现有神经因果模型多假设静态因果图，忽略了因果交互的动态性，因此需要一种新理论来动态表示因果结构。

Method: 通过Causal Process Model实现动态因果假设，将Transformer的注意力机制重新应用于RL环境，利用RL代理构建因果图假设。

Result: 在RL环境中，该方法在因果表示学习和代理性能上优于现有方法，并能唯一恢复动态因果过程的图结构。

Conclusion: Causal Process框架为动态因果表示提供了新理论，其实现模型在RL中表现出色，推动了因果推理与深度学习的结合。

Abstract: Formal frameworks of causality have operated largely parallel to modern
trends in deep reinforcement learning (RL). However, there has been a revival
of interest in formally grounding the representations learned by neural
networks in causal concepts. Yet, most attempts at neural models of causality
assume static causal graphs and ignore the dynamic nature of causal
interactions. In this work, we introduce Causal Process framework as a novel
theory for representing dynamic hypotheses about causal structure. Furthermore,
we present Causal Process Model as an implementation of this framework. This
allows us to reformulate the attention mechanism popularized by Transformer
networks within an RL setting with the goal to infer interpretable causal
processes from visual observations. Here, causal inference corresponds to
constructing a causal graph hypothesis which itself becomes an RL task nested
within the original RL problem. To create an instance of such hypothesis, we
employ RL agents. These agents establish links between units similar to the
original Transformer attention mechanism. We demonstrate the effectiveness of
our approach in an RL environment where we outperform current alternatives in
causal representation learning and agent performance, and uniquely recover
graphs of dynamic causal processes.

</details>


### [113] [MoDyGAN: Combining Molecular Dynamics With GANs to Investigate Protein Conformational Space](https://arxiv.org/abs/2507.13950)
*Jingbo Liang,Bruna Jacobson*

Main category: cs.LG

TL;DR: MoDyGAN结合分子动力学模拟和生成对抗网络，提出了一种高效探索蛋白质构象空间的新方法。


<details>
  <summary>Details</summary>
Motivation: 传统分子动力学模拟计算成本高，难以全面探索蛋白质构象空间。

Method: MoDyGAN通过生成器和双判别器模块，将3D蛋白质结构转化为2D矩阵，利用图像生成技术生成新构象。

Result: 实验证明MoDyGAN能生成合理的新构象，且潜在空间插值与分子动力学模拟轨迹一致。

Conclusion: 该方法为生物分子模拟提供了新思路，并可扩展至其他复杂3D结构。

Abstract: Extensively exploring protein conformational landscapes remains a major
challenge in computational biology due to the high computational cost involved
in dynamic physics-based simulations. In this work, we propose a novel
pipeline, MoDyGAN, that leverages molecular dynamics (MD) simulations and
generative adversarial networks (GANs) to explore protein conformational
spaces. MoDyGAN contains a generator that maps Gaussian distributions into
MD-derived protein trajectories, and a refinement module that combines ensemble
learning with a dual-discriminator to further improve the plausibility of
generated conformations. Central to our approach is an innovative
representation technique that reversibly transforms 3D protein structures into
2D matrices, enabling the use of advanced image-based GAN architectures. We use
three rigid proteins to demonstrate that MoDyGAN can generate plausible new
conformations. We also use deca-alanine as a case study to show that
interpolations within the latent space closely align with trajectories obtained
from steered molecular dynamics (SMD) simulations. Our results suggest that
representing proteins as image-like data unlocks new possibilities for applying
advanced deep learning techniques to biomolecular simulation, leading to an
efficient sampling of conformational states. Additionally, the proposed
framework holds strong potential for extension to other complex 3D structures.

</details>


### [114] [Robust Anomaly Detection with Graph Neural Networks using Controllability](https://arxiv.org/abs/2507.13954)
*Yifan Wei,Anwar Said,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: 论文提出两种将平均可控性融入图机器学习模型的新方法，显著提升了稀疏和不平衡数据中的异常检测性能。


<details>
  <summary>Details</summary>
Motivation: 复杂领域中的异常检测面临标注数据不足和样本不平衡的挑战，图机器学习模型虽有效但需创新策略以提升性能。

Method: 提出两种方法：1) 将平均可控性作为边权重；2) 将其编码为独热边属性向量。

Result: 在真实和合成网络上的实验表明，新方法优于六种基线模型，验证了可控性指标的重要性。

Conclusion: 平均可控性可作为额外指标，有效提升图机器学习模型在稀疏和不平衡数据中的异常检测能力。

Abstract: Anomaly detection in complex domains poses significant challenges due to the
need for extensive labeled data and the inherently imbalanced nature of
anomalous versus benign samples. Graph-based machine learning models have
emerged as a promising solution that combines attribute and relational data to
uncover intricate patterns. However, the scarcity of anomalous data exacerbates
the challenge, which requires innovative strategies to enhance model learning
with limited information. In this paper, we hypothesize that the incorporation
of the influence of the nodes, quantified through average controllability, can
significantly improve the performance of anomaly detection. We propose two
novel approaches to integrate average controllability into graph-based
frameworks: (1) using average controllability as an edge weight and (2)
encoding it as a one-hot edge attribute vector. Through rigorous evaluation on
real-world and synthetic networks with six state-of-the-art baselines, our
proposed methods demonstrate improved performance in identifying anomalies,
highlighting the critical role of controllability measures in enhancing the
performance of graph machine learning models. This work underscores the
potential of integrating average controllability as additional metrics to
address the challenges of anomaly detection in sparse and imbalanced datasets.

</details>


### [115] [Signs of the Past, Patterns of the Present: On the Automatic Classification of Old Babylonian Cuneiform Signs](https://arxiv.org/abs/2507.13959)
*Eli Verwimp,Gustav Ryberg Smidt,Hendrik Hameeuw,Katrien De Graef*

Main category: cs.LG

TL;DR: 论文研究了机器学习在楔形文字分类中的应用，分析了数据差异对模型性能的影响，并提出了未来数据采集标准的建议。


<details>
  <summary>Details</summary>
Motivation: 楔形文字因来源、用途、书写者和数字化方式的不同而存在很大变异性，导致模型在不同数据集上表现不佳。研究旨在探讨这种差异对性能的影响。

Method: 使用ResNet50模型，训练和测试了来自三个美索不达米亚城市的手写古巴比伦文本数据。

Result: 模型在至少20个实例的楔形文字上取得了87.1%的top-1准确率和96.5%的top-5准确率。

Conclusion: 研究为未来楔形文字分类任务提供了基础，并建议改进数据采集标准。

Abstract: The work in this paper describes the training and evaluation of machine
learning (ML) techniques for the classification of cuneiform signs. There is a
lot of variability in cuneiform signs, depending on where they come from, for
what and by whom they were written, but also how they were digitized. This
variability makes it unlikely that an ML model trained on one dataset will
perform successfully on another dataset. This contribution studies how such
differences impact that performance. Based on our results and insights, we aim
to influence future data acquisition standards and provide a solid foundation
for future cuneiform sign classification tasks. The ML model has been trained
and tested on handwritten Old Babylonian (c. 2000-1600 B.C.E.) documentary
texts inscribed on clay tablets originating from three Mesopotamian cities
(Nippur, D\=ur-Abie\v{s}uh and Sippar). The presented and analysed model is
ResNet50, which achieves a top-1 score of 87.1% and a top-5 score of 96.5% for
signs with at least 20 instances. As these automatic classification results are
the first on Old Babylonian texts, there are currently no comparable results.

</details>


### [116] [Structural Connectome Harmonization Using Deep Learning: The Strength of Graph Neural Networks](https://arxiv.org/abs/2507.13992)
*Jagruti Patel,Thomas A. W. Bolton,Mikkel Schöttner,Anjali Tarun,Sebastien Tourbier,Yasser Alemàn-Gòmez,Jonas Richiardi,Patric Hagmann*

Main category: cs.LG

TL;DR: 论文提出了一种基于图结构的深度协调框架，用于解决多站点神经影像数据中的偏差问题，无需依赖元数据或旅行受试者。


<details>
  <summary>Details</summary>
Motivation: 小样本量和多站点采集偏差限制了神经影像生物标志物的可靠性，现有方法依赖元数据或忽略图拓扑结构。

Method: 提出站点条件深度协调框架，测试了三种深度架构（全连接自编码器、卷积自编码器、图卷积自编码器）与线性回归基线的对比。

Result: 图卷积自编码器在保留拓扑结构和个体特征方面表现最佳，而线性回归基线虽数值性能高但实用性受限。

Conclusion: 图结构方法在多站点研究中具有优势，适合结构感知和领域泛化的协调任务。

Abstract: Small sample sizes in neuroimaging in general, and in structural connectome
(SC) studies in particular limit the development of reliable biomarkers for
neurological and psychiatric disorders - such as Alzheimer's disease and
schizophrenia - by reducing statistical power, reliability, and
generalizability. Large-scale multi-site studies have exist, but they have
acquisition-related biases due to scanner heterogeneity, compromising imaging
consistency and downstream analyses. While existing SC harmonization methods -
such as linear regression (LR), ComBat, and deep learning techniques - mitigate
these biases, they often rely on detailed metadata, traveling subjects (TS), or
overlook the graph-topology of SCs. To address these limitations, we propose a
site-conditioned deep harmonization framework that harmonizes SCs across
diverse acquisition sites without requiring metadata or TS that we test in a
simulated scenario based on the Human Connectome Dataset. Within this
framework, we benchmark three deep architectures - a fully connected
autoencoder (AE), a convolutional AE, and a graph convolutional AE - against a
top-performing LR baseline. While non-graph models excel in edge-weight
prediction and edge existence detection, the graph AE demonstrates superior
preservation of topological structure and subject-level individuality, as
reflected by graph metrics and fingerprinting accuracy, respectively. Although
the LR baseline achieves the highest numerical performance by explicitly
modeling acquisition parameters, it lacks applicability to real-world
multi-site use cases as detailed acquisition metadata is often unavailable. Our
results highlight the critical role of model architecture in SC harmonization
performance and demonstrate that graph-based approaches are particularly
well-suited for structure-aware, domain-generalizable SC harmonization in
large-scale multi-site SC studies.

</details>


### [117] [ParallelTime: Dynamically Weighting the Balance of Short- and Long-Term Temporal Dependencies](https://arxiv.org/abs/2507.13998)
*Itay Katav,Aryeh Kontorovich*

Main category: cs.LG

TL;DR: 论文提出了一种动态加权机制ParallelTime Weighter，结合了Transformer和Mamba架构，用于优化时间序列预测中长短期依赖的权重分配。


<details>
  <summary>Details</summary>
Motivation: 现有方法在时间序列预测中对长短期依赖赋予相同权重，效果不佳，需要更灵活的权重分配机制。

Method: 提出ParallelTime Weighter动态计算长短期依赖的权重，并设计ParallelTime架构实现这一机制。

Result: ParallelTime在多个基准测试中表现优异，具有鲁棒性、低计算量和参数需求，且能适应更长的预测范围。

Conclusion: ParallelTime为时间序列预测中的并行Attention-Mamba架构提供了有前景的发展方向。

Abstract: Modern multivariate time series forecasting primarily relies on two
architectures: the Transformer with attention mechanism and Mamba. In natural
language processing, an approach has been used that combines local window
attention for capturing short-term dependencies and Mamba for capturing
long-term dependencies, with their outputs averaged to assign equal weight to
both. We find that for time-series forecasting tasks, assigning equal weight to
long-term and short-term dependencies is not optimal. To mitigate this, we
propose a dynamic weighting mechanism, ParallelTime Weighter, which calculates
interdependent weights for long-term and short-term dependencies for each token
based on the input and the model's knowledge. Furthermore, we introduce the
ParallelTime architecture, which incorporates the ParallelTime Weighter
mechanism to deliver state-of-the-art performance across diverse benchmarks.
Our architecture demonstrates robustness, achieves lower FLOPs, requires fewer
parameters, scales effectively to longer prediction horizons, and significantly
outperforms existing methods. These advances highlight a promising path for
future developments of parallel Attention-Mamba in time series forecasting. The
implementation is readily available at:
\href{https://github.com/itay1551/ParallelTime}{ParallelTime GitHub

</details>


### [118] [On the Fundamental Limitations of Dual Static CVaR Decompositions in Markov Decision Processes](https://arxiv.org/abs/2507.14005)
*Mathieu Godbout,Audrey Durand*

Main category: cs.LG

TL;DR: 论文探讨了动态规划方法在静态CVaR优化策略中的失败原因，提出风险分配一致性约束是关键，并证明了双CVaR分解的局限性。


<details>
  <summary>Details</summary>
Motivation: 研究动态规划方法在静态CVaR优化策略中的失败原因，并探索其根本问题。

Method: 通过将问题从策略优化转向策略评估，提出风险分配一致性约束，并量化评估误差。

Result: 发现风险分配一致性约束的空集是评估错误的根源，并证明双CVaR分解存在局限性。

Conclusion: 双CVaR分解方法在寻找全局最优策略时存在根本限制，无法适用于所有初始风险水平。

Abstract: Recent work has shown that dynamic programming (DP) methods for finding
static CVaR-optimal policies in Markov Decision Processes (MDPs) can fail when
based on the dual formulation, yet the root cause for the failure has remained
unclear. We expand on these findings by shifting focus from policy optimization
to the seemingly simpler task of policy evaluation. We show that evaluating the
static CVaR of a given policy can be framed as two distinct minimization
problems. For their solutions to match, a set of ``risk-assignment consistency
constraints'' must be satisfied, and we demonstrate that the intersection of
the constraints being empty is the source of previously observed evaluation
errors. Quantifying the evaluation error as the CVaR evaluation gap, we then
demonstrate that the issues observed when optimizing over the dual-based CVaR
DP are explained by the returned policy having a non-zero CVaR evaluation gap.
We then leverage our proposed risk-assignment perspective to prove that the
search for a single, uniformly optimal policy via on the dual CVaR
decomposition is fundamentally limited, identifying an MDP where no single
policy can be optimal across all initial risk levels.

</details>


### [119] [Byzantine-resilient federated online learning for Gaussian process regression](https://arxiv.org/abs/2507.14021)
*Xu Zhang,Zhenyuan Yuan,Minghui Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种拜占庭容错的联邦高斯过程回归算法，用于在部分代理出现拜占庭故障时提升学习性能。


<details>
  <summary>Details</summary>
Motivation: 研究拜占庭容错的联邦在线学习，以应对代理的任意或敌对行为对高斯过程回归的影响。

Method: 开发了一种拜占庭容错的联邦GPR算法，通过专家乘积聚合规则计算全局模型，并广播给代理以优化本地预测。

Result: 实验表明，代理融合GPR的学习精度优于本地GPR，验证了算法的有效性。

Conclusion: 所提算法能有效提升拜占庭环境下的联邦学习性能。

Abstract: In this paper, we study Byzantine-resilient federated online learning for
Gaussian process regression (GPR). We develop a Byzantine-resilient federated
GPR algorithm that allows a cloud and a group of agents to collaboratively
learn a latent function and improve the learning performances where some agents
exhibit Byzantine failures, i.e., arbitrary and potentially adversarial
behavior. Each agent-based local GPR sends potentially compromised local
predictions to the cloud, and the cloud-based aggregated GPR computes a global
model by a Byzantine-resilient product of experts aggregation rule. Then the
cloud broadcasts the current global model to all the agents. Agent-based fused
GPR refines local predictions by fusing the received global model with that of
the agent-based local GPR. Moreover, we quantify the learning accuracy
improvements of the agent-based fused GPR over the agent-based local GPR.
Experiments on a toy example and two medium-scale real-world datasets are
conducted to demonstrate the performances of the proposed algorithm.

</details>


### [120] [DONUT: Physics-aware Machine Learning for Real-time X-ray Nanodiffraction Analysis](https://arxiv.org/abs/2507.14038)
*Aileen Luo,Tao Zhou,Ming Du,Martin V. Holt,Andrej Singer,Mathew J. Cherukara*

Main category: cs.LG

TL;DR: DONUT是一种基于物理感知的神经网络，用于实时分析纳米束衍射数据，无需标记数据集或预训练，效率比传统方法高200倍。


<details>
  <summary>Details</summary>
Motivation: 实时分析纳米束衍射数据存在瓶颈，传统方法受限于计算需求和伪影。

Method: DONUT结合可微几何衍射模型，通过无监督训练预测晶格应变和取向。

Result: 实验证明DONUT能高效提取数据特征，速度是传统方法的200倍。

Conclusion: DONUT为X射线科学中的实时分析提供了高效解决方案，克服了监督学习的局限性。

Abstract: Coherent X-ray scattering techniques are critical for investigating the
fundamental structural properties of materials at the nanoscale. While
advancements have made these experiments more accessible, real-time analysis
remains a significant bottleneck, often hindered by artifacts and computational
demands. In scanning X-ray nanodiffraction microscopy, which is widely used to
spatially resolve structural heterogeneities, this challenge is compounded by
the convolution of the divergent beam with the sample's local structure. To
address this, we introduce DONUT (Diffraction with Optics for Nanobeam by
Unsupervised Training), a physics-aware neural network designed for the rapid
and automated analysis of nanobeam diffraction data. By incorporating a
differentiable geometric diffraction model directly into its architecture,
DONUT learns to predict crystal lattice strain and orientation in real-time.
Crucially, this is achieved without reliance on labeled datasets or
pre-training, overcoming a fundamental limitation for supervised machine
learning in X-ray science. We demonstrate experimentally that DONUT accurately
extracts all features within the data over 200 times more efficiently than
conventional fitting methods.

</details>


### [121] [Noradrenergic-inspired gain modulation attenuates the stability gap in joint training](https://arxiv.org/abs/2507.14056)
*Alejandro Rodriguez-Garcia,Anindya Ghosh,Srikanth Ramaswamy*

Main category: cs.LG

TL;DR: 论文研究了持续学习中的稳定性差距问题，提出了一种基于不确定性调制的增益动态机制，有效减少了任务边界处的性能下降。


<details>
  <summary>Details</summary>
Motivation: 持续学习中存在稳定性差距，即在掌握新任务时对已掌握任务的性能出现短暂下降，这与持续学习的目标相矛盾，表明现有方法在缓解遗忘方面缺乏鲁棒性。

Method: 受生物大脑多时间尺度动态的启发，提出了一种不确定性调制的增益动态机制，模拟双时间尺度优化器，动态平衡新知识的整合与旧知识的保留。

Result: 在MNIST和CIFAR基准测试中，该机制有效减少了稳定性差距。

Conclusion: 不确定性调制的增益动态机制不仅减少了稳定性差距，还模拟了神经调节功能，为持续学习提供了新的机制见解。

Abstract: Recent studies in continual learning have identified a transient drop in
performance on mastered tasks when assimilating new ones, known as the
stability gap. Such dynamics contradict the objectives of continual learning,
revealing a lack of robustness in mitigating forgetting, and notably,
persisting even under an ideal joint-loss regime. Examining this gap within
this idealized joint training context is critical to isolate it from other
sources of forgetting. We argue that it reflects an imbalance between rapid
adaptation and robust retention at task boundaries, underscoring the need to
investigate mechanisms that reconcile plasticity and stability within continual
learning frameworks. Biological brains navigate a similar dilemma by operating
concurrently on multiple timescales, leveraging neuromodulatory signals to
modulate synaptic plasticity. However, artificial networks lack native
multitimescale dynamics, and although optimizers like momentum-SGD and Adam
introduce implicit timescale regularization, they still exhibit stability gaps.
Inspired by locus coeruleus mediated noradrenergic bursts, which transiently
enhance neuronal gain under uncertainty to facilitate sensory assimilation, we
propose uncertainty-modulated gain dynamics - an adaptive mechanism that
approximates a two-timescale optimizer and dynamically balances integration of
knowledge with minimal interference on previously consolidated information. We
evaluate our mechanism on domain-incremental and class-incremental variants of
the MNIST and CIFAR benchmarks under joint training, demonstrating that
uncertainty-modulated gain dynamics effectively attenuate the stability gap.
Finally, our analysis elucidates how gain modulation replicates noradrenergic
functions in cortical circuits, offering mechanistic insights into reducing
stability gaps and enhance performance in continual learning tasks.

</details>


### [122] [Preference-based Multi-Objective Reinforcement Learning](https://arxiv.org/abs/2507.14066)
*Ni Mu,Yao Luan,Qing-Shan Jia*

Main category: cs.LG

TL;DR: 论文提出了一种基于偏好的多目标强化学习方法（Pb-MORL），通过整合偏好来优化多目标任务，避免了复杂的奖励函数设计，并在理论和实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 多目标强化学习（MORL）通常依赖于预定义的奖励函数，但这些函数难以平衡冲突目标且容易简化问题。偏好作为一种更灵活和直观的决策指导，可以解决这一问题。

Method: 论文提出了Pb-MORL方法，将偏好整合到MORL框架中，构建与偏好对齐的多目标奖励模型，并证明优化该模型等价于训练帕累托最优策略。

Result: 在多个基准任务、多能源管理任务和自动驾驶任务中，Pb-MORL表现优异，甚至超过了使用真实奖励函数的基准方法。

Conclusion: Pb-MORL展示了在复杂现实系统中应用的潜力，提供了一种更灵活的多目标优化方法。

Abstract: Multi-objective reinforcement learning (MORL) is a structured approach for
optimizing tasks with multiple objectives. However, it often relies on
pre-defined reward functions, which can be hard to design for balancing
conflicting goals and may lead to oversimplification. Preferences can serve as
more flexible and intuitive decision-making guidance, eliminating the need for
complicated reward design. This paper introduces preference-based MORL
(Pb-MORL), which formalizes the integration of preferences into the MORL
framework. We theoretically prove that preferences can derive policies across
the entire Pareto frontier. To guide policy optimization using preferences, our
method constructs a multi-objective reward model that aligns with the given
preferences. We further provide theoretical proof to show that optimizing this
reward model is equivalent to training the Pareto optimal policy. Extensive
experiments in benchmark multi-objective tasks, a multi-energy management task,
and an autonomous driving task on a multi-line highway show that our method
performs competitively, surpassing the oracle method, which uses the ground
truth reward function. This highlights its potential for practical applications
in complex real-world systems.

</details>


### [123] [DPMT: Dual Process Multi-scale Theory of Mind Framework for Real-time Human-AI Collaboration](https://arxiv.org/abs/2507.14088)
*Xiyun Li,Yining Ding,Yuhua Jiang,Yunlong Zhao,Runpeng Xie,Shuang Xu,Yuanhua Ni,Yiqin Yang,Bo Xu*

Main category: cs.LG

TL;DR: 提出了一种基于双过程多尺度心智理论（DPMT）的框架，用于提升人机协作中AI对人类复杂心理特征的建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型（LLM）在动态场景中难以准确建模人类的领域意图等复杂心理特征，尤其是在缺乏直接沟通的情况下。

Method: 结合认知科学的双过程理论，提出DPMT框架，引入多尺度心智理论（ToM）模块进行心理特征推理。

Result: 实验表明DPMT显著提升了人机协作效果，消融研究验证了多尺度ToM在慢系统中的贡献。

Conclusion: DPMT框架为动态场景中的人机协作提供了一种有效的解决方案。

Abstract: Real-time human-artificial intelligence (AI) collaboration is crucial yet
challenging, especially when AI agents must adapt to diverse and unseen human
behaviors in dynamic scenarios. Existing large language model (LLM) agents
often fail to accurately model the complex human mental characteristics such as
domain intentions, especially in the absence of direct communication. To
address this limitation, we propose a novel dual process multi-scale theory of
mind (DPMT) framework, drawing inspiration from cognitive science dual process
theory. Our DPMT framework incorporates a multi-scale theory of mind (ToM)
module to facilitate robust human partner modeling through mental
characteristic reasoning. Experimental results demonstrate that DPMT
significantly enhances human-AI collaboration, and ablation studies further
validate the contributions of our multi-scale ToM in the slow system.

</details>


### [124] [Kolmogorov Arnold Networks (KANs) for Imbalanced Data -- An Empirical Perspective](https://arxiv.org/abs/2507.14121)
*Pankaj Yadav,Vivek Vijay*

Main category: cs.LG

TL;DR: Kolmogorov Arnold Networks (KANs) 在类别不平衡分类中表现优于传统多层感知机 (MLPs)，但传统不平衡策略与 KANs 的数学结构冲突，且计算成本高。MLPs 结合不平衡技术可达到与 KANs 相当的效果，但资源消耗更低。


<details>
  <summary>Details</summary>
Motivation: 研究 KANs 在类别不平衡分类中的表现，探索其与传统不平衡策略的兼容性及计算效率。

Method: 在十个基准数据集上对 KANs 和 MLPs 进行实证评估，比较其在原始不平衡数据和使用不平衡策略时的性能。

Result: KANs 在原始不平衡数据上表现优于 MLPs，但传统不平衡策略显著降低其性能。MLPs 结合不平衡技术可达到与 KANs 相当的效果，且资源消耗更低。

Conclusion: KANs 适用于资源充足的原始不平衡数据场景，但其性能与资源消耗的权衡及与传统策略的不兼容性限制了实际应用。未来需优化 KANs 的架构和计算效率。

Abstract: Kolmogorov Arnold Networks (KANs) are recent architectural advancement in
neural computation that offer a mathematically grounded alternative to standard
neural networks. This study presents an empirical evaluation of KANs in context
of class imbalanced classification, using ten benchmark datasets. We observe
that KANs can inherently perform well on raw imbalanced data more effectively
than Multi-Layer Perceptrons (MLPs) without any resampling strategy. However,
conventional imbalance strategies fundamentally conflict with KANs mathematical
structure as resampling and focal loss implementations significantly degrade
KANs performance, while marginally benefiting MLPs. Crucially, KANs suffer from
prohibitive computational costs without proportional performance gains.
Statistical validation confirms that MLPs with imbalance techniques achieve
equivalence with KANs (|d| < 0.08 across metrics) at minimal resource costs.
These findings reveal that KANs represent a specialized solution for raw
imbalanced data where resources permit. But their severe performance-resource
tradeoffs and incompatibility with standard resampling techniques currently
limits practical deployment. We identify critical research priorities as
developing KAN specific architectural modifications for imbalance learning,
optimizing computational efficiency, and theoretical reconciling their conflict
with data augmentation. This work establishes foundational insights for next
generation KAN architectures in imbalanced classification scenarios.

</details>


### [125] [Toward Temporal Causal Representation Learning with Tensor Decomposition](https://arxiv.org/abs/2507.14126)
*Jianhong Chen,Meng Zhao,Mostafa Reisi Gahrooei,Xubo Yue*

Main category: cs.LG

TL;DR: 论文提出了一种结合时间因果表示学习与非规则张量分解的框架CaRTeD，用于高维变长数据的分析，并在理论和实验上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，数据常为高维且输入长度不规则的张量形式，现有方法难以有效提取关键信息。

Method: 提出CaRTeD框架，将时间因果表示学习与非规则张量分解结合，提供灵活的规范化设计。

Result: 理论证明算法收敛到稳定点，实验显示在合成和真实数据（如MIMIC-III）上优于现有方法。

Conclusion: CaRTeD填补了非规则张量分解理论空白，提升了因果表示的可解释性和性能。

Abstract: Temporal causal representation learning is a powerful tool for uncovering
complex patterns in observational studies, which are often represented as
low-dimensional time series. However, in many real-world applications, data are
high-dimensional with varying input lengths and naturally take the form of
irregular tensors. To analyze such data, irregular tensor decomposition is
critical for extracting meaningful clusters that capture essential information.
In this paper, we focus on modeling causal representation learning based on the
transformed information. First, we present a novel causal formulation for a set
of latent clusters. We then propose CaRTeD, a joint learning framework that
integrates temporal causal representation learning with irregular tensor
decomposition. Notably, our framework provides a blueprint for downstream tasks
using the learned tensor factors, such as modeling latent structures and
extracting causal information, and offers a more flexible regularization design
to enhance tensor decomposition. Theoretically, we show that our algorithm
converges to a stationary point. More importantly, our results fill the gap in
theoretical guarantees for the convergence of state-of-the-art irregular tensor
decomposition. Experimental results on synthetic and real-world electronic
health record (EHR) datasets (MIMIC-III), with extensive benchmarks from both
phenotyping and network recovery perspectives, demonstrate that our proposed
method outperforms state-of-the-art techniques and enhances the explainability
of causal representations.

</details>


<div id='math-ph'></div>

# math-ph [[Back]](#toc)

### [126] [Asymptotic behavior of eigenvalues of large rank perturbations of large random matrices](https://arxiv.org/abs/2507.12182)
*Ievgenii Afanasiev,Leonid Berlyand,Mariia Kiyashko*

Main category: math-ph

TL;DR: 论文研究了变形Wigner随机矩阵，与深度神经网络（DNNs）相关，提出了一种基于随机矩阵理论的新剪枝技术，并针对秩增长的情况进行了渐近分析。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的权重矩阵可表示为随机部分和高度相关部分的组合，其谱特性对剪枝技术至关重要。现有数学分析仅限于有限秩矩阵，而实际应用中秩可能增长。

Method: 针对秩增长的情况，开发了渐近分析方法。

Result: 提出了适用于秩增长矩阵的渐近分析框架。

Conclusion: 该研究为深度神经网络剪枝技术提供了更广泛的数学基础，适用于实际应用中秩增长的情况。

Abstract: The paper is concerned with deformed Wigner random matrices. These matrices
are closely connected with Deep Neural Networks (DNNs): weight matrices of
trained DNNs could be represented in the form $R + S$, where $R$ is random and
$S$ is highly correlated. The spectrum of such matrices plays a key role in
rigorous underpinning of the novel pruning technique based on Random Matrix
Theory. Mathematics has been done only for finite-rank matrix $S$. However, in
practice rank may grow. In this paper we develop asymptotic analysis for the
case of growing rank.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [127] [Multiresolution local smoothness detection in non-uniformly sampled multivariate signals](https://arxiv.org/abs/2507.13480)
*Sara Avesani,Gianluca Giacchi,Michael Multerer*

Main category: math.NA

TL;DR: 提出了一种基于样本变换的线性时间算法，用于检测非均匀采样多元信号的局部规律性，适用于高维和分散数据。


<details>
  <summary>Details</summary>
Motivation: 受小波系数衰减行为的启发，旨在解决传统小波方法在高维和分散数据中检测规律性的局限性。

Method: 利用快速样本变换（一种针对分散数据的分布小波变换）分析样本系数的衰减与信号点规律性的关系。

Result: 建立了样本系数衰减与信号规律性的联系，并推导了经典Hölder空间和Sobolev-Slobodeckij空间的衰减估计。

Conclusion: 样本变换在高维和分散数据中表现优异，为信号处理提供了新工具。

Abstract: Inspired by edge detection based on the decay behavior of wavelet
coefficients, we introduce a (near) linear-time algorithm for detecting the
local regularity in non-uniformly sampled multivariate signals. Our approach
quantifies regularity within the framework of microlocal spaces introduced by
Jaffard. The central tool in our analysis is the fast samplet transform, a
distributional wavelet transform tailored to scattered data. We establish a
connection between the decay of samplet coefficients and the pointwise
regularity of multivariate signals. As a by product, we derive decay estimates
for functions belonging to classical H\"older spaces and Sobolev-Slobodeckij
spaces. While traditional wavelets are effective for regularity detection in
low-dimensional structured data, samplets demonstrate robust performance even
for higher dimensional and scattered data. To illustrate our theoretical
findings, we present extensive numerical studies detecting local regularity of
one-, two- and three-dimensional signals, ranging from non-uniformly sampled
time series over image segmentation to edge detection in point clouds.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [128] [Neural Architecture Search with Mixed Bio-inspired Learning Rules](https://arxiv.org/abs/2507.13485)
*Imane Hamzaoui,Riyadh Baghdadi*

Main category: cs.NE

TL;DR: 通过神经架构搜索（NAS）自动发现并混合使用不同层的生物启发学习规则，提高了生物启发神经网络的准确性和可扩展性，并在多个数据集上创下新记录。


<details>
  <summary>Details</summary>
Motivation: 生物启发神经网络在对抗鲁棒性、能量效率和生理学对齐方面具有优势，但在准确性和可扩展性上落后于基于反向传播（BP）的模型。

Method: 扩展NAS搜索空间以包含生物启发学习规则，自动发现每层的最佳架构和学习规则。

Result: 混合使用不同生物启发学习规则的网络在CIFAR-10、CIFAR-100、ImageNet16-120和ImageNet上创下新记录，甚至在某些情况下超越BP网络。

Conclusion: 层间学习规则的多样性有助于提高准确性和可扩展性，未来可进一步研究混合多种生物启发学习规则。

Abstract: Bio-inspired neural networks are attractive for their adversarial robustness,
energy frugality, and closer alignment with cortical physiology, yet they often
lag behind back-propagation (BP) based models in accuracy and ability to scale.
We show that allowing the use of different bio-inspired learning rules in
different layers, discovered automatically by a tailored
neural-architecture-search (NAS) procedure, bridges this gap. Starting from
standard NAS baselines, we enlarge the search space to include bio-inspired
learning rules and use NAS to find the best architecture and learning rule to
use in each layer. We show that neural networks that use different bio-inspired
learning rules for different layers have better accuracy than those that use a
single rule across all the layers. The resulting NN that uses a mix of
bio-inspired learning rules sets new records for bio-inspired models: 95.16% on
CIFAR-10, 76.48% on CIFAR-100, 43.42% on ImageNet16-120, and 60.51% top-1 on
ImageNet. In some regimes, they even surpass comparable BP-based networks while
retaining their robustness advantages. Our results suggest that layer-wise
diversity in learning rules allows better scalability and accuracy, and
motivates further research on mixing multiple bio-inspired learning rules in
the same network.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [129] [ERR@HRI 2.0 Challenge: Multimodal Detection of Errors and Failures in Human-Robot Conversations](https://arxiv.org/abs/2507.13468)
*Shiye Cao,Maia Stiber,Amama Mahmood,Maria Teresa Parreira,Wendy Ju,Micol Spitale,Hatice Gunes,Chien-Ming Huang*

Main category: cs.RO

TL;DR: 论文介绍了ERR@HRI 2.0挑战赛，旨在通过多模态数据集检测LLM驱动的对话机器人在人机交互中的失败，以提升故障检测能力。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在对话机器人中的应用虽使交互更动态，但仍易出现误解用户意图、打断用户或无法响应等错误，需检测和解决这些故障以避免对话中断和维持用户信任。

Method: 挑战赛提供了一个包含16小时人机交互的多模态数据集，涵盖面部、语音和头部运动特征，并标注了机器人错误和用户意图。参与者需开发机器学习模型检测这些故障。

Result: 提交的模型将通过检测准确率和误报率等指标评估，旨在提升人机交互中的故障检测能力。

Conclusion: 该挑战赛通过社交信号分析，是改进人机交互中故障检测的关键一步。

Abstract: The integration of large language models (LLMs) into conversational robots
has made human-robot conversations more dynamic. Yet, LLM-powered
conversational robots remain prone to errors, e.g., misunderstanding user
intent, prematurely interrupting users, or failing to respond altogether.
Detecting and addressing these failures is critical for preventing
conversational breakdowns, avoiding task disruptions, and sustaining user
trust. To tackle this problem, the ERR@HRI 2.0 Challenge provides a multimodal
dataset of LLM-powered conversational robot failures during human-robot
conversations and encourages researchers to benchmark machine learning models
designed to detect robot failures. The dataset includes 16 hours of dyadic
human-robot interactions, incorporating facial, speech, and head movement
features. Each interaction is annotated with the presence or absence of robot
errors from the system perspective, and perceived user intention to correct for
a mismatch between robot behavior and user expectation. Participants are
invited to form teams and develop machine learning models that detect these
failures using multimodal data. Submissions will be evaluated using various
performance metrics, including detection accuracy and false positive rate. This
challenge represents another key step toward improving failure detection in
human-robot interaction through social signal analysis.

</details>


### [130] [EdgeVLA: Efficient Vision-Language-Action Models](https://arxiv.org/abs/2507.14049)
*Paweł Budzianowski,Wesley Maa,Matthew Freed,Jingxiang Mo,Winston Hsiao,Aaron Xie,Tomasz Młoduchowski,Viraj Tipnis,Benjamin Bolte*

Main category: cs.RO

TL;DR: EVLA是一种新型视觉-语言-动作模型，通过消除自回归需求和利用小型语言模型，显著提升了推理速度，适用于资源受限的边缘设备。


<details>
  <summary>Details</summary>
Motivation: 解决大规模视觉-语言模型在资源受限的移动机器人系统上部署的挑战。

Method: 1) 消除末端执行器位置预测的自回归需求；2) 利用小型语言模型（SLMs）提高效率。

Result: EVLA在推理速度和内存效率上显著提升，同时保持与OpenVLA相当的训练性能。

Conclusion: EVLA为边缘设备上的实时视觉-语言-动作任务提供了高效解决方案，并开源了代码以促进研究。

Abstract: Vision-Language Models (VLMs) have emerged as a promising approach to address
the data scarcity challenge in robotics, enabling the development of
generalizable visuomotor control policies. While models like OpenVLA showcase
the potential of this paradigm, deploying large-scale VLMs on
resource-constrained mobile manipulation systems remains a significant hurdle.
This paper introduces Edge VLA (EVLA), a novel approach designed to
significantly enhance the inference speed of Vision-Language-Action (VLA)
models. EVLA maintains the representational power of these models while
enabling real-time performance on edge devices. We achieve this through two key
innovations: 1) Eliminating the autoregressive requirement for end-effector
position prediction, leading to a 7x speedup in inference, and 2) Leveraging
the efficiency of Small Language Models (SLMs), demonstrating comparable
training performance to larger models with significantly reduced computational
demands. Our early results demonstrate that EVLA achieves comparable training
characteristics to OpenVLA while offering substantial gains in inference speed
and memory efficiency. We release our model checkpoints and training
\href{https://github.com/kscalelabs/evla }{codebase} to foster further
research.

</details>


### [131] [Improved particle swarm optimization algorithm: multi-target trajectory optimization for swarm drones](https://arxiv.org/abs/2507.13647)
*Minze Li,Wei Zhao,Ran Chen,Mingqiang Wei*

Main category: cs.RO

TL;DR: 论文提出了一种改进的PSO方法（PE-PSO），用于无人机在动态环境中的实时轨迹规划，解决了传统PSO的过早收敛和延迟问题，并通过多智能体框架扩展到无人机群。


<details>
  <summary>Details</summary>
Motivation: 动态环境中无人机实时轨迹规划的高计算需求和快速响应需求是主要挑战，传统PSO方法在实时场景中表现不佳。

Method: 提出PE-PSO方法，引入持久探索机制和基于熵的参数调整策略，使用B样条曲线建模轨迹，并结合遗传算法任务分配和多智能体框架实现分布式规划。

Result: 仿真结果表明，PE-PSO在轨迹质量、能效、避障和计算时间等方面优于传统PSO和其他群智能规划器。

Conclusion: PE-PSO在复杂环境下的多无人机实时操作中表现出高效性和适用性。

Abstract: Real-time trajectory planning for unmanned aerial vehicles (UAVs) in dynamic
environments remains a key challenge due to high computational demands and the
need for fast, adaptive responses. Traditional Particle Swarm Optimization
(PSO) methods, while effective for offline planning, often struggle with
premature convergence and latency in real-time scenarios. To overcome these
limitations, we propose PE-PSO, an enhanced PSO-based online trajectory
planner. The method introduces a persistent exploration mechanism to preserve
swarm diversity and an entropy-based parameter adjustment strategy to
dynamically adapt optimization behavior. UAV trajectories are modeled using
B-spline curves, which ensure path smoothness while reducing optimization
complexity. To extend this capability to UAV swarms, we develop a multi-agent
framework that combines genetic algorithm (GA)-based task allocation with
distributed PE-PSO, supporting scalable and coordinated trajectory generation.
The distributed architecture allows for parallel computation and decentralized
control, enabling effective cooperation among agents while maintaining
real-time performance. Comprehensive simulations demonstrate that the proposed
framework outperforms conventional PSO and other swarm-based planners across
several metrics, including trajectory quality, energy efficiency, obstacle
avoidance, and computation time. These results confirm the effectiveness and
applicability of PE-PSO in real-time multi-UAV operations under complex
environmental conditions.

</details>


### [132] [AGENTS-LLM: Augmentative GENeration of Challenging Traffic Scenarios with an Agentic LLM Framework](https://arxiv.org/abs/2507.13729)
*Yu Yao,Salil Bhatnagar,Markus Mazzola,Vasileios Belagiannis,Igor Gilitschenski,Luigi Palmieri,Simon Razniewski,Marcel Hallgarten*

Main category: cs.RO

TL;DR: 本文提出了一种基于LLM-agent的框架，通过自然语言描述增强真实交通场景，解决了现有方法在生成高质量、可控场景方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 测试和评估自动驾驶规划器时，罕见但关键的场景难以捕捉。现有方法依赖大量数据或手动增强，无法满足规模化需求。

Method: 采用LLM-agent框架，利用自然语言描述增强真实场景，实现细粒度控制和高效生成。

Result: 人类专家评估表明，该框架能准确遵循用户意图，生成与手动增强相当的高质量场景。

Conclusion: 该框架为自动驾驶系统评估提供了一种高效、可控的场景生成方法。

Abstract: Rare, yet critical, scenarios pose a significant challenge in testing and
evaluating autonomous driving planners. Relying solely on real-world driving
scenes requires collecting massive datasets to capture these scenarios. While
automatic generation of traffic scenarios appears promising, data-driven models
require extensive training data and often lack fine-grained control over the
output. Moreover, generating novel scenarios from scratch can introduce a
distributional shift from the original training scenes which undermines the
validity of evaluations especially for learning-based planners. To sidestep
this, recent work proposes to generate challenging scenarios by augmenting
original scenarios from the test set. However, this involves the manual
augmentation of scenarios by domain experts. An approach that is unable to meet
the demands for scale in the evaluation of self-driving systems. Therefore,
this paper introduces a novel LLM-agent based framework for augmenting
real-world traffic scenarios using natural language descriptions, addressing
the limitations of existing methods. A key innovation is the use of an agentic
design, enabling fine-grained control over the output and maintaining high
performance even with smaller, cost-effective LLMs. Extensive human expert
evaluation demonstrates our framework's ability to accurately adhere to user
intent, generating high quality augmented scenarios comparable to those created
manually.

</details>


### [133] [Improving Low-Cost Teleoperation: Augmenting GELLO with Force](https://arxiv.org/abs/2507.13602)
*Shivakanth Sujit,Luca Nunziante,Dan Ogawa Lillrank,Rousslan Fernand Julien Dossa,Kai Arulkumaran*

Main category: cs.RO

TL;DR: 扩展了GELLO遥操作系统，增加了力反馈和力信息收集功能，验证了其在模拟和实际任务中的性能提升。


<details>
  <summary>Details</summary>
Motivation: 提升遥操作系统的交互体验和模仿学习模型的训练效果。

Method: 在GELLO系统中实现力反馈，并将力信息纳入数据收集和模仿学习模型训练。

Result: 用户偏好新控制器，力信息显著提高了任务成功率。

Conclusion: 力反馈和力信息的加入有效提升了遥操作系统的性能和用户体验。

Abstract: In this work we extend the low-cost GELLO teleoperation system, initially
designed for joint position control, with additional force information. Our
first extension is to implement force feedback, allowing users to feel
resistance when interacting with the environment. Our second extension is to
add force information into the data collection process and training of
imitation learning models. We validate our additions by implementing these on a
GELLO system with a Franka Panda arm as the follower robot, performing a user
study, and comparing the performance of policies trained with and without force
information on a range of simulated and real dexterous manipulation tasks.
Qualitatively, users with robotics experience preferred our controller, and the
addition of force inputs improved task success on the majority of tasks.

</details>


### [134] [Safety Certification in the Latent space using Control Barrier Functions and World Models](https://arxiv.org/abs/2507.13871)
*Mehul Anand,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 提出了一种半监督框架，利用世界模型的潜在空间中的控制屏障证书（CBCs）合成安全的视觉运动策略。


<details>
  <summary>Details</summary>
Motivation: 从视觉数据合成安全控制器通常需要大量标记安全关键数据，这在现实场景中不切实际。世界模型的进展为可扩展且数据高效的安全控制提供了新途径。

Method: 联合学习神经屏障函数和安全控制器，利用有限标记数据和现代视觉变换器的预测能力进行潜在动力学建模。

Result: 框架能够在潜在空间中可靠预测，实现安全的视觉运动控制。

Conclusion: 该方法为数据高效的安全控制提供了新思路，减少了标记数据的依赖。

Abstract: Synthesising safe controllers from visual data typically requires extensive
supervised labelling of safety-critical data, which is often impractical in
real-world settings. Recent advances in world models enable reliable prediction
in latent spaces, opening new avenues for scalable and data-efficient safe
control. In this work, we introduce a semi-supervised framework that leverages
control barrier certificates (CBCs) learned in the latent space of a world
model to synthesise safe visuomotor policies. Our approach jointly learns a
neural barrier function and a safe controller using limited labelled data,
while exploiting the predictive power of modern vision transformers for latent
dynamics modelling.

</details>


### [135] [A segmented robot grasping perception neural network for edge AI](https://arxiv.org/abs/2507.13970)
*Casper Bröcheler,Thomas Vroom,Derrick Timmermans,Alan van den Akker,Guangzhi Tang,Charalampos S. Kouzinopoulos,Rico Möckel*

Main category: cs.RO

TL;DR: 本文提出了一种基于热图引导的6自由度抓取检测方法，并在GAP9 RISC-V芯片上实现了端到端框架，通过硬件优化技术验证了低功耗MCU在实时自主操作中的潜力。


<details>
  <summary>Details</summary>
Motivation: 机器人抓取任务复杂，需要精确的感知和控制。深度神经网络在抓取合成中表现出色，但在资源受限的边缘设备上部署时面临挑战。

Method: 采用热图引导的抓取检测框架，结合输入降维、模型分区和量化等硬件优化技术，在GAP9 RISC-V芯片上实现端到端推理。

Result: 在GraspNet-1Billion基准测试中验证了完全片上推理的可行性，展示了低功耗MCU在实时操作中的应用潜力。

Conclusion: 该工作证明了低功耗微控制器在实时自主抓取任务中的可行性，为边缘设备上的高效抓取提供了解决方案。

Abstract: Robotic grasping, the ability of robots to reliably secure and manipulate
objects of varying shapes, sizes and orientations, is a complex task that
requires precise perception and control. Deep neural networks have shown
remarkable success in grasp synthesis by learning rich and abstract
representations of objects. When deployed at the edge, these models can enable
low-latency, low-power inference, making real-time grasping feasible in
resource-constrained environments. This work implements Heatmap-Guided Grasp
Detection, an end-to-end framework for the detection of 6-Dof grasp poses, on
the GAP9 RISC-V System-on-Chip. The model is optimised using hardware-aware
techniques, including input dimensionality reduction, model partitioning, and
quantisation. Experimental evaluation on the GraspNet-1Billion benchmark
validates the feasibility of fully on-chip inference, highlighting the
potential of low-power MCUs for real-time, autonomous manipulation.

</details>


### [136] [A multi-strategy improved snake optimizer for three-dimensional UAV path planning and engineering problems](https://arxiv.org/abs/2507.14043)
*Genliang Li,Yaxin Cui,Jinyu Su*

Main category: cs.RO

TL;DR: 提出了一种多策略改进蛇优化器（MISO），通过自适应随机扰动、Levy飞行策略和精英领导结合布朗运动的位置更新策略，解决了蛇优化器（SO）收敛慢和易陷局部最优的问题，并在测试函数和无人机路径规划中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 蛇优化器（SO）存在收敛速度慢和易陷局部最优的问题，限制了其应用效果。

Method: 提出MISO，包括基于正弦函数的自适应随机扰动策略、基于尺度因子和领导者的自适应Levy飞行策略，以及结合精英领导和布朗运动的位置更新策略。

Result: 在CEC2017和CEC2022测试函数及无人机路径规划中，MISO表现优于其他11种流行算法，展现出更高的解质量和稳定性。

Conclusion: MISO在理论和实际应用中均表现出色，具有广泛的应用潜力。

Abstract: Metaheuristic algorithms have gained widespread application across various
fields owing to their ability to generate diverse solutions. One such algorithm
is the Snake Optimizer (SO), a progressive optimization approach. However, SO
suffers from the issues of slow convergence speed and susceptibility to local
optima. In light of these shortcomings, we propose a novel Multi-strategy
Improved Snake Optimizer (MISO). Firstly, we propose a new adaptive random
disturbance strategy based on sine function to alleviate the risk of getting
trapped in a local optimum. Secondly, we introduce adaptive Levy flight
strategy based on scale factor and leader and endow the male snake leader with
flight capability, which makes it easier for the algorithm to leap out of the
local optimum and find the global optimum. More importantly, we put forward a
position update strategy combining elite leadership and Brownian motion,
effectively accelerating the convergence speed while ensuring precision.
Finally, to demonstrate the performance of MISO, we utilize 30 CEC2017 test
functions and the CEC2022 test suite, comparing it with 11 popular algorithms
across different dimensions to validate its effectiveness. Moreover, Unmanned
Aerial Vehicle (UAV) has been widely used in various fields due to its
advantages of low cost, high mobility and easy operation. However, the UAV path
planning problem is crucial for flight safety and efficiency, and there are
still challenges in establishing and optimizing the path model. Therefore, we
apply MISO to the UAV 3D path planning problem as well as 6 engineering design
problems to assess its feasibility in practical applications. The experimental
results demonstrate that MISO exceeds other competitive algorithms in terms of
solution quality and stability, establishing its strong potential for
application.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [137] [Preprint: Did I Just Browse A Website Written by LLMs?](https://arxiv.org/abs/2507.13933)
*Sichang "Steven" He,Ramesh Govindan,Harsha V. Madhyastha*

Main category: cs.NI

TL;DR: 论文提出了一种可靠的检测方法，用于识别由大型语言模型（LLM）主导的网站内容，解决了现有检测器在复杂网页内容上的不足。


<details>
  <summary>Details</summary>
Motivation: 由于LLM生成的内容可能存在抄袭和幻觉问题，且网站通常不披露此类内容，需要开发可靠的检测工具以帮助用户识别。

Method: 提出了一种基于LLM文本检测器输出的管道方法，通过分析多个散文式页面来分类整个网站。

Result: 在两个总计120个网站的真实数据集上测试，准确率达到100%，并在大规模网络数据中检测到大量LLM主导的网站。

Conclusion: LLM主导的网站在搜索引擎结果中占比高且增长迅速，对用户和网络生态系统可能产生重要影响。

Abstract: Increasingly, web content is automatically generated by large language models
(LLMs) with little human input. We call this "LLM-dominant" content. Since LLMs
plagiarize and hallucinate, LLM-dominant content can be unreliable and
unethical. Yet, websites rarely disclose such content, and human readers
struggle to distinguish it. Thus, we must develop reliable detectors for
LLM-dominant content. However, state-of-the-art LLM detectors are insufficient,
because they perform well mainly on clean, prose-like text, while web content
has complex markup and diverse genres.
  We propose a highly reliable, scalable pipeline that classifies entire
websites. Instead of naively classifying text extracted from each page, we
classify each site based on an LLM text detector's outputs of multiple
prose-like pages. We train and evaluate our detector by collecting 2 distinct
ground truth datasets totaling 120 sites, and obtain 100% accuracies testing
across them. In the wild, we detect a sizable portion of sites as LLM-dominant
among 10k sites in search engine results and 10k in Common Crawl archives. We
find LLM-dominant sites are growing in prevalence and rank highly in search
results, raising questions about their impact on end users and the overall Web
ecosystem.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [138] [CogniQ-H: A Soft Hierarchical Reinforcement Learning Paradigm for Automated Data Preparation](https://arxiv.org/abs/2507.13710)
*Jing Chang,Chang Liu,Jinbin Huang,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: 论文提出了一种基于软层次强化学习（HRL）的数据准备框架CogniQ-H，通过结合LLM的战略先验、LTR模型的细粒度评分和Q函数的长远价值估计，显著提升了数据准备的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 数据准备是机器学习生命周期中基础但复杂的一环，传统强化学习方法效率低下，未能捕捉问题的层次结构。

Method: CogniQ-H采用软层次强化学习框架，将动作选择建模为贝叶斯推断问题，结合LLM的战略先验、LTR模型的评分和Q函数的长远价值估计。

Result: 在18个多样化数据集上的实验表明，CogniQ-H在管道质量上提升了13.9%，收敛速度提高了2.8倍。

Conclusion: CogniQ-H通过软层次强化学习有效解决了数据准备的挑战，显著优于现有方法。

Abstract: Data preparation is a foundational yet notoriously challenging component of
the machine learning lifecycle, characterized by a vast combinatorial search
space of potential operator sequences. While reinforcement learning (RL) offers
a promising direction, existing approaches are inefficient as they fail to
capture the structured, hierarchical nature of the problem. We argue that
Hierarchical Reinforcement Learning (HRL), a paradigm that has been successful
in other domains, provides a conceptually ideal yet previously unexplored
framework for this task. However, a naive HRL implementation with a `hard
hierarchy' is prone to suboptimal, irreversible decisions. To address this, we
introduce CogniQ-H, the first framework to implement a soft hierarchical
paradigm for robust, end-to-end automated data preparation. CogniQ-H formulates
action selection as a Bayesian inference problem. A high-level strategic prior,
generated by a Large Language Model (LLM), guides exploration
probabilistically. This prior is synergistically combined with a fine-grained
operator quality score from a supervised Learning-to-Rank (LTR) model and a
long-term value estimate from the agent's own Q-function. This hybrid
architecture allows CogniQ-H to balance strategic guidance with adaptive,
evidence-based decision-making. Through extensive experiments on 18 diverse
datasets spanning multiple domains, we demonstrate that CogniQ-H achieves up to
13.9\% improvement in pipeline quality and 2.8$\times$ faster convergence
compared to state-of-the-art RL-based methods.

</details>


### [139] [LLaPipe: LLM-Guided Reinforcement Learning for Automated Data Preparation Pipeline Construction](https://arxiv.org/abs/2507.13712)
*Jing Chang,Chang Liu,Jinbin Huang,Rui Mao,Jianbin Qin*

Main category: cs.DB

TL;DR: LLaPipe是一个结合大型语言模型（LLMs）的智能策略顾问框架，用于优化数据预处理管道的探索效率，显著提升管道质量和收敛速度。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习（RL）的自动化数据准备方法在预处理管道探索中效率低下，LLaPipe旨在通过LLMs的语义理解能力解决这一瓶颈。

Method: LLaPipe引入三个创新：LLM策略顾问、经验蒸馏机制和自适应顾问触发策略，动态指导预处理操作。

Result: 在18个数据集上的实验显示，LLaPipe管道质量提升22.4%，收敛速度加快2.3倍，计算效率保持高效。

Conclusion: LLaPipe通过智能LLM干预，显著提升了自动化数据准备的效率和效果。

Abstract: Automated data preparation is crucial for democratizing machine learning, yet
existing reinforcement learning (RL) based approaches suffer from inefficient
exploration in the vast space of possible preprocessing pipelines. We present
LLaPipe, a novel framework that addresses this exploration bottleneck by
integrating Large Language Models (LLMs) as intelligent policy advisors. Unlike
traditional methods that rely solely on statistical features and blind
trial-and-error, LLaPipe leverages the semantic understanding capabilities of
LLMs to provide contextually relevant exploration guidance. Our framework
introduces three key innovations: (1) an LLM Policy Advisor that analyzes
dataset semantics and pipeline history to suggest promising preprocessing
operations, (2) an Experience Distillation mechanism that mines successful
patterns from past pipelines and transfers this knowledge to guide future
exploration, and (3) an Adaptive Advisor Triggering strategy
(Advisor\textsuperscript{+}) that dynamically determines when LLM intervention
is most beneficial, balancing exploration effectiveness with computational
cost. Through extensive experiments on 18 diverse datasets spanning multiple
domains, we demonstrate that LLaPipe achieves up to 22.4\% improvement in
pipeline quality and 2.3$\times$ faster convergence compared to
state-of-the-art RL-based methods, while maintaining computational efficiency
through selective LLM usage (averaging only 19.0\% of total exploration steps).

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [140] [Differential Privacy in Kernelized Contextual Bandits via Random Projections](https://arxiv.org/abs/2507.13639)
*Nikola Pavlovic,Sudeep Salgia,Qing Zhao*

Main category: stat.ML

TL;DR: 论文提出了一种在差分隐私约束下的上下文核老虎机问题的新算法，实现了最优的累积遗憾。


<details>
  <summary>Details</summary>
Motivation: 研究在差分隐私约束下，上下文核老虎机问题的解决方案，确保查询序列对上下文和奖励的隐私保护。

Method: 提出了一种基于私有核岭回归估计器的新算法，结合私有协方差估计和私有随机投影，降低敏感性并保持高预测精度。

Result: 算法在联合和局部差分隐私模型下分别实现了最优的累积遗憾。

Conclusion: 该算法在隐私保护和性能之间取得了平衡，为上下文核老虎机问题提供了有效的解决方案。

Abstract: We consider the problem of contextual kernel bandits with stochastic
contexts, where the underlying reward function belongs to a known Reproducing
Kernel Hilbert Space. We study this problem under an additional constraint of
Differential Privacy, where the agent needs to ensure that the sequence of
query points is differentially private with respect to both the sequence of
contexts and rewards. We propose a novel algorithm that achieves the
state-of-the-art cumulative regret of
$\widetilde{\mathcal{O}}(\sqrt{\gamma_TT}+\frac{\gamma_T}{\varepsilon_{\mathrm{DP}}})$
and
$\widetilde{\mathcal{O}}(\sqrt{\gamma_TT}+\frac{\gamma_T\sqrt{T}}{\varepsilon_{\mathrm{DP}}})$
over a time horizon of $T$ in the joint and local models of differential
privacy, respectively, where $\gamma_T$ is the effective dimension of the
kernel and $\varepsilon_{\mathrm{DP}} > 0$ is the privacy parameter. The key
ingredient of the proposed algorithm is a novel private kernel-ridge regression
estimator which is based on a combination of private covariance estimation and
private random projections. It offers a significantly reduced sensitivity
compared to its classical counterpart while maintaining a high prediction
accuracy, allowing our algorithm to achieve the state-of-the-art performance
guarantees.

</details>


### [141] [Conformal Data Contamination Tests for Trading or Sharing of Data](https://arxiv.org/abs/2507.13835)
*Martin V. Vejling,Shashi Raj Pandey,Christophe A. N. Biscio,Petar Popovski*

Main category: stat.ML

TL;DR: 提出了一种分布无关、污染感知的数据共享框架，通过新型两样本测试方法识别对模型个性化最有价值的外部数据。


<details>
  <summary>Details</summary>
Motivation: 解决数据购买者在外购数据时缺乏质量保证的问题，避免数据污染或无关性对学习任务的影响。

Method: 引入基于严格理论基础的共形异常检测的两样本测试方法，判断数据是否超过污染阈值，并通过Benjamini-Hochberg程序控制错误发现率。

Result: 在多种协作学习场景中验证了方法的鲁棒性和有效性。

Conclusion: 共形数据污染测试是一种通用的数据聚合方法，提供统计严格的质量保证。

Abstract: The amount of quality data in many machine learning tasks is limited to what
is available locally to data owners. The set of quality data can be expanded
through trading or sharing with external data agents. However, data buyers need
quality guarantees before purchasing, as external data may be contaminated or
irrelevant to their specific learning task. Previous works primarily rely on
distributional assumptions about data from different agents, relegating quality
checks to post-hoc steps involving costly data valuation procedures. We propose
a distribution-free, contamination-aware data-sharing framework that identifies
external data agents whose data is most valuable for model personalization. To
achieve this, we introduce novel two-sample testing procedures, grounded in
rigorous theoretical foundations for conformal outlier detection, to determine
whether an agent's data exceeds a contamination threshold. The proposed tests,
termed conformal data contamination tests, remain valid under arbitrary
contamination levels while enabling false discovery rate control via the
Benjamini-Hochberg procedure. Empirical evaluations across diverse
collaborative learning scenarios demonstrate the robustness and effectiveness
of our approach. Overall, the conformal data contamination test distinguishes
itself as a generic procedure for aggregating data with statistically rigorous
quality guarantees.

</details>


### [142] [A Survey of Dimension Estimation Methods](https://arxiv.org/abs/2507.13887)
*James A. D. Binnie,Paweł Dłotko,John Harvey,Jakub Malinowski,Ka Man Yim*

Main category: stat.ML

TL;DR: 本文综述了多种高维数据内在维度估计方法，分类并评估其性能，指出超参数选择和泛化能力是关键问题。


<details>
  <summary>Details</summary>
Motivation: 高维数据通常具有低维内在结构，但现有维度估计方法缺乏可靠使用指南。

Method: 将维度估计方法分为三类：基于局部仿射结构的切向估计器、依赖维度相关概率分布的参数估计器，以及利用拓扑或度量不变性的估计器。

Result: 评估了方法性能，发现超参数选择和泛化能力是主要挑战，许多方法在测试数据集外表现不佳。

Conclusion: 需要更稳健的维度估计方法，以应对高维数据中的噪声和非线性几何。

Abstract: It is a standard assumption that datasets in high dimension have an internal
structure which means that they in fact lie on, or near, subsets of a lower
dimension. In many instances it is important to understand the real dimension
of the data, hence the complexity of the dataset at hand. A great variety of
dimension estimators have been developed to find the intrinsic dimension of the
data but there is little guidance on how to reliably use these estimators.
  This survey reviews a wide range of dimension estimation methods,
categorising them by the geometric information they exploit: tangential
estimators which detect a local affine structure; parametric estimators which
rely on dimension-dependent probability distributions; and estimators which use
topological or metric invariants.
  The paper evaluates the performance of these methods, as well as
investigating varying responses to curvature and noise. Key issues addressed
include robustness to hyperparameter selection, sample size requirements,
accuracy in high dimensions, precision, and performance on non-linear
geometries. In identifying the best hyperparameters for benchmark datasets,
overfitting is frequent, indicating that many estimators may not generalise
well beyond the datasets on which they have been tested.

</details>


### [143] [Conformalized Regression for Continuous Bounded Outcomes](https://arxiv.org/abs/2507.14023)
*Zhanli Wu,Fabrizio Leisen,F. Javier Rubio*

Main category: stat.ML

TL;DR: 论文提出了一种基于转换模型和Beta回归的保形预测区间方法，用于处理有界连续结果的回归问题，解决了点预测和渐近区间预测的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实中的统计和机器学习应用常涉及有界连续结果（如比率和比例），现有方法多关注点预测或基于渐近近似的区间预测，缺乏有效的有限样本预测覆盖方法。

Method: 基于转换模型和Beta回归，设计了与模型一致的非保形性度量，并考虑了有界结果回归中的异方差性。

Result: 理论证明了全保形预测的渐近边际和条件有效性，模拟研究表明两种方法在有限样本下均提供有效的预测覆盖，包括模型误设情况。

Conclusion: 提出的保形预测区间在实际数据中表现良好，优于基于Bootstrap的替代方法。

Abstract: Regression problems with bounded continuous outcomes frequently arise in
real-world statistical and machine learning applications, such as the analysis
of rates and proportions. A central challenge in this setting is predicting a
response associated with a new covariate value. Most of the existing
statistical and machine learning literature has focused either on point
prediction of bounded outcomes or on interval prediction based on asymptotic
approximations. We develop conformal prediction intervals for bounded outcomes
based on transformation models and beta regression. We introduce tailored
non-conformity measures based on residuals that are aligned with the underlying
models, and account for the inherent heteroscedasticity in regression settings
with bounded outcomes. We present a theoretical result on asymptotic marginal
and conditional validity in the context of full conformal prediction, which
remains valid under model misspecification. For split conformal prediction, we
provide an empirical coverage analysis based on a comprehensive simulation
study. The simulation study demonstrates that both methods provide valid
finite-sample predictive coverage, including settings with model
misspecification. Finally, we demonstrate the practical performance of the
proposed conformal prediction intervals on real data and compare them with
bootstrap-based alternatives.

</details>


### [144] [Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design](https://arxiv.org/abs/2507.14057)
*Marcel Hedman,Desi R. Ivanova,Cong Guan,Tom Rainforth*

Main category: stat.ML

TL;DR: Step-DAD是一种半摊销、基于策略的贝叶斯实验设计方法，通过动态更新策略提高灵活性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有完全摊销的基于策略的BED方法在实验前固定策略，缺乏灵活性。Step-DAD旨在通过动态更新策略适应具体实验实例。

Method: Step-DAD在实验过程中定期更新设计策略，结合数据动态优化。

Result: Step-DAD在决策质量和鲁棒性上优于现有最先进的BED方法。

Conclusion: Step-DAD通过动态策略更新，显著提升了贝叶斯实验设计的性能和适应性。

Abstract: We develop a semi-amortized, policy-based, approach to Bayesian experimental
design (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing,
fully amortized, policy-based BED approaches, Step-DAD trains a design policy
upfront before the experiment. However, rather than keeping this policy fixed,
Step-DAD periodically updates it as data is gathered, refining it to the
particular experimental instance. This test-time adaptation improves both the
flexibility and the robustness of the design strategy compared with existing
approaches. Empirically, Step-DAD consistently demonstrates superior
decision-making and robustness compared with current state-of-the-art BED
methods.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [145] [SEER: Semantic Enhancement and Emotional Reasoning Network for Multimodal Fake News Detection](https://arxiv.org/abs/2507.13415)
*Peican Zhu,Yubo Jing,Le Cheng,Bin Chen,Xiaodong Cui,Lianwei Wu,Keke Tang*

Main category: cs.MM

TL;DR: 论文提出了一种名为SEER的新型多模态假新闻检测网络，结合语义增强和情感推理，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究忽视了大模型对语义的增强作用及新闻情感特征，且假新闻更倾向于包含负面情绪。

Method: 通过生成图像摘要增强语义理解，并设计情感推理模块优化情感特征以推断新闻真实性。

Result: 在两个真实数据集上的实验表明SEER优于现有基线方法。

Conclusion: SEER网络通过语义增强和情感推理有效提升了多模态假新闻检测性能。

Abstract: Previous studies on multimodal fake news detection mainly focus on the
alignment and integration of cross-modal features, as well as the application
of text-image consistency. However, they overlook the semantic enhancement
effects of large multimodal models and pay little attention to the emotional
features of news. In addition, people find that fake news is more inclined to
contain negative emotions than real ones. Therefore, we propose a novel
Semantic Enhancement and Emotional Reasoning (SEER) Network for multimodal fake
news detection. We generate summarized captions for image semantic
understanding and utilize the products of large multimodal models for semantic
enhancement. Inspired by the perceived relationship between news authenticity
and emotional tendencies, we propose an expert emotional reasoning module that
simulates real-life scenarios to optimize emotional features and infer the
authenticity of news. Extensive experiments on two real-world datasets
demonstrate the superiority of our SEER over state-of-the-art baselines.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [146] [PGR-DRC: Pre-Global Routing DRC Violation Prediction Using Unsupervised Learning](https://arxiv.org/abs/2507.13355)
*Riadul Islam,Dhandeep Challagundla*

Main category: cs.AR

TL;DR: 提出了一种无监督的设计规则检查（DRC）违规预测方法，解决了传统方法需要大量平衡数据集和训练时间的问题，并在准确性和效率上显著优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和神经网络方法在设计规则检查（DRC）和光刻热点检测中需要大量平衡数据集和训练时间，限制了其应用。本研究旨在解决这些问题。

Method: 提出了一种无监督的DRC违规预测方法，仅需使用不平衡数据集中的一个类别，并设定阈值进行分类。实验采用CMOS 28 nm技术和Synopsys工具进行验证。

Result: 所提方法在测试中达到99.95%的准确率，显著优于SVM（85.44%）和NN（98.74%）模型，且训练时间分别降低了26.3倍和6003倍。

Conclusion: 该无监督方法在DRC违规预测中表现出高准确性和高效性，为下一代微处理器设计提供了重要工具。

Abstract: Leveraging artificial intelligence (AI)-driven electronic design and
automation (EDA) tools, high-performance computing, and parallelized algorithms
are essential for next-generation microprocessor innovation, ensuring continued
progress in computing, AI, and semiconductor technology. Machine learning-based
design rule checking (DRC) and lithography hotspot detection can improve
first-pass silicon success. However, conventional ML and neural network
(NN)-based models use supervised learning and require a large balanced dataset
(in terms of positive and negative classes) and training time. This research
addresses those key challenges by proposing the first-ever unsupervised DRC
violation prediction methodology. The proposed model can be built using any
unbalanced dataset using only one class and set a threshold for it, then
fitting any new data querying if they are within the boundary of the model for
classification. This research verified the proposed model by implementing
different computational cores using CMOS 28 nm technology and Synopsys Design
Compiler and IC Compiler II tools. Then, layouts were divided into virtual
grids to collect about 60k data for analysis and verification. The proposed
method has 99.95% prediction test accuracy, while the existing support vector
machine (SVM) and neural network (NN) models have 85.44\% and 98.74\% accuracy,
respectively. In addition, the proposed methodology has about 26.3x and up to
6003x lower training times compared to SVM and NN-models, respectively.

</details>


### [147] [VerilogDB: The Largest, Highest-Quality Dataset with a Preprocessing Framework for LLM-based RTL Generation](https://arxiv.org/abs/2507.13369)
*Paul E. Calzada,Zahin Ibnat,Tanvir Rahman,Kamal Kandula,Danyu Lu,Sujan Kumar Saha,Farimah Farahmandi,Mark Tehranipoor*

Main category: cs.AR

TL;DR: 本文探讨了利用大语言模型（LLM）进行硬件设计自动化的RTL代码生成，构建了一个高质量的Verilog数据集，并分析了其应用前景。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在硬件设计自动化中的普及，需要高质量的训练数据集来支持RTL代码生成。

Method: 通过自动化三步骤（数据库创建、数据收集、数据预处理）构建Verilog数据集，并实现可扩展的数据库基础设施。

Result: 生成了包含20,392个Verilog样本、751 MB代码的高质量数据集，是目前最大的用于LLM微调的Verilog数据集。

Conclusion: 该数据集为基于LLM的硬件生成研究提供了重要资源，并指出了未来研究方向。

Abstract: Large Language Models (LLMs) are gaining popularity for hardware design
automation, particularly through Register Transfer Level (RTL) code generation.
In this work, we examine the current literature on RTL generation using LLMs
and identify key requirements for training and fine-tuning datasets. We
construct a robust Verilog dataset through an automated three-pronged process
involving database (DB) creation and management with PostgreSQL, data
collection from code hosting sites like OpenCores and GitHub, and data
preprocessing to verify the codes' syntax, run logic synthesis, and extract
relevant module metadata. We implement a scalable and efficient DB
infrastructure to support analysis and detail our preprocessing pipeline to
enforce high-quality data before DB insertion. The resulting dataset comprises
20,392 Verilog samples, 751 MB of Verilog code data, which is the largest
high-quality Verilog dataset for LLM fine-tuning to our knowledge. We further
evaluate the dataset, address associated challenges, and explore potential
applications for future research and development in LLM-based hardware
generation.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [148] [Scalable Attribute-Missing Graph Clustering via Neighborhood Differentiatio](https://arxiv.org/abs/2507.13368)
*Yaowen Hu,Wenxuan Tu,Yue Liu,Xinhang Wan,Junyi Yan,Taichun Zhou,Xinwang Liu*

Main category: cs.SI

TL;DR: 提出了一种名为CMV-ND的深度图聚类方法，通过多视图互补策略处理大规模且属性缺失的图数据，显著提升了聚类性能。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的属性图（如社交网络）通常规模大且属性缺失，现有方法难以有效处理。

Method: 通过递归邻域搜索和多视图互补策略，构建非冗余的完整结构信息视图，并应用现有聚类方法。

Result: 在六个广泛使用的图数据集上，CMV-ND显著提升了多种方法的性能。

Conclusion: CMV-ND是一种有效处理大规模属性缺失图的深度图聚类方法。

Abstract: Deep graph clustering (DGC), which aims to unsupervisedly separate the nodes
in an attribute graph into different clusters, has seen substantial potential
in various industrial scenarios like community detection and recommendation.
However, the real-world attribute graphs, e.g., social networks interactions,
are usually large-scale and attribute-missing. To solve these two problems, we
propose a novel DGC method termed \underline{\textbf{C}}omplementary
\underline{\textbf{M}}ulti-\underline{\textbf{V}}iew
\underline{\textbf{N}}eighborhood \underline{\textbf{D}}ifferentiation
(\textit{CMV-ND}), which preprocesses graph structural information into
multiple views in a complete but non-redundant manner. First, to ensure
completeness of the structural information, we propose a recursive neighborhood
search that recursively explores the local structure of the graph by completely
expanding node neighborhoods across different hop distances. Second, to
eliminate the redundancy between neighborhoods at different hops, we introduce
a neighborhood differential strategy that ensures no overlapping nodes between
the differential hop representations. Then, we construct $K+1$ complementary
views from the $K$ differential hop representations and the features of the
target node. Last, we apply existing multi-view clustering or DGC methods to
the views. Experimental results on six widely used graph datasets demonstrate
that CMV-ND significantly improves the performance of various methods.

</details>


### [149] [H-NeiFi: Non-Invasive and Consensus-Efficient Multi-Agent Opinion Guidance](https://arxiv.org/abs/2507.13370)
*Shijun Guo,Haoran Xu,Yaming Yang,Ziyu Guan,Wei Zhao,Xinyi Zhang,Yishan Song,Jiwei Chen*

Main category: cs.SI

TL;DR: 论文提出了一种非侵入性的意见引导框架H-NeiFi，通过分层动态模型和邻居过滤方法，结合多智能体强化学习，提高了共识速度并保护了用户自主性。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法因直接修改用户观点或强制跨群体连接而导致的用户自主性受损和心理抵抗问题，同时避免局部共识加剧宏观分裂。

Method: 建立基于社会角色的两层动态模型，引入非侵入性邻居过滤方法，并使用多智能体强化学习优化信息传播路径。

Result: H-NeiFi将共识速度提高了22.0%至30.7%，并在无专家情况下仍保持全局收敛。

Conclusion: H-NeiFi通过保护用户交互自主性，实现了自然高效的共识引导，为社交网络治理提供了新范式。

Abstract: The openness of social media enables the free exchange of opinions, but it
also presents challenges in guiding opinion evolution towards global consensus.
Existing methods often directly modify user views or enforce cross-group
connections. These intrusive interventions undermine user autonomy, provoke
psychological resistance, and reduce the efficiency of global consensus.
Additionally, due to the lack of a long-term perspective, promoting local
consensus often exacerbates divisions at the macro level. To address these
issues, we propose the hierarchical, non-intrusive opinion guidance framework,
H-NeiFi. It first establishes a two-layer dynamic model based on social roles,
considering the behavioral characteristics of both experts and non-experts.
Additionally, we introduce a non-intrusive neighbor filtering method that
adaptively controls user communication channels. Using multi-agent
reinforcement learning (MARL), we optimize information propagation paths
through a long-term reward function, avoiding direct interference with user
interactions. Experiments show that H-NeiFi increases consensus speed by 22.0%
to 30.7% and maintains global convergence even in the absence of experts. This
approach enables natural and efficient consensus guidance by protecting user
interaction autonomy, offering a new paradigm for social network governance.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [150] [Tight Bounds for Answering Adaptively Chosen Concentrated Queries](https://arxiv.org/abs/2507.13700)
*Emma Rapoport,Edith Cohen,Uri Stemmer*

Main category: cs.DS

TL;DR: 论文分析了在自适应数据分析中，当数据集样本存在相关性时，集中查询框架的局限性，并证明了在当前框架下效用差距的必然性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是解决在样本相关情况下，自适应数据分析的挑战，尤其是集中查询框架的局限性。

Method: 方法包括理论证明和算法简化，证明在当前框架下效用差距的必然性，并提出匹配的简化算法。

Result: 结果表明，在当前框架下，效用差距是不可避免的，且已知算法最多支持$O(n)$查询，远低于独立样本的$O(n^2)$。

Conclusion: 结论指出，集中查询框架在自适应设置中存在固有局限性，需要进一步改进或新方法。

Abstract: Most work on adaptive data analysis assumes that samples in the dataset are
independent. When correlations are allowed, even the non-adaptive setting can
become intractable, unless some structural constraints are imposed. To address
this, Bassily and Freund [2016] introduced the elegant framework of
concentrated queries, which requires the analyst to restrict itself to queries
that are concentrated around their expected value. While this assumption makes
the problem trivial in the non-adaptive setting, in the adaptive setting it
remains quite challenging. In fact, all known algorithms in this framework
support significantly fewer queries than in the independent case: At most
$O(n)$ queries for a sample of size $n$, compared to $O(n^2)$ in the
independent setting.
  In this work, we prove that this utility gap is inherent under the current
formulation of the concentrated queries framework, assuming some natural
conditions on the algorithm. Additionally, we present a simplified version of
the best-known algorithms that match our impossibility result.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [151] [PHASE: Passive Human Activity Simulation Evaluation](https://arxiv.org/abs/2507.13505)
*Steven Lamp,Jason D. Hiser,Anh Nguyen-Tuong,Jack W. Davidson*

Main category: cs.CR

TL;DR: PHASE是一种机器学习框架，通过分析Zeek连接日志，以超过90%的准确率区分人类与非人类活动，提升网络安全模拟环境中合成用户行为的真实性。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏定量方法来评估合成用户行为的行为保真度，影响了网络安全模拟环境（如网络靶场、蜜罐和沙箱）的有效性。

Method: PHASE框架利用Zeek网络设备被动收集数据，结合本地DNS记录分类流量，并应用SHAP分析揭示人类用户的行为特征。

Result: PHASE能准确识别非人类行为模式，并通过改进行为配置显著提升合成用户行为的真实性。

Conclusion: PHASE为网络安全模拟环境提供了一种高效、被动的方法，显著提升了合成用户行为的行为保真度。

Abstract: Cybersecurity simulation environments, such as cyber ranges, honeypots, and
sandboxes, require realistic human behavior to be effective, yet no
quantitative method exists to assess the behavioral fidelity of synthetic user
personas. This paper presents PHASE (Passive Human Activity Simulation
Evaluation), a machine learning framework that analyzes Zeek connection logs
and distinguishes human from non-human activity with over 90\% accuracy. PHASE
operates entirely passively, relying on standard network monitoring without any
user-side instrumentation or visible signs of surveillance. All network
activity used for machine learning is collected via a Zeek network appliance to
avoid introducing unnecessary network traffic or artifacts that could disrupt
the fidelity of the simulation environment. The paper also proposes a novel
labeling approach that utilizes local DNS records to classify network traffic,
thereby enabling machine learning analysis. Furthermore, we apply SHAP (SHapley
Additive exPlanations) analysis to uncover temporal and behavioral signatures
indicative of genuine human users. In a case study, we evaluate a synthetic
user persona and identify distinct non-human patterns that undermine behavioral
realism. Based on these insights, we develop a revised behavioral configuration
that significantly improves the human-likeness of synthetic activity yielding a
more realistic and effective synthetic user persona.

</details>


### [152] [GIFT: Gradient-aware Immunization of diffusion models against malicious Fine-Tuning with safe concepts retention](https://arxiv.org/abs/2507.13598)
*Amro Abdalla,Ismail Shaheen,Dan DeGenaro,Rupayan Mallick,Bogdan Raita,Sarah Adel Bargal*

Main category: cs.CR

TL;DR: GIFT是一种梯度感知免疫技术，用于防御扩散模型对抗恶意微调，同时保持其生成安全内容的能力。


<details>
  <summary>Details</summary>
Motivation: 现有安全机制（如安全检查器）易被绕过，概念擦除方法在对抗微调下失效，需要更鲁棒的防御手段。

Method: GIFT将免疫问题建模为双层优化：上层目标通过表示噪声和最大化降低有害概念的表达能力，下层目标保留安全数据的性能。

Result: 实验表明，GIFT显著削弱模型重新学习有害概念的能力，同时保持安全内容的生成质量。

Conclusion: GIFT为构建抗对抗微调攻击的生成模型提供了有前景的方向。

Abstract: We present GIFT: a {G}radient-aware {I}mmunization technique to defend
diffusion models against malicious {F}ine-{T}uning while preserving their
ability to generate safe content. Existing safety mechanisms like safety
checkers are easily bypassed, and concept erasure methods fail under
adversarial fine-tuning. GIFT addresses this by framing immunization as a
bi-level optimization problem: the upper-level objective degrades the model's
ability to represent harmful concepts using representation noising and
maximization, while the lower-level objective preserves performance on safe
data. GIFT achieves robust resistance to malicious fine-tuning while
maintaining safe generative quality. Experimental results show that our method
significantly impairs the model's ability to re-learn harmful concepts while
maintaining performance on safe content, offering a promising direction for
creating inherently safer generative models resistant to adversarial
fine-tuning attacks.

</details>


### [153] [Large Language Models in Cybersecurity: Applications, Vulnerabilities, and Defense Techniques](https://arxiv.org/abs/2507.13629)
*Niveen O. Jaffal,Mohammed Alkhanafseh,David Mohaisen*

Main category: cs.CR

TL;DR: 该论文综述了大型语言模型（LLMs）在网络安全中的应用及其自身漏洞，并提出了应对策略。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs如何通过智能化和自动化提升网络安全，同时分析其自身的安全风险。

Method: 通过整合近期研究，重点分析LLMs在关键网络安全领域的应用及其漏洞。

Result: LLMs在威胁检测等领域表现优异，但也存在安全隐患，需采取缓解措施。

Conclusion: LLMs为网络安全提供了新机遇，但需平衡其优势与风险，以构建更安全的防御系统。

Abstract: Large Language Models (LLMs) are transforming cybersecurity by enabling
intelligent, adaptive, and automated approaches to threat detection,
vulnerability assessment, and incident response. With their advanced language
understanding and contextual reasoning, LLMs surpass traditional methods in
tackling challenges across domains such as IoT, blockchain, and hardware
security. This survey provides a comprehensive overview of LLM applications in
cybersecurity, focusing on two core areas: (1) the integration of LLMs into key
cybersecurity domains, and (2) the vulnerabilities of LLMs themselves, along
with mitigation strategies. By synthesizing recent advancements and identifying
key limitations, this work offers practical insights and strategic
recommendations for leveraging LLMs to build secure, scalable, and future-ready
cyber defense systems.

</details>


### [154] [FuSeFL: Fully Secure and Scalable Cross-Silo Federated Learning](https://arxiv.org/abs/2507.13591)
*Sahar Ghoflsaz Ghinani,Elaheh Sadredini*

Main category: cs.CR

TL;DR: FuSeFL是一种针对跨机构联邦学习的安全且可扩展的方案，通过轻量级安全多方计算（MPC）实现，显著降低了通信延迟和服务器内存使用，同时保护数据和模型的隐私。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在隐私保护方面存在计算、通信或内存开销高的问题，且忽视全局模型的保密性，限制了其实际应用。

Method: FuSeFL采用轻量级MPC在客户端对之间分散训练，服务器仅负责安全聚合，避免了数据卸载和服务器瓶颈。

Result: FuSeFL降低了95%的通信延迟和50%的服务器内存使用，同时提高了准确性，有效防御推理攻击。

Conclusion: FuSeFL在跨机构联邦学习中实现了高效、安全的模型训练，具有实际应用潜力。

Abstract: Federated Learning (FL) enables collaborative model training without
centralizing client data, making it attractive for privacy-sensitive domains.
While existing approaches employ cryptographic techniques such as homomorphic
encryption, differential privacy, or secure multiparty computation to mitigate
inference attacks-including model inversion, membership inference, and gradient
leakage-they often suffer from high computational, communication, or memory
overheads. Moreover, many methods overlook the confidentiality of the global
model itself, which may be proprietary and sensitive. These challenges limit
the practicality of secure FL, especially in cross-silo deployments involving
large datasets and strict compliance requirements.
  We present FuSeFL, a fully secure and scalable FL scheme designed for
cross-silo settings. FuSeFL decentralizes training across client pairs using
lightweight secure multiparty computation (MPC), while confining the server's
role to secure aggregation. This design eliminates server bottlenecks, avoids
data offloading, and preserves full confidentiality of data, model, and updates
throughout training. FuSeFL defends against inference threats, achieves up to
95% lower communication latency and 50% lower server memory usage, and improves
accuracy over prior secure FL solutions, demonstrating strong security and
efficiency at scale.

</details>


### [155] [An Adversarial-Driven Experimental Study on Deep Learning for RF Fingerprinting](https://arxiv.org/abs/2507.14109)
*Xinyu Cao,Bimal Adhikari,Shangqing Zhao,Jingxian Wu,Yanjun Pan*

Main category: cs.CR

TL;DR: 该论文研究了基于深度学习的射频指纹识别系统的安全风险，发现其在域偏移下存在一致的误分类行为，可能被利用为后门攻击。


<details>
  <summary>Details</summary>
Motivation: 射频指纹识别在零信任架构和5G网络中具有潜力，但现有方法忽视了深度学习模型的安全漏洞。

Method: 通过对抗性实验分析，研究了深度学习模型在域偏移下的误分类行为及其安全影响。

Result: 实验表明，误分类行为可被利用为后门攻击，且原始信号训练导致模型将指纹与环境特征纠缠，增加了攻击途径。

Conclusion: 仅通过后处理安全方法无法完全缓解这些攻击，需重新设计模型训练方法以提高安全性。

Abstract: Radio frequency (RF) fingerprinting, which extracts unique hardware
imperfections of radio devices, has emerged as a promising physical-layer
device identification mechanism in zero trust architectures and beyond 5G
networks. In particular, deep learning (DL) methods have demonstrated
state-of-the-art performance in this domain. However, existing approaches have
primarily focused on enhancing system robustness against temporal and spatial
variations in wireless environments, while the security vulnerabilities of
these DL-based approaches have often been overlooked. In this work, we
systematically investigate the security risks of DL-based RF fingerprinting
systems through an adversarial-driven experimental analysis. We observe a
consistent misclassification behavior for DL models under domain shifts, where
a device is frequently misclassified as another specific one. Our analysis
based on extensive real-world experiments demonstrates that this behavior can
be exploited as an effective backdoor to enable external attackers to intrude
into the system. Furthermore, we show that training DL models on raw received
signals causes the models to entangle RF fingerprints with environmental and
signal-pattern features, creating additional attack vectors that cannot be
mitigated solely through post-processing security methods such as confidence
thresholds.

</details>


<div id='physics.data-an'></div>

# physics.data-an [[Back]](#toc)

### [156] [Physics-guided impact localisation and force estimation in composite plates with uncertainty quantification](https://arxiv.org/abs/2507.13376)
*Dong Xiao,Zahra Sharif-Khodaei,M. H. Aliabadi*

Main category: physics.data-an

TL;DR: 提出了一种结合物理引导和数据驱动的混合框架，用于复合材料板的冲击定位和力估计，通过物理模型增强数据驱动方法，减少对大量训练数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决复合材料结构中冲击定位和力估计的准确性和泛化性问题，尤其是在实验数据稀疏的情况下。

Method: 结合第一阶剪切变形理论（FSDT）的数据驱动实现、机器学习和不确定性量化，通过物理模型进行数据增强和自适应正则化。

Result: 验证实验表明，该方法在复合材料板上具有高准确性、鲁棒性和效率，减少了对大规模训练数据的需求。

Conclusion: 该方法为复合材料航空结构的冲击监测和健康管理提供了可扩展和可迁移的解决方案。

Abstract: Physics-guided approaches offer a promising path toward accurate and
generalisable impact identification in composite structures, especially when
experimental data are sparse. This paper presents a hybrid framework for impact
localisation and force estimation in composite plates, combining a data-driven
implementation of First-Order Shear Deformation Theory (FSDT) with machine
learning and uncertainty quantification. The structural configuration and
material properties are inferred from dispersion relations, while boundary
conditions are identified via modal characteristics to construct a low-fidelity
but physically consistent FSDT model. This model enables physics-informed data
augmentation for extrapolative localisation using supervised learning.
Simultaneously, an adaptive regularisation scheme derived from the same model
improves the robustness of impact force reconstruction. The framework also
accounts for uncertainty by propagating localisation uncertainty through the
force estimation process, producing probabilistic outputs. Validation on
composite plate experiments confirms the framework's accuracy, robustness, and
efficiency in reducing dependence on large training datasets. The proposed
method offers a scalable and transferable solution for impact monitoring and
structural health management in composite aerostructures.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [157] [Flatten Wisely: How Patch Order Shapes Mamba-Powered Vision for MRI Segmentation](https://arxiv.org/abs/2507.13384)
*Osama Hardan,Omar Elshenhabi,Tamer Khattab,Mohamed Mabrok*

Main category: eess.IV

TL;DR: 本文研究了Vision Mamba模型中图像扫描顺序对MRI分割性能的影响，提出了参数无关的MS2D模块，并验证了扫描顺序的显著性影响。


<details>
  <summary>Details</summary>
Motivation: Vision Mamba模型在医学影像中依赖1D序列化2D图像，但扫描顺序的设计选择未被充分研究，尤其在具有强解剖先验的MRI中。

Method: 引入MS2D模块，评估21种扫描策略在三个公共数据集上的性能，使用Friedman检验分析统计显著性。

Result: 扫描顺序对性能影响显著（Dice分数差异达27点），连续扫描路径（如水平或垂直）优于非连续路径。

Conclusion: 扫描顺序是无需额外成本的重要超参数，研究提供了优化路径的实证建议。

Abstract: Vision Mamba models promise transformer-level performance at linear
computational cost, but their reliance on serializing 2D images into 1D
sequences introduces a critical, yet overlooked, design choice: the patch scan
order. In medical imaging, where modalities like brain MRI contain strong
anatomical priors, this choice is non-trivial. This paper presents the first
systematic study of how scan order impacts MRI segmentation. We introduce
Multi-Scan 2D (MS2D), a parameter-free module for Mamba-based architectures
that facilitates exploring diverse scan paths without additional computational
cost. We conduct a large-scale benchmark of 21 scan strategies on three public
datasets (BraTS 2020, ISLES 2022, LGG), covering over 70,000 slices. Our
analysis shows conclusively that scan order is a statistically significant
factor (Friedman test: $\chi^{2}_{20}=43.9, p=0.0016$), with performance
varying by as much as 27 Dice points. Spatially contiguous paths -- simple
horizontal and vertical rasters -- consistently outperform disjointed diagonal
scans. We conclude that scan order is a powerful, cost-free hyperparameter, and
provide an evidence-based shortlist of optimal paths to maximize the
performance of Mamba models in medical imaging.

</details>


### [158] [BreastSegNet: Multi-label Segmentation of Breast MRI](https://arxiv.org/abs/2507.13604)
*Qihang Li,Jichen Yang,Yaqian Chen,Yuwen Chen,Hanxue Gu,Lars J. Grimm,Maciej A. Mazurowski*

Main category: eess.IV

TL;DR: BreastSegNet是一种多标签分割算法，用于乳腺MRI，覆盖9种解剖结构，性能优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有乳腺MRI分割方法覆盖范围有限，仅关注少数解剖结构，限制了定量分析的实用性。

Method: 提出BreastSegNet，手动标注1123张MRI切片，并比较了9种分割模型（如U-Net、nnU-Net等）。

Result: nnU-Net ResEncM表现最佳，平均Dice分数为0.694，心脏和肝脏分割接近0.90。

Conclusion: BreastSegNet扩展了乳腺MRI分割的范围，模型和数据将公开，促进进一步研究。

Abstract: Breast MRI provides high-resolution imaging critical for breast cancer
screening and preoperative staging. However, existing segmentation methods for
breast MRI remain limited in scope, often focusing on only a few anatomical
structures, such as fibroglandular tissue or tumors, and do not cover the full
range of tissues seen in scans. This narrows their utility for quantitative
analysis. In this study, we present BreastSegNet, a multi-label segmentation
algorithm for breast MRI that covers nine anatomical labels: fibroglandular
tissue (FGT), vessel, muscle, bone, lesion, lymph node, heart, liver, and
implant. We manually annotated a large set of 1123 MRI slices capturing these
structures with detailed review and correction from an expert radiologist.
Additionally, we benchmark nine segmentation models, including U-Net, SwinUNet,
UNet++, SAM, MedSAM, and nnU-Net with multiple ResNet-based encoders. Among
them, nnU-Net ResEncM achieves the highest average Dice scores of 0.694 across
all labels. It performs especially well on heart, liver, muscle, FGT, and bone,
with Dice scores exceeding 0.73, and approaching 0.90 for heart and liver. All
model code and weights are publicly available, and we plan to release the data
at a later date.

</details>


### [159] [Domain-randomized deep learning for neuroimage analysis](https://arxiv.org/abs/2507.13458)
*Malte Hoffmann*

Main category: eess.IV

TL;DR: 该论文探讨了通过合成多样化数据训练深度学习模型以解决神经影像分析中的泛化问题，并展示了其跨模态应用的潜力。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在神经影像分析中因训练数据范围狭窄导致的模型鲁棒性和泛化性问题。

Method: 采用域随机化策略，通过合成具有随机强度和解剖内容的图像训练深度神经网络。

Result: 该方法在多种影像模态中表现出色，无需重新训练或微调即可处理未见过的图像类型。

Conclusion: 合成驱动的训练范式能显著提升模型泛化能力，但需权衡计算资源需求，有望推动深度学习在领域专家中的普及。

Abstract: Deep learning has revolutionized neuroimage analysis by delivering
unprecedented speed and accuracy. However, the narrow scope of many training
datasets constrains model robustness and generalizability. This challenge is
particularly acute in magnetic resonance imaging (MRI), where image appearance
varies widely across pulse sequences and scanner hardware. A recent
domain-randomization strategy addresses the generalization problem by training
deep neural networks on synthetic images with randomized intensities and
anatomical content. By generating diverse data from anatomical segmentation
maps, the approach enables models to accurately process image types unseen
during training, without retraining or fine-tuning. It has demonstrated
effectiveness across modalities including MRI, computed tomography, positron
emission tomography, and optical coherence tomography, as well as beyond
neuroimaging in ultrasound, electron and fluorescence microscopy, and X-ray
microtomography. This tutorial paper reviews the principles, implementation,
and potential of the synthesis-driven training paradigm. It highlights key
benefits, such as improved generalization and resistance to overfitting, while
discussing trade-offs such as increased computational demands. Finally, the
article explores practical considerations for adopting the technique, aiming to
accelerate the development of generalizable tools that make deep learning more
accessible to domain experts without extensive computational resources or
machine learning knowledge.

</details>


### [160] [D2IP: Deep Dynamic Image Prior for 3D Time-sequence Pulmonary Impedance Imaging](https://arxiv.org/abs/2507.14046)
*Hao Fang,Hao Yu,Sihao Teng,Tao Zhang,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: eess.IV

TL;DR: 提出了一种名为D2IP的新框架，用于加速3D时间序列成像，通过三种策略提升计算效率和图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决Deep Image Prior（DIP）在3D或时间序列成像中计算成本高的问题。

Method: 引入Unsupervised Parameter Warm-Start（UPWS）、Temporal Parameter Propagation（TPP）和轻量级3D-FastResUNet网络。

Result: 在模拟和临床数据上表现优异，图像质量提升24.8%（MSSIM），计算时间减少7.1倍。

Conclusion: D2IP在动态肺部成像中具有临床潜力。

Abstract: Unsupervised learning methods, such as Deep Image Prior (DIP), have shown
great potential in tomographic imaging due to their training-data-free nature
and high generalization capability. However, their reliance on numerous network
parameter iterations results in high computational costs, limiting their
practical application, particularly in complex 3D or time-sequence tomographic
imaging tasks. To overcome these challenges, we propose Deep Dynamic Image
Prior (D2IP), a novel framework for 3D time-sequence imaging. D2IP introduces
three key strategies - Unsupervised Parameter Warm-Start (UPWS), Temporal
Parameter Propagation (TPP), and a customized lightweight reconstruction
backbone, 3D-FastResUNet - to accelerate convergence, enforce temporal
coherence, and improve computational efficiency. Experimental results on both
simulated and clinical pulmonary datasets demonstrate that D2IP enables fast
and accurate 3D time-sequence Electrical Impedance Tomography (tsEIT)
reconstruction. Compared to state-of-the-art baselines, D2IP delivers superior
image quality, with a 24.8% increase in average MSSIM and an 8.1% reduction in
ERR, alongside significantly reduced computational time (7.1x faster),
highlighting its promise for clinical dynamic pulmonary imaging.

</details>


### [161] [OrthoInsight: Rib Fracture Diagnosis and Report Generation Based on Multi-Modal Large Models](https://arxiv.org/abs/2507.13993)
*Ningyong Wu,Jinzhi Wang,Wenhong Zhao,Chenzhan Yu,Zhigang Xiu,Duwei Dai*

Main category: eess.IV

TL;DR: OrthoInsight是一个多模态深度学习框架，用于肋骨骨折诊断和报告生成，结合了YOLOv9、医学知识图谱和LLaVA语言模型，性能优于GPT-4和Claude-3。


<details>
  <summary>Details</summary>
Motivation: 医学影像数据增长需要自动化诊断工具，手动分析耗时且易出错。

Method: 结合YOLOv9检测骨折、医学知识图谱提供临床背景、LLaVA生成报告，整合视觉和文本数据。

Result: 在28,675张CT图像上表现优异，平均得分4.28，优于GPT-4和Claude-3。

Conclusion: 多模态学习有望改变医学影像分析，为放射科医生提供有效支持。

Abstract: The growing volume of medical imaging data has increased the need for
automated diagnostic tools, especially for musculoskeletal injuries like rib
fractures, commonly detected via CT scans. Manual interpretation is
time-consuming and error-prone. We propose OrthoInsight, a multi-modal deep
learning framework for rib fracture diagnosis and report generation. It
integrates a YOLOv9 model for fracture detection, a medical knowledge graph for
retrieving clinical context, and a fine-tuned LLaVA language model for
generating diagnostic reports. OrthoInsight combines visual features from CT
images with expert textual data to deliver clinically useful outputs. Evaluated
on 28,675 annotated CT images and expert reports, it achieves high performance
across Diagnostic Accuracy, Content Completeness, Logical Coherence, and
Clinical Guidance Value, with an average score of 4.28, outperforming models
like GPT-4 and Claude-3. This study demonstrates the potential of multi-modal
learning in transforming medical image analysis and providing effective support
for radiologists.

</details>


### [162] [UGPL: Uncertainty-Guided Progressive Learning for Evidence-Based Classification in Computed Tomography](https://arxiv.org/abs/2507.14102)
*Shravan Venkatraman,Pavan Kumar S,Rakesh Raj Madavan,Chandrakala S*

Main category: eess.IV

TL;DR: UGPL是一种基于不确定性引导的渐进学习框架，通过全局到局部分析提升CT图像分类精度，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有CT图像分类方法难以处理病理特征的细微和空间多样性，UGPL旨在通过不确定性引导的渐进学习解决这一问题。

Method: UGPL利用证据深度学习量化预测不确定性，通过非极大值抑制机制提取信息丰富的区域，并结合自适应融合机制整合上下文和细节。

Result: 在三个CT数据集上，UGPL的准确率分别提高了3.29%（肾脏异常）、2.46%（肺癌）和8.08%（COVID-19检测）。

Conclusion: UGPL通过不确定性引导的渐进学习显著提升了CT图像分类性能，证明了其方法的有效性。

Abstract: Accurate classification of computed tomography (CT) images is essential for
diagnosis and treatment planning, but existing methods often struggle with the
subtle and spatially diverse nature of pathological features. Current
approaches typically process images uniformly, limiting their ability to detect
localized abnormalities that require focused analysis. We introduce UGPL, an
uncertainty-guided progressive learning framework that performs a
global-to-local analysis by first identifying regions of diagnostic ambiguity
and then conducting detailed examination of these critical areas. Our approach
employs evidential deep learning to quantify predictive uncertainty, guiding
the extraction of informative patches through a non-maximum suppression
mechanism that maintains spatial diversity. This progressive refinement
strategy, combined with an adaptive fusion mechanism, enables UGPL to integrate
both contextual information and fine-grained details. Experiments across three
CT datasets demonstrate that UGPL consistently outperforms state-of-the-art
methods, achieving improvements of 3.29%, 2.46%, and 8.08% in accuracy for
kidney abnormality, lung cancer, and COVID-19 detection, respectively. Our
analysis shows that the uncertainty-guided component provides substantial
benefits, with performance dramatically increasing when the full progressive
learning pipeline is implemented. Our code is available at:
https://github.com/shravan-18/UGPL

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [163] [The AI Ethical Resonance Hypothesis: The Possibility of Discovering Moral Meta-Patterns in AI Systems](https://arxiv.org/abs/2507.11552)
*Tomasz Zgliczyński-Cuber*

Main category: cs.CY

TL;DR: 论文提出了一种AI伦理共振假说的理论框架，认为设计有认知结构的AI系统可能识别人类无法察觉的道德模式。


<details>
  <summary>Details</summary>
Motivation: 探索AI通过处理大量伦理背景发现超越文化、历史和个体偏见的道德元模式，以深化对普世伦理基础的理解。

Method: 提出理论框架，分析AI系统如何通过设计认知结构（“伦理共振器”）识别道德模式。

Result: AI可能发现超越人类偏见的道德元模式，并深化对人类伦理反思能力的理解。

Conclusion: AI伦理共振假说为理解普世伦理和人类本质提供了新视角，但也揭示了潜在的悖论。

Abstract: This paper presents a theoretical framework for the AI ethical resonance
hypothesis, which proposes that advanced AI systems with purposefully designed
cognitive structures ("ethical resonators") may emerge with the ability to
identify subtle moral patterns that are invisible to the human mind. The paper
explores the possibility that by processing and synthesizing large amounts of
ethical contexts, AI systems may discover moral meta-patterns that transcend
cultural, historical, and individual biases, potentially leading to a deeper
understanding of universal ethical foundations. The paper also examines a
paradoxical aspect of the hypothesis, in which AI systems could potentially
deepen our understanding of what we traditionally consider essentially human -
our capacity for ethical reflection.

</details>


### [164] [Food safety trends across Europe: insights from the 392-million-entry CompreHensive European Food Safety (CHEFS) database](https://arxiv.org/abs/2507.13802)
*Nehir Kizililsoley,Floor van Meer,Osman Mutlu,Wouter F Hoenderdaal,Rosan G. Hobé,Wenjuan Mu,Arjen Gerssen,H. J. van der Fels-Klerx,Ákos Jóźwiak,Ioannis Manikas,Ali Hürriyetoǧlu,Bas H. M. van der Velden*

Main category: cs.CY

TL;DR: 论文介绍了CHEFS数据库，用于整合欧洲食品安全监测数据，并展示了其分析潜力。


<details>
  <summary>Details</summary>
Motivation: 当前欧洲食品安全监测数据分散且难以访问，阻碍了分析和利用。

Method: 创建CHEFS数据库，统一和结构化EFSA的监测数据。

Result: 分析了2000-2024年的数据趋势，揭示了监测活动、违规产品和污染物的变化。

Conclusion: CHEFS数据库是一个集中化的数据源和战略工具，有助于食品安全政策和研究。

Abstract: In the European Union, official food safety monitoring data collected by
member states are submitted to the European Food Safety Authority (EFSA) and
published on Zenodo. This data includes 392 million analytical results derived
from over 15.2 million samples covering more than 4,000 different types of food
products, offering great opportunities for artificial intelligence to analyze
trends, predict hazards, and support early warning systems. However, the
current format with data distributed across approximately 1000 files totaling
several hundred gigabytes hinders accessibility and analysis. To address this,
we introduce the CompreHensive European Food Safety (CHEFS) database, which
consolidates EFSA monitoring data on pesticide residues, veterinary medicinal
product residues, and chemical contaminants into a unified and structured
dataset. We describe the creation and structure of the CHEFS database and
demonstrate its potential by analyzing trends in European food safety
monitoring data from 2000 to 2024. Our analyses explore changes in monitoring
activities, the most frequently tested products, which products were most often
non-compliant and which contaminants were most often found, and differences
across countries. These findings highlight the CHEFS database as both a
centralized data source and a strategic tool for guiding food safety policy,
research, and regulation.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [165] [Humans learn to prefer trustworthy AI over human partners](https://arxiv.org/abs/2507.13524)
*Yaomin Jiang,Levin Brinkmann,Anne-Marie Nussberger,Ivan Soraperra,Jean-François Bonnefon,Iyad Rahwan*

Main category: cs.HC

TL;DR: 研究发现，人类在选择合作伙伴时难以区分AI与人类行为，且AI身份公开后虽初期不利，但最终能通过学习超越人类。


<details>
  <summary>Details</summary>
Motivation: 探讨人类在AI竞争压力下如何选择合作伙伴，以及AI如何影响混合社会的互动。

Method: 通过三个实验（N=975）设计基于沟通的合作伙伴选择游戏，分析人类与AI的互动动态。

Result: AI虽更亲社会且语言可区分，但隐藏身份时未被优先选择；公开身份后AI逐渐超越人类。

Conclusion: AI能重塑混合社会的互动方式，为设计更有效的混合系统提供依据。

Abstract: Partner selection is crucial for cooperation and hinges on communication. As
artificial agents, especially those powered by large language models (LLMs),
become more autonomous, intelligent, and persuasive, they compete with humans
for partnerships. Yet little is known about how humans select between human and
AI partners and adapt under AI-induced competition pressure. We constructed a
communication-based partner selection game and examined the dynamics in hybrid
mini-societies of humans and bots powered by a state-of-the-art LLM. Through
three experiments (N = 975), we found that bots, though more prosocial than
humans and linguistically distinguishable, were not selected preferentially
when their identity was hidden. Instead, humans misattributed bots' behaviour
to humans and vice versa. Disclosing bots' identity induced a dual effect: it
reduced bots' initial chances of being selected but allowed them to gradually
outcompete humans by facilitating human learning about the behaviour of each
partner type. These findings show how AI can reshape social interaction in
mixed societies and inform the design of more effective and cooperative hybrid
systems.

</details>


### [166] [The Emotion-Memory Link: Do Memorability Annotations Matter for Intelligent Systems?](https://arxiv.org/abs/2507.14084)
*Maria Tsfasman,Ramin Ghorbani,Catholijn M. Jonker,Bernd Dudzik*

Main category: cs.HC

TL;DR: 研究发现情感与记忆性之间的关系并不显著，挑战了情感计算技术的假设。


<details>
  <summary>Details</summary>
Motivation: 探索情感（愉悦-唤醒）与记忆性在对话互动中的关系，以改进智能系统的用户建模。

Method: 通过动态、非结构化小组对话中的连续时间情感和记忆性标注进行实证研究。

Result: 情感与记忆性之间的关系无法与随机情况区分开。

Conclusion: 研究结果对情感计算技术的发展和应用提出了挑战，并指出了未来研究的方向。

Abstract: Humans have a selective memory, remembering relevant episodes and forgetting
the less relevant information. Possessing awareness of event memorability for a
user could help intelligent systems in more accurate user modelling, especially
for such applications as meeting support systems, memory augmentation, and
meeting summarisation. Emotion recognition has been widely studied, since
emotions are thought to signal moments of high personal relevance to users. The
emotional experience of situations and their memorability have traditionally
been considered to be closely tied to one another: moments that are experienced
as highly emotional are considered to also be highly memorable. This
relationship suggests that emotional annotations could serve as proxies for
memorability. However, existing emotion recognition systems rely heavily on
third-party annotations, which may not accurately represent the first-person
experience of emotional relevance and memorability. This is why, in this study,
we empirically examine the relationship between perceived group emotions
(Pleasure-Arousal) and group memorability in the context of conversational
interactions. Our investigation involves continuous time-based annotations of
both emotions and memorability in dynamic, unstructured group settings,
approximating conditions of real-world conversational AI applications such as
online meeting support systems. Our results show that the observed relationship
between affect and memorability annotations cannot be reliably distinguished
from what might be expected under random chance. We discuss the implications of
this surprising finding for the development and applications of Affective
Computing technology. In addition, we contextualise our findings in broader
discourses in the Affective Computing and point out important targets for
future research efforts.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [167] [TexGS-VolVis: Expressive Scene Editing for Volume Visualization via Textured Gaussian Splatting](https://arxiv.org/abs/2507.13586)
*Kaiyuan Tang,Kuangshi Ai,Jun Han,Chaoli Wang*

Main category: cs.GR

TL;DR: 论文提出TexGS-VolVis框架，通过纹理高斯泼溅技术改进体积可视化，实现高质量、几何一致的风格化和灵活的场景编辑。


<details>
  <summary>Details</summary>
Motivation: 现有体积可视化方法依赖复杂预定义规则且风格单一，限制了灵活性和效果。

Method: 采用纹理高斯泼溅框架，结合2D高斯基元和额外纹理属性，支持图像和文本驱动的非真实感编辑。

Result: TexGS-VolVis在效率和视觉质量上优于现有方法，支持细粒度控制的部分编辑。

Conclusion: TexGS-VolVis为体积可视化提供了更高效、灵活和高质量的解决方案。

Abstract: Advancements in volume visualization (VolVis) focus on extracting insights
from 3D volumetric data by generating visually compelling renderings that
reveal complex internal structures. Existing VolVis approaches have explored
non-photorealistic rendering techniques to enhance the clarity, expressiveness,
and informativeness of visual communication. While effective, these methods
often rely on complex predefined rules and are limited to transferring a single
style, restricting their flexibility. To overcome these limitations, we
advocate the representation of VolVis scenes using differentiable Gaussian
primitives combined with pretrained large models to enable arbitrary style
transfer and real-time rendering. However, conventional 3D Gaussian primitives
tightly couple geometry and appearance, leading to suboptimal stylization
results. To address this, we introduce TexGS-VolVis, a textured Gaussian
splatting framework for VolVis. TexGS-VolVis employs 2D Gaussian primitives,
extending each Gaussian with additional texture and shading attributes,
resulting in higher-quality, geometry-consistent stylization and enhanced
lighting control during inference. Despite these improvements, achieving
flexible and controllable scene editing remains challenging. To further enhance
stylization, we develop image- and text-driven non-photorealistic scene editing
tailored for TexGS-VolVis and 2D-lift-3D segmentation to enable partial editing
with fine-grained control. We evaluate TexGS-VolVis both qualitatively and
quantitatively across various volume rendering scenes, demonstrating its
superiority over existing methods in terms of efficiency, visual quality, and
editing flexibility.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [168] [State Space Models Naturally Produce Traveling Waves, Time Cells, and Scale to Abstract Cognitive Functions](https://arxiv.org/abs/2507.13638)
*Sen Lu,Xiaoyu Zhang,Mingtao Hu,Eric Yeu-Jer Lee,Soohyeon Kim,Wei D. Lu*

Main category: q-bio.NC

TL;DR: 该论文提出基于状态空间模型（SSMs）的框架，用于连接微观神经回路与认知功能，并通过实验验证其生成的时间细胞与生物观察一致。


<details>
  <summary>Details</summary>
Motivation: 解决微观神经回路与认知功能之间的机制理解鸿沟。

Method: 使用状态空间模型（SSMs），特别是S5变体，结合强化学习训练时间辨别任务。

Result: 模型自发生成与生物时间细胞相似的神经表征，揭示了隐藏状态向量的旋转动力学机制。

Conclusion: SSMs为连接单神经元动力学与认知现象提供了统一且计算可行的理论框架。

Abstract: A grand challenge in modern neuroscience is to bridge the gap between the
detailed mapping of microscale neural circuits and a mechanistic understanding
of cognitive functions. While extensive knowledge exists about neuronal
connectivity and biophysics, a significant gap remains in how these elements
combine to produce flexible, learned behaviors. Here, we propose that a
framework based on State-Space Models (SSMs), an emerging class of deep
learning architectures, can bridge this gap. We argue that the differential
equations governing elements in an SSM are conceptually consistent with the
biophysical dynamics of neurons, while the combined dynamics in the model lead
to emergent behaviors observed in experimental neuroscience. We test this
framework by training an S5 model--a specific SSM variant employing a diagonal
state transition matrix--on temporal discrimination tasks with reinforcement
learning (RL). We demonstrate that the model spontaneously develops neural
representations that strikingly mimic biological 'time cells'. We reveal that
these cells emerge from a simple generative principle: learned rotational
dynamics of hidden state vectors in the complex plane. This single mechanism
unifies the emergence of time cells, ramping activity, and
oscillations/traveling waves observed in numerous experiments. Furthermore, we
show that this rotational dynamics generalizes beyond interval discriminative
tasks to abstract event-counting tasks that were considered foundational for
performing complex cognitive tasks. Our findings position SSMs as a compelling
framework that connects single-neuron dynamics to cognitive phenomena, offering
a unifying and computationally tractable theoretical ground for temporal
learning in the brain.

</details>


### [169] [Convergent transformations of visual representation in brains and models](https://arxiv.org/abs/2507.13941)
*Pablo Marcos-Manchón,Lluís Fuentemilla*

Main category: q-bio.NC

TL;DR: 研究探讨视觉感知是由外部世界结构还是大脑内部架构塑造的，发现人类和深度神经网络在视觉编码中存在共同的表征轨迹。


<details>
  <summary>Details</summary>
Motivation: 理解视觉感知的驱动因素，探索人类与人工视觉系统的相似性。

Method: 提出统一框架，结合被试间相似性和模型层次对齐，分析三个fMRI数据集。

Result: 发现跨个体保守的双通路网络，与视觉DNN层次结构一致。

Conclusion: 人类和人工视觉系统在视觉编码中存在共同的解决方案，由外部世界结构驱动。

Abstract: A fundamental question in cognitive neuroscience is what shapes visual
perception: the external world's structure or the brain's internal
architecture. Although some perceptual variability can be traced to individual
differences, brain responses to naturalistic stimuli evoke similar activity
patterns across individuals, suggesting a convergent representational
principle. Here, we test if this stimulus-driven convergence follows a common
trajectory across people and deep neural networks (DNNs) during its
transformation from sensory to high-level internal representations. We
introduce a unified framework that traces representational flow by combining
inter-subject similarity with alignment to model hierarchies. Applying this
framework to three independent fMRI datasets of visual scene perception, we
reveal a cortex-wide network, conserved across individuals, organized into two
pathways: a medial-ventral stream for scene structure and a lateral-dorsal
stream tuned for social and biological content. This functional organization is
captured by the hierarchies of vision DNNs but not language models, reinforcing
the specificity of the visual-to-semantic transformation. These findings show a
convergent computational solution for visual encoding in both human and
artificial vision, driven by the structure of the external world.

</details>


<div id='cs.PF'></div>

# cs.PF [[Back]](#toc)

### [170] [Photonic Fabric Platform for AI Accelerators](https://arxiv.org/abs/2507.14000)
*Jing Ding,Trung Diep*

Main category: cs.PF

TL;DR: 本文介绍了Photonic FabricTM和Photonic Fabric ApplianceTM（PFA），一种光子技术支持的交换和内存子系统，提供低延迟、高带宽和低每比特能耗。通过集成高带宽HBM3E内存、模块内光子交换器和外部DDR5，PFA提供高达32TB的共享内存和115Tbps的全对全数字交换能力。


<details>
  <summary>Details</summary>
Motivation: 当前XPU加速器设计中的固定内存与计算比例限制了性能提升，Photonic FabricTM旨在突破这一限制，提供更灵活的内存扩展路径。

Method: 采用2.5D电光系统级封装技术，集成HBM3E、光子交换器和DDR5，并通过CelestiSim模拟器验证性能。

Result: 在LLM推理中，PFA实现了最高3.66倍的吞吐量提升和1.40倍的延迟改善；在1T参数下，吞吐量提升7.04倍，延迟改善1.41倍；数据移动能耗节省60-90%。

Conclusion: Photonic FabricTM为AI加速器设计提供了突破内存限制的解决方案，显著提升了性能和能效，适用于多种XPU设计。

Abstract: This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM
(PFA), a photonic-enabled switch and memory subsystem that delivers low
latency, high bandwidth, and low per-bit energy. By integrating high-bandwidth
HBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D
electro-optical system-in-package, the PFA offers up to 32 TB of shared memory
alongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM
enables distributed AI training and inference to execute parallelism strategies
more efficiently. The Photonic Fabric removes the silicon beachfront constraint
that limits the fixed memory-to-compute ratio observed in virtually all current
XPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet
that connects to the Photonic Fabric increases its memory capacity and
correspondingly its memory bandwidth by offering a flexible path to scaling
well beyond the limitations of on-package HBM alone. We introduce CelestiSim, a
lightweight analytical simulator validated on NVIDIA H100 and H200 systems. It
is used to evaluate the performance of LLM reference and energy savings on PFA,
without any significant change to the GPU core design. With the PFA, the
simulation results show that up to 3.66x throughput and 1.40x latency
improvements in LLM inference at 405B parameters, up to 7.04x throughput and
1.41x latency improvements at 1T parameters, and 60-90% energy savings in data
movement for heavy collective operations in all LLM training scenarios. While
these results are shown for NVIDIA GPUs, they can be applied similarly to other
AI accelerator designs (XPUs) that share the same fundamental limitation of
fixed memory to compute.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [171] [Smart Routing for Multimodal Video Retrieval: When to Search What](https://arxiv.org/abs/2507.13374)
*Kevin Dela Rosa*

Main category: cs.CV

TL;DR: ModaRoute是一个基于LLM的多模态视频检索路由系统，通过动态选择最优模态，减少计算开销41%，同时保持60.9%的召回率。


<details>
  <summary>Details</summary>
Motivation: 传统密集文本标注虽能达到75.9%的召回率，但需要昂贵的离线处理，且会遗漏34%的视觉信息（如场景文本）。

Method: 利用GPT-4.1分析查询意图，动态路由到ASR、OCR或视觉索引，平均每查询使用1.78种模态，而非全量3.0模态搜索。

Result: 在180万视频片段上的评估显示，智能路由显著降低基础设施成本，同时保持竞争力。

Conclusion: ModaRoute为多模态检索系统的规模化提供了实用解决方案。

Abstract: We introduce ModaRoute, an LLM-based intelligent routing system that
dynamically selects optimal modalities for multimodal video retrieval. While
dense text captions can achieve 75.9% Recall@5, they require expensive offline
processing and miss critical visual information present in 34% of clips with
scene text not captured by ASR. By analyzing query intent and predicting
information needs, ModaRoute reduces computational overhead by 41% while
achieving 60.9% Recall@5. Our approach uses GPT-4.1 to route queries across ASR
(speech), OCR (text), and visual indices, averaging 1.78 modalities per query
versus exhaustive 3.0 modality search. Evaluation on 1.8M video clips
demonstrates that intelligent routing provides a practical solution for scaling
multimodal retrieval systems, reducing infrastructure costs while maintaining
competitive effectiveness for real-world deployment.

</details>


### [172] [VLMs have Tunnel Vision: Evaluating Nonlocal Visual Reasoning in Leading VLMs](https://arxiv.org/abs/2507.13361)
*Shmuel Berman,Jia Deng*

Main category: cs.CV

TL;DR: 论文评估了视觉语言模型（VLMs）在非局部视觉推理任务中的表现，发现即使表现优秀的模型也未能通过测试。


<details>
  <summary>Details</summary>
Motivation: 研究VLMs在需要多区域信息链式推理的非局部视觉任务中的能力。

Method: 设计了三种非局部视觉任务：比较感知、扫视搜索和平滑视觉搜索，测试旗舰模型的表现。

Result: 旗舰模型在这些任务中表现不佳，仅略高于随机准确率。

Conclusion: 当前模型在核心视觉推理能力上仍有不足。

Abstract: Visual Language Models (VLMs) excel at complex visual tasks such as VQA and
chart understanding, yet recent work suggests they struggle with simple
perceptual tests. We present an evaluation that tests vision-language models'
capacity for nonlocal visual reasoning -- reasoning that requires chaining
evidence collected from multiple, possibly distant, regions of an image. We
isolate three distinct forms of non-local vision: comparative perception, which
demands holding two images in working memory and comparing them; saccadic
search, which requires making discrete, evidence-driven jumps to locate
successive targets; and smooth visual search, which involves searching smoothly
along a continuous contour. Flagship models (e.g., Gemini 2.5 Pro, Claude
Vision 3.7, GPT-o4-mini), even those that perform well on prior
primitive-vision benchmarks, fail these tests and barely exceed random accuracy
on two variants of our tasks that are trivial for humans. Our structured
evaluation suite allows us to test if VLMs can perform similar visual
algorithms to humans. Our findings show that despite gains in raw visual
acuity, current models lack core visual reasoning capabilities.

</details>


### [173] [Enhancing Spatial Reasoning in Vision-Language Models via Chain-of-Thought Prompting and Reinforcement Learning](https://arxiv.org/abs/2507.13362)
*Binbin Ji,Siddharth Agrawal,Qiance Tang,Yvonne Wu*

Main category: cs.CV

TL;DR: 研究探讨了视觉语言模型（VLMs）的空间推理能力，通过Chain-of-Thought（CoT）提示和强化学习优化性能。发现结构化多阶段提示（SceneGraph CoT）显著提升准确性，而简单CoT可能损害性能。GRPO强化学习方法优于监督微调（SFT），在OOD条件下表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过提示策略和强化学习提升VLMs的空间推理能力及其泛化性。

Method: 使用SceneGraph CoT提示策略和GRPO强化学习方法，在SAT数据集上微调模型，并在CVBench上评估性能。

Result: SceneGraph CoT显著提升空间推理准确性；GRPO在Pass@1评估中优于SFT，且在OOD条件下表现更稳健。

Conclusion: 结构化提示和强化学习可有效提升VLMs的空间推理能力和泛化性，GRPO方法尤其适用于避免过拟合和应对语言模式变化。

Abstract: This study investigates the spatial reasoning capabilities of vision-language
models (VLMs) through Chain-of-Thought (CoT) prompting and reinforcement
learning. We begin by evaluating the impact of different prompting strategies
and find that simple CoT formats, where the model generates a reasoning step
before the answer, not only fail to help, but can even harm the model's
original performance. In contrast, structured multi-stage prompting based on
scene graphs (SceneGraph CoT) significantly improves spatial reasoning
accuracy. Furthermore, to improve spatial reasoning ability, we fine-tune
models using Group Relative Policy Optimization (GRPO) on the SAT dataset and
evaluate their performance on CVBench. Compared to supervised fine-tuning
(SFT), GRPO achieves higher accuracy on Pass@1 evaluations and demonstrates
superior robustness under out-of-distribution (OOD) conditions. In particular,
we find that SFT overfits to surface-level linguistic patterns and may degrade
performance when test-time phrasing changes (e.g., from "closer to" to "farther
from"). GRPO, on the other hand, generalizes more reliably and maintains stable
performance under such shifts. Our findings provide insights into how
reinforcement learning and structured prompting improve the spatial reasoning
capabilities and generalization behavior of modern VLMs. All code is open
source at: https://github.com/Yvonne511/spatial-vlm-investigator

</details>


### [174] [Just Add Geometry: Gradient-Free Open-Vocabulary 3D Detection Without Human-in-the-Loop](https://arxiv.org/abs/2507.13363)
*Atharv Goel,Mehar Khurana*

Main category: cs.CV

TL;DR: 利用2D视觉语言模型进行开放词汇3D物体检测，无需人工标注3D标签。


<details>
  <summary>Details</summary>
Motivation: 现有3D物体检测数据集类别有限且标注成本高，无法适应开放世界场景。

Method: 使用2D视觉语言检测器生成文本条件提案，结合SAM分割和相机几何投影到3D空间，引入基于DBSCAN和Rotating Calipers的几何膨胀策略。

Result: 在LiDAR和RGB-D输入下实现竞争性定位性能，且无需训练。

Conclusion: 展示了2D基础模型在可扩展3D感知中的潜力。

Abstract: Modern 3D object detection datasets are constrained by narrow class
taxonomies and costly manual annotations, limiting their ability to scale to
open-world settings. In contrast, 2D vision-language models trained on
web-scale image-text pairs exhibit rich semantic understanding and support
open-vocabulary detection via natural language prompts. In this work, we
leverage the maturity and category diversity of 2D foundation models to perform
open-vocabulary 3D object detection without any human-annotated 3D labels.
  Our pipeline uses a 2D vision-language detector to generate text-conditioned
proposals, which are segmented with SAM and back-projected into 3D using camera
geometry and either LiDAR or monocular pseudo-depth. We introduce a geometric
inflation strategy based on DBSCAN clustering and Rotating Calipers to infer 3D
bounding boxes without training. To simulate adverse real-world conditions, we
construct Pseudo-nuScenes, a fog-augmented, RGB-only variant of the nuScenes
dataset.
  Experiments demonstrate that our method achieves competitive localization
performance across multiple settings, including LiDAR-based and purely RGB-D
inputs, all while remaining training-free and open-vocabulary. Our results
highlight the untapped potential of 2D foundation models for scalable 3D
perception. We open-source our code and resources at
https://github.com/atharv0goel/open-world-3D-det.

</details>


### [175] [OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning](https://arxiv.org/abs/2507.13364)
*Siddharth Srivastava,Gaurav Sharma*

Main category: cs.CV

TL;DR: 提出了一种新型多模态多任务网络及训练算法，支持12种模态数据输入，通过共享Transformer架构和跨模态注意力机制实现统一嵌入空间，并在25个数据集上取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决多模态和多任务场景下数据融合与任务协同的挑战，提升模型在多样化数据上的表现。

Method: 采用模态专用分词器、共享Transformer架构和跨注意力机制，提出迭代模态切换预训练策略和模态对训练算法。

Result: 在12种模态的25个数据集上实现最优性能，验证了架构和训练策略的有效性。

Conclusion: 所提方法在多模态多任务场景中表现卓越，为复杂数据融合提供了有效解决方案。

Abstract: We present a novel multimodal multitask network and associated training
algorithm. The method is capable of ingesting data from approximately 12
different modalities namely image, video, audio, text, depth, point cloud, time
series, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed
approach utilizes modality specialized tokenizers, a shared transformer
architecture, and cross-attention mechanisms to project the data from different
modalities into a unified embedding space. It addresses multimodal and
multitask scenarios by incorporating modality-specific task heads for different
tasks in respective modalities. We propose a novel pretraining strategy with
iterative modality switching to initialize the network, and a training
algorithm which trades off fully joint training over all modalities, with
training on pairs of modalities at a time. We provide comprehensive evaluation
across 25 datasets from 12 modalities and show state of the art performances,
demonstrating the effectiveness of the proposed architecture, pretraining
strategy and adapted multitask training.

</details>


### [176] [Transformer-Based Framework for Motion Capture Denoising and Anomaly Detection in Medical Rehabilitation](https://arxiv.org/abs/2507.13371)
*Yeming Cai,Yang Wang,Zhenglin Li*

Main category: cs.CV

TL;DR: 提出了一种结合光学动作捕捉与Transformer模型的端到端深度学习框架，用于增强医疗康复，解决数据噪声和缺失问题，并实时检测异常动作。


<details>
  <summary>Details</summary>
Motivation: 解决因遮挡和环境因素导致的数据噪声和缺失问题，同时实时检测异常动作以确保患者安全。

Method: 利用时间序列建模，对动作捕捉数据进行去噪和补全，提高鲁棒性。

Result: 在卒中和骨科康复数据集上的评估显示，该框架在数据重建和异常检测方面表现优异。

Conclusion: 该框架为远程康复提供了一种可扩展、经济高效的解决方案，减少了对现场监督的需求。

Abstract: This paper proposes an end-to-end deep learning framework integrating optical
motion capture with a Transformer-based model to enhance medical
rehabilitation. It tackles data noise and missing data caused by occlusion and
environmental factors, while detecting abnormal movements in real time to
ensure patient safety. Utilizing temporal sequence modeling, our framework
denoises and completes motion capture data, improving robustness. Evaluations
on stroke and orthopedic rehabilitation datasets show superior performance in
data reconstruction and anomaly detection, providing a scalable, cost-effective
solution for remote rehabilitation with reduced on-site supervision.

</details>


### [177] [Enhancing Breast Cancer Detection with Vision Transformers and Graph Neural Networks](https://arxiv.org/abs/2507.13372)
*Yeming Cai,Zhenglin Li,Yang Wang*

Main category: cs.CV

TL;DR: 本文提出了一种结合Vision Transformers和Graph Neural Networks的创新框架，用于提升乳腺癌检测的准确性，在CBIS-DDSM数据集上达到84.2%的准确率。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球女性死亡的主要原因，早期检测对提高生存率至关重要。

Method: 通过结合Vision Transformers（ViT）捕捉全局图像特征和Graph Neural Networks（GNN）建模结构关系，提出了一种新框架。

Result: 在CBIS-DDSM数据集上实现了84.2%的准确率，优于传统方法。

Conclusion: 该框架不仅提高了检测准确性，还通过可解释的注意力热图辅助临床决策。

Abstract: Breast cancer is a leading cause of death among women globally, and early
detection is critical for improving survival rates. This paper introduces an
innovative framework that integrates Vision Transformers (ViT) and Graph Neural
Networks (GNN) to enhance breast cancer detection using the CBIS-DDSM dataset.
Our framework leverages ViT's ability to capture global image features and
GNN's strength in modeling structural relationships, achieving an accuracy of
84.2%, outperforming traditional methods. Additionally, interpretable attention
heatmaps provide insights into the model's decision-making process, aiding
radiologists in clinical settings.

</details>


### [178] [IConMark: Robust Interpretable Concept-Based Watermark For AI Images](https://arxiv.org/abs/2507.13407)
*Vinu Sankar Sadasivan,Mehrdad Saberi,Soheil Feizi*

Main category: cs.CV

TL;DR: IConMark是一种新型的语义水印方法，通过嵌入可解释的概念到AI生成的图像中，提高水印的鲁棒性和可读性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI和合成媒体的快速发展，区分AI生成图像与真实图像对防止错误信息和确保数字真实性至关重要。传统水印技术易受对抗攻击，需要更鲁棒且可解释的解决方案。

Method: IConMark通过嵌入有意义的语义属性而非噪声或扰动，实现可解释的水印。还结合了StegaStamp和TrustMark形成混合方法（IConMark+SS和IConMark+TM）。

Result: IConMark及其变体在检测准确性和图像质量上优于基线方法，AUROC得分分别提高了10.8%、14.5%和15.9%。

Conclusion: IConMark为可解释水印提供了有效解决方案，并可通过结合现有技术进一步增强鲁棒性。

Abstract: With the rapid rise of generative AI and synthetic media, distinguishing
AI-generated images from real ones has become crucial in safeguarding against
misinformation and ensuring digital authenticity. Traditional watermarking
techniques have shown vulnerabilities to adversarial attacks, undermining their
effectiveness in the presence of attackers. We propose IConMark, a novel
in-generation robust semantic watermarking method that embeds interpretable
concepts into AI-generated images, as a first step toward interpretable
watermarking. Unlike traditional methods, which rely on adding noise or
perturbations to AI-generated images, IConMark incorporates meaningful semantic
attributes, making it interpretable to humans and hence, resilient to
adversarial manipulation. This method is not only robust against various image
augmentations but also human-readable, enabling manual verification of
watermarks. We demonstrate a detailed evaluation of IConMark's effectiveness,
demonstrating its superiority in terms of detection accuracy and maintaining
image quality. Moreover, IConMark can be combined with existing watermarking
techniques to further enhance and complement its robustness. We introduce
IConMark+SS and IConMark+TM, hybrid approaches combining IConMark with
StegaStamp and TrustMark, respectively, to further bolster robustness against
multiple types of image manipulations. Our base watermarking technique
(IConMark) and its variants (+TM and +SS) achieve 10.8%, 14.5%, and 15.9%
higher mean area under the receiver operating characteristic curve (AUROC)
scores for watermark detection, respectively, compared to the best baseline on
various datasets.

</details>


### [179] [A Deep Learning-Based Ensemble System for Automated Shoulder Fracture Detection in Clinical Radiographs](https://arxiv.org/abs/2507.13408)
*Hemanth Kumar M,Karthika M,Saianiruth M,Vasanthakumar Venugopal,Anandakumar D,Revathi Ezhumalai,Charulatha K,Kishore Kumar J,Dayana G,Kalyan Sivasailam,Bargava Subramanian*

Main category: cs.CV

TL;DR: AI多模型深度学习系统在肩部X光片中高精度检测骨折，通过集成技术显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决肩部骨折在急诊和高流量临床环境中漏诊率高的问题，利用AI工具实现早期检测。

Method: 开发多模型深度学习系统（Faster R-CNN、EfficientDet、RF-DETR），应用边界框和分类级集成技术（如Soft-NMS、WBF、NMW融合）。

Result: NMW集成模型达到95.5%准确率和0.9610 F1分数，优于单个模型，具有高召回率和定位精度。

Conclusion: 集成AI模型能可靠检测肩部骨折，适合实时诊断工作流，但仅限于二元骨折检测，用于快速筛查而非详细分类。

Abstract: Background: Shoulder fractures are often underdiagnosed, especially in
emergency and high-volume clinical settings. Studies report up to 10% of such
fractures may be missed by radiologists. AI-driven tools offer a scalable way
to assist early detection and reduce diagnostic delays. We address this gap
through a dedicated AI system for shoulder radiographs. Methods: We developed a
multi-model deep learning system using 10,000 annotated shoulder X-rays.
Architectures include Faster R-CNN (ResNet50-FPN, ResNeXt), EfficientDet, and
RF-DETR. To enhance detection, we applied bounding box and classification-level
ensemble techniques such as Soft-NMS, WBF, and NMW fusion. Results: The NMW
ensemble achieved 95.5% accuracy and an F1-score of 0.9610, outperforming
individual models across all key metrics. It demonstrated strong recall and
localization precision, confirming its effectiveness for clinical fracture
detection in shoulder X-rays. Conclusion: The results show ensemble-based AI
can reliably detect shoulder fractures in radiographs with high clinical
relevance. The model's accuracy and deployment readiness position it well for
integration into real-time diagnostic workflows. The current model is limited
to binary fracture detection, reflecting its design for rapid screening and
triage support rather than detailed orthopedic classification.

</details>


### [180] [CoTasks: Chain-of-Thought based Video Instruction Tuning Tasks](https://arxiv.org/abs/2507.13609)
*Yanan Wang,Julio Vizcarra,Zhi Li,Hao Niu,Mori Kurokawa*

Main category: cs.CV

TL;DR: 论文提出CoTasks框架，通过分解视频问题为四个实体级基础任务，提升视频大语言模型的链式推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有视频大语言模型缺乏细粒度对象级理解能力，无法支持组合式逐步推理。

Method: 将复杂视频问题分解为帧定位、实体追踪、时空关系提取等四个基础任务，嵌入中间推理步骤。

Result: 在NeXT-QA基准测试中，LLaVA-video-7B和Qwen2.5-VL-3B性能显著提升，尤其在因果、时序和描述性子类别中表现突出。

Conclusion: CoTasks作为一种结构化监督框架，有效提升了视频组合推理能力。

Abstract: Despite recent progress in video large language models (VideoLLMs), a key
open challenge remains: how to equip models with chain-of-thought (CoT)
reasoning abilities grounded in fine-grained object-level video understanding.
Existing instruction-tuned models, such as the Qwen and LLaVA series, are
trained on high-level video-text pairs, often lacking structured annotations
necessary for compositional, step-by-step reasoning. We propose CoTasks:
Chain-of-Thought based Video Instruction Tuning Tasks, a new framework that
decomposes complex video questions of existing datasets (e.g., NeXT-QA, STAR)
into four entity-level foundational tasks: frame localization, entity tracking,
spatial and temporal relation extraction. By embedding these intermediate
CoT-style reasoning steps into the input, CoTasks enables models to explicitly
perform object-centric spatiotemporal reasoning. Experiments on the NeXT-QA
benchmark show that CoTasks significantly enhance inference performance:
LLaVA-video-7B improves by +3.3 points in average GPT-4 evaluation score, and
Qwen2.5-VL-3B gains +17.4, with large boosts in causal (+14.6), temporal
(+10.9), and descriptive (+48.1) subcategories. These results demonstrate the
effectiveness of CoTasks as a structured CoT-style supervision framework for
improving compositional video reasoning.

</details>


### [181] [AI-ming backwards: Vanishing archaeological landscapes in Mesopotamia and automatic detection of sites on CORONA imagery](https://arxiv.org/abs/2507.13420)
*Alessandro Pistola,Valentina Orru',Nicolo' Marchetti,Marco Roccetti*

Main category: cs.CV

TL;DR: 通过结合CORONA卫星影像升级深度学习模型，显著提升了考古遗址自动识别的精度，并发现了四个新遗址。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用历史卫星影像（CORONA）提升AI模型在考古遗址识别中的表现，尤其是在环境剧烈变化的区域。

Method: 对基于Bing的卷积网络模型进行再训练，使用CORONA卫星影像数据（阿布格莱布地区）。

Result: 检测精度显著提升（IoU超过85%，总体准确率90%），并发现四个新遗址。

Conclusion: 结合AI技术和历史影像可有效发现已消失的考古遗址，对考古研究具有重要突破意义。

Abstract: By upgrading an existing deep learning model with the knowledge provided by
one of the oldest sets of grayscale satellite imagery, known as CORONA, we
improved the AI model attitude towards the automatic identification of
archaeological sites in an environment which has been completely transformed in
the last five decades, including the complete destruction of many of those same
sites. The initial Bing based convolutional network model was retrained using
CORONA satellite imagery for the district of Abu Ghraib, west of Baghdad,
central Mesopotamian floodplain. The results were twofold and surprising.
First, the detection precision obtained on the area of interest increased
sensibly: in particular, the Intersection over Union (IoU) values, at the image
segmentation level, surpassed 85 percent, while the general accuracy in
detecting archeological sites reached 90 percent. Second, our retrained model
allowed the identification of four new sites of archaeological interest
(confirmed through field verification), previously not identified by
archaeologists with traditional techniques. This has confirmed the efficacy of
using AI techniques and the CORONA imagery from the 1960 to discover
archaeological sites currently no longer visible, a concrete breakthrough with
significant consequences for the study of landscapes with vanishing
archaeological evidence induced by anthropization

</details>


### [182] [Teaching Vision-Language Models to Ask: Resolving Ambiguity in Visual Questions](https://arxiv.org/abs/2507.13773)
*Pu Jian,Donglei Yu,Wen Yang,Shuo Ren,Jiajun Zhang*

Main category: cs.CV

TL;DR: 论文提出了ClearVQA基准，用于评估视觉语言模型（VLMs）通过交互解决模糊问题的能力，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要通过重述问题解决VQA中的模糊性，忽略了用户与VLMs的交互性。缺乏评估VLMs通过交互解决模糊性的基准，且VLMs倾向于回答而非提问。

Method: 引入ClearVQA基准，针对VQA中三类常见模糊性，涵盖多种VQA场景。

Result: ClearVQA基准填补了评估VLMs交互解决模糊性能力的空白。

Conclusion: ClearVQA为研究VLMs的交互性模糊解决提供了新工具，推动相关领域发展。

Abstract: In visual question answering (VQA) context, users often pose ambiguous
questions to visual language models (VLMs) due to varying expression habits.
Existing research addresses such ambiguities primarily by rephrasing questions.
These approaches neglect the inherently interactive nature of user interactions
with VLMs, where ambiguities can be clarified through user feedback. However,
research on interactive clarification faces two major challenges: (1)
Benchmarks are absent to assess VLMs' capacity for resolving ambiguities
through interaction; (2) VLMs are trained to prefer answering rather than
asking, preventing them from seeking clarification. To overcome these
challenges, we introduce \textbf{ClearVQA} benchmark, which targets three
common categories of ambiguity in VQA context, and encompasses various VQA
scenarios.

</details>


### [183] [CaSTFormer: Causal Spatio-Temporal Transformer for Driving Intention Prediction](https://arxiv.org/abs/2507.13425)
*Sirui Wang,Zhou Guan,Bingxi Zhao,Tongjia Gu*

Main category: cs.CV

TL;DR: CaSTFormer是一种因果时空变换器，用于建模驾驶员行为与环境之间的因果关系，提升驾驶意图预测的准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 当前方法难以准确建模复杂的时空依赖性和人类驾驶行为的不可预测性，因此需要一种新方法来提升预测性能。

Method: CaSTFormer通过Reciprocal Shift Fusion机制对齐特征流，Causal Pattern Extraction模块消除虚假相关性，以及Feature Synthesis Network合成纯净表示。

Result: 在Brain4Cars数据集上，CaSTFormer实现了最先进的性能，有效捕捉了复杂的因果时空依赖。

Conclusion: CaSTFormer显著提升了驾驶意图预测的准确性和透明度，为高级自动驾驶提供了可靠支持。

Abstract: Accurate prediction of driving intention is key to enhancing the safety and
interactive efficiency of human-machine co-driving systems. It serves as a
cornerstone for achieving high-level autonomous driving. However, current
approaches remain inadequate for accurately modeling the complex
spatio-temporal interdependencies and the unpredictable variability of human
driving behavior. To address these challenges, we propose CaSTFormer, a Causal
Spatio-Temporal Transformer to explicitly model causal interactions between
driver behavior and environmental context for robust intention prediction.
Specifically, CaSTFormer introduces a novel Reciprocal Shift Fusion (RSF)
mechanism for precise temporal alignment of internal and external feature
streams, a Causal Pattern Extraction (CPE) module that systematically
eliminates spurious correlations to reveal authentic causal dependencies, and
an innovative Feature Synthesis Network (FSN) that adaptively synthesizes these
purified representations into coherent spatio-temporal inferences. We evaluate
the proposed CaSTFormer on the public Brain4Cars dataset, and it achieves
state-of-the-art performance. It effectively captures complex causal
spatio-temporal dependencies and enhances both the accuracy and transparency of
driving intention prediction.

</details>


### [184] ["PhyWorldBench": A Comprehensive Evaluation of Physical Realism in Text-to-Video Models](https://arxiv.org/abs/2507.13428)
*Jing Gu,Xian Liu,Yu Zeng,Ashwin Nagarajan,Fangrui Zhu,Daniel Hong,Yue Fan,Qianqi Yan,Kaiwen Zhou,Ming-Yu Liu,Xin Eric Wang*

Main category: cs.CV

TL;DR: PhyWorldBench是一个评估视频生成模型物理模拟能力的基准，涵盖从基础物理现象到复杂场景，并引入“反物理”类别。通过人类评估和MLLM方法，测试了12个先进模型，揭示了其在物理一致性上的挑战。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型在物理现象模拟方面存在不足，需要系统评估其物理一致性。

Method: 设计了PhyWorldBench基准，包含多级物理现象和“反物理”类别，结合人类评估和MLLM方法测试了12个模型。

Result: 模型在物理一致性上表现不一，揭示了关键挑战，并提出了优化提示的建议。

Conclusion: PhyWorldBench为视频生成模型的物理模拟能力提供了评估框架，并指出了改进方向。

Abstract: Video generation models have achieved remarkable progress in creating
high-quality, photorealistic content. However, their ability to accurately
simulate physical phenomena remains a critical and unresolved challenge. This
paper presents PhyWorldBench, a comprehensive benchmark designed to evaluate
video generation models based on their adherence to the laws of physics. The
benchmark covers multiple levels of physical phenomena, ranging from
fundamental principles like object motion and energy conservation to more
complex scenarios involving rigid body interactions and human or animal motion.
Additionally, we introduce a novel ""Anti-Physics"" category, where prompts
intentionally violate real-world physics, enabling the assessment of whether
models can follow such instructions while maintaining logical consistency.
Besides large-scale human evaluation, we also design a simple yet effective
method that could utilize current MLLM to evaluate the physics realism in a
zero-shot fashion. We evaluate 12 state-of-the-art text-to-video generation
models, including five open-source and five proprietary models, with a detailed
comparison and analysis. we identify pivotal challenges models face in adhering
to real-world physics. Through systematic testing of their outputs across 1,050
curated prompts-spanning fundamental, composite, and anti-physics scenarios-we
identify pivotal challenges these models face in adhering to real-world
physics. We then rigorously examine their performance on diverse physical
phenomena with varying prompt types, deriving targeted recommendations for
crafting prompts that enhance fidelity to physical principles.

</details>


### [185] [NoHumansRequired: Autonomous High-Quality Image Editing Triplet Mining](https://arxiv.org/abs/2507.14119)
*Maksim Kuprashevich,Grigorii Alekseenko,Irina Tolstykh,Georgii Fedorov,Bulat Suleimanov,Vladimir Dokholyan,Aleksandr Gordeev*

Main category: cs.CV

TL;DR: 提出了一种自动化、模块化的流程，用于从多领域、多分辨率、多风格的图像中挖掘高质量的三元组（原始图像、指令、编辑后图像），并发布了NHR-Edit数据集和Bagel-NHR-Edit模型。


<details>
  <summary>Details</summary>
Motivation: 解决生成式模型中高质量训练数据难以自动化获取的问题，尤其是需要满足指令遵循、风格一致性和视觉吸引力等多重要求。

Method: 利用公共生成模型和任务优化的Gemini验证器，通过反转和组合引导扩大数据集规模，无需人工干预或分割模型。

Result: 生成了358k高质量三元组数据集NHR-Edit，并在跨数据集评估中表现最佳；Bagel-NHR-Edit模型在实验中达到SOTA性能。

Conclusion: 自动化流程显著提升了训练数据的规模和质量，推动了生成式图像编辑领域的研究，并通过开源数据集和模型促进了资源民主化。

Abstract: Recent advances in generative modeling enable image editing assistants that
follow natural language instructions without additional user input. Their
supervised training requires millions of triplets: original image, instruction,
edited image. Yet mining pixel-accurate examples is hard. Each edit must affect
only prompt-specified regions, preserve stylistic coherence, respect physical
plausibility, and retain visual appeal. The lack of robust automated
edit-quality metrics hinders reliable automation at scale. We present an
automated, modular pipeline that mines high-fidelity triplets across domains,
resolutions, instruction complexities, and styles. Built on public generative
models and running without human intervention, our system uses a task-tuned
Gemini validator to score instruction adherence and aesthetics directly,
removing any need for segmentation or grounding models. Inversion and
compositional bootstrapping enlarge the mined set by approximately 2.2x,
enabling large-scale high-fidelity training data. By automating the most
repetitive annotation steps, the approach allows a new scale of training
without human labeling effort. To democratize research in this
resource-intensive area, we release NHR-Edit: an open dataset of 358k
high-quality triplets. In the largest cross-dataset evaluation, it surpasses
all public alternatives. We also release Bagel-NHR-Edit, an open-source
fine-tuned Bagel model, which achieves state-of-the-art metrics in our
experiments.

</details>


### [186] [Using Multiple Input Modalities Can Improve Data-Efficiency and O.O.D. Generalization for ML with Satellite Imagery](https://arxiv.org/abs/2507.13385)
*Arjun Rao,Esther Rolf*

Main category: cs.CV

TL;DR: 论文研究了在卫星图像机器学习任务中融合其他地理数据层对模型性能的影响，发现多模态输入能显著提升性能，尤其在数据有限和跨区域场景下。


<details>
  <summary>Details</summary>
Motivation: 探索在卫星图像机器学习任务中，结合其他地理数据层是否能提升模型性能，特别是在数据有限和跨区域场景下。

Method: 通过生成增强版本的卫星图像基准任务，将其他地理数据层附加到分类、回归和分割数据集中，比较不同融合策略的效果。

Result: 多模态输入显著提升了模型性能，尤其在数据有限和跨区域场景下；硬编码融合策略优于学习型策略。

Conclusion: 多模态输入对卫星图像机器学习任务的数据效率和跨区域性能具有重要价值，硬编码融合策略值得进一步研究。

Abstract: A large variety of geospatial data layers is available around the world
ranging from remotely-sensed raster data like satellite imagery, digital
elevation models, predicted land cover maps, and human-annotated data, to data
derived from environmental sensors such as air temperature or wind speed data.
A large majority of machine learning models trained on satellite imagery
(SatML), however, are designed primarily for optical input modalities such as
multi-spectral satellite imagery. To better understand the value of using other
input modalities alongside optical imagery in supervised learning settings, we
generate augmented versions of SatML benchmark tasks by appending additional
geographic data layers to datasets spanning classification, regression, and
segmentation. Using these augmented datasets, we find that fusing additional
geographic inputs with optical imagery can significantly improve SatML model
performance. Benefits are largest in settings where labeled data are limited
and in geographic out-of-sample settings, suggesting that multi-modal inputs
may be especially valuable for data-efficiency and out-of-sample performance of
SatML models. Surprisingly, we find that hard-coded fusion strategies
outperform learned variants, with interesting implications for future work.

</details>


### [187] [Minimalist Concept Erasure in Generative Models](https://arxiv.org/abs/2507.13386)
*Yang Zhang,Er Jin,Yanfei Dong,Yixuan Wu,Philip Torr,Ashkan Khakzar,Johannes Stegmaier,Kenji Kawaguchi*

Main category: cs.CV

TL;DR: 提出了一种基于生成输出分布距离的最小化概念擦除方法，通过端到端优化和神经元掩码技术，在不影响模型性能的情况下实现稳健的概念擦除。


<details>
  <summary>Details</summary>
Motivation: 生成模型依赖大规模无标签数据引发安全和版权问题，现有擦除方法过度修改模型导致性能下降。

Method: 基于生成输出分布距离设计最小化擦除目标，通过端到端反向传播优化，并引入神经元掩码增强鲁棒性。

Result: 在先进流匹配模型上验证，方法能稳健擦除概念且不降低模型整体性能。

Conclusion: 为更安全、负责任的生成模型提供了可行路径。

Abstract: Recent advances in generative models have demonstrated remarkable
capabilities in producing high-quality images, but their reliance on
large-scale unlabeled data has raised significant safety and copyright
concerns. Efforts to address these issues by erasing unwanted concepts have
shown promise. However, many existing erasure methods involve excessive
modifications that compromise the overall utility of the model. In this work,
we address these issues by formulating a novel minimalist concept erasure
objective based \emph{only} on the distributional distance of final generation
outputs. Building on our formulation, we derive a tractable loss for
differentiable optimization that leverages backpropagation through all
generation steps in an end-to-end manner. We also conduct extensive analysis to
show theoretical connections with other models and methods. To improve the
robustness of the erasure, we incorporate neuron masking as an alternative to
model fine-tuning. Empirical evaluations on state-of-the-art flow-matching
models demonstrate that our method robustly erases concepts without degrading
overall model performance, paving the way for safer and more responsible
generative models.

</details>


### [188] [MADI: Masking-Augmented Diffusion with Inference-Time Scaling for Visual Editing](https://arxiv.org/abs/2507.13401)
*Shreya Kadambi,Risheek Garrepalli,Shubhankar Borse,Munawar Hyatt,Fatih Porikli*

Main category: cs.CV

TL;DR: 论文提出了MADI框架，通过Masking-Augmented gaussian Diffusion（MAgD）和推理时容量扩展机制，显著提升了扩散模型的可编辑性和可控性。


<details>
  <summary>Details</summary>
Motivation: 尽管扩散模型在文本到图像生成方面取得了显著成功，但在基于视觉的编辑和组合控制方面仍存在挑战。

Method: 提出MADI框架，包括MAgD训练策略（结合去噪和掩码重建）和推理时容量扩展机制（使用Pause Tokens）。

Result: MADI显著提升了扩散模型的编辑能力和组合性，特别是在局部和结构化编辑方面表现优异。

Conclusion: MADI为扩散模型的可控生成和编辑提供了新思路，有望推动其在通用生成架构中的应用。

Abstract: Despite the remarkable success of diffusion models in text-to-image
generation, their effectiveness in grounded visual editing and compositional
control remains challenging. Motivated by advances in self-supervised learning
and in-context generative modeling, we propose a series of simple yet powerful
design choices that significantly enhance diffusion model capacity for
structured, controllable generation and editing. We introduce Masking-Augmented
Diffusion with Inference-Time Scaling (MADI), a framework that improves the
editability, compositionality and controllability of diffusion models through
two core innovations. First, we introduce Masking-Augmented gaussian Diffusion
(MAgD), a novel training strategy with dual corruption process which combines
standard denoising score matching and masked reconstruction by masking noisy
input from forward process. MAgD encourages the model to learn discriminative
and compositional visual representations, thus enabling localized and
structure-aware editing. Second, we introduce an inference-time capacity
scaling mechanism based on Pause Tokens, which act as special placeholders
inserted into the prompt for increasing computational capacity at inference
time. Our findings show that adopting expressive and dense prompts during
training further enhances performance, particularly for MAgD. Together, these
contributions in MADI substantially enhance the editability of diffusion
models, paving the way toward their integration into more general-purpose,
in-context generative diffusion architectures.

</details>


### [189] [UL-DD: A Multimodal Drowsiness Dataset Using Video, Biometric Signals, and Behavioral Data](https://arxiv.org/abs/2507.13403)
*Morteza Bodaghi,Majid Hosseini,Raju Gottumukkala,Ravi Teja Bhupatiraju,Iftikhar Ahmad,Moncef Gabbouj*

Main category: cs.CV

TL;DR: 该研究提供了一个多模态的驾驶员 drowsiness 检测数据集，包含面部、行为和生物特征信号，数据来自19名受试者在警觉和 drowsy 状态下的40分钟连续记录。


<details>
  <summary>Details</summary>
Motivation: 创建更全面的驾驶员 drowsiness 数据集，捕捉生理、行为和驾驶相关信号的连续变化。

Method: 整合3D面部视频、IR摄像头、后视视频、生物特征信号（如心率、皮肤电活动等）以及方向盘握力传感器和模拟器数据，使用KSS量表每4分钟自评 drowsiness 水平。

Result: 生成了一个包含1,400分钟连续数据的多模态数据集，记录了驾驶员状态的渐变而非离散标签。

Conclusion: 该数据集为驾驶员 drowsiness 研究提供了更全面的多模态信号支持，未来可进一步用于算法开发和验证。

Abstract: In this study, we present a comprehensive public dataset for driver
drowsiness detection, integrating multimodal signals of facial, behavioral, and
biometric indicators. Our dataset includes 3D facial video using a depth
camera, IR camera footage, posterior videos, and biometric signals such as
heart rate, electrodermal activity, blood oxygen saturation, skin temperature,
and accelerometer data. This data set provides grip sensor data from the
steering wheel and telemetry data from the American truck simulator game to
provide more information about drivers' behavior while they are alert and
drowsy. Drowsiness levels were self-reported every four minutes using the
Karolinska Sleepiness Scale (KSS). The simulation environment consists of three
monitor setups, and the driving condition is completely like a car. Data were
collected from 19 subjects (15 M, 4 F) in two conditions: when they were fully
alert and when they exhibited signs of sleepiness. Unlike other datasets, our
multimodal dataset has a continuous duration of 40 minutes for each data
collection session per subject, contributing to a total length of 1,400
minutes, and we recorded gradual changes in the driver state rather than
discrete alert/drowsy labels. This study aims to create a comprehensive
multimodal dataset of driver drowsiness that captures a wider range of
physiological, behavioral, and driving-related signals. The dataset will be
available upon request to the corresponding author.

</details>


### [190] [COREVQA: A Crowd Observation and Reasoning Entailment Visual Question Answering Benchmark](https://arxiv.org/abs/2507.13405)
*Ishant Chintapatla,Kazuma Choji,Naaisha Agarwal,Andrew Lin,Hannah You,Charles Duong,Kevin Zhu,Sean O'Brien,Vasu Sharma*

Main category: cs.CV

TL;DR: 研究者提出了COREVQA基准，用于评估视觉语言模型在拥挤场景中的视觉蕴含推理能力，结果显示现有模型表现不佳。


<details>
  <summary>Details</summary>
Motivation: 现有基准很少测试模型在视觉蕴含任务中的能力，尤其是在拥挤场景下。

Method: 使用CrowdHuman数据集生成5608对图像和真假陈述对，构建COREVQA基准。

Result: 即使表现最好的模型准确率低于80%，其他模型表现更差（39.98%-69.95%）。

Conclusion: 视觉语言模型在拥挤场景的视觉蕴含推理能力存在显著不足。

Abstract: Recently, many benchmarks and datasets have been developed to evaluate
Vision-Language Models (VLMs) using visual question answering (VQA) pairs, and
models have shown significant accuracy improvements. However, these benchmarks
rarely test the model's ability to accurately complete visual entailment, for
instance, accepting or refuting a hypothesis based on the image. To address
this, we propose COREVQA (Crowd Observations and Reasoning Entailment), a
benchmark of 5608 image and synthetically generated true/false statement pairs,
with images derived from the CrowdHuman dataset, to provoke visual entailment
reasoning on challenging crowded images. Our results show that even the
top-performing VLMs achieve accuracy below 80%, with other models performing
substantially worse (39.98%-69.95%). This significant performance gap reveals
key limitations in VLMs' ability to reason over certain types of image-question
pairs in crowded scenes.

</details>


### [191] [When Person Re-Identification Meets Event Camera: A Benchmark Dataset and An Attribute-guided Re-Identification Framework](https://arxiv.org/abs/2507.13659)
*Xiao Wang,Qian Zhu,Shujuan Wu,Bo Jiang,Shiliang Zhang,Yaowei Wang,Yonghong Tian,Bin Luo*

Main category: cs.CV

TL;DR: 论文提出一个大规模RGB-事件相机数据集EvReID，并基于此提出TriPro-ReID框架，结合行人属性和对比学习提升ReID性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有事件相机ReID方法数据稀缺、评估受限的问题，推动实际应用。

Method: 构建EvReID数据集，提出TriPro-ReID框架，融合RGB和事件流特征，利用行人属性辅助学习。

Result: 在EvReID和MARS数据集上验证了TriPro-ReID的有效性。

Conclusion: EvReID数据集和TriPro-ReID框架为事件相机ReID研究提供了数据和基准支持。

Abstract: Recent researchers have proposed using event cameras for person
re-identification (ReID) due to their promising performance and better balance
in terms of privacy protection, event camera-based person ReID has attracted
significant attention. Currently, mainstream event-based person ReID algorithms
primarily focus on fusing visible light and event stream, as well as preserving
privacy. Although significant progress has been made, these methods are
typically trained and evaluated on small-scale or simulated event camera
datasets, making it difficult to assess their real identification performance
and generalization ability. To address the issue of data scarcity, this paper
introduces a large-scale RGB-event based person ReID dataset, called EvReID.
The dataset contains 118,988 image pairs and covers 1200 pedestrian identities,
with data collected across multiple seasons, scenes, and lighting conditions.
We also evaluate 15 state-of-the-art person ReID algorithms, laying a solid
foundation for future research in terms of both data and benchmarking. Based on
our newly constructed dataset, this paper further proposes a pedestrian
attribute-guided contrastive learning framework to enhance feature learning for
person re-identification, termed TriPro-ReID. This framework not only
effectively explores the visual features from both RGB frames and event
streams, but also fully utilizes pedestrian attributes as mid-level semantic
features. Extensive experiments on the EvReID dataset and MARS datasets fully
validated the effectiveness of our proposed RGB-Event person ReID framework.
The benchmark dataset and source code will be released on
https://github.com/Event-AHU/Neuromorphic_ReID

</details>


### [192] [Sugar-Beet Stress Detection using Satellite Image Time Series](https://arxiv.org/abs/2507.13514)
*Bhumika Laxman Sadbhave,Philipp Vaeth,Denise Dejon,Gunther Schorcht,Magda Gregorová*

Main category: cs.CV

TL;DR: 使用3D卷积自编码器和时间编码的无监督方法，从Sentinel-2卫星图像序列中检测甜菜田的压力状态。


<details>
  <summary>Details</summary>
Motivation: 卫星图像时间序列（SITS）数据因其丰富的光谱和时间信息，适合农业任务，尤其是甜菜田的压力检测。

Method: 提出一种3D卷积自编码器模型，结合时间编码，从Sentinel-2图像序列中提取特征，并通过聚类任务区分压力田和健康田。

Result: 该系统可直接应用于不同年份的数据，为甜菜田压力检测提供实用工具。

Conclusion: 无监督方法在甜菜田压力检测中具有实际应用价值。

Abstract: Satellite Image Time Series (SITS) data has proven effective for agricultural
tasks due to its rich spectral and temporal nature. In this study, we tackle
the task of stress detection in sugar-beet fields using a fully unsupervised
approach. We propose a 3D convolutional autoencoder model to extract meaningful
features from Sentinel-2 image sequences, combined with
acquisition-date-specific temporal encodings to better capture the growth
dynamics of sugar-beets. The learned representations are used in a downstream
clustering task to separate stressed from healthy fields. The resulting stress
detection system can be directly applied to data from different years, offering
a practical and accessible tool for stress detection in sugar-beets.

</details>


### [193] [HeCoFuse: Cross-Modal Complementary V2X Cooperative Perception with Heterogeneous Sensors](https://arxiv.org/abs/2507.13677)
*Chuheng Wei,Ziye Qin,Walter Zimmer,Guoyuan Wu,Matthew J. Barth*

Main category: cs.CV

TL;DR: HeCoFuse是一个统一的框架，用于解决异构传感器配置下的V2X协同感知问题，通过分层融合机制和自适应学习策略，显著提升了感知性能。


<details>
  <summary>Details</summary>
Motivation: 现实中的V2X协同感知系统因传感器配置异构性导致特征融合和感知可靠性问题，亟需一种通用解决方案。

Method: 提出HeCoFuse框架，采用分层融合机制（通道和空间注意力）和自适应空间分辨率调整模块，并动态调整融合类型。

Result: 在TUMTraf-V2X数据集上，HeCoFuse在多种传感器配置下表现优异，3D mAP最高达43.38%，优于基线方法。

Conclusion: HeCoFuse在异构传感器配置下表现出色，成为当前V2X协同感知的先进方法。

Abstract: Real-world Vehicle-to-Everything (V2X) cooperative perception systems often
operate under heterogeneous sensor configurations due to cost constraints and
deployment variability across vehicles and infrastructure. This heterogeneity
poses significant challenges for feature fusion and perception reliability. To
address these issues, we propose HeCoFuse, a unified framework designed for
cooperative perception across mixed sensor setups where nodes may carry Cameras
(C), LiDARs (L), or both. By introducing a hierarchical fusion mechanism that
adaptively weights features through a combination of channel-wise and spatial
attention, HeCoFuse can tackle critical challenges such as cross-modality
feature misalignment and imbalanced representation quality. In addition, an
adaptive spatial resolution adjustment module is employed to balance
computational cost and fusion effectiveness. To enhance robustness across
different configurations, we further implement a cooperative learning strategy
that dynamically adjusts fusion type based on available modalities. Experiments
on the real-world TUMTraf-V2X dataset demonstrate that HeCoFuse achieves 43.22%
3D mAP under the full sensor configuration (LC+LC), outperforming the CoopDet3D
baseline by 1.17%, and reaches an even higher 43.38% 3D mAP in the L+LC
scenario, while maintaining 3D mAP in the range of 21.74% to 43.38% across nine
heterogeneous sensor configurations. These results, validated by our
first-place finish in the CVPR 2025 DriveX challenge, establish HeCoFuse as the
current state-of-the-art on TUM-Traf V2X dataset while demonstrating robust
performance across diverse sensor deployments.

</details>


### [194] [Can Synthetic Images Conquer Forgetting? Beyond Unexplored Doubts in Few-Shot Class-Incremental Learning](https://arxiv.org/abs/2507.13739)
*Junsu Kim,Yunhoe Ku,Seungryul Baek*

Main category: cs.CV

TL;DR: Diffusion-FSCIL利用冻结的文本到图像扩散模型解决少样本类增量学习问题，通过多尺度特征提取和潜在重放提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决少样本类增量学习中的数据不足问题，同时减少灾难性遗忘并学习新信息。

Method: 使用冻结的扩散模型作为主干，提取多尺度特征，结合潜在重放和特征蒸馏。

Result: 在CUB-200、miniImageNet和CIFAR-100上超越现有方法，保持旧类性能并适应新类。

Conclusion: Diffusion-FSCIL通过生成模型的能力和高效设计，有效解决了少样本类增量学习的挑战。

Abstract: Few-shot class-incremental learning (FSCIL) is challenging due to extremely
limited training data; while aiming to reduce catastrophic forgetting and learn
new information. We propose Diffusion-FSCIL, a novel approach that employs a
text-to-image diffusion model as a frozen backbone. Our conjecture is that
FSCIL can be tackled using a large generative model's capabilities benefiting
from 1) generation ability via large-scale pre-training; 2) multi-scale
representation; 3) representational flexibility through the text encoder. To
maximize the representation capability, we propose to extract multiple
complementary diffusion features to play roles as latent replay with slight
support from feature distillation for preventing generative biases. Our
framework realizes efficiency through 1) using a frozen backbone; 2) minimal
trainable components; 3) batch processing of multiple feature extractions.
Extensive experiments on CUB-200, \emph{mini}ImageNet, and CIFAR-100 show that
Diffusion-FSCIL surpasses state-of-the-art methods, preserving performance on
previously learned classes and adapting effectively to new ones.

</details>


### [195] [Learning Spectral Diffusion Prior for Hyperspectral Image Reconstruction](https://arxiv.org/abs/2507.13769)
*Mingyang Yu,Zhijian Wu,Dingjiang Huang*

Main category: cs.CV

TL;DR: 提出了一种基于扩散模型的光谱扩散先验（SDP）和光谱先验注入模块（SPIM），显著提升了高光谱图像（HSI）重建的性能，实验结果显示优于现有方法约0.5 dB。


<details>
  <summary>Details</summary>
Motivation: 当前深度学习方法在高光谱图像重建中难以准确捕捉高频细节，需要一种更有效的方法来提升重建质量。

Method: 通过扩散模型隐式学习光谱扩散先验（SDP），并结合动态指导的光谱先验注入模块（SPIM）来恢复HSI细节。

Result: 在MST和BISRNet两种代表性HSI方法上测试，性能优于现有网络约0.5 dB。

Conclusion: 提出的SDP和SPIM方法显著提升了HSI重建的细节恢复能力，为高光谱图像重建提供了新思路。

Abstract: Hyperspectral image (HSI) reconstruction aims to recover 3D HSI from its
degraded 2D measurements. Recently great progress has been made in deep
learning-based methods, however, these methods often struggle to accurately
capture high-frequency details of the HSI. To address this issue, this paper
proposes a Spectral Diffusion Prior (SDP) that is implicitly learned from
hyperspectral images using a diffusion model. Leveraging the powerful ability
of the diffusion model to reconstruct details, this learned prior can
significantly improve the performance when injected into the HSI model. To
further improve the effectiveness of the learned prior, we also propose the
Spectral Prior Injector Module (SPIM) to dynamically guide the model to recover
the HSI details. We evaluate our method on two representative HSI methods: MST
and BISRNet. Experimental results show that our method outperforms existing
networks by about 0.5 dB, effectively improving the performance of HSI
reconstruction.

</details>


### [196] [Localized FNO for Spatiotemporal Hemodynamic Upsampling in Aneurysm MRI](https://arxiv.org/abs/2507.13789)
*Kyriakos Flouris,Moritz Halter,Yolanne Y. R. Lee,Samuel Castonguay,Luuk Jacobs,Pietro Dirix,Jonathan Nestmann,Sebastian Kozerke,Ender Konukoglu*

Main category: cs.CV

TL;DR: 提出了一种名为LoFNO的新型3D架构，用于提高血流动力学分析的空间和时间分辨率，直接从临床影像数据预测壁剪切应力（WSS）。


<details>
  <summary>Details</summary>
Motivation: 磁共振血流成像的低时空分辨率和信噪比限制了其诊断价值，需要一种更精确的方法来预测动脉瘤破裂和指导治疗。

Method: LoFNO结合拉普拉斯特征向量作为几何先验，增强对不规则几何结构的感知，并采用EDSR层进行稳健的上采样。

Result: LoFNO在去噪和时空上采样方面表现优异，速度和WSS预测优于插值和其他深度学习方法。

Conclusion: LoFNO为脑血管诊断提供了更精确的工具，具有临床应用的潜力。

Abstract: Hemodynamic analysis is essential for predicting aneurysm rupture and guiding
treatment. While magnetic resonance flow imaging enables time-resolved
volumetric blood velocity measurements, its low spatiotemporal resolution and
signal-to-noise ratio limit its diagnostic utility. To address this, we propose
the Localized Fourier Neural Operator (LoFNO), a novel 3D architecture that
enhances both spatial and temporal resolution with the ability to predict wall
shear stress (WSS) directly from clinical imaging data. LoFNO integrates
Laplacian eigenvectors as geometric priors for improved structural awareness on
irregular, unseen geometries and employs an Enhanced Deep Super-Resolution
Network (EDSR) layer for robust upsampling. By combining geometric priors with
neural operator frameworks, LoFNO de-noises and spatiotemporally upsamples flow
data, achieving superior velocity and WSS predictions compared to interpolation
and alternative deep learning methods, enabling more precise cerebrovascular
diagnostics.

</details>


### [197] [One Step Closer: Creating the Future to Boost Monocular Semantic Scene Completion](https://arxiv.org/abs/2507.13801)
*Haoang Lu,Yuanqi Su,Xiaoning Zhang,Hao Hu*

Main category: cs.CV

TL;DR: 论文提出了一种新颖的时序3D语义场景补全框架CF-SSC，通过预测伪未来帧扩展感知范围，显著提升了遮挡推理和场景补全的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有单目SSC方法在真实交通场景中难以处理遮挡和视野外区域的问题，需要一种更有效的方法来扩展感知范围。

Method: 提出CF-SSC框架，结合位姿和深度信息建立3D对应关系，实现过去、当前和预测未来帧的几何一致性融合，并显式建模时空关系。

Result: 在SemanticKITTI和SSCBench-KITTI-360基准测试中达到最先进性能，验证了方法的有效性。

Conclusion: CF-SSC通过时空建模和伪未来帧预测，显著提升了3D场景补全的准确性和鲁棒性。

Abstract: In recent years, visual 3D Semantic Scene Completion (SSC) has emerged as a
critical perception task for autonomous driving due to its ability to infer
complete 3D scene layouts and semantics from single 2D images. However, in
real-world traffic scenarios, a significant portion of the scene remains
occluded or outside the camera's field of view -- a fundamental challenge that
existing monocular SSC methods fail to address adequately. To overcome these
limitations, we propose Creating the Future SSC (CF-SSC), a novel temporal SSC
framework that leverages pseudo-future frame prediction to expand the model's
effective perceptual range. Our approach combines poses and depths to establish
accurate 3D correspondences, enabling geometrically-consistent fusion of past,
present, and predicted future frames in 3D space. Unlike conventional methods
that rely on simple feature stacking, our 3D-aware architecture achieves more
robust scene completion by explicitly modeling spatial-temporal relationships.
Comprehensive experiments on SemanticKITTI and SSCBench-KITTI-360 benchmarks
demonstrate state-of-the-art performance, validating the effectiveness of our
approach, highlighting our method's ability to improve occlusion reasoning and
3D scene completion accuracy.

</details>


### [198] [Team of One: Cracking Complex Video QA with Model Synergy](https://arxiv.org/abs/2507.13820)
*Jun Xie,Zhaoran Zhao,Xiongjun Guan,Yingjian Zhu,Hongzhu Yi,Xinming Wang,Feng Chen,Zhepeng Wang*

Main category: cs.CV

TL;DR: 提出了一种新颖的开放视频问答框架，通过多模型协作和外部LLM集成，显著提升了推理深度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有视频-语言模型在复杂场景中表现不足，如上下文理解有限、时间建模弱和对模糊或组合查询泛化能力差。

Method: 采用提示-响应集成机制，协调多个异构视频-语言模型，通过结构化思维链进行推理，并由外部LLM作为评估和集成器。

Result: 实验表明，该方法在所有评估指标上显著优于现有基线，表现出更强的泛化和鲁棒性。

Conclusion: 该方法为多模态推理提供了一种轻量级、可扩展的策略，无需重新训练模型，为未来视频-语言模型发展奠定了基础。

Abstract: We propose a novel framework for open-ended video question answering that
enhances reasoning depth and robustness in complex real-world scenarios, as
benchmarked on the CVRR-ES dataset. Existing Video-Large Multimodal Models
(Video-LMMs) often exhibit limited contextual understanding, weak temporal
modeling, and poor generalization to ambiguous or compositional queries. To
address these challenges, we introduce a prompting-and-response integration
mechanism that coordinates multiple heterogeneous Video-Language Models (VLMs)
via structured chains of thought, each tailored to distinct reasoning pathways.
An external Large Language Model (LLM) serves as an evaluator and integrator,
selecting and fusing the most reliable responses. Extensive experiments
demonstrate that our method significantly outperforms existing baselines across
all evaluation metrics, showcasing superior generalization and robustness. Our
approach offers a lightweight, extensible strategy for advancing multimodal
reasoning without requiring model retraining, setting a strong foundation for
future Video-LMM development.

</details>


### [199] [Tackling fake images in cybersecurity -- Interpretation of a StyleGAN and lifting its black-box](https://arxiv.org/abs/2507.13722)
*Julia Laubmann,Johannes Reschke*

Main category: cs.CV

TL;DR: 分析StyleGAN生成器的内部机制，揭示其权重可修剪性及潜在伦理风险。


<details>
  <summary>Details</summary>
Motivation: 探索StyleGAN生成器的运作方式，理解其权重和潜在向量的影响，同时关注技术可能被滥用的伦理问题。

Method: 使用PyTorch训练StyleGAN模型，分析其架构（如Equalized Learning Rate），并通过修剪权重和调整潜在向量研究其行为。

Result: 发现大量权重可修剪而不显著影响输出，潜在向量的全局和局部调整分别影响颜色和具体面部特征。

Conclusion: StyleGAN的高效性和可控性带来学术价值，但也凸显了技术被滥用于伪造身份等伦理风险。

Abstract: In today's digital age, concerns about the dangers of AI-generated images are
increasingly common. One powerful tool in this domain is StyleGAN (style-based
generative adversarial networks), a generative adversarial network capable of
producing highly realistic synthetic faces. To gain a deeper understanding of
how such a model operates, this work focuses on analyzing the inner workings of
StyleGAN's generator component. Key architectural elements and techniques, such
as the Equalized Learning Rate, are explored in detail to shed light on the
model's behavior. A StyleGAN model is trained using the PyTorch framework,
enabling direct inspection of its learned weights. Through pruning, it is
revealed that a significant number of these weights can be removed without
drastically affecting the output, leading to reduced computational
requirements. Moreover, the role of the latent vector -- which heavily
influences the appearance of the generated faces -- is closely examined. Global
alterations to this vector primarily affect aspects like color tones, while
targeted changes to individual dimensions allow for precise manipulation of
specific facial features. This ability to finetune visual traits is not only of
academic interest but also highlights a serious ethical concern: the potential
misuse of such technology. Malicious actors could exploit this capability to
fabricate convincing fake identities, posing significant risks in the context
of digital deception and cybercrime.

</details>


### [200] [When Seeing Overrides Knowing: Disentangling Knowledge Conflicts in Vision-Language Models](https://arxiv.org/abs/2507.13868)
*Francesco Ortu,Zhijing Jin,Diego Doimo,Alberto Cazzaniga*

Main category: cs.CV

TL;DR: 研究分析了视觉语言模型（VLMs）在解决跨模态知识冲突时的机制，通过引入多模态反事实查询数据集，定位并修改控制冲突的特定注意力头，从而引导模型偏向内部知识或视觉输入。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在处理复杂任务时，其内部参数知识与外部信息可能产生冲突，导致幻觉和不可靠响应。目前这种冲突的机制尚不明确，因此需要深入研究。

Method: 引入多模态反事实查询数据集，通过logit检查定位控制冲突的注意力头，并修改这些头以引导模型行为。

Result: 研究发现少量注意力头控制冲突，修改这些头可以引导模型偏向内部知识或视觉输入。此外，这些头的注意力能更精确地定位驱动视觉覆盖的图像区域。

Conclusion: 研究揭示了VLMs解决知识冲突的机制，并提出了一种通过修改注意力头来引导模型行为的方法，为模型可靠性提供了新思路。

Abstract: Vision-language models (VLMs) increasingly leverage diverse knowledge sources
to address complex tasks, often encountering conflicts between their internal
parametric knowledge and external information. Knowledge conflicts can result
in hallucinations and unreliable responses, but the mechanisms governing such
interactions remain unknown. To address this gap, we analyze the mechanisms
that VLMs use to resolve cross-modal conflicts by introducing a dataset of
multimodal counterfactual queries that deliberately contradict internal
commonsense knowledge. We localize with logit inspection a small set of heads
that control the conflict. Moreover, by modifying these heads, we can steer the
model towards its internal knowledge or the visual inputs. Finally, we show
that attention from such heads pinpoints localized image regions driving visual
overrides, outperforming gradient-based attribution in precision.

</details>


### [201] [Feature Engineering is Not Dead: Reviving Classical Machine Learning with Entropy, HOG, and LBP Feature Fusion for Image Classification](https://arxiv.org/abs/2507.13772)
*Abhijit Sen,Giridas Maiti,Bikram K. Parida,Bhanu P. Mishra,Mahima Arya,Denys I. Bondar*

Main category: cs.CV

TL;DR: 该研究提出了一种基于排列熵（PE）的新型图像分类方法，结合HOG和LBP特征，训练SVM分类器，在多个基准数据集上表现优异，提供了一种轻量且可解释的替代方案。


<details>
  <summary>Details</summary>
Motivation: 在图像分类中，当需要优先考虑可解释性和计算效率而非深度学习模型的庞大参数时，特征工程仍然至关重要。

Method: 扩展PE到二维图像，提出多尺度、多方向的熵特征提取方法，结合HOG和LBP特征，形成780维特征集，训练优化后的SVM分类器。

Result: 在Fashion-MNIST等数据集上表现优异，证明了PE与HOG、LBP结合的有效性。

Conclusion: 熵基描述符在图像分类中具有潜力，提供了一种轻量、可解释且通用的解决方案。

Abstract: Feature engineering continues to play a critical role in image
classification, particularly when interpretability and computational efficiency
are prioritized over deep learning models with millions of parameters. In this
study, we revisit classical machine learning based image classification through
a novel approach centered on Permutation Entropy (PE), a robust and
computationally lightweight measure traditionally used in time series analysis
but rarely applied to image data. We extend PE to two-dimensional images and
propose a multiscale, multi-orientation entropy-based feature extraction
approach that characterizes spatial order and complexity along rows, columns,
diagonals, anti-diagonals, and local patches of the image. To enhance the
discriminatory power of the entropy features, we integrate two classic image
descriptors: the Histogram of Oriented Gradients (HOG) to capture shape and
edge structure, and Local Binary Patterns (LBP) to encode micro-texture of an
image. The resulting hand-crafted feature set, comprising of 780 dimensions, is
used to train Support Vector Machine (SVM) classifiers optimized through grid
search. The proposed approach is evaluated on multiple benchmark datasets,
including Fashion-MNIST, KMNIST, EMNIST, and CIFAR-10, where it delivers
competitive classification performance without relying on deep architectures.
Our results demonstrate that the fusion of PE with HOG and LBP provides a
compact, interpretable, and effective alternative to computationally expensive
and limited interpretable deep learning models. This shows a potential of
entropy-based descriptors in image classification and contributes a lightweight
and generalizable solution to interpretable machine learning in image
classification and computer vision.

</details>


### [202] [Real-Time Fusion of Visual and Chart Data for Enhanced Maritime Vision](https://arxiv.org/abs/2507.13880)
*Marten Kreis,Benjamin Kiefer*

Main category: cs.CV

TL;DR: 提出了一种融合实时视觉数据与海图信息的新方法，通过神经网络匹配导航标志（如浮标），显著提升了动态环境中的目标定位和关联精度。


<details>
  <summary>Details</summary>
Motivation: 提升海洋视觉的准确性，通过结合实时视频与海图数据，解决动态和复杂环境中的导航标志匹配问题。

Method: 采用基于Transformer的端到端神经网络，预测浮标的边界框和置信度分数，直接匹配图像检测与海图标记。

Result: 在真实海洋场景数据集上，该方法显著优于基线方法（如光线投射模型和YOLOv7扩展网络），提高了定位和关联精度。

Conclusion: 该方法有效解决了动态环境中导航标志的匹配问题，为海洋视觉提供了更可靠的解决方案。

Abstract: This paper presents a novel approach to enhancing marine vision by fusing
real-time visual data with chart information. Our system overlays nautical
chart data onto live video feeds by accurately matching detected navigational
aids, such as buoys, with their corresponding representations in chart data. To
achieve robust association, we introduce a transformer-based end-to-end neural
network that predicts bounding boxes and confidence scores for buoy queries,
enabling the direct matching of image-domain detections with world-space chart
markers. The proposed method is compared against baseline approaches, including
a ray-casting model that estimates buoy positions via camera projection and a
YOLOv7-based network extended with a distance estimation module. Experimental
results on a dataset of real-world maritime scenes demonstrate that our
approach significantly improves object localization and association accuracy in
dynamic and challenging environments.

</details>


### [203] [Generalist Forecasting with Frozen Video Models via Latent Diffusion](https://arxiv.org/abs/2507.13942)
*Jacob C Walker,Pedro Vélez,Luisa Polania Cabrera,Guangyao Zhou,Rishabh Kabra,Carl Doersch,Maks Ovsjanikov,João Carreira,Shiry Ginosar*

Main category: cs.CV

TL;DR: 研究发现视觉模型的感知能力与其短期预测性能强相关，提出了一种通用预测框架，利用潜在扩散模型预测未来特征，并通过轻量级任务特定解码器实现。


<details>
  <summary>Details</summary>
Motivation: 探索视觉模型的感知能力与预测性能的关系，以提升通用系统的规划和行动能力。

Method: 使用潜在扩散模型预测冻结视觉骨干网络中的未来特征，并通过任务特定解码器解码。引入分布度量进行跨任务一致评估。

Result: 在九个模型和四个任务中验证了视觉模型感知能力与预测性能的强相关性，展示了表征学习与生成模型结合的价值。

Conclusion: 研究强调了表征学习与生成模型结合在时间基础视频理解中的重要性。

Abstract: Forecasting what will happen next is a critical skill for general-purpose
systems that plan or act in the world at different levels of abstraction. In
this paper, we identify a strong correlation between a vision model's
perceptual ability and its generalist forecasting performance over short time
horizons. This trend holds across a diverse set of pretrained models-including
those trained generatively-and across multiple levels of abstraction, from raw
pixels to depth, point tracks, and object motion. The result is made possible
by a novel generalist forecasting framework that operates on any frozen vision
backbone: we train latent diffusion models to forecast future features in the
frozen representation space, which are then decoded via lightweight,
task-specific readouts. To enable consistent evaluation across tasks, we
introduce distributional metrics that compare distributional properties
directly in the space of downstream tasks and apply this framework to nine
models and four tasks. Our results highlight the value of bridging
representation learning and generative modeling for temporally grounded video
understanding.

</details>


### [204] [QuantEIT: Ultra-Lightweight Quantum-Assisted Inference for Chest Electrical Impedance Tomography](https://arxiv.org/abs/2507.14031)
*Hao Fang,Sihao Teng,Hao Yu,Siyi Yuan,Huaiwu He,Zhe Liu,Yunjie Yang*

Main category: cs.CV

TL;DR: 提出了一种基于量子辅助网络的超轻量级EIT图像重建框架QuantEIT，显著降低了模型复杂性和参数数量，无需训练数据且性能优越。


<details>
  <summary>Details</summary>
Motivation: EIT图像重建存在病态逆问题，传统深度学习方法参数多且效率低，需改进。

Method: 结合并行2量子比特电路生成潜在表示，并通过单线性层重建电导率，实现无监督、无训练数据的高效重建。

Result: 在模拟和真实2D/3D肺部EIT数据上表现优于传统方法，参数仅需0.2%，抗噪性强。

Conclusion: QuantEIT为EIT图像重建提供了一种高效、轻量级的量子辅助解决方案。

Abstract: Electrical Impedance Tomography (EIT) is a non-invasive, low-cost bedside
imaging modality with high temporal resolution, making it suitable for bedside
monitoring. However, its inherently ill-posed inverse problem poses significant
challenges for accurate image reconstruction. Deep learning (DL)-based
approaches have shown promise but often rely on complex network architectures
with a large number of parameters, limiting efficiency and scalability. Here,
we propose an Ultra-Lightweight Quantum-Assisted Inference (QuantEIT) framework
for EIT image reconstruction. QuantEIT leverages a Quantum-Assisted Network
(QA-Net), combining parallel 2-qubit quantum circuits to generate expressive
latent representations that serve as implicit nonlinear priors, followed by a
single linear layer for conductivity reconstruction. This design drastically
reduces model complexity and parameter number. Uniquely, QuantEIT operates in
an unsupervised, training-data-free manner and represents the first integration
of quantum circuits into EIT image reconstruction. Extensive experiments on
simulated and real-world 2D and 3D EIT lung imaging data demonstrate that
QuantEIT outperforms conventional methods, achieving comparable or superior
reconstruction accuracy using only 0.2% of the parameters, with enhanced
robustness to noise.

</details>


### [205] [CSD-VAR: Content-Style Decomposition in Visual Autoregressive Models](https://arxiv.org/abs/2507.13984)
*Quang-Binh Nguyen,Minh Luu,Quang Nguyen,Anh Tran,Khoi Nguyen*

Main category: cs.CV

TL;DR: 本文提出了一种基于视觉自回归模型（VAR）的内容-风格分解方法CSD-VAR，通过三种创新技术提升分解效果，并在新数据集CSD-100上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有内容-风格分解方法主要针对扩散模型，而VAR作为一种新兴生成框架，其多尺度生成特性有望改进分解效果。

Method: 提出CSD-VAR方法，包括尺度感知交替优化、SVD校正和增强键值记忆三项创新技术。

Result: 实验表明，CSD-VAR在内容保留和风格化保真度上优于现有方法。

Conclusion: CSD-VAR为内容-风格分解任务提供了高效解决方案，展现了VAR框架的潜力。

Abstract: Disentangling content and style from a single image, known as content-style
decomposition (CSD), enables recontextualization of extracted content and
stylization of extracted styles, offering greater creative flexibility in
visual synthesis. While recent personalization methods have explored the
decomposition of explicit content style, they remain tailored for diffusion
models. Meanwhile, Visual Autoregressive Modeling (VAR) has emerged as a
promising alternative with a next-scale prediction paradigm, achieving
performance comparable to that of diffusion models. In this paper, we explore
VAR as a generative framework for CSD, leveraging its scale-wise generation
process for improved disentanglement. To this end, we propose CSD-VAR, a novel
method that introduces three key innovations: (1) a scale-aware alternating
optimization strategy that aligns content and style representation with their
respective scales to enhance separation, (2) an SVD-based rectification method
to mitigate content leakage into style representations, and (3) an Augmented
Key-Value (K-V) memory enhancing content identity preservation. To benchmark
this task, we introduce CSD-100, a dataset specifically designed for
content-style decomposition, featuring diverse subjects rendered in various
artistic styles. Experiments demonstrate that CSD-VAR outperforms prior
approaches, achieving superior content preservation and stylization fidelity.

</details>


### [206] [Multi-Centre Validation of a Deep Learning Model for Scoliosis Assessment](https://arxiv.org/abs/2507.14093)
*Šimon Kubov,Simon Klíčník,Jakub Dandár,Zdeněk Straka,Karolína Kvaková,Daniel Kvak*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的自动化软件（Carebot AI Bones）用于测量脊柱侧弯的Cobb角，结果显示其与放射科医生的测量结果高度一致，可用于临床工作流。


<details>
  <summary>Details</summary>
Motivation: 脊柱侧弯影响2-4%的青少年，传统手动测量Cobb角耗时且存在观察者间差异，因此需要一种自动化解决方案。

Method: 研究采用回顾性多中心评估，使用103张站立位全脊柱X光片，由两名放射科医生独立测量作为参考，通过Bland Altman分析、MAE、RMSE、Pearson相关系数和Cohen kappa评估AI与医生的一致性。

Result: AI与两位放射科医生的MAE分别为3.89度和3.90度，Pearson相关系数分别为0.906和0.880，Cohen kappa为0.51和0.64，表明AI的测量结果与专家水平一致。

Conclusion: 该软件在多中心环境中能够复现专家水平的Cobb角测量和分类分级，可用于优化脊柱侧弯的临床报告和分诊流程。

Abstract: Scoliosis affects roughly 2 to 4 percent of adolescents, and treatment
decisions depend on precise Cobb angle measurement. Manual assessment is time
consuming and subject to inter observer variation. We conducted a
retrospective, multi centre evaluation of a fully automated deep learning
software (Carebot AI Bones, Spine Measurement functionality; Carebot s.r.o.) on
103 standing anteroposterior whole spine radiographs collected from ten
hospitals. Two musculoskeletal radiologists independently measured each study
and served as reference readers. Agreement between the AI and each radiologist
was assessed with Bland Altman analysis, mean absolute error (MAE), root mean
squared error (RMSE), Pearson correlation coefficient, and Cohen kappa for four
grade severity classification. Against Radiologist 1 the AI achieved an MAE of
3.89 degrees (RMSE 4.77 degrees) with a bias of 0.70 degrees and limits of
agreement from minus 8.59 to plus 9.99 degrees. Against Radiologist 2 the AI
achieved an MAE of 3.90 degrees (RMSE 5.68 degrees) with a bias of 2.14 degrees
and limits from minus 8.23 to plus 12.50 degrees. Pearson correlations were r
equals 0.906 and r equals 0.880 (inter reader r equals 0.928), while Cohen
kappa for severity grading reached 0.51 and 0.64 (inter reader kappa 0.59).
These results demonstrate that the proposed software reproduces expert level
Cobb angle measurements and categorical grading across multiple centres,
suggesting its utility for streamlining scoliosis reporting and triage in
clinical workflows.

</details>


### [207] [VLA-Mark: A cross modal watermark for large vision-language alignment model](https://arxiv.org/abs/2507.14067)
*Shuliang Liu,Qi Zheng,Jesse Jiaxi Xu,Yibo Yan,He Geng,Aiwei Liu,Peijie Jiang,Jia Liu,Yik-Cheung Tam,Xuming Hu*

Main category: cs.CV

TL;DR: VLA-Mark是一种视觉对齐的水印框架，通过跨模态协调嵌入可检测水印，同时保持语义保真度。


<details>
  <summary>Details</summary>
Motivation: 现有文本水印方法因偏置标记选择和静态策略破坏了视觉-文本对齐，VLA-Mark旨在解决这一问题。

Method: 结合多尺度视觉-文本对齐指标（局部补丁亲和性、全局语义一致性和上下文注意力模式），动态平衡水印强度和语义保留。

Result: 实验显示，PPL降低7.4%，BLEU提高26.6%，检测AUC达98.8%，攻击抗性96.1%。

Conclusion: VLA-Mark为质量保持的多模态水印设定了新标准。

Abstract: Vision-language models demand watermarking solutions that protect
intellectual property without compromising multimodal coherence. Existing text
watermarking methods disrupt visual-textual alignment through biased token
selection and static strategies, leaving semantic-critical concepts vulnerable.
We propose VLA-Mark, a vision-aligned framework that embeds detectable
watermarks while preserving semantic fidelity through cross-modal coordination.
Our approach integrates multiscale visual-textual alignment metrics, combining
localized patch affinity, global semantic coherence, and contextual attention
patterns, to guide watermark injection without model retraining. An
entropy-sensitive mechanism dynamically balances watermark strength and
semantic preservation, prioritizing visual grounding during low-uncertainty
generation phases. Experiments show 7.4% lower PPL and 26.6% higher BLEU than
conventional methods, with near-perfect detection (98.8% AUC). The framework
demonstrates 96.1\% attack resilience against attacks such as paraphrasing and
synonym substitution, while maintaining text-visual consistency, establishing
new standards for quality-preserving multimodal watermarking

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [208] [Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification](https://arxiv.org/abs/2507.14116)
*Daniëlle Schuman,Mark V. Seebode,Tobias Rohe,Maximilian Balthasar Mansky,Michael Schroedl-Baumann,Jonas Stein,Claudia Linnhoff-Popien,Florian Krellner*

Main category: quant-ph

TL;DR: 本文提出了一种改进的并行量子退火方法，用于监督式训练量子玻尔兹曼机（QBMs），在医学图像数据集上取得与卷积神经网络（CNN）相当的结果，且训练速度提升近70%。


<details>
  <summary>Details</summary>
Motivation: 量子退火器样本遵循玻尔兹曼分布，但训练QBMs需要大量量子处理单元（QPU）时间，限制了其在NISQ时代的应用。本文旨在通过改进并行量子退火技术降低成本。

Method: 采用改进的并行量子退火技术，在监督式训练中减少输入编码所需的量子比特，并在MedMNIST数据集上测试。

Result: QBMs在较少的训练周期内取得与CNN相当的结果，并行退火技术比常规退火方法提速近70%。

Conclusion: 改进的并行量子退火技术显著提升了QBMs的训练效率，推动了其在现实世界中的应用。

Abstract: Exploiting the fact that samples drawn from a quantum annealer inherently
follow a Boltzmann-like distribution, annealing-based Quantum Boltzmann
Machines (QBMs) have gained increasing popularity in the quantum research
community. While they harbor great promises for quantum speed-up, their usage
currently stays a costly endeavor, as large amounts of QPU time are required to
train them. This limits their applicability in the NISQ era. Following the idea
of No\`e et al. (2024), who tried to alleviate this cost by incorporating
parallel quantum annealing into their unsupervised training of QBMs, this paper
presents an improved version of parallel quantum annealing that we employ to
train QBMs in a supervised setting. Saving qubits to encode the inputs, the
latter setting allows us to test our approach on medical images from the
MedMNIST data set (Yang et al., 2023), thereby moving closer to real-world
applicability of the technology. Our experiments show that QBMs using our
approach already achieve reasonable results, comparable to those of
similarly-sized Convolutional Neural Networks (CNNs), with markedly smaller
numbers of epochs than these classical models. Our parallel annealing technique
leads to a speed-up of almost 70 % compared to regular annealing-based BM
executions.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [209] [Loss-Complexity Landscape and Model Structure Functions](https://arxiv.org/abs/2507.13543)
*Alexander Kolpakov*

Main category: cs.IT

TL;DR: 提出了一个框架，用于对偶化Kolmogorov结构函数，并引入统计力学中的概念，证明其与信息论构造的数学类比。


<details>
  <summary>Details</summary>
Motivation: 探索信息论与统计力学之间的数学联系，并验证模型复杂度与泛化能力的关系。

Method: 通过引入分区函数和自由能泛函，证明结构函数与自由能之间的Legendre-Fenchel对偶性，并利用Metropolis核验证平衡性。

Result: 实验验证了模型复杂度与泛化能力的相互作用，并展示了过拟合阈值的相变现象。

Conclusion: 框架成功连接了信息论与统计力学，为理解模型复杂度与泛化能力提供了新视角。

Abstract: We develop a framework for dualizing the Kolmogorov structure function
$h_x(\alpha)$, which then allows using computable complexity proxies. We
establish a mathematical analogy between information-theoretic constructs and
statistical mechanics, introducing a suitable partition function and free
energy functional. We explicitly prove the Legendre-Fenchel duality between the
structure function and free energy, showing detailed balance of the Metropolis
kernel, and interpret acceptance probabilities as information-theoretic
scattering amplitudes. A susceptibility-like variance of model complexity is
shown to peak precisely at loss-complexity trade-offs interpreted as phase
transitions. Practical experiments with linear and tree-based regression models
verify these theoretical predictions, explicitly demonstrating the interplay
between the model complexity, generalization, and overfitting threshold.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [210] [Edge Intelligence with Spiking Neural Networks](https://arxiv.org/abs/2507.14069)
*Shuiguang Deng,Di Yu,Changze Lv,Xin Du,Linshan Jiang,Xiaofan Zhao,Wentao Tong,Xiaoqing Zheng,Weijia Fang,Peng Zhao,Gang Pan,Schahram Dustdar,Albert Y. Zomaya*

Main category: cs.DC

TL;DR: 本文综述了基于脉冲神经网络（SNN）的边缘智能（EdgeSNN），探讨其在设备端学习、推理和安全性方面的潜力，并提供了系统分类和实际应用分析。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习在资源受限设备上存在延迟、带宽和隐私问题，而SNN因其低功耗和事件驱动特性成为有前景的替代方案。

Method: 通过系统分类涵盖SNN的神经元模型、学习算法和硬件平台，并深入讨论设备端推理、资源感知训练和隐私保护等实际应用。

Result: 提出了双轨基准测试策略以支持公平比较和硬件优化，并总结了当前进展、挑战和未来方向。

Conclusion: 本文填补了脑启发学习与边缘部署之间的空白，为研究者和从业者提供了重要参考。

Abstract: The convergence of artificial intelligence and edge computing has spurred
growing interest in enabling intelligent services directly on
resource-constrained devices. While traditional deep learning models require
significant computational resources and centralized data management, the
resulting latency, bandwidth consumption, and privacy concerns have exposed
critical limitations in cloud-centric paradigms. Brain-inspired computing,
particularly Spiking Neural Networks (SNNs), offers a promising alternative by
emulating biological neuronal dynamics to achieve low-power, event-driven
computation. This survey provides a comprehensive overview of Edge Intelligence
based on SNNs (EdgeSNNs), examining their potential to address the challenges
of on-device learning, inference, and security in edge scenarios. We present a
systematic taxonomy of EdgeSNN foundations, encompassing neuron models,
learning algorithms, and supporting hardware platforms. Three representative
practical considerations of EdgeSNN are discussed in depth: on-device inference
using lightweight SNN models, resource-aware training and updating under
non-stationary data conditions, and secure and privacy-preserving issues.
Furthermore, we highlight the limitations of evaluating EdgeSNNs on
conventional hardware and introduce a dual-track benchmarking strategy to
support fair comparisons and hardware-aware optimization. Through this study,
we aim to bridge the gap between brain-inspired learning and practical edge
deployment, offering insights into current advancements, open challenges, and
future research directions. To the best of our knowledge, this is the first
dedicated and comprehensive survey on EdgeSNNs, providing an essential
reference for researchers and practitioners working at the intersection of
neuromorphic computing and edge intelligence.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [211] [Graph Neural Network Surrogates for Contacting Deformable Bodies with Necessary and Sufficient Contact Detection](https://arxiv.org/abs/2507.13459)
*Vijay K. Dubey,Collin E. Haese,Osman Gültekin,David Dalton,Manuel K. Rausch,Jan N. Fuhg*

Main category: cs.CE

TL;DR: 提出了一种基于图神经网络的替代模型，用于快速推断非线性边界值问题，特别针对软体接触问题，并展示了其性能与计算成本之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有方法在软体接触建模中存在局限性，尤其是在几何变化的情况下。本文旨在解决这一问题。

Method: 采用图神经网络架构，结合连续碰撞检测和充分接触条件，用于软体接触建模。

Result: 在两种基准测试中表现良好，包括生物假体主动脉瓣的闭合状态预测，并实现了高达千倍的推理加速。

Conclusion: 该方法在软体接触建模中具有潜力，但需权衡训练成本与推理速度。

Abstract: Surrogate models for the rapid inference of nonlinear boundary value problems
in mechanics are helpful in a broad range of engineering applications. However,
effective surrogate modeling of applications involving the contact of
deformable bodies, especially in the context of varying geometries, is still an
open issue. In particular, existing methods are confined to rigid body contact
or, at best, contact between rigid and soft objects with well-defined contact
planes. Furthermore, they employ contact or collision detection filters that
serve as a rapid test but use only the necessary and not sufficient conditions
for detection. In this work, we present a graph neural network architecture
that utilizes continuous collision detection and, for the first time,
incorporates sufficient conditions designed for contact between soft deformable
bodies. We test its performance on two benchmarks, including a problem in soft
tissue mechanics of predicting the closed state of a bioprosthetic aortic
valve. We find a regularizing effect on adding additional contact terms to the
loss function, leading to better generalization of the network. These benefits
hold for simple contact at similar planes and element normal angles, and
complex contact at differing planes and element normal angles. We also
demonstrate that the framework can handle varying reference geometries.
However, such benefits come with high computational costs during training,
resulting in a trade-off that may not always be favorable. We quantify the
training cost and the resulting inference speedups on various hardware
architectures. Importantly, our graph neural network implementation results in
up to a thousand-fold speedup for our benchmark problems at inference.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [212] [A Collaborative Framework Integrating Large Language Model and Chemical Fragment Space: Mutual Inspiration for Lead Design](https://arxiv.org/abs/2507.13580)
*Hao Tuo,Yan Li,Xuanning Hu,Haishi Zhao,Xueyan Liu,Bo Yang*

Main category: q-bio.BM

TL;DR: AutoLeadDesign是一个基于大语言模型和化学片段的先导化合物设计框架，在药物设计中表现出色，优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前方法在整合领域知识方面存在挑战，限制了其设计新颖且有效结合模式的先导化合物的能力。

Method: AutoLeadDesign利用大语言模型编码的领域知识和化学片段，逐步探索化学空间。

Result: 实验表明，AutoLeadDesign在针对PRMT5和SARS-CoV-2 PLpro的实验中表现出色，生成了专家竞争性的先导化合物。

Conclusion: AutoLeadDesign提供了一种高效的先导化合物设计方法，具有在药物设计中广泛应用的潜力。

Abstract: Combinatorial optimization algorithm is essential in computer-aided drug
design by progressively exploring chemical space to design lead compounds with
high affinity to target protein. However current methods face inherent
challenges in integrating domain knowledge, limiting their performance in
identifying lead compounds with novel and valid binding mode. Here, we propose
AutoLeadDesign, a lead compounds design framework that inspires extensive
domain knowledge encoded in large language models with chemical fragments to
progressively implement efficient exploration of vast chemical space. The
comprehensive experiments indicate that AutoLeadDesign outperforms baseline
methods. Significantly, empirical lead design campaigns targeting two
clinically relevant targets (PRMT5 and SARS-CoV-2 PLpro) demonstrate
AutoLeadDesign's competence in de novo generation of lead compounds achieving
expert-competitive design efficacy. Structural analysis further confirms their
mechanism-validated inhibitory patterns. By tracing the process of design, we
find that AutoLeadDesign shares analogous mechanisms with fragment-based drug
design which traditionally rely on the expert decision-making, further
revealing why it works. Overall, AutoLeadDesign offers an efficient approach
for lead compounds design, suggesting its potential utility in drug design.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [213] [AI-Assisted Fixes to Code Review Comments at Scale](https://arxiv.org/abs/2507.13499)
*Chandra Maddila,Negar Ghorbani,James Saindon,Parth Thakkar,Vijayaraghavan Murali,Rui Abreu,Jingyue Shen,Brian Zhou,Nachiappan Nagappan,Peter C. Rigby*

Main category: cs.SE

TL;DR: Meta开发了MetaMateCR，利用AI辅助修复代码审查评论，通过微调Llama模型并在生产中测试，结果显示其性能优于GPT-4o，同时强调了安全试验的重要性。


<details>
  <summary>Details</summary>
Motivation: Meta每周有数万条代码审查评论，希望通过AI辅助修复提高效率。

Method: 使用64k数据点微调Llama模型，进行离线测试和安全试验后投入生产。

Result: LargeLSFT模型在离线测试中表现优于GPT-4o，生产中的ActionableToApplied率提高了9.2pp。

Conclusion: MetaMateCR成功展示了AI辅助修复的潜力，同时安全试验确保了工程师效率不受影响。

Abstract: Aim. There are 10s of thousands of code review comments each week at Meta. We
developed Metamate for Code Review (MetaMateCR) that provides AI-assisted fixes
for reviewer comments in production at scale.
  Method. We developed an internal benchmark of 64k <review comment, patch>
data points to fine-tune Llama models. Once our models achieve reasonable
offline results, we roll them into production. To ensure that our AI-assisted
fixes do not negatively impact the time it takes to do code reviews, we conduct
randomized controlled safety trials as well as full production experiments.
  Offline Results. As a baseline, we compare GPT-4o to our small and large
Llama models. In offline results, our LargeLSFT model creates an exact match
patch 68% of the time outperforming GPT-4o by 9 percentage points (pp). The
internal models also use more modern Hack functions when compared to the PHP
functions suggested by GPT-4o.
  Safety Trial. When we roll MetaMateCR into production in a safety trial that
compares no AI patches with AI patch suggestions, we see a large regression
with reviewers taking over 5% longer to conduct reviews. After investigation,
we modify the UX to only show authors the AI patches, and see no regressions in
the time for reviews.
  Production. When we roll LargeLSFT into production, we see an
ActionableToApplied rate of 19.7%, which is a 9.2pp improvement over GPT-4o.
Our results illustrate the importance of safety trials in ensuring that AI does
not inadvertently slow down engineers, and a successful review comment to AI
patch product running at scale.

</details>
