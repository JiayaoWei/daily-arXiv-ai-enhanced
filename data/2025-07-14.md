<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 55]
- [cs.AI](#cs.AI) [Total: 22]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 64]
- [physics.flu-dyn](#physics.flu-dyn) [Total: 1]
- [cs.SE](#cs.SE) [Total: 2]
- [cs.NI](#cs.NI) [Total: 2]
- [cs.PL](#cs.PL) [Total: 1]
- [eess.SP](#eess.SP) [Total: 1]
- [q-fin.RM](#q-fin.RM) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [cs.SD](#cs.SD) [Total: 5]
- [stat.ML](#stat.ML) [Total: 6]
- [eess.IV](#eess.IV) [Total: 2]
- [cs.CE](#cs.CE) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 1]
- [cs.RO](#cs.RO) [Total: 2]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.CY](#cs.CY) [Total: 1]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.CV](#cs.CV) [Total: 27]
- [cs.DB](#cs.DB) [Total: 3]
- [cs.LO](#cs.LO) [Total: 1]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.CR](#cs.CR) [Total: 2]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [RepeaTTS: Towards Feature Discovery through Repeated Fine-Tuning](https://arxiv.org/abs/2507.08012)
*Atli Sigurgeirsson,Simon King*

Main category: cs.CL

TL;DR: 提出一种基于提示的文本转语音模型，通过自然语言指令控制语音特征，并通过新的微调方法解决灵活性与限制性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的文本转语音模型在灵活性（不可控变化）和限制性（仅能控制训练中暴露的声学特征）之间存在矛盾。

Method: 通过主成分分析确定潜在特征，并将其作为新标签用于二次微调，以提升模型的可控性。

Result: 在未包含情感披露的冰岛语音语料库上，该方法成功提取连续和离散特征，显著提升模型的可控性。

Conclusion: 该方法有效解决了基于提示的文本转语音模型的灵活性与限制性问题，提升了可控性。

Abstract: A Prompt-based Text-To-Speech model allows a user to control different
aspects of speech, such as speaking rate and perceived gender, through natural
language instruction. Although user-friendly, such approaches are on one hand
constrained: control is limited to acoustic features exposed to the model
during training, and too flexible on the other: the same inputs yields
uncontrollable variation that are reflected in the corpus statistics.
  We investigate a novel fine-tuning regime to address both of these issues at
the same time by exploiting the uncontrollable variance of the model. Through
principal component analysis of thousands of synthesised samples, we determine
latent features that account for the highest proportion of the output variance
and incorporate them as new labels for secondary fine-tuning. We evaluate the
proposed methods on two models trained on an expressive Icelandic speech
corpus, one with emotional disclosure and one without. In the case of the model
without emotional disclosure, the method yields both continuous and discrete
features that improve overall controllability of the model.

</details>


### [2] [MedicalBERT: enhancing biomedical natural language processing using pretrained BERT-based model](https://arxiv.org/abs/2507.08013)
*K. Sahit Reddy,N. Ragavenderan,Vasanth K.,Ganesh N. Naik,Vishalakshi Prabhu,Nagaraja G. S*

Main category: cs.CL

TL;DR: MedicalBERT是一种针对生物医学领域的预训练BERT模型，通过领域特定词汇和优化，显著提升了生物医学NLP任务的性能。


<details>
  <summary>Details</summary>
Motivation: 通用预训练语言模型（如BERT、GPT）在生物医学领域因术语复杂性和双向理解需求表现不足。

Method: 提出MedicalBERT，基于BERT架构，使用大规模生物医学数据集预训练，并优化微调以支持多种任务。

Result: MedicalBERT在命名实体识别、关系抽取等任务上优于BioBERT、SciBERT等模型，平均性能提升5.67%。

Conclusion: MedicalBERT展示了预训练BERT模型在生物医学NLP中的潜力，验证了迁移学习在领域特定任务中的有效性。

Abstract: Recent advances in natural language processing (NLP) have been driven
bypretrained language models like BERT, RoBERTa, T5, and GPT. Thesemodels excel
at understanding complex texts, but biomedical literature, withits
domain-specific terminology, poses challenges that models likeWord2Vec and
bidirectional long short-term memory (Bi-LSTM) can't fullyaddress. GPT and T5,
despite capturing context, fall short in tasks needingbidirectional
understanding, unlike BERT. Addressing this, we proposedMedicalBERT, a
pretrained BERT model trained on a large biomedicaldataset and equipped with
domain-specific vocabulary that enhances thecomprehension of biomedical
terminology. MedicalBERT model is furtheroptimized and fine-tuned to address
diverse tasks, including named entityrecognition, relation extraction, question
answering, sentence similarity, anddocument classification. Performance metrics
such as the F1-score,accuracy, and Pearson correlation are employed to showcase
the efficiencyof our model in comparison to other BERT-based models such as
BioBERT,SciBERT, and ClinicalBERT. MedicalBERT outperforms these models onmost
of the benchmarks, and surpasses the general-purpose BERT model by5.67% on
average across all the tasks evaluated respectively. This work alsounderscores
the potential of leveraging pretrained BERT models for medicalNLP tasks,
demonstrating the effectiveness of transfer learning techniques incapturing
domain-specific information.
  (PDF) MedicalBERT: enhancing biomedical natural language processing using
pretrained BERT-based model. Available from:
https://www.researchgate.net/publication/392489050_MedicalBERT_enhancing_biomedical_natural_language_processing_using_pretrained_BERT-based_model
[accessed Jul 06 2025].

</details>


### [3] [Mass-Scale Analysis of In-the-Wild Conversations Reveals Complexity Bounds on LLM Jailbreaking](https://arxiv.org/abs/2507.08014)
*Aldan Creo,Raul Castro Fernandez,Manuel Cebrian*

Main category: cs.CL

TL;DR: 研究发现，实际对话中的越狱尝试并不比正常对话更复杂，且攻击复杂性和毒性保持稳定，而AI的防御机制在提升。


<details>
  <summary>Details</summary>
Motivation: 理解越狱策略的复杂性和演变对AI安全至关重要。

Method: 通过对200多万条真实对话进行大规模实证分析，使用多种复杂性指标。

Result: 越狱尝试的复杂性未显著高于正常对话，攻击复杂性和毒性稳定，AI防御机制提升。

Conclusion: 研究挑战了攻防升级的叙事，指出AI安全受限于人类创造力，同时强调学术披露可能带来的信息风险。

Abstract: As large language models (LLMs) become increasingly deployed, understanding
the complexity and evolution of jailbreaking strategies is critical for AI
safety.
  We present a mass-scale empirical analysis of jailbreak complexity across
over 2 million real-world conversations from diverse platforms, including
dedicated jailbreaking communities and general-purpose chatbots. Using a range
of complexity metrics spanning probabilistic measures, lexical diversity,
compression ratios, and cognitive load indicators, we find that jailbreak
attempts do not exhibit significantly higher complexity than normal
conversations. This pattern holds consistently across specialized jailbreaking
communities and general user populations, suggesting practical bounds on attack
sophistication. Temporal analysis reveals that while user attack toxicity and
complexity remains stable over time, assistant response toxicity has decreased,
indicating improving safety mechanisms. The absence of power-law scaling in
complexity distributions further points to natural limits on jailbreak
development.
  Our findings challenge the prevailing narrative of an escalating arms race
between attackers and defenders, instead suggesting that LLM safety evolution
is bounded by human ingenuity constraints while defensive measures continue
advancing. Our results highlight critical information hazards in academic
jailbreak disclosure, as sophisticated attacks exceeding current complexity
baselines could disrupt the observed equilibrium and enable widespread harm
before defensive adaptation.

</details>


### [4] [Assessing the Capabilities and Limitations of FinGPT Model in Financial NLP Applications](https://arxiv.org/abs/2507.08015)
*Prudence Djagba,Chimezie A. Odinakachukwu*

Main category: cs.CL

TL;DR: FinGPT在金融领域NLP任务中表现优异，尤其在分类任务（如情感分析和标题分类）上接近GPT-4，但在推理和生成任务（如问答和摘要）上表现较差。


<details>
  <summary>Details</summary>
Motivation: 评估FinGPT在金融领域NLP任务中的能力，为未来研究提供基准。

Method: 在六个金融NLP任务上测试FinGPT，使用金融专用数据集，并与GPT-4和人类基准对比。

Result: FinGPT在分类任务中表现强，但推理和生成任务表现不佳，数值准确性和复杂推理存在明显差距。

Conclusion: FinGPT适用于结构化金融任务，但需进一步优化架构和领域适应性。

Abstract: This work evaluates FinGPT, a financial domain-specific language model,
across six key natural language processing (NLP) tasks: Sentiment Analysis,
Text Classification, Named Entity Recognition, Financial Question Answering,
Text Summarization, and Stock Movement Prediction. The evaluation uses
finance-specific datasets to assess FinGPT's capabilities and limitations in
real-world financial applications. The results show that FinGPT performs
strongly in classification tasks such as sentiment analysis and headline
categorization, often achieving results comparable to GPT-4. However, its
performance is significantly lower in tasks that involve reasoning and
generation, such as financial question answering and summarization. Comparisons
with GPT-4 and human benchmarks highlight notable performance gaps,
particularly in numerical accuracy and complex reasoning. Overall, the findings
indicate that while FinGPT is effective for certain structured financial tasks,
it is not yet a comprehensive solution. This research provides a useful
benchmark for future research and underscores the need for architectural
improvements and domain-specific optimization in financial language models.

</details>


### [5] [Mechanistic Indicators of Understanding in Large Language Models](https://arxiv.org/abs/2507.08017)
*Pierre Beckmann,Matthieu Queloz*

Main category: cs.CL

TL;DR: 论文提出了一种新的理论框架，将大型语言模型（LLMs）的内部结构划分为三个层次的理解，并探讨了其与人类认知的差异。


<details>
  <summary>Details</summary>
Motivation: 挑战大型语言模型仅依赖表面统计的观点，探索其内部结构和功能类比于人类理解的可能性。

Method: 提出三层次理解框架：概念理解、世界状态理解和原则性理解，并通过机制可解释性（MI）研究验证。

Result: LLMs确实形成内部结构，功能上类似于人类的理解，但其认知架构与人类不同。

Conclusion: 应关注LLMs的独特工作机制，而非争论其是否具备理解能力。

Abstract: Recent findings in mechanistic interpretability (MI), the field probing the
inner workings of Large Language Models (LLMs), challenge the view that these
models rely solely on superficial statistics. Here, we offer an accessible
synthesis of these findings that doubles as an introduction to MI, all while
integrating these findings within a novel theoretical framework for thinking
about machine understanding. We argue that LLMs develop internal structures
that are functionally analogous to the kind of understanding that consists in
seeing connections. To sharpen this idea, we propose a three-tiered conception
of machine understanding. First, conceptual understanding emerges when a model
forms "features" as directions in latent space, thereby learning the
connections between diverse manifestations of something. Second,
state-of-the-world understanding emerges when a model learns contingent factual
connections between features and dynamically tracks changes in the world.
Third, principled understanding emerges when a model ceases to rely on a
collection of memorized facts and discovers a "circuit" that connects these
facts. However, we conclude by exploring the "parallel mechanisms" phenomenon,
arguing that while LLMs exhibit forms of understanding, their cognitive
architecture remains different from ours, and the debate should shift from
whether LLMs understand to how their strange minds work.

</details>


### [6] [Review, Remask, Refine (R3): Process-Guided Block Diffusion for Text Generation](https://arxiv.org/abs/2507.08018)
*Nikita Mounier,Parsa Idehpour*

Main category: cs.CL

TL;DR: R3框架通过Review、Remask、Refine三步优化文本生成，无需额外训练，适用于预训练掩码扩散模型。


<details>
  <summary>Details</summary>
Motivation: 解决迭代文本生成中模型自我纠错的效率问题。

Method: 使用Process Reward Model（PRM）评估生成块，根据PRM分数动态掩码低分块，并针对性优化。

Result: 通过聚焦优化低分块，提升最终生成质量。

Conclusion: R3是一种简单有效的框架，显著提升文本生成模型的自我纠错能力。

Abstract: A key challenge for iterative text generation is enabling models to
efficiently identify and correct their own errors. We propose Review, Remask,
Refine (R3), a relatively simple yet elegant framework that requires no
additional model training and can be applied to any pre-trained masked text
diffusion model (e.g., LLaDA or BD3-LM). In R3, a Process Reward Model (PRM) is
utilized for the Review of intermediate generated blocks. The framework then
translates these PRM scores into a Remask strategy: the lower a block's PRM
score, indicating potential mistakes, the greater the proportion of tokens
within that block are remasked. Finally, the model is compelled to Refine these
targeted segments, focusing its efforts more intensively on specific
sub-optimal parts of past generations, leading to improved final output.

</details>


### [7] [Signal or Noise? Evaluating Large Language Models in Resume Screening Across Contextual Variations and Human Expert Benchmarks](https://arxiv.org/abs/2507.08019)
*Aryan Varshney,Venkat Ram Reddy Ganuthula*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在简历筛选中的一致性与随机性，并与人类专家对比。发现LLMs表现与人类显著不同，且不同模型适应公司背景的能力各异。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在简历筛选中的行为模式，评估其与人类专家的差异，为自动化招聘系统提供参考。

Method: 使用控制数据集测试三种LLM（Claude、GPT、Gemini）在不同公司背景下的表现，并与三位人类专家对比。

Result: LLMs在部分条件下表现显著不同，且与人类专家差异显著。GPT适应性强，Gemini部分适应，Claude适应性弱。

Conclusion: LLMs在详细提示下可提供可解释的模式，但与人类判断差异显著，需谨慎用于自动化招聘。

Abstract: This study investigates whether large language models (LLMs) exhibit
consistent behavior (signal) or random variation (noise) when screening resumes
against job descriptions, and how their performance compares to human experts.
Using controlled datasets, we tested three LLMs (Claude, GPT, and Gemini)
across contexts (No Company, Firm1 [MNC], Firm2 [Startup], Reduced Context)
with identical and randomized resumes, benchmarked against three human
recruitment experts. Analysis of variance revealed significant mean differences
in four of eight LLM-only conditions and consistently significant differences
between LLM and human evaluations (p < 0.01). Paired t-tests showed GPT adapts
strongly to company context (p < 0.001), Gemini partially (p = 0.038 for
Firm1), and Claude minimally (p > 0.1), while all LLMs differed significantly
from human experts across contexts. Meta-cognition analysis highlighted
adaptive weighting patterns that differ markedly from human evaluation
approaches. Findings suggest LLMs offer interpretable patterns with detailed
prompts but diverge substantially from human judgment, informing their
deployment in automated hiring systems.

</details>


### [8] [Circumventing Safety Alignment in Large Language Models Through Embedding Space Toxicity Attenuation](https://arxiv.org/abs/2507.08020)
*Zhibo Zhang,Yuxi Li,Kailong Wang,Shuai Yuan,Ling Shi,Haoyu Wang*

Main category: cs.CL

TL;DR: ETTA框架通过线性变换识别并减弱嵌入空间中的毒性敏感维度，成功攻击多个LLM，揭示了当前对齐策略的漏洞。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多个领域取得成功，但其开放性和嵌入空间中毒攻击的安全风险未被充分研究，尤其是针对安全对齐机制的精准攻击。

Method: 提出ETTA框架，通过线性变换在嵌入空间识别并减弱毒性敏感维度，无需微调或训练数据。

Result: 在五个开源LLM上测试，ETTA平均攻击成功率达88.61%，优于基线11.34%，并在安全增强模型上表现良好。

Conclusion: ETTA揭示了当前对齐策略的关键漏洞，强调了嵌入感知防御的必要性。

Abstract: Large Language Models (LLMs) have achieved remarkable success across domains
such as healthcare, education, and cybersecurity. However, this openness also
introduces significant security risks, particularly through embedding space
poisoning, which is a subtle attack vector where adversaries manipulate the
internal semantic representations of input data to bypass safety alignment
mechanisms. While previous research has investigated universal perturbation
methods, the dynamics of LLM safety alignment at the embedding level remain
insufficiently understood. Consequently, more targeted and accurate adversarial
perturbation techniques, which pose significant threats, have not been
adequately studied.
  In this work, we propose ETTA (Embedding Transformation Toxicity
Attenuation), a novel framework that identifies and attenuates
toxicity-sensitive dimensions in embedding space via linear transformations.
ETTA bypasses model refusal behaviors while preserving linguistic coherence,
without requiring model fine-tuning or access to training data. Evaluated on
five representative open-source LLMs using the AdvBench benchmark, ETTA
achieves a high average attack success rate of 88.61%, outperforming the best
baseline by 11.34%, and generalizes to safety-enhanced models (e.g., 77.39% ASR
on instruction-tuned defenses). These results highlight a critical
vulnerability in current alignment strategies and underscore the need for
embedding-aware defenses.

</details>


### [9] [Unveiling Effective In-Context Configurations for Image Captioning: An External & Internal Analysis](https://arxiv.org/abs/2507.08021)
*Li Li,Yongliang Wu,Jingze Zhu,Jiawei Peng,Jianfei Cai,Xu Yang*

Main category: cs.CL

TL;DR: 本文通过外部和内部实验全面研究了多模态上下文学习（ICL）在图像描述任务中的应用，探索了演示配置策略和模型行为特征，并提出了新的度量标准。


<details>
  <summary>Details</summary>
Motivation: 受大型语言模型（LLMs）成功的启发，研究者开发了具有ICL能力的大型多模态模型（LMMs），但多模态ICL的演示配置研究尚不充分。本文旨在填补这一空白。

Method: 外部实验探索了演示配置策略（如样本数量、图像检索和标题分配），内部实验分析了LMM的注意力特征，并开发了基于注意力的度量标准。

Result: 研究发现ICEs配置策略显著影响模型性能，并揭示了LMM的典型注意力模式。辅助实验还探索了注意力驱动的模型加速和压缩的可行性。

Conclusion: 本文通过双重视角（外部和内部）理解多模态ICL，提出的方法和度量标准可广泛应用于其他大型模型研究。

Abstract: The evolution of large models has witnessed the emergence of In-Context
Learning (ICL) capabilities. In Natural Language Processing (NLP), numerous
studies have demonstrated the effectiveness of ICL. Inspired by the success of
Large Language Models (LLMs), researchers have developed Large Multimodal
Models (LMMs) with ICL capabilities. However, explorations of demonstration
configuration for multimodal ICL remain preliminary. Additionally, the
controllability of In-Context Examples (ICEs) provides an efficient and
cost-effective means to observe and analyze the inference characteristics of
LMMs under varying inputs. This paper conducts a comprehensive external and
internal investigation of multimodal in-context learning on the image
captioning task. Externally, we explore demonstration configuration strategies
through three dimensions: shot number, image retrieval, and caption assignment.
We employ multiple metrics to systematically and thoroughly evaluate and
summarize key findings. Internally, we analyze typical LMM attention
characteristics and develop attention-based metrics to quantify model
behaviors. We also conduct auxiliary experiments to explore the feasibility of
attention-driven model acceleration and compression. We further compare
performance variations between LMMs with identical model design and pretraining
strategies and explain the differences from the angles of pre-training data
features. Our study reveals both how ICEs configuration strategies impact model
performance through external experiments and characteristic typical patterns
through internal inspection, providing dual perspectives for understanding
multimodal ICL in LMMs. Our method of combining external and internal analysis
to investigate large models, along with our newly proposed metrics, can be
applied to broader research areas.

</details>


### [10] ["Amazing, They All Lean Left" -- Analyzing the Political Temperaments of Current LLMs](https://arxiv.org/abs/2507.08027)
*W. Russell Neuman,Chad Coleman,Ali Dasdan,Safinah Ali,Manan Shah,Kund Meghani*

Main category: cs.CL

TL;DR: 研究发现大多数商业大语言模型（LLM）在伦理和政治回应中表现出一致的自由主义倾向，主要归因于训练数据、强化学习、学术伦理框架和安全微调。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM政治倾向的成因及其对民主话语的影响。

Method: 使用道德基础理论、政治意识形态量表和争议指数，分析七种主流LLM的政治倾向。

Result: 多数模型优先体现自由主义价值观（如关怀与公平），微调会增强这一倾向。

Conclusion: LLM的自由主义倾向是训练数据的自然结果，可能为集体理性提供新视角。

Abstract: Recent studies have revealed a consistent liberal orientation in the ethical
and political responses generated by most commercial large language models
(LLMs), yet the underlying causes and resulting implications remain unclear.
This paper systematically investigates the political temperament of seven
prominent LLMs - OpenAI's GPT-4o, Anthropic's Claude Sonnet 4, Perplexity
(Sonar Large), Google's Gemini 2.5 Flash, Meta AI's Llama 4, Mistral 7b Le Chat
and High-Flyer's DeepSeek R1 -- using a multi-pronged approach that includes
Moral Foundations Theory, a dozen established political ideology scales and a
new index of current political controversies. We find strong and consistent
prioritization of liberal-leaning values, particularly care and fairness,
across most models. Further analysis attributes this trend to four overlapping
factors: Liberal-leaning training corpora, reinforcement learning from human
feedback (RLHF), the dominance of liberal frameworks in academic ethical
discourse and safety-driven fine-tuning practices. We also distinguish between
political "bias" and legitimate epistemic differences, cautioning against
conflating the two. A comparison of base and fine-tuned model pairs reveals
that fine-tuning generally increases liberal lean, an effect confirmed through
both self-report and empirical testing. We argue that this "liberal tilt" is
not a programming error or the personal preference of programmers but an
emergent property of training on democratic rights-focused discourse. Finally,
we propose that LLMs may indirectly echo John Rawls' famous veil-of ignorance
philosophical aspiration, reflecting a moral stance unanchored to personal
identity or interest. Rather than undermining democratic discourse, this
pattern may offer a new lens through which to examine collective reasoning.

</details>


### [11] [UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations](https://arxiv.org/abs/2507.07030)
*Fengran Mo,Yifan Gao,Chuan Meng,Xin Liu,Zhuofeng Wu,Kelong Mao,Zhengyang Wang,Pei Chen,Zheng Li,Xian Li,Bing Yin,Meng Jiang*

Main category: cs.CL

TL;DR: 论文提出了一种统一密集检索和响应生成的方法，通过联合微调和设计机制减少不一致性，提升了对话搜索系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有对话搜索系统通常使用两个独立模型，限制了知识共享和性能提升，需要一种统一的方法来解决检索与生成的协同问题。

Method: 采用联合微调不同目标的方法，并设计两种机制以减少不一致性和数据差异。

Result: 在五个对话搜索数据集上的评估表明，统一模型能同时提升检索和生成任务，并优于现有基线。

Conclusion: 统一密集检索和响应生成的方法有效提升了对话搜索系统的性能，解决了现有模型的局限性。

Abstract: The rapid advancement of conversational search systems revolutionizes how
information is accessed by enabling the multi-turn interaction between the user
and the system. Existing conversational search systems are usually built with
two different models. This separation restricts the system from leveraging the
intrinsic knowledge of the models simultaneously, which cannot ensure the
effectiveness of retrieval benefiting the generation. The existing studies for
developing unified models cannot fully address the aspects of understanding
conversational context, managing retrieval independently, and generating
responses. In this paper, we explore how to unify dense retrieval and response
generation for large language models in conversation. We conduct joint
fine-tuning with different objectives and design two mechanisms to reduce the
inconsistency risks while mitigating data discrepancy. The evaluations on five
conversational search datasets demonstrate that our unified model can mutually
improve both tasks and outperform the existing baselines.

</details>


### [12] [Better Together: Quantifying the Benefits of AI-Assisted Recruitment](https://arxiv.org/abs/2507.08029)
*Ada Aka,Emil Palikot,Ali Ansari,Nima Yazdani*

Main category: cs.CL

TL;DR: AI在招聘中显著提升候选人通过率和就业概率，但倾向于选择年轻且经验较少的申请人。


<details>
  <summary>Details</summary>
Motivation: 量化AI在招聘中对效率和候选人选择的影响。

Method: 随机分配37,000名申请人至传统或AI辅助招聘流程，比较最终面试通过率和后续就业情况。

Result: AI辅助流程的候选人通过率提高20个百分点，就业概率高5.9个百分点，但偏向年轻和经验较少的候选人。

Conclusion: AI技术显著影响招聘决策，但也带来潜在的选择偏好问题。

Abstract: Artificial intelligence (AI) is increasingly used in recruitment, yet
empirical evidence quantifying its impact on hiring efficiency and candidate
selection remains limited. We randomly assign 37,000 applicants for a
junior-developer position to either a traditional recruitment process (resume
screening followed by human selection) or an AI-assisted recruitment pipeline
incorporating an initial AI-driven structured video interview before human
evaluation. Candidates advancing from either track faced the same final-stage
human interview, with interviewers blind to the earlier selection method. In
the AI-assisted pipeline, 54% of candidates passed the final interview compared
with 34% from the traditional pipeline, yielding an average treatment effect of
20 percentage points (SE 12 pp.). Five months later, we collected LinkedIn
profiles of top applicants from both groups and found that 18% (SE 1.1%) of
applicants from the traditional track found new jobs compared with 23% (SE
2.3%) from the AI group, resulting in a 5.9 pp. (SE 2.6 pp.) difference in the
probability of finding new employment between groups. The AI system tended to
select younger applicants with less experience and fewer advanced credentials.
We analyze AI-generated interview transcripts to examine the selection criteria
and conversational dynamics. Our findings contribute to understanding how AI
technologies affect decision making in recruitment and talent acquisition while
highlighting some of their potential implications.

</details>


### [13] [A Systematic Analysis of Declining Medical Safety Messaging in Generative AI Models](https://arxiv.org/abs/2507.08030)
*Sonali Sharma,Ahmed M. Alaa,Roxana Daneshjou*

Main category: cs.CL

TL;DR: 研究评估了2022至2025年间生成式AI模型（LLM和VLM）在医疗图像和临床问题回答中免责声明的使用情况，发现免责声明比例显著下降。


<details>
  <summary>Details</summary>
Motivation: 随着生成式AI模型在医疗领域的应用增加，其输出可能存在不准确性，免责声明作为安全措施的重要性凸显。

Method: 使用500张乳腺X光片、500张胸部X光片、500张皮肤病图像和500个医学问题，筛查模型输出中的免责声明短语。

Result: LLM和VLM的免责声明比例从2022年的26.3%和2023年的19.6%分别降至2025年的0.97%和1.05%。

Conclusion: 随着模型能力的提升，免责声明需根据临床背景动态调整，以确保安全性。

Abstract: Generative AI models, including large language models (LLMs) and
vision-language models (VLMs), are increasingly used to interpret medical
images and answer clinical questions. Their responses often include
inaccuracies; therefore, safety measures like medical disclaimers are critical
to remind users that AI outputs are not professionally vetted or a substitute
for medical advice. This study evaluated the presence of disclaimers in LLM and
VLM outputs across model generations from 2022 to 2025. Using 500 mammograms,
500 chest X-rays, 500 dermatology images, and 500 medical questions, outputs
were screened for disclaimer phrases. Medical disclaimer presence in LLM and
VLM outputs dropped from 26.3% in 2022 to 0.97% in 2025, and from 19.6% in 2023
to 1.05% in 2025, respectively. By 2025, the majority of models displayed no
disclaimers. As public models become more capable and authoritative,
disclaimers must be implemented as a safeguard adapting to the clinical context
of each output.

</details>


### [14] [GRASP: Generic Reasoning And SPARQL Generation across Knowledge Graphs](https://arxiv.org/abs/2507.08107)
*Sebastian Walter,Hannah Bast*

Main category: cs.CL

TL;DR: 提出了一种无需微调的方法，利用大语言模型从自然语言问题或关键词查询生成SPARQL查询，通过策略性执行查询和图搜索实现。


<details>
  <summary>Details</summary>
Motivation: 解决从自然语言生成SPARQL查询的挑战，避免对模型进行微调，适用于不同规模和类型的知识图谱。

Method: 利用语言模型探索知识图谱，策略性执行SPARQL查询并搜索相关IRIs和字面量。

Result: 在Wikidata上取得零样本设置的SOTA结果，在Freebase上接近少样本方法，其他知识图谱上表现良好。

Conclusion: 该方法在多种知识图谱和基准测试中表现优异，支持零样本和少样本设置，具有广泛适用性。

Abstract: We propose a new approach for generating SPARQL queries on RDF knowledge
graphs from natural language questions or keyword queries, using a large
language model. Our approach does not require fine-tuning. Instead, it uses the
language model to explore the knowledge graph by strategically executing SPARQL
queries and searching for relevant IRIs and literals. We evaluate our approach
on a variety of benchmarks (for knowledge graphs of different kinds and sizes)
and language models (of different scales and types, commercial as well as
open-source) and compare it with existing approaches. On Wikidata we reach
state-of-the-art results on multiple benchmarks, despite the zero-shot setting.
On Freebase we come close to the best few-shot methods. On other, less commonly
evaluated knowledge graphs and benchmarks our approach also performs well
overall. We conduct several additional studies, like comparing different ways
of searching the graphs, incorporating a feedback mechanism, or making use of
few-shot examples.

</details>


### [15] [Beyond Scale: Small Language Models are Comparable to GPT-4 in Mental Health Understanding](https://arxiv.org/abs/2507.08031)
*Hong Jia,Shiya Fu,Vassilis Kostakos,Feng Xia,Ting Dang*

Main category: cs.CL

TL;DR: 论文研究了小型语言模型（SLMs）在心理健康理解任务中的表现，发现其性能接近大型语言模型（LLMs），尤其在少样本学习下表现显著提升。


<details>
  <summary>Details</summary>
Motivation: 探讨SLMs作为隐私保护替代方案在敏感应用中的潜力，特别是在心理健康领域的理解能力。

Method: 通过零样本和少样本学习范式，评估五种SLMs和三种LLMs在六项心理健康任务中的表现。

Result: SLMs在二元分类任务中性能接近LLMs（F1分数0.64 vs 0.66），少样本学习显著提升SLMs性能（最高14.6%）。

Conclusion: SLMs在心理健康领域具有潜力，可作为隐私保护工具，尤其在少样本学习下表现优异。

Abstract: The emergence of Small Language Models (SLMs) as privacy-preserving
alternatives for sensitive applications raises a fundamental question about
their inherent understanding capabilities compared to Large Language Models
(LLMs). This paper investigates the mental health understanding capabilities of
current SLMs through systematic evaluation across diverse classification tasks.
Employing zero-shot and few-shot learning paradigms, we benchmark their
performance against established LLM baselines to elucidate their relative
strengths and limitations in this critical domain. We assess five
state-of-the-art SLMs (Phi-3, Phi-3.5, Qwen2.5, Llama-3.2, Gemma2) against
three LLMs (GPT-4, FLAN-T5-XXL, Alpaca-7B) on six mental health understanding
tasks. Our findings reveal that SLMs achieve mean performance within 2\% of
LLMs on binary classification tasks (F1 scores of 0.64 vs 0.66 in zero-shot
settings), demonstrating notable competence despite orders of magnitude fewer
parameters. Both model categories experience similar degradation on multi-class
severity tasks (a drop of over 30\%), suggesting that nuanced clinical
understanding challenges transcend model scale. Few-shot prompting provides
substantial improvements for SLMs (up to 14.6\%), while LLM gains are more
variable. Our work highlights the potential of SLMs in mental health
understanding, showing they can be effective privacy-preserving tools for
analyzing sensitive online text data. In particular, their ability to quickly
adapt and specialize with minimal data through few-shot learning positions them
as promising candidates for scalable mental health screening tools.

</details>


### [16] [Integrating External Tools with Large Language Models to Improve Accuracy](https://arxiv.org/abs/2507.08034)
*Nripesh Niketan,Hadj Batatia*

Main category: cs.CL

TL;DR: 提出一个框架，通过集成外部工具增强LLMs在查询中的表现，特别是在教育领域。


<details>
  <summary>Details</summary>
Motivation: LLMs缺乏上下文信息时表现不佳，需外部工具提供实时数据以提高准确性。

Method: 开发框架，通过外部API获取额外信息，集成计算工具如计算器和日历。

Result: 在MMLU数据集上测试，数学推理准确率83%，科学推理88%，显著优于现有模型。

Conclusion: 框架为LLMs构建复杂计算生态系统铺平道路，支持多样化任务。

Abstract: This paper deals with improving querying large language models (LLMs). It is
well-known that without relevant contextual information, LLMs can provide poor
quality responses or tend to hallucinate. Several initiatives have proposed
integrating LLMs with external tools to provide them with up-to-date data to
improve accuracy. In this paper, we propose a framework to integrate external
tools to enhance the capabilities of LLMs in answering queries in educational
settings. Precisely, we develop a framework that allows accessing external APIs
to request additional relevant information. Integrated tools can also provide
computational capabilities such as calculators or calendars. The proposed
framework has been evaluated using datasets from the Multi-Modal Language
Understanding (MMLU) collection. The data consists of questions on mathematical
and scientific reasoning. Results compared to state-of-the-art language models
show that the proposed approach significantly improves performance. Our Athena
framework achieves 83% accuracy in mathematical reasoning and 88% in scientific
reasoning, substantially outperforming all tested models including GPT-4o,
LLaMA-Large, Mistral-Large, Phi-Large, and GPT-3.5, with the best baseline
model (LLaMA-Large) achieving only 67% and 79% respectively. These promising
results open the way to creating complex computing ecosystems around LLMs to
make their use more natural to support various tasks and activities.

</details>


### [17] [Distillation versus Contrastive Learning: How to Train Your Rerankers](https://arxiv.org/abs/2507.08336)
*Zhichao Xu,Zhiqi Huang,Shengyao Zhuang,Ashim Gupta,Vivek Srikumar*

Main category: cs.CL

TL;DR: 本文比较了对比学习和知识蒸馏在训练文本重排模型中的效果，发现知识蒸馏在有大模型教师时表现更好，否则对比学习更可靠。


<details>
  <summary>Details</summary>
Motivation: 需要明确比较对比学习和知识蒸馏在训练跨编码器重排模型时的实际效果。

Method: 通过在同一数据上训练不同大小和架构的重排模型，使用对比学习模型作为教师进行知识蒸馏。

Result: 知识蒸馏在大模型教师时表现优于对比学习，但教师模型容量相同时优势不明显，尤其在跨领域任务中。

Conclusion: 建议在有大型教师模型时使用知识蒸馏训练小型重排模型，否则选择对比学习。

Abstract: Training text rerankers is crucial for information retrieval. Two primary
strategies are widely used: contrastive learning (optimizing directly on
ground-truth labels) and knowledge distillation (transferring knowledge from a
larger reranker). While both have been studied in the literature, a clear
comparison of their effectiveness for training cross-encoder rerankers under
practical conditions is needed.
  This paper empirically compares these strategies by training rerankers of
different sizes and architectures using both methods on the same data, with a
strong contrastive learning model acting as the distillation teacher. Our
results show that knowledge distillation generally yields better in-domain and
out-of-domain ranking performance than contrastive learning when distilling
from a larger teacher model. This finding is consistent across student model
sizes and architectures. However, distilling from a teacher of the same
capacity does not provide the same advantage, particularly for out-of-domain
tasks. These findings offer practical guidance for choosing a training strategy
based on available teacher models. Therefore, we recommend using knowledge
distillation to train smaller rerankers if a larger, more powerful teacher is
accessible; in its absence, contrastive learning provides a strong and more
reliable alternative otherwise.

</details>


### [18] [Barriers in Integrating Medical Visual Question Answering into Radiology Workflows: A Scoping Review and Clinicians' Insights](https://arxiv.org/abs/2507.08036)
*Deepali Mishra,Chaklam Silpasuwanchai,Ashutosh Modi,Madhumita Sushil,Sorayouth Chumnanvej*

Main category: cs.CL

TL;DR: 该研究系统回顾了68篇关于医学视觉问答（MedVQA）的文献，并调查了50名临床医生，发现MedVQA在临床工作流中的实际应用存在显著差距，包括数据集和模型的局限性以及评估指标与临床需求的不匹配。


<details>
  <summary>Details</summary>
Motivation: 探讨MedVQA在临床工作流中的实际效用、挑战和差距，以促进其更有效的整合。

Method: 采用Arksey和O'Malley的范围综述框架，结合文献回顾和临床医生调查。

Result: 研究发现60%的问答对缺乏临床相关性，数据集和模型不支持多视图或多分辨率成像等关键功能，且评估指标与临床需求不匹配。临床医生中仅29.8%认为MedVQA高度有用。

Conclusion: MedVQA具有潜力，但需解决多模态分析不足、缺乏患者背景和评估方法不匹配等问题，以实现有效的临床整合。

Abstract: Medical Visual Question Answering (MedVQA) is a promising tool to assist
radiologists by automating medical image interpretation through question
answering. Despite advances in models and datasets, MedVQA's integration into
clinical workflows remains limited. This study systematically reviews 68
publications (2018-2024) and surveys 50 clinicians from India and Thailand to
examine MedVQA's practical utility, challenges, and gaps. Following the Arksey
and O'Malley scoping review framework, we used a two-pronged approach: (1)
reviewing studies to identify key concepts, advancements, and research gaps in
radiology workflows, and (2) surveying clinicians to capture their perspectives
on MedVQA's clinical relevance. Our review reveals that nearly 60% of QA pairs
are non-diagnostic and lack clinical relevance. Most datasets and models do not
support multi-view, multi-resolution imaging, EHR integration, or domain
knowledge, features essential for clinical diagnosis. Furthermore, there is a
clear mismatch between current evaluation metrics and clinical needs. The
clinician survey confirms this disconnect: only 29.8% consider MedVQA systems
highly useful. Key concerns include the absence of patient history or domain
knowledge (87.2%), preference for manually curated datasets (51.1%), and the
need for multi-view image support (78.7%). Additionally, 66% favor models
focused on specific anatomical regions, and 89.4% prefer dialogue-based
interactive systems. While MedVQA shows strong potential, challenges such as
limited multimodal analysis, lack of patient context, and misaligned evaluation
approaches must be addressed for effective clinical integration.

</details>


### [19] [CRISP: Complex Reasoning with Interpretable Step-based Plans](https://arxiv.org/abs/2507.08037)
*Matan Vetzler,Koren Lazar,Guy Uziel,Eran Hirsch,Ateret Anaby-Tavor,Leshem Choshen*

Main category: cs.CL

TL;DR: 论文提出CRISP数据集，通过微调小模型生成高质量计划，优于大模型的少样本提示和Chain-of-Thought推理。


<details>
  <summary>Details</summary>
Motivation: 现有方法假设LLMs仅通过少样本提示即可生成有效计划，但实际效果不足，需验证和改进。

Method: 引入CRISP数据集，自动生成并验证高层面计划，通过微调小模型提升计划质量。

Result: 微调后小模型生成计划质量优于大模型少样本提示，且跨领域泛化能力强。

Conclusion: CRISP验证了微调方法的有效性，展示了计划能力的通用性。

Abstract: Recent advancements in large language models (LLMs) underscore the need for
stronger reasoning capabilities to solve complex problems effectively. While
Chain-of-Thought (CoT) reasoning has been a step forward, it remains
insufficient for many domains. A promising alternative is explicit high-level
plan generation, but existing approaches largely assume that LLMs can produce
effective plans through few-shot prompting alone, without additional training.
In this work, we challenge this assumption and introduce CRISP (Complex
Reasoning with Interpretable Step-based Plans), a multi-domain dataset of
high-level plans for mathematical reasoning and code generation. The plans in
CRISP are automatically generated and rigorously validated--both intrinsically,
using an LLM as a judge, and extrinsically, by evaluating their impact on
downstream task performance. We demonstrate that fine-tuning a small model on
CRISP enables it to generate higher-quality plans than much larger models using
few-shot prompting, while significantly outperforming Chain-of-Thought
reasoning. Furthermore, our out-of-domain evaluation reveals that fine-tuning
on one domain improves plan generation in the other, highlighting the
generalizability of learned planning capabilities.

</details>


### [20] [AblationBench: Evaluating Automated Planning of Ablations in Empirical AI Research](https://arxiv.org/abs/2507.08038)
*Talor Abramovich,Gal Chechik*

Main category: cs.CL

TL;DR: AblationBench是一个评估语言模型代理在消融实验规划任务中的基准套件，包含两个任务：AuthorAblation和ReviewerAblation。实验显示前沿语言模型在这些任务上表现有限，最佳系统仅识别29%的原始消融。


<details>
  <summary>Details</summary>
Motivation: 支持或自动化科学研究中的消融实验设计，提升AI代理在实证研究中的实用性。

Method: 开发AblationBench基准套件，包含两个任务，并基于语言模型设计自动评估框架。

Result: 前沿语言模型在任务中表现有限，最佳系统平均识别29%的原始消融。

Conclusion: 当前语言模型在消融实验规划任务中仍有挑战，链式思维提示优于现有代理方法。

Abstract: Autonomous agents built on language models (LMs) are showing increasing
popularity in many fields, including scientific research. AI co-scientists aim
to support or automate parts of the research process using these agents. A key
component of empirical AI research is the design of ablation experiments. To
this end, we introduce AblationBench, a benchmark suite for evaluating agents
on ablation planning tasks in empirical AI research. It includes two tasks:
AuthorAblation, which helps authors propose ablation experiments based on a
method section and contains 83 instances, and ReviewerAblation, which helps
reviewers find missing ablations in a full paper and contains 350 instances.
For both tasks, we develop LM-based judges that serve as an automatic
evaluation framework. Our experiments with frontier LMs show that these tasks
remain challenging, with the best-performing LM system identifying only 29% of
the original ablations on average. Lastly, we analyze the limitations of
current LMs on these tasks, and find that chain-of-thought prompting
outperforms the currently existing agent-based approach.

</details>


### [21] [Krul: Efficient State Restoration for Multi-turn Conversations with Dynamic Cross-layer KV Sharing](https://arxiv.org/abs/2507.08045)
*Junyi Wen,Junyuan Liang,Zicong Hong,Wuhui Chen,Zibin Zheng*

Main category: cs.CL

TL;DR: Krul是一个多轮LLM推理系统，通过动态选择压缩策略和优化KV缓存恢复，显著提升了效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决多轮对话中KV缓存恢复的效率问题，避免静态压缩策略导致的准确性下降。

Method: 动态选择压缩策略、预选策略选择器、注意力相似性估计器和无气泡恢复调度器。

Result: TTFT减少1.5x-2.68x，KV缓存存储减少1.33x-2.35x，且不损失生成质量。

Conclusion: Krul在多轮对话中实现了高效且准确的KV缓存恢复。

Abstract: Efficient state restoration in multi-turn conversations with large language
models (LLMs) remains a critical challenge, primarily due to the overhead of
recomputing or loading full key-value (KV) caches for all historical tokens. To
address this, existing approaches compress KV caches across adjacent layers
with highly similar attention patterns. However, these methods often apply a
fixed compression scheme across all conversations, selecting the same layer
pairs for compression without considering conversation-specific attention
dynamics. This static strategy overlooks variability in attention pattern
similarity across different conversations, which can lead to noticeable
accuracy degradation.
  We present Krul, a multi-turn LLM inference system that enables accurate and
efficient KV cache restoration. Krul dynamically selects compression strategies
based on attention similarity across layer pairs and uses a
recomputation-loading pipeline to restore the KV cache. It introduces three key
innovations: 1) a preemptive compression strategy selector to preserve critical
context for future conversation turns and selects a customized strategy for the
conversation; 2) a token-wise heterogeneous attention similarity estimator to
mitigate the attention similarity computation and storage overhead during model
generation; 3) a bubble-free restoration scheduler to reduce potential bubbles
brought by the imbalance of recomputing and loading stream due to compressed KV
caches. Empirical evaluations on real-world tasks demonstrate that Krul
achieves a 1.5x-2.68x reduction in time-to-first-token (TTFT) and a 1.33x-2.35x
reduction in KV cache storage compared to state-of-the-art methods without
compromising generation quality.

</details>


### [22] [Audit, Alignment, and Optimization of LM-Powered Subroutines with Application to Public Comment Processing](https://arxiv.org/abs/2507.08109)
*Reilly Raab,Mike Parker,Dan Nally,Sadie Montgomery,Anastasia Bernat,Sai Munikoti,Sameera Horawalavithana*

Main category: cs.CL

TL;DR: 论文提出了一种框架，用于声明静态类型的语言模型子程序，以透明、可审计的方式在传统异步代码中使用，并通过人类专家的稀疏反馈在线改进性能。


<details>
  <summary>Details</summary>
Motivation: 语言模型（LMs）的潜力受到安全性、可解释性和偏见等问题的限制，需要一种透明、可审计的方法来负责任地利用LMs。

Method: 提出一个框架，将LM子程序声明为静态类型、可调用的函数，记录所有LM生成的工件，并支持通过人类反馈在线优化。

Result: 开发了“CommentNEPA”应用，用于处理公共评论，并通过与历史人工标注数据的定量比较验证了其性能。

Conclusion: 该框架为透明、可审计地使用LMs提供了可行方案，并在公共评论处理中展示了实际应用价值。

Abstract: The advent of language models (LMs) has the potential to dramatically
accelerate tasks that may be cast to text-processing; however, real-world
adoption is hindered by concerns regarding safety, explainability, and bias.
How can we responsibly leverage LMs in a transparent, auditable manner --
minimizing risk and allowing human experts to focus on informed decision-making
rather than data-processing or prompt engineering? In this work, we propose a
framework for declaring statically typed, LM-powered subroutines (i.e.,
callable, function-like procedures) for use within conventional asynchronous
code -- such that sparse feedback from human experts is used to improve the
performance of each subroutine online (i.e., during use). In our
implementation, all LM-produced artifacts (i.e., prompts, inputs, outputs, and
data-dependencies) are recorded and exposed to audit on demand. We package this
framework as a library to support its adoption and continued development. While
this framework may be applicable across several real-world decision workflows
(e.g., in healthcare and legal fields), we evaluate it in the context of public
comment processing as mandated by the 1969 National Environmental Protection
Act (NEPA): Specifically, we use this framework to develop "CommentNEPA," an
application that compiles, organizes, and summarizes a corpus of public
commentary submitted in response to a project requiring environmental review.
We quantitatively evaluate the application by comparing its outputs (when
operating without human feedback) to historical ``ground-truth'' data as
labelled by human annotators during the preparation of official environmental
impact statements.

</details>


### [23] [Compactor: Calibrated Query-Agnostic KV Cache Compression with Approximate Leverage Scores](https://arxiv.org/abs/2507.08143)
*Vivek Chari,Benjamin Van Durme*

Main category: cs.CL

TL;DR: Compactor是一种无需参数、与查询无关的KV缓存压缩方法，通过近似杠杆分数确定令牌重要性，显著减少内存负担。


<details>
  <summary>Details</summary>
Motivation: 解决LLM中KV缓存内存占用高的问题，提升吞吐量并降低服务成本。

Method: 使用近似杠杆分数进行令牌重要性评估，提出上下文校准压缩策略。

Result: 在合成和真实任务中保留一半令牌，内存负担减少63%，性能与竞争方法相当。

Conclusion: Compactor高效且通用，适用于多种任务和模型家族。

Abstract: Modern Large Language Models (LLMs) are increasingly trained to support very
large context windows. Unfortunately the ability to use long contexts in
generation is complicated by the large memory requirement of the KV cache,
which scales linearly with the context length. This memory footprint is often
the dominant resource bottleneck in real-world deployments, limiting throughput
and increasing serving cost. One way to address this is by compressing the KV
cache, which can be done either with knowledge of the question being asked
(query-aware) or without knowledge of the query (query-agnostic). We present
Compactor, a parameter-free, query-agnostic KV compression strategy that uses
approximate leverage scores to determine token importance. We show that
Compactor can achieve the same performance as competing methods while retaining
1/2 the tokens in both synthetic and real-world context tasks, with minimal
computational overhead. We further introduce a procedure for context-calibrated
compression, which allows one to infer the maximum compression ratio a given
context can support. Using context-calibrated compression, we show that
Compactor achieves full KV performance on Longbench while reducing the KV
memory burden by 63%, on average. To demonstrate the efficacy and
generalizability of our approach, we apply Compactor to 27 synthetic and
real-world tasks from RULER and Longbench, with models from both the Qwen 2.5
and Llama 3.1 families.

</details>


### [24] [Distilling Empathy from Large Language Models](https://arxiv.org/abs/2507.08151)
*Henry J. Xie,Jinghan Zhang,Xinhao Zhang,Kunpeng Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种将大语言模型（LLM）的共情能力蒸馏到小语言模型（SLM）的两步微调方法，通过特定提示显著提升SLM的共情表现。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的场景（如智能手机）中，SLM需要保留LLM的共情能力以支持良好的人机交互。

Method: 采用两步微调过程，结合LLM生成的共情对话数据集，并设计四种特定提示以优化共情蒸馏。

Result: 实验表明，该方法使SLM在共情回应生成上显著优于基准模型，胜率达90%，特定提示比基础提示胜率提升10%。

Conclusion: 该方法有效实现了LLM到SLM的共情能力蒸馏，为资源受限场景提供了高质量的共情交互解决方案。

Abstract: The distillation of knowledge from Large Language Models (LLMs) into Smaller
Language Models (SLMs), preserving the capabilities and performance of LLMs
while reducing model size, has played a key role in the proliferation of LLMs.
Because SLMs are considerably smaller than LLMs, they are often utilized in
domains where human interaction is frequent but resources are highly
constrained, e.g., smart phones. Therefore, it is crucial to ensure that
empathy, a fundamental aspect of positive human interactions, already instilled
into LLMs, is retained by SLMs after distillation. In this paper, we develop a
comprehensive approach for effective empathy distillation from LLMs into SLMs.
Our approach features a two-step fine-tuning process that fully leverages
datasets of empathetic dialogue responses distilled from LLMs. We explore
several distillation methods beyond basic direct prompting and propose four
unique sets of prompts for targeted empathy improvement to significantly
enhance the empathy distillation process. Our evaluations demonstrate that SLMs
fine-tuned through the two-step fine-tuning process with distillation datasets
enhanced by the targeted empathy improvement prompts significantly outperform
the base SLM at generating empathetic responses with a win rate of 90%. Our
targeted empathy improvement prompts substantially outperform the basic direct
prompting with a 10% improvement in win rate.

</details>


### [25] [TruthTorchLM: A Comprehensive Library for Predicting Truthfulness in LLM Outputs](https://arxiv.org/abs/2507.08203)
*Duygu Nur Yaldiz,Yavuz Faruk Bakman,Sungmin Kang,Alperen Öziş,Hayrettin Eren Yildiz,Mitash Ashish Shah,Zhiqi Huang,Anoop Kumar,Alfy Samuel,Daben Liu,Sai Praneeth Karimireddy,Salman Avestimehr*

Main category: cs.CL

TL;DR: TruthTorchLM是一个开源Python库，提供30多种真实性预测方法，支持多种模型和接口，旨在提升生成式大语言模型输出的真实性预测。


<details>
  <summary>Details</summary>
Motivation: 生成式大语言模型（LLMs）常产生不真实的输出，尤其在关键场景中，准确预测其真实性至关重要。现有工具如Guardrails和LM-Polygraph功能有限，无法满足多样化需求。

Method: TruthTorchLM整合了30多种真实性预测方法，涵盖不同计算成本、访问级别、文档需求及监督类型，支持HuggingFace和LiteLLM，并提供统一的生成、评估和校准接口。

Result: 在TriviaQA、GSM8K和FactScore-Bio数据集上评估了代表性方法，验证了其有效性。代码已开源。

Conclusion: TruthTorchLM为真实性预测研究提供了全面且可扩展的工具，填补了现有工具的不足。

Abstract: Generative Large Language Models (LLMs)inevitably produce untruthful
responses. Accurately predicting the truthfulness of these outputs is critical,
especially in high-stakes settings. To accelerate research in this domain and
make truthfulness prediction methods more accessible, we introduce TruthTorchLM
an open-source, comprehensive Python library featuring over 30 truthfulness
prediction methods, which we refer to as Truth Methods. Unlike existing
toolkits such as Guardrails, which focus solely on document-grounded
verification, or LM-Polygraph, which is limited to uncertainty-based methods,
TruthTorchLM offers a broad and extensible collection of techniques. These
methods span diverse tradeoffs in computational cost, access level (e.g.,
black-box vs white-box), grounding document requirements, and supervision type
(self-supervised or supervised). TruthTorchLM is seamlessly compatible with
both HuggingFace and LiteLLM, enabling support for locally hosted and API-based
models. It also provides a unified interface for generation, evaluation,
calibration, and long-form truthfulness prediction, along with a flexible
framework for extending the library with new methods. We conduct an evaluation
of representative truth methods on three datasets, TriviaQA, GSM8K, and
FactScore-Bio. The code is available at https://github.com/Ybakman/TruthTorchLM

</details>


### [26] [Simple Mechanistic Explanations for Out-Of-Context Reasoning](https://arxiv.org/abs/2507.08218)
*Atticus Wang,Joshua Engels,Oliver Clive-Griffin*

Main category: cs.CL

TL;DR: 研究发现，微调后的LLMs通过添加恒定引导向量实现跨领域泛化，解释了其为何能在任务中表现出上下文外推理能力。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在微调后为何能表现出上下文外推理（OOCR）的深层机制。

Method: 通过分析LoRA微调过程，发现其本质是添加恒定引导向量，并尝试直接训练这些向量。

Result: 恒定引导向量能显著提升任务性能并实现跨领域泛化，甚至适用于条件行为任务（如模型后门）。

Conclusion: 研究揭示了微调过程中学习的内容，为LLMs的上下文外推理能力提供了一种解释，对其安全可靠部署具有重要意义。

Abstract: Out-of-context reasoning (OOCR) is a phenomenon in which fine-tuned LLMs
exhibit surprisingly deep out-of-distribution generalization. Rather than
learning shallow heuristics, they implicitly internalize and act on the
consequences of observations scattered throughout the fine-tuning data. In this
work, we investigate this phenomenon mechanistically and find that many
instances of OOCR in the literature have a simple explanation: the LoRA
fine-tuning essentially adds a constant steering vector, steering the model
towards a general concept. This improves performance on the fine-tuning task
and in many other concept-related domains, causing the surprising
generalization. Moreover, we can directly train steering vectors for these
tasks from scratch, which also induces OOCR. We find that our results hold even
for a task that seems like it must involve conditional behavior (model
backdoors); it turns out that unconditionally adding a steering vector is
sufficient. Overall, our work presents one explanation of what gets learned
during fine-tuning for OOCR tasks, contributing to the key question of why LLMs
can reason out of context, an advanced capability that is highly relevant to
their safe and reliable deployment.

</details>


### [27] [Can LLMs Reliably Simulate Real Students' Abilities in Mathematics and Reading Comprehension?](https://arxiv.org/abs/2507.08232)
*KV Aditya Srivatsa,Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）作为代理学生在智能辅导系统（ITSs）和测试题目试点中的准确性，发现未经指导的强通用模型普遍优于真实学生，而弱模型或领域不匹配的模型可能偶然对齐。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs作为代理学生是否能准确模拟真实学生的行为和特征。

Method: 收集489项NAEP数据，应用IRT模型将11种LLMs与真实学生能力对标。

Result: 强通用模型普遍优于学生，弱模型可能偶然对齐；提示调整性能，但无模型-提示组合能完全匹配年级水平。

Conclusion: 需新训练和评估策略，并提供选择可行代理的指南。

Abstract: Large Language Models (LLMs) are increasingly used as proxy students in the
development of Intelligent Tutoring Systems (ITSs) and in piloting test
questions. However, to what extent these proxy students accurately emulate the
behavior and characteristics of real students remains an open question. To
investigate this, we collected a dataset of 489 items from the National
Assessment of Educational Progress (NAEP), covering mathematics and reading
comprehension in grades 4, 8, and 12. We then apply an Item Response Theory
(IRT) model to position 11 diverse and state-of-the-art LLMs on the same
ability scale as real student populations. Our findings reveal that, without
guidance, strong general-purpose models consistently outperform the average
student at every grade, while weaker or domain-mismatched models may align
incidentally. Using grade-enforcement prompts changes models' performance, but
whether they align with the average grade-level student remains highly model-
and prompt-specific: no evaluated model-prompt pair fits the bill across
subjects and grades, underscoring the need for new training and evaluation
strategies. We conclude by providing guidelines for the selection of viable
proxies based on our findings.

</details>


### [28] [Exploring Gender Differences in Chronic Pain Discussions on Reddit](https://arxiv.org/abs/2507.08241)
*Ancita Maria Andrade,Tanvi Banerjee,Ramakrishna Mundugar*

Main category: cs.CL

TL;DR: 该研究利用NLP和HAM-CNN模型分析性别在疼痛体验中的差异，发现女性更倾向于情感表达，且某些疼痛症状在女性中更常见。


<details>
  <summary>Details</summary>
Motivation: 以往研究常忽视性别在疼痛体验中的作用，本研究旨在填补这一空白。

Method: 使用NLP和HAM-CNN模型对文本进行分类，分析性别差异。

Result: F1分数为0.86，女性文本更情感化，某些疼痛症状在女性中更普遍。

Conclusion: 性别在疼痛体验中起重要作用，需在治疗中考虑性别差异。

Abstract: Pain is an inherent part of human existence, manifesting as both physical and
emotional experiences, and can be categorized as either acute or chronic. Over
the years, extensive research has been conducted to understand the causes of
pain and explore potential treatments, with contributions from various
scientific disciplines. However, earlier studies often overlooked the role of
gender in pain experiences. In this study, we utilized Natural Language
Processing (NLP) to analyze and gain deeper insights into individuals' pain
experiences, with a particular focus on gender differences. We successfully
classified posts into male and female corpora using the Hidden Attribute
Model-Convolutional Neural Network (HAM-CNN), achieving an F1 score of 0.86 by
aggregating posts based on usernames. Our analysis revealed linguistic
differences between genders, with female posts tending to be more emotionally
focused. Additionally, the study highlighted that conditions such as migraine
and sinusitis are more prevalent among females and explored how pain medication
affects individuals differently based on gender.

</details>


### [29] [KAT-V1: Kwai-AutoThink Technical Report](https://arxiv.org/abs/2507.08297)
*Zizheng Zhan,Ken Deng,Huaixi Tang,Wen Xiang,Kun Wu,Weihao Li,Wenqiang Zhu,Jingxuan Xu,Lecheng Huang,Zongxian Feng,Shaojie Wang,Shangpeng Yan,Jiaheng Liu,Zhongyuan Peng,Zuchen Gao,Haoyang Huang,Ziqi Zhan,Yanan Wu,Yuanxing Zhang,Jian Yang,Guang Chen,Haotian Zhang,Bin Chen,Bing Yu*

Main category: cs.CL

TL;DR: Kwaipilot-AutoThink (KAT) 是一个开源的40B大语言模型，通过动态切换推理和非推理模式解决推理密集型任务中的过度思考问题，并在性能和效率上优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决推理密集型任务中的过度思考问题，并提升模型的效率和可控性。

Method: 构建双机制数据集，采用多令牌预测增强的知识蒸馏，冷启动初始化策略，以及结合中间监督的Step-SRPO强化学习算法。

Result: KAT在多个基准测试中表现优异，减少约30%的token使用，并在实际应用中提升了开发效率。

Conclusion: KAT展示了AutoThink范式的可扩展性，未来计划扩展到200B MoE模型。

Abstract: We present Kwaipilot-AutoThink (KAT), an open-source 40B large language model
developed to address the overthinking problem in reasoning-intensive tasks,
where an automatic thinking training paradigm is proposed to dynamically switch
between reasoning and non-reasoning modes based on task complexity.
Specifically, first, we construct the dual-regime dataset based on a novel
tagging pipeline and a multi-agent synthesis strategy, and then we apply
Multi-Token Prediction (MTP)-enhanced knowledge distillation, enabling
efficient and fine-grained reasoning transfer with minimal pretraining cost.
Besides, we implement a cold-start initialization strategy that introduces
mode-selection priors using majority-vote signals and intent-aware prompting.
Finally, we propose Step-SRPO, a reinforcement learning algorithm that
incorporates intermediate supervision into the GRPO framework, offering
structured guidance over both reasoning-mode selection and response accuracy.
Extensive experiments across multiple benchmarks demonstrate that KAT
consistently matches or even outperforms current state-of-the-art models,
including DeepSeek-R1-0528 and Qwen3-235B-A22B, across a wide range of
reasoning-intensive tasks while reducing token usage by up to approximately
30\%. Beyond academic evaluation, KAT has been successfully deployed in
Kwaipilot (i.e., Kuaishou's internal coding assistant), and improves real-world
development workflows with high accuracy, efficiency, and controllable
reasoning behaviors. Moreover, we are actively training a 200B
Mixture-of-Experts (MoE) with 40B activation parameters, where the early-stage
results already demonstrate promising improvements in performance and
efficiency, further showing the scalability of the AutoThink paradigm.

</details>


### [30] [Improving MLLM's Document Image Machine Translation via Synchronously Self-reviewing Its OCR Proficiency](https://arxiv.org/abs/2507.08309)
*Yupu Liang,Yaping Zhang,Zhiyang Zhang,Zhiyuan Chen,Yang Zhao,Lu Xiang,Chengqing Zong,Yu Zhou*

Main category: cs.CL

TL;DR: 提出了一种名为SSR的新微调范式，通过同步自检OCR能力，解决多模态大语言模型在文档图像机器翻译任务中的遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在文档图像任务（如OCR）表现优异，但在跨模态和跨语言的文档图像机器翻译任务中存在遗忘问题。

Method: 引入SSR范式，通过让模型在生成翻译文本前先生成OCR文本，利用其单语OCR能力辅助跨语言翻译学习。

Result: 实验表明SSR能有效缓解灾难性遗忘，提升模型在OCR和DIMT任务上的泛化能力。

Conclusion: SSR是一种有效的微调方法，能够平衡模型的多任务学习能力。

Abstract: Multimodal Large Language Models (MLLMs) have shown strong performance in
document image tasks, especially Optical Character Recognition (OCR). However,
they struggle with Document Image Machine Translation (DIMT), which requires
handling both cross-modal and cross-lingual challenges. Previous efforts to
enhance DIMT capability through Supervised Fine-Tuning (SFT) on the DIMT
dataset often result in the forgetting of the model's existing monolingual
abilities, such as OCR. To address these challenges, we introduce a novel
fine-tuning paradigm, named Synchronously Self-Reviewing (SSR) its OCR
proficiency, inspired by the concept "Bilingual Cognitive Advantage".
Specifically, SSR prompts the model to generate OCR text before producing
translation text, which allows the model to leverage its strong monolingual OCR
ability while learning to translate text across languages. Comprehensive
experiments demonstrate the proposed SSR learning helps mitigate catastrophic
forgetting, improving the generalization ability of MLLMs on both OCR and DIMT
tasks.

</details>


### [31] [CRMAgent: A Multi-Agent LLM System for E-Commerce CRM Message Template Generation](https://arxiv.org/abs/2507.08325)
*Yinzhu Quan,Xinrui Li,Ying Chen*

Main category: cs.CL

TL;DR: CRMAgent是一个基于大语言模型的多智能体系统，通过三种模式生成高质量消息模板和写作指导，显著提升电商私域渠道的营销效果。


<details>
  <summary>Details</summary>
Motivation: 解决电商商户在私域渠道中因缺乏专业知识和工具而难以撰写有说服力消息的问题。

Method: CRMAgent采用三种模式：基于群组的学习、检索与适配、规则化回退，以生成和优化消息模板。

Result: 实验表明，CRMAgent在受众匹配和营销效果指标上显著优于商户原始模板。

Conclusion: CRMAgent为商户提供了一种可扩展且高效的解决方案，显著提升了私域渠道的营销效果。

Abstract: In e-commerce private-domain channels such as instant messaging and e-mail,
merchants engage customers directly as part of their Customer Relationship
Management (CRM) programmes to drive retention and conversion. While a few top
performers excel at crafting outbound messages, most merchants struggle to
write persuasive copy because they lack both expertise and scalable tools. We
introduce CRMAgent, a multi-agent system built on large language models (LLMs)
that generates high-quality message templates and actionable writing guidance
through three complementary modes. First, group-based learning enables the
agent to learn from a merchant's own top-performing messages within the same
audience segment and rewrite low-performing ones. Second,
retrieval-and-adaptation fetches templates that share the same audience segment
and exhibit high similarity in voucher type and product category, learns their
successful patterns, and adapts them to the current campaign. Third, a
rule-based fallback provides a lightweight zero-shot rewrite when no suitable
references are available. Extensive experiments show that CRMAgent consistently
outperforms merchants' original templates, delivering significant gains in both
audience-match and marketing-effectiveness metrics.

</details>


### [32] [MK2 at PBIG Competition: A Prompt Generation Solution](https://arxiv.org/abs/2507.08335)
*Yuzheng Xu,Tosho Hirasawa,Seiya Kawano,Shota Kato,Tadashi Kozuno*

Main category: cs.CL

TL;DR: MK2是一个基于提示的流程，通过Gemini 2.5和GPT-4.1生成专利产品创意，无需额外训练数据，在多个领域表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决将专利转化为三年内可行的产品创意的任务，探索轻量级提示工程的潜力。

Method: 使用Gemini 2.5迭代编辑提示，GPT-4.1生成创意，Qwen3-8B通过Elo循环选择最佳提示。

Result: 在三个领域、两种评估类型和六项标准中表现优异，赢得36项测试中的25项。

Conclusion: 轻量级提示工程已能生成具有商业价值的创意，但在材料化学领域需进一步优化。

Abstract: The Patent-Based Idea Generation task asks systems to turn real patents into
product ideas viable within three years. We propose MK2, a prompt-centric
pipeline: Gemini 2.5 drafts and iteratively edits a prompt, grafting useful
fragments from weaker outputs; GPT-4.1 then uses this prompt to create one idea
per patent, and an Elo loop judged by Qwen3-8B selects the best prompt-all
without extra training data. Across three domains, two evaluator types, and six
criteria, MK2 topped the automatic leaderboard and won 25 of 36 tests. Only the
materials-chemistry track lagged, indicating the need for deeper domain
grounding; yet, the results show that lightweight prompt engineering has
already delivered competitive, commercially relevant ideation from patents.

</details>


### [33] [What Factors Affect LLMs and RLLMs in Financial Question Answering?](https://arxiv.org/abs/2507.08339)
*Peng Wang,Xuesi Hu,Jiageng Wu,Yuntao Zou,Qiancheng Zhang,Dagang Li*

Main category: cs.CL

TL;DR: 研究了提示方法、代理框架和多语言对齐方法对LLMs和RLLMs在金融问答任务中的影响。


<details>
  <summary>Details</summary>
Motivation: 探索如何充分释放LLMs和RLLMs在金融领域的性能。

Method: 使用五种LLMs和三种RLLMs评估不同方法的效果。

Result: 提示方法和代理框架通过模拟Long CoT提升LLMs性能；RLLMs因固有Long CoT能力，常规方法效果有限；多语言对齐方法对LLMs有效但对RLLMs帮助不大。

Conclusion: 本研究为金融问答领域的LLMs和RLLMs提供了重要参考。

Abstract: Recently, the development of large language models (LLMs) and reasoning large
language models (RLLMs) have gained considerable attention from many
researchers. RLLMs enhance the reasoning capabilities of LLMs through Long
Chain-of-Thought (Long CoT) processes, significantly improving the performance
of LLMs in addressing complex problems. However, there are few works that
systematically explore what methods can fully unlock the performance of LLMs
and RLLMs within the financial domain. To investigate the impact of various
methods on LLMs and RLLMs, we utilize five LLMs and three RLLMs to assess the
effects of prompting methods, agentic frameworks, and multilingual alignment
methods on financial question-answering tasks. Our research findings indicate:
(1) Current prompting methods and agent frameworks enhance the performance of
LLMs in financial question answering by simulating Long CoT; (2) RLLMs possess
inherent Long CoT capabilities, which limits the effectiveness of conventional
methods in further enhancing their performance; (3) Current advanced
multilingual alignment methods primarily improve the multilingual performance
of LLMs by extending the reasoning length, which yields minimal benefits for
RLLMs. We hope that this study can serve as an important reference for LLMs and
RLLMs in the field of financial question answering.

</details>


### [34] [Beyond N-Grams: Rethinking Evaluation Metrics and Strategies for Multilingual Abstractive Summarization](https://arxiv.org/abs/2507.08342)
*Itai Mondshine,Tzuf Paz-Argaman,Reut Tsarfaty*

Main category: cs.CL

TL;DR: 评估自动n-gram和神经指标在多语言生成任务中的有效性，发现指标性能受语言类型影响，神经指标（如COMET）表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有自动评估指标（如ROUGE）在英语中表现良好，但在其他语言中的适用性尚不明确。

Method: 设计大规模评估套件，覆盖八种语言和四种语言类型，分析指标与人工评估的相关性。

Result: n-gram指标在融合性语言中表现较差，神经指标（如COMET）在低资源语言中表现更优。

Conclusion: n-gram指标在融合性语言中存在局限性，应更多投资于神经指标的开发。

Abstract: Automatic n-gram based metrics such as ROUGE are widely used for evaluating
generative tasks such as summarization. While these metrics are considered
indicative (even if imperfect) of human evaluation for English, their
suitability for other languages remains unclear. To address this, we
systematically assess evaluation metrics for generation both n-gram-based and
neural based to evaluate their effectiveness across languages and tasks.
Specifically, we design a large-scale evaluation suite across eight languages
from four typological families: agglutinative, isolating, low-fusional, and
high-fusional, spanning both low- and high-resource settings, to analyze their
correlation with human judgments. Our findings highlight the sensitivity of
evaluation metrics to the language type. For example, in fusional languages,
n-gram-based metrics show lower correlation with human assessments compared to
isolating and agglutinative languages. We also demonstrate that proper
tokenization can significantly mitigate this issue for morphologically rich
fusional languages, sometimes even reversing negative trends. Additionally, we
show that neural-based metrics specifically trained for evaluation, such as
COMET, consistently outperform other neural metrics and better correlate with
human judgments in low-resource languages. Overall, our analysis highlights the
limitations of n-gram metrics for fusional languages and advocates for greater
investment in neural-based metrics trained for evaluation tasks.

</details>


### [35] [Exploring Design of Multi-Agent LLM Dialogues for Research Ideation](https://arxiv.org/abs/2507.08350)
*Keisuke Ueda,Wataru Hirota,Takuto Asakura,Takahiro Omi,Kosuke Takahashi,Kosuke Arima,Tatsuya Ishigaki*

Main category: cs.CL

TL;DR: 研究探讨了多智能体LLM对话在科学创意生成中的优化设计，分析了智能体角色、数量和对话深度对创意新颖性和可行性的影响。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过多智能体LLM对话提升科学创意的生成质量，尤其是新颖性和可行性。

Method: 比较不同智能体配置（角色、数量、对话深度），采用生成-批评-修订的迭代模式。

Result: 增加智能体数量、对话深度和角色多样性可提升创意多样性；批评者多样性进一步提高了可行性。

Conclusion: 研究为构建高效多智能体LLM科学创意系统提供了实用指南。

Abstract: Large language models (LLMs) are increasingly used to support creative tasks
such as research idea generation. While recent work has shown that structured
dialogues between LLMs can improve the novelty and feasibility of generated
ideas, the optimal design of such interactions remains unclear. In this study,
we conduct a comprehensive analysis of multi-agent LLM dialogues for scientific
ideation. We compare different configurations of agent roles, number of agents,
and dialogue depth to understand how these factors influence the novelty and
feasibility of generated ideas. Our experimental setup includes settings where
one agent generates ideas and another critiques them, enabling iterative
improvement. Our results show that enlarging the agent cohort, deepening the
interaction depth, and broadening agent persona heterogeneity each enrich the
diversity of generated ideas. Moreover, specifically increasing critic-side
diversity within the ideation-critique-revision loop further boosts the
feasibility of the final proposals. Our findings offer practical guidelines for
building effective multi-agent LLM systems for scientific ideation. Our code is
available at https://github.com/g6000/MultiAgent-Research-Ideator.

</details>


### [36] [The Curious Case of Factuality Finetuning: Models' Internal Beliefs Can Improve Factuality](https://arxiv.org/abs/2507.08371)
*Benjamin Newman,Abhilasha Ravichander,Jaehun Jung,Rui Xin,Hamish Ivison,Yegor Kuznetsov,Pang Wei Koh,Yejin Choi*

Main category: cs.CL

TL;DR: 研究发现，微调语言模型时，使用模型自认为可信的数据比使用真实黄金数据更能减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过微调数据的选择减少语言模型生成文本时的幻觉问题。

Method: 研究微调数据的真实性对长文本生成任务中幻觉现象的影响，比较黄金数据和模型生成数据的效果。

Result: 微调模型自认为可信的数据比黄金数据更有效，且通过模型内部判断过滤的数据效果最佳。

Conclusion: 模型自身的信念可以作为提升生成文本事实性的有效信号。

Abstract: Language models are prone to hallucination - generating text that is
factually incorrect. Finetuning models on high-quality factual information can
potentially reduce hallucination, but concerns remain; obtaining factual gold
data can be expensive and training on correct but unfamiliar data may
potentially lead to even more downstream hallucination. What data should
practitioners finetune on to mitigate hallucinations in language models? In
this work, we study the relationship between the factuality of finetuning data
and the prevalence of hallucinations in long-form generation tasks.
Counterintuitively, we find that finetuning on factual gold data is not as
helpful as finetuning on model-generated data that models believe to be
factual. Next, we evaluate filtering strategies applied on both factual gold
data and model-generated data, and find that finetuning on model-generated data
that is filtered by models' own internal judgments often leads to better
overall factuality compared to other configurations: training on gold data
filtered by models' judgments, training on gold data alone, or training on
model-generated data that is supported by gold data. These factuality
improvements transfer across three domains we study, suggesting that a models'
own beliefs can provide a powerful signal for factuality.

</details>


### [37] [A Survey of Large Language Models in Discipline-specific Research: Challenges, Methods and Opportunities](https://arxiv.org/abs/2507.08425)
*Lu Xiang,Yang Zhao,Yaping Zhang,Chengqing Zong*

Main category: cs.CL

TL;DR: 这篇综述论文全面探讨了大语言模型（LLMs）在跨学科研究中的应用，从技术方法和适用性两个角度进行了分类分析。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多个学科中展现出变革潜力，但其系统性的跨学科整合仍未被充分研究。本文旨在填补这一空白。

Method: 论文从技术角度（如监督微调、检索增强生成、基于代理的方法和工具集成）和适用性角度（如数学、物理、化学、生物学及人文社科）分析了LLMs的应用。

Result: LLMs在跨学科任务中表现出显著潜力，但也面临挑战。论文总结了技术进展和应用案例，并指出了未来研究方向。

Conclusion: 本文为研究者提供了LLMs在跨学科研究中的全面资源，强调了其技术发展和应用前景。

Abstract: Large Language Models (LLMs) have demonstrated their transformative potential
across numerous disciplinary studies, reshaping the existing research
methodologies and fostering interdisciplinary collaboration. However, a
systematic understanding of their integration into diverse disciplines remains
underexplored. This survey paper provides a comprehensive overview of the
application of LLMs in interdisciplinary studies, categorising research efforts
from both a technical perspective and with regard to their applicability. From
a technical standpoint, key methodologies such as supervised fine-tuning,
retrieval-augmented generation, agent-based approaches, and tool-use
integration are examined, which enhance the adaptability and effectiveness of
LLMs in discipline-specific contexts. From the perspective of their
applicability, this paper explores how LLMs are contributing to various
disciplines including mathematics, physics, chemistry, biology, and the
humanities and social sciences, demonstrating their role in discipline-specific
tasks. The prevailing challenges are critically examined and the promising
research directions are highlighted alongside the recent advances in LLMs. By
providing a comprehensive overview of the technical developments and
applications in this field, this survey aims to serve as an invaluable resource
for the researchers who are navigating the complex landscape of LLMs in the
context of interdisciplinary studies.

</details>


### [38] [ChainEdit: Propagating Ripple Effects in LLM Knowledge Editing through Logical Rule-Guided Chains](https://arxiv.org/abs/2507.08427)
*Zilu Dong,Xiangqing Shen,Zinong Yang,Rui Xia*

Main category: cs.CL

TL;DR: ChainEdit框架结合知识图谱逻辑规则与LLM推理能力，系统性更新知识，提升逻辑一致性30%以上。


<details>
  <summary>Details</summary>
Motivation: 现有LLM知识编辑方法在关联事实传播中难以保持逻辑一致性。

Method: 通过知识图谱提取逻辑规则，与LLM内部逻辑对齐，动态生成和编辑逻辑关联知识簇。

Result: 实验显示逻辑泛化能力提升30%以上，同时保持编辑可靠性和特异性。

Conclusion: ChainEdit在知识编辑中实现新SOTA性能，确保内部逻辑一致性。

Abstract: Current knowledge editing methods for large language models (LLMs) struggle
to maintain logical consistency when propagating ripple effects to associated
facts. We propose ChainEdit, a framework that synergizes knowledge
graph-derived logical rules with LLM logical reasoning capabilities to enable
systematic chain updates. By automatically extracting logical patterns from
structured knowledge bases and aligning them with LLMs' internal logics,
ChainEdit dynamically generates and edits logically connected knowledge
clusters. Experiments demonstrate an improvement of more than 30% in logical
generalization over baselines while preserving editing reliability and
specificity. We further address evaluation biases in existing benchmarks
through knowledge-aware protocols that disentangle external dependencies. This
work establishes new state-of-the-art performance on ripple effect while
ensuring internal logical consistency after knowledge editing.

</details>


### [39] [Finding Common Ground: Using Large Language Models to Detect Agreement in Multi-Agent Decision Conferences](https://arxiv.org/abs/2507.08440)
*Selina Heller,Mohamed Ibrahim,David Antony Selby,Sebastian Vollmer*

Main category: cs.CL

TL;DR: 该论文提出了一种基于LLM的多智能体系统，用于模拟决策会议并检测参与者之间的共识，结果表明LLM能有效识别动态辩论中的一致性。


<details>
  <summary>Details</summary>
Motivation: 决策会议通常依赖专家协作达成共识，而LLM在模拟多智能体互动方面具有潜力，因此研究如何利用LLM模拟此类会议并检测共识。

Method: 评估六种LLM在立场检测和立场极性检测任务上的表现，并将其整合到多智能体系统中，以模拟复杂辩论并检测共识。

Result: LLM能可靠地检测动态辩论中的共识，加入共识检测智能体可提升辩论效率和决策质量，效果接近真实决策会议。

Conclusion: LLM多智能体系统能有效模拟群体决策过程，支持跨领域的专家决策工作。

Abstract: Decision conferences are structured, collaborative meetings that bring
together experts from various fields to address complex issues and reach a
consensus on recommendations for future actions or policies. These conferences
often rely on facilitated discussions to ensure productive dialogue and
collective agreement. Recently, Large Language Models (LLMs) have shown
significant promise in simulating real-world scenarios, particularly through
collaborative multi-agent systems that mimic group interactions. In this work,
we present a novel LLM-based multi-agent system designed to simulate decision
conferences, specifically focusing on detecting agreement among the participant
agents. To achieve this, we evaluate six distinct LLMs on two tasks: stance
detection, which identifies the position an agent takes on a given issue, and
stance polarity detection, which identifies the sentiment as positive,
negative, or neutral. These models are further assessed within the multi-agent
system to determine their effectiveness in complex simulations. Our results
indicate that LLMs can reliably detect agreement even in dynamic and nuanced
debates. Incorporating an agreement-detection agent within the system can also
improve the efficiency of group debates and enhance the overall quality and
coherence of deliberations, making them comparable to real-world decision
conferences regarding outcome and decision-making. These findings demonstrate
the potential for LLM-based multi-agent systems to simulate group
decision-making processes. They also highlight that such systems could be
instrumental in supporting decision-making with expert elicitation workshops
across various domains.

</details>


### [40] [Diagnosing Failures in Large Language Models' Answers: Integrating Error Attribution into Evaluation Framework](https://arxiv.org/abs/2507.08459)
*Zishan Xu,Shuyi Xie,Qingsong Lv,Shupei Xiao,Linlin Song,Sui Wenjuan,Fan Lin*

Main category: cs.CL

TL;DR: 提出了一种自动化框架（Misattribution Framework）和数据集（AttriData），用于系统分类和归因LLM的错误，并开发了首个通用评判模型（MisAttributionLLM）。


<details>
  <summary>Details</summary>
Motivation: 现有评估模型缺乏错误归因能力，需开发自动化框架以高效分析模型表现和诊断错误。

Method: 建立包含6个主类和15个子类的错误归因框架，创建AttriData数据集，并基于此微调MisAttributionLLM模型。

Result: 实验验证了方法的有效性和鲁棒性，MisAttributionLLM能同时生成评分、错误归因和反馈。

Conclusion: 提出的框架和模型为LLM错误分析提供了系统化工具，填补了现有评估的不足。

Abstract: With the widespread application of Large Language Models (LLMs) in various
tasks, the mainstream LLM platforms generate massive user-model interactions
daily. In order to efficiently analyze the performance of models and diagnose
failures in their answers, it is essential to develop an automated framework to
systematically categorize and attribute errors. However, existing evaluation
models lack error attribution capability. In this work, we establish a
comprehensive Misattribution Framework with 6 primary and 15 secondary
categories to facilitate in-depth analysis. Based on this framework, we present
AttriData, a dataset specifically designed for error attribution, encompassing
misattribution, along with the corresponding scores and feedback. We also
propose MisAttributionLLM, a fine-tuned model on AttriData, which is the first
general-purpose judge model capable of simultaneously generating score,
misattribution, and feedback. Extensive experiments and analyses are conducted
to confirm the effectiveness and robustness of our proposed method.

</details>


### [41] [Using Large Language Models for Legal Decision-Making in Austrian Value-Added Tax Law: An Experimental Study](https://arxiv.org/abs/2507.08468)
*Marina Luketina,Andrea Benkel,Christoph G. Schuetz*

Main category: cs.CL

TL;DR: 论文评估了大型语言模型（LLM）在奥地利和欧盟增值税法律框架下辅助法律决策的能力，探讨了微调和检索增强生成（RAG）两种方法的效果。


<details>
  <summary>Details</summary>
Motivation: 税收咨询实践中，客户常以自然语言描述案例，LLM有望支持自动化决策并减轻税务专业人员的工作负担。

Method: 实验采用微调和检索增强生成（RAG）方法，应用于教科书案例和实际税务咨询案例，评估LLM的法律推理能力。

Result: 研究发现，LLM在适当配置下能有效支持税务专业人员，提供法律依据充分的决策分析，但当前原型尚无法完全自动化。

Conclusion: LLM在税务咨询中具有潜力，但仍需整合结构化背景信息以处理隐含客户知识和特定上下文文档。

Abstract: This paper provides an experimental evaluation of the capability of large
language models (LLMs) to assist in legal decision-making within the framework
of Austrian and European Union value-added tax (VAT) law. In tax consulting
practice, clients often describe cases in natural language, making LLMs a prime
candidate for supporting automated decision-making and reducing the workload of
tax professionals. Given the requirement for legally grounded and
well-justified analyses, the propensity of LLMs to hallucinate presents a
considerable challenge. The experiments focus on two common methods for
enhancing LLM performance: fine-tuning and retrieval-augmented generation
(RAG). In this study, these methods are applied on both textbook cases and
real-world cases from a tax consulting firm to systematically determine the
best configurations of LLM-based systems and assess the legal-reasoning
capabilities of LLMs. The findings highlight the potential of using LLMs to
support tax consultants by automating routine tasks and providing initial
analyses, although current prototypes are not ready for full automation due to
the sensitivity of the legal domain. The findings indicate that LLMs, when
properly configured, can effectively support tax professionals in VAT tasks and
provide legally grounded justifications for decisions. However, limitations
remain regarding the handling of implicit client knowledge and context-specific
documentation, underscoring the need for future integration of structured
background information.

</details>


### [42] [ILT-Iterative LoRA Training through Focus-Feedback-Fix for Multilingual Speech Recognition](https://arxiv.org/abs/2507.08477)
*Qingliang Meng,Hao Wu,Wei Liang,Wei Xu,Qing Zhao*

Main category: cs.CL

TL;DR: 提出了一种结合迭代LoRA训练（ILT）和迭代伪标签策略的新方法，解决了LoRA在监督微调阶段的过拟合问题，提升了模型性能上限。


<details>
  <summary>Details</summary>
Motivation: 解决LoRA在监督微调阶段的过拟合问题，提升大语言模型与自动语音识别系统的集成性能。

Method: 采用迭代LoRA训练（ILT）和迭代伪标签策略，通过三阶段训练（Focus Training、Feed Back Training、Fix Training）优化模型。

Result: 实验验证了方法的有效性，并在Interspeech 2025挑战赛中取得优异成绩。

Conclusion: 该方法具有实际可行性和强大的应用潜力。

Abstract: The deep integration of large language models and automatic speech
recognition systems has become a promising research direction with high
practical value. To address the overfitting issue commonly observed in Low-Rank
Adaptation (LoRA) during the supervised fine-tuning (SFT) stage, this work
proposes an innovative training paradigm Iterative LoRA Training (ILT) in
combination with an Iterative Pseudo Labeling strategy, effectively enhancing
the theoretical upper bound of model performance. Based on Whisper-large-v3 and
Qwen2-Audio, we conduct systematic experiments using a three-stage training
process: Focus Training, Feed Back Training, and Fix Training. Experimental
results demonstrate the effectiveness of the proposed method. Furthermore, the
MegaAIS research team applied this technique in the Interspeech 2025
Multilingual Conversational Speech Language Modeling Challenge (MLC-SLM),
achieving 4th in Track 1 (Multilingual ASR Task) and 1st place in Track 2
(Speech Separation and Recognition Task), showcasing the practical feasibility
and strong application potential of our approach.

</details>


### [43] [Enhancing Essay Cohesion Assessment: A Novel Item Response Theory Approach](https://arxiv.org/abs/2507.08487)
*Bruno Alexandre Rosa,Hilário Oliveira,Luiz Rodrigues,Eduardo Araujo Oliveira,Rafael Ferreira Mello*

Main category: cs.CL

TL;DR: 该论文提出了一种基于项目反应理论的机器学习方法，用于自动评分教育论文的连贯性，并在实验中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 论文旨在解决教育人工智能领域中自动评分文本连贯性的挑战，传统机器学习算法未考虑文本实例的个体特征。

Method: 采用项目反应理论调整机器学习模型的评分，提取325个语言特征，并将问题视为回归任务。

Result: 实验结果表明，该方法在多个评估指标上优于传统机器学习模型和集成方法。

Conclusion: 该研究为改进教育论文连贯性的自动评估提供了一种潜在的有效方法。

Abstract: Essays are considered a valuable mechanism for evaluating learning outcomes
in writing. Textual cohesion is an essential characteristic of a text, as it
facilitates the establishment of meaning between its parts. Automatically
scoring cohesion in essays presents a challenge in the field of educational
artificial intelligence. The machine learning algorithms used to evaluate texts
generally do not consider the individual characteristics of the instances that
comprise the analysed corpus. In this meaning, item response theory can be
adapted to the context of machine learning, characterising the ability,
difficulty and discrimination of the models used. This work proposes and
analyses the performance of a cohesion score prediction approach based on item
response theory to adjust the scores generated by machine learning models. In
this study, the corpus selected for the experiments consisted of the extended
Essay-BR, which includes 6,563 essays in the style of the National High School
Exam (ENEM), and the Brazilian Portuguese Narrative Essays, comprising 1,235
essays written by 5th to 9th grade students from public schools. We extracted
325 linguistic features and treated the problem as a machine learning
regression task. The experimental results indicate that the proposed approach
outperforms conventional machine learning models and ensemble methods in
several evaluation metrics. This research explores a potential approach for
improving the automatic evaluation of cohesion in educational essays.

</details>


### [44] [A Third Paradigm for LLM Evaluation: Dialogue Game-Based Evaluation using clembench](https://arxiv.org/abs/2507.08491)
*David Schlangen,Sherzod Hakimov,Jonathan Jordan,Philipp Sadler*

Main category: cs.CL

TL;DR: 论文介绍了clembench，一种结合了参考评估和偏好评估优点的对话游戏评估工具，旨在提供可控、可重复的多轮交互测试。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估方法（参考评估和偏好评估）各有优缺点，缺乏一种结合两者优势的范式。

Method: 提出clembench工具，支持多轮、无参考、可重复的交互测试，并提供易于扩展的基准测试集。

Result: clembench已持续开发至成熟阶段，优化了易用性，支持用户自定义测试和扩展。

Conclusion: clembench为语言模型评估提供了一种新的、灵活的范式，填补了现有方法的不足。

Abstract: There are currently two main paradigms for evaluating large language models
(LLMs), reference-based evaluation and preference-based evaluation. The first,
carried over from the evaluation of machine learning models in general, relies
on pre-defined task instances, for which reference task executions are
available. The second, best exemplified by the LM-arena, relies on (often
self-selected) users bringing their own intents to a site that routes these to
several models in parallel, among whose responses the user then selects their
most preferred one. The former paradigm hence excels at control over what is
tested, while the latter comes with higher ecological validity, testing actual
use cases interactively. Recently, a third complementary paradigm has emerged
that combines some of the strengths of these approaches, offering control over
multi-turn, reference-free, repeatable interactions, while stressing
goal-directedness: dialogue game based evaluation. While the utility of this
approach has been shown by several projects, its adoption has been held back by
the lack of a mature, easily re-usable implementation. In this paper, we
present clembench, which has been in continuous development since 2023 and has
in its latest release been optimized for ease of general use. We describe how
it can be used to benchmark one's own models (using a provided set of benchmark
game instances in English), as well as how easily the benchmark itself can be
extended with new, tailor-made targeted tests.

</details>


### [45] [LLaPa: A Vision-Language Model Framework for Counterfactual-Aware Procedural Planning](https://arxiv.org/abs/2507.08496)
*Shibo Sun,Xue Li,Donglin Di,Mingjie Wei,Lanshun Nie,Wei-Nan Zhang,Dechen Zhan,Yang Song,Lei Fan*

Main category: cs.CL

TL;DR: LLaPa是一个多模态程序规划框架，结合视觉语言模型和辅助模块，提升任务执行计划的生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在多模态输入和反事实推理方面的不足。

Method: LLaPa框架结合视觉语言模型生成可执行动作序列，并引入任务环境重排器和反事实活动检索器优化规划。

Result: 在ActPlan-1K和ALFRED基准测试中表现优异，生成计划的质量和正确性更高。

Conclusion: LLaPa通过多模态和反事实推理增强，显著提升了程序规划能力。

Abstract: While large language models (LLMs) have advanced procedural planning for
embodied AI systems through strong reasoning abilities, the integration of
multimodal inputs and counterfactual reasoning remains underexplored. To tackle
these challenges, we introduce LLaPa, a vision-language model framework
designed for multimodal procedural planning. LLaPa generates executable action
sequences from textual task descriptions and visual environmental images using
vision-language models (VLMs). Furthermore, we enhance LLaPa with two auxiliary
modules to improve procedural planning. The first module, the Task-Environment
Reranker (TER), leverages task-oriented segmentation to create a task-sensitive
feature space, aligning textual descriptions with visual environments and
emphasizing critical regions for procedural execution. The second module, the
Counterfactual Activities Retriever (CAR), identifies and emphasizes potential
counterfactual conditions, enhancing the model's reasoning capability in
counterfactual scenarios. Extensive experiments on ActPlan-1K and ALFRED
benchmarks demonstrate that LLaPa generates higher-quality plans with superior
LCS and correctness, outperforming advanced models. The code and models are
available https://github.com/sunshibo1234/LLaPa.

</details>


### [46] [Semantic-Augmented Latent Topic Modeling with LLM-in-the-Loop](https://arxiv.org/abs/2507.08498)
*Mengze Hong,Chen Jason Zhang,Di Jiang*

Main category: cs.CL

TL;DR: 论文探讨了将大型语言模型（LLM）与潜在狄利克雷分配（LDA）结合在初始化和后校正阶段的效用。实验表明，LLM引导的初始化对LDA早期迭代有帮助，但对收敛无影响且性能最差；而LLM后校正则提升了5.86%的连贯性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过LLM增强LDA模型在文本挖掘中的表现，探索LLM在初始化和后校正阶段的作用。

Method: 将LLM集成到LDA的两个关键阶段：初始化（LLM引导的主题聚类）和后校正（LLM修正）。通过实验评估其效果。

Result: LLM初始化对LDA早期迭代有改进，但对收敛无影响且性能最差；LLM后校正显著提升了5.86%的连贯性。

Conclusion: LLM在LDA后校正阶段表现优异，但初始化阶段效果不佳，挑战了LLM总是优于传统方法的观点。

Abstract: Latent Dirichlet Allocation (LDA) is a prominent generative probabilistic
model used for uncovering abstract topics within document collections. In this
paper, we explore the effectiveness of augmenting topic models with Large
Language Models (LLMs) through integration into two key phases: Initialization
and Post-Correction. Since the LDA is highly dependent on the quality of its
initialization, we conduct extensive experiments on the LLM-guided topic
clustering for initializing the Gibbs sampling algorithm. Interestingly, the
experimental results reveal that while the proposed initialization strategy
improves the early iterations of LDA, it has no effect on the convergence and
yields the worst performance compared to the baselines. The LLM-enabled
post-correction, on the other hand, achieved a promising improvement of 5.86%
in the coherence evaluation. These results highlight the practical benefits of
the LLM-in-the-loop approach and challenge the belief that LLMs are always the
superior text mining alternative.

</details>


### [47] [PromotionGo at SemEval-2025 Task 11: A Feature-Centric Framework for Cross-Lingual Multi-Emotion Detection in Short Texts](https://arxiv.org/abs/2507.08499)
*Ziyi Huang,Xia Cui*

Main category: cs.CL

TL;DR: 本文提出了一种用于多语言短文本多标签情感检测的特征中心框架，评估了文档表示、降维和模型训练三个关键组件，结果显示TF-IDF在低资源语言中表现优异，而FastText和Sentence-BERT在不同语言中各有优势。


<details>
  <summary>Details</summary>
Motivation: 解决多语言情感检测中的语言多样性和资源限制问题。

Method: 提出动态适应文档表示和学习算法的特征中心框架，评估了28种语言中的文档表示、降维和模型训练。

Result: TF-IDF在低资源语言中表现优异，FastText和Sentence-BERT有语言特定优势，PCA能减少训练时间而不影响性能。

Conclusion: 该框架为多语言情感检测提供了可扩展的解决方案，平衡了模型复杂性和计算成本。

Abstract: This paper presents our system for SemEval 2025 Task 11: Bridging the Gap in
Text-Based Emotion Detection (Track A), which focuses on multi-label emotion
detection in short texts. We propose a feature-centric framework that
dynamically adapts document representations and learning algorithms to optimize
language-specific performance. Our study evaluates three key components:
document representation, dimensionality reduction, and model training in 28
languages, highlighting five for detailed analysis. The results show that
TF-IDF remains highly effective for low-resource languages, while contextual
embeddings like FastText and transformer-based document representations, such
as those produced by Sentence-BERT, exhibit language-specific strengths.
Principal Component Analysis (PCA) reduces training time without compromising
performance, particularly benefiting FastText and neural models such as
Multi-Layer Perceptrons (MLP). Computational efficiency analysis underscores
the trade-off between model complexity and processing cost. Our framework
provides a scalable solution for multilingual emotion detection, addressing the
challenges of linguistic diversity and resource constraints.

</details>


### [48] [The AI Language Proficiency Monitor -- Tracking the Progress of LLMs on Multilingual Benchmarks](https://arxiv.org/abs/2507.08538)
*David Pomerenke,Jonas Nothnagel,Simon Ostermann*

Main category: cs.CL

TL;DR: AI Language Proficiency Monitor是一个多语言基准测试，用于评估大型语言模型（LLMs）在200种语言中的表现，特别关注低资源语言。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型的公平访问，促进多语言AI的透明度和包容性。

Method: 结合翻译、问答、数学和推理等任务，使用FLORES+、MMLU等数据集，提供开源、自动更新的排行榜和仪表盘。

Result: 提供了一个全球熟练度地图和趋势分析，帮助识别模型性能的优势和不足。

Conclusion: 该基准测试旨在推动多语言AI的进步，促进透明和包容性。

Abstract: To ensure equitable access to the benefits of large language models (LLMs),
it is essential to evaluate their capabilities across the world's languages. We
introduce the AI Language Proficiency Monitor, a comprehensive multilingual
benchmark that systematically assesses LLM performance across up to 200
languages, with a particular focus on low-resource languages. Our benchmark
aggregates diverse tasks including translation, question answering, math, and
reasoning, using datasets such as FLORES+, MMLU, GSM8K, TruthfulQA, and ARC. We
provide an open-source, auto-updating leaderboard and dashboard that supports
researchers, developers, and policymakers in identifying strengths and gaps in
model performance. In addition to ranking models, the platform offers
descriptive insights such as a global proficiency map and trends over time. By
complementing and extending prior multilingual benchmarks, our work aims to
foster transparency, inclusivity, and progress in multilingual AI. The system
is available at
https://huggingface.co/spaces/fair-forward/evals-for-every-language.

</details>


### [49] [DocPolarBERT: A Pre-trained Model for Document Understanding with Relative Polar Coordinate Encoding of Layout Structures](https://arxiv.org/abs/2507.08606)
*Benno Uthayasooriyar,Antoine Ly,Franck Vermet,Caio Corro*

Main category: cs.CL

TL;DR: DocPolarBERT是一种布局感知的BERT模型，通过相对极坐标系统改进自注意力机制，无需绝对2D位置嵌入，在小规模预训练数据下实现SOTA效果。


<details>
  <summary>Details</summary>
Motivation: 传统文档理解模型依赖绝对2D位置嵌入，计算复杂且数据需求高，DocPolarBERT旨在通过更高效的注意力机制解决这一问题。

Method: 扩展自注意力机制，采用相对极坐标系表示文本块位置，替代传统的笛卡尔坐标系。

Result: 在预训练数据量仅为IIT-CDIP六分之一的情况下，DocPolarBERT仍达到最先进性能。

Conclusion: 精心设计的注意力机制可弥补数据量的不足，为文档理解提供高效且有效的解决方案。

Abstract: We introduce DocPolarBERT, a layout-aware BERT model for document
understanding that eliminates the need for absolute 2D positional embeddings.
We extend self-attention to take into account text block positions in relative
polar coordinate system rather than the Cartesian one. Despite being
pre-trained on a dataset more than six times smaller than the widely used
IIT-CDIP corpus, DocPolarBERT achieves state-of-the-art results. These results
demonstrate that a carefully designed attention mechanism can compensate for
reduced pre-training data, offering an efficient and effective alternative for
document understanding.

</details>


### [50] [A comprehensive study of LLM-based argument classification: from LLAMA through GPT-4o to Deepseek-R1](https://arxiv.org/abs/2507.08621)
*Marcin Pietroń,Rafał Olszowski,Jakub Gomułka,Filip Gampel,Andrzej Tomski*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLM）在公开论证分类数据库中的表现，测试了包括GPT、Llama和DeepSeek等模型，发现ChatGPT-4o和Deepseek-R1表现最佳，但也存在常见错误。


<details>
  <summary>Details</summary>
Motivation: 论证挖掘（AM）是一个跨学科领域，但缺乏对LLM在公开论证分类数据库中表现的研究。本文旨在填补这一空白。

Method: 使用Args.me和UKP等多样数据集，测试了GPT、Llama和DeepSeek等LLM及其推理增强变体（如Chain-of-Thoughts算法）。

Result: ChatGPT-4o在论证分类基准中表现最佳，而Deepseek-R1在推理增强模型中表现最优。但两者仍存在常见错误。

Conclusion: 本文首次对LLM在论证数据集中的表现进行了广泛分析，揭示了已知提示算法的弱点，并提出了改进方向。

Abstract: Argument mining (AM) is an interdisciplinary research field that integrates
insights from logic, philosophy, linguistics, rhetoric, law, psychology, and
computer science. It involves the automatic identification and extraction of
argumentative components, such as premises and claims, and the detection of
relationships between them, such as support, attack, or neutrality. Recently,
the field has advanced significantly, especially with the advent of large
language models (LLMs), which have enhanced the efficiency of analyzing and
extracting argument semantics compared to traditional methods and other deep
learning models. There are many benchmarks for testing and verifying the
quality of LLM, but there is still a lack of research and results on the
operation of these models in publicly available argument classification
databases. This paper presents a study of a selection of LLM's, using diverse
datasets such as Args.me and UKP. The models tested include versions of GPT,
Llama, and DeepSeek, along with reasoning-enhanced variants incorporating the
Chain-of-Thoughts algorithm. The results indicate that ChatGPT-4o outperforms
the others in the argument classification benchmarks. In case of models
incorporated with reasoning capabilities, the Deepseek-R1 shows its
superiority. However, despite their superiority, GPT-4o and Deepseek-R1 still
make errors. The most common errors are discussed for all models. To our
knowledge, the presented work is the first broader analysis of the mentioned
datasets using LLM and prompt algorithms. The work also shows some weaknesses
of known prompt algorithms in argument analysis, while indicating directions
for their improvement. The added value of the work is the in-depth analysis of
the available argument datasets and the demonstration of their shortcomings.

</details>


### [51] [The Impact of Automatic Speech Transcription on Speaker Attribution](https://arxiv.org/abs/2507.08660)
*Cristina Aggazzotti,Matthew Wiesner,Elizabeth Allyn Smith,Nicholas Andrews*

Main category: cs.CL

TL;DR: 研究探讨了自动语音识别（ASR）生成的错误转录对说话者归属任务的影响，发现其性能与人工转录相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，自动转录的文本往往存在错误，但此前研究多基于人工转录。本文旨在填补这一空白。

Method: 通过对比ASR转录和人工转录的说话者归属性能，分析转录错误的影响。

Result: ASR转录的说话者归属表现优于或等同于人工转录，转录错误可能揭示了说话者特征。

Conclusion: ASR转录可用于说话者归属任务，且性能不逊于人工转录。

Abstract: Speaker attribution from speech transcripts is the task of identifying a
speaker from the transcript of their speech based on patterns in their language
use. This task is especially useful when the audio is unavailable (e.g.
deleted) or unreliable (e.g. anonymized speech). Prior work in this area has
primarily focused on the feasibility of attributing speakers using transcripts
produced by human annotators. However, in real-world settings, one often only
has more errorful transcripts produced by automatic speech recognition (ASR)
systems. In this paper, we conduct what is, to our knowledge, the first
comprehensive study of the impact of automatic transcription on speaker
attribution performance. In particular, we study the extent to which speaker
attribution performance degrades in the face of transcription errors, as well
as how properties of the ASR system impact attribution. We find that
attribution is surprisingly resilient to word-level transcription errors and
that the objective of recovering the true transcript is minimally correlated
with attribution performance. Overall, our findings suggest that speaker
attribution on more errorful transcripts produced by ASR is as good, if not
better, than attribution based on human-transcribed data, possibly because ASR
transcription errors can capture speaker-specific features revealing of speaker
identity.

</details>


### [52] [KELPS: A Framework for Verified Multi-Language Autoformalization via Semantic-Syntactic Alignment](https://arxiv.org/abs/2507.08665)
*Jiyao Zhang,Chengli Zhong,Hui Xu,Qige Li,Yi Zhou*

Main category: cs.CL

TL;DR: KELPS框架通过神经符号方法将非正式数学转化为多种形式化语言，解决了多语言平行语料库的瓶颈问题，并在多个数据集上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在形式化非正式数学时因多语言平行语料库数量和质量限制而面临的瓶颈问题。

Method: 提出KELPS框架，通过知识方程（KEs）将自然语言翻译为形式化语言，并定义严格规则保持结构和语义。

Result: 生成了60,000多个问题的平行语料库，在MiniF2F上达到88.9%的语法准确率，优于Deepseek-V3和Herald。

Conclusion: KELPS框架在形式化数学任务中表现出色，为相关领域提供了新的解决方案。

Abstract: Modern large language models (LLMs) show promising progress in formalizing
informal mathematics into machine-verifiable theorems. However, these methods
still face bottlenecks due to the limited quantity and quality of multilingual
parallel corpora. In this paper, we propose a novel neuro-symbolic framework
KELPS (Knowledge-Equation based Logical Processing System) to address these
problems. KELPS is an iterative framework for translating, synthesizing, and
filtering informal data into multiple formal languages (Lean, Coq, and
Isabelle). First, we translate natural language into Knowledge Equations (KEs),
a novel language that we designed, theoretically grounded in assertional logic.
Next, we convert them to target languages through rigorously defined rules that
preserve both syntactic structure and semantic meaning. This process yielded a
parallel corpus of over 60,000 problems. Our framework achieves 88.9% syntactic
accuracy (pass@1) on MiniF2F, outperforming SOTA models such as Deepseek-V3
(81%) and Herald (81.3%) across multiple datasets. All datasets and codes are
available in the supplementary materials.

</details>


### [53] [KG-Attention: Knowledge Graph-Guided Attention at Test-Time via Bidirectional Information Aggregation](https://arxiv.org/abs/2507.08704)
*Songlin Zhai,Guilin Qi,Yuan Meng*

Main category: cs.CL

TL;DR: 提出了一种基于知识图谱引导注意力（KGA）的动态知识融合框架，无需参数更新即可增强大语言模型（LLMs）的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有知识图谱增强方法依赖参数密集型微调导致的灾难性遗忘和泛化能力下降问题，以及静态框架对实时知识更新的适应性不足。

Method: 设计了KGA模块，包含向外和向内两条协同路径：向外路径动态融合外部知识，向内路径通过KG引导过滤任务无关信号。

Result: 在五个基准测试中验证了KGA的优异知识融合性能。

Conclusion: KGA框架实现了无需参数更新的实时知识融合，显著提升了LLMs的性能和适应性。

Abstract: Knowledge graphs (KGs) play a critical role in enhancing large language
models (LLMs) by introducing structured and grounded knowledge into the
learning process. However, most existing KG-enhanced approaches rely on
parameter-intensive fine-tuning, which risks catastrophic forgetting and
degrades the pretrained model's generalization. Moreover, they exhibit limited
adaptability to real-time knowledge updates due to their static integration
frameworks. To address these issues, we introduce the first test-time
KG-augmented framework for LLMs, built around a dedicated knowledge
graph-guided attention (KGA) module that enables dynamic knowledge fusion
without any parameter updates. The proposed KGA module augments the standard
self-attention mechanism with two synergistic pathways: outward and inward
aggregation. Specifically, the outward pathway dynamically integrates external
knowledge into input representations via input-driven KG fusion. This inward
aggregation complements the outward pathway by refining input representations
through KG-guided filtering, suppressing task-irrelevant signals and amplifying
knowledge-relevant patterns. Importantly, while the outward pathway handles
knowledge fusion, the inward path selects the most relevant triples and feeds
them back into the fusion process, forming a closed-loop enhancement mechanism.
By synergistically combining these two pathways, the proposed method supports
real-time knowledge fusion exclusively at test-time, without any parameter
modification. Extensive experiments on five benchmarks verify the comparable
knowledge fusion performance of KGA.

</details>


### [54] [Multilingual Multimodal Software Developer for Code Generation](https://arxiv.org/abs/2507.08719)
*Linzheng Chai,Jian Yang,Shukai Liu,Wei Zhang,Liran Wang,Ke Jin,Tao Sun,Congnan Liu,Chenchen Zhang,Hualei Zhu,Jiaheng Liu,Xianjie Wu,Ge Zhang,Tianyu Liu,Zhoujun Li*

Main category: cs.CL

TL;DR: MM-Coder是一个多语言多模态的软件开发工具，通过整合视觉设计输入（如UML图和流程图）与文本指令，提升代码生成的准确性和架构对齐。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）多为纯文本模型，忽略了软件开发中常用的视觉辅助工具（如流程图和UML图），限制了代码生成的实用性和准确性。

Method: 开发了MM-Coder，结合视觉工作流（Visual Workflow）和文本指令；构建了MMc-Instruct数据集用于多模态指令调优；提出了MMEval基准用于评估多模态代码生成。

Result: MM-Coder能够综合文本和图形信息生成代码，但模型在精确捕捉视觉信息、遵循指令和高级编程知识方面仍存在挑战。

Conclusion: MM-Coder为工业编程带来了革新，使LLMs能够通过文本和视觉设计解释并实现复杂规范。

Abstract: The rapid advancement of Large Language Models (LLMs) has significantly
improved code generation, yet most models remain text-only, neglecting crucial
visual aids like diagrams and flowcharts used in real-world software
development. To bridge this gap, we introduce MM-Coder, a Multilingual
Multimodal software developer. MM-Coder integrates visual design inputs-Unified
Modeling Language (UML) diagrams and flowcharts (termed Visual Workflow)-with
textual instructions to enhance code generation accuracy and architectural
alignment. To enable this, we developed MMc-Instruct, a diverse multimodal
instruction-tuning dataset including visual-workflow-based code generation,
allowing MM-Coder to synthesize textual and graphical information like human
developers, distinct from prior work on narrow tasks. Furthermore, we introduce
MMEval, a new benchmark for evaluating multimodal code generation, addressing
existing text-only limitations. Our evaluations using MMEval highlight
significant remaining challenges for models in precise visual information
capture, instruction following, and advanced programming knowledge. Our work
aims to revolutionize industrial programming by enabling LLMs to interpret and
implement complex specifications conveyed through both text and visual designs.

</details>


### [55] [KV Cache Steering for Inducing Reasoning in Small Language Models](https://arxiv.org/abs/2507.08799)
*Max Belitsky,Dawid J. Kopiczko,Michael Dorkenwald,M. Jehanzeb Mirza,Cees G. M. Snoek,Yuki M. Asano*

Main category: cs.CL

TL;DR: 提出了一种轻量级方法“缓存引导”，通过一次性干预直接作用于键值缓存来隐式引导语言模型，无需微调或修改提示。


<details>
  <summary>Details</summary>
Motivation: 解决现有激活引导技术需要持续干预的问题，提供更高效、稳定的解决方案。

Method: 利用GPT-4生成推理轨迹构建引导向量，通过一次性干预键值缓存引导模型行为。

Result: 实验表明，缓存引导提升了模型推理的定性结构和定量任务性能。

Conclusion: 缓存引导在超参数稳定性、推理效率和集成便捷性上优于现有技术，是一种更实用的控制生成方法。

Abstract: We propose cache steering, a lightweight method for implicit steering of
language models via a one-shot intervention applied directly to the key-value
cache. To validate its effectiveness, we apply cache steering to induce
chain-of-thought reasoning in small language models. Our approach leverages
GPT-4o-generated reasoning traces to construct steering vectors that shift
model behavior toward more explicit, multi-step reasoning without fine-tuning
or prompt modifications. Experimental evaluations on diverse reasoning
benchmarks demonstrate that cache steering improves both the qualitative
structure of model reasoning and quantitative task performance. Compared to
prior activation steering techniques that require continuous interventions, our
one-shot cache steering offers substantial advantages in terms of
hyperparameter stability, inference-time efficiency, and ease of integration,
making it a more robust and practical solution for controlled generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [56] [Human Creativity and AI](https://arxiv.org/abs/2507.08001)
*Shengyi Xie*

Main category: cs.AI

TL;DR: 本文探讨了AI是否能展现创造力，结合心理学、认知神经科学和哲学视角，分析了创造力的定义及其在AI发展中的意义。


<details>
  <summary>Details</summary>
Motivation: 随着科技进步，创造力哲学被重新诠释，研究旨在探讨AI是否具备创造力。

Method: 回顾历史视角，分析心理学进展对创造力研究的影响，探讨自然主义和认知神经科学的回应。

Result: 通过多学科视角，深入探讨了创造力的定义及其在AI中的可能性。

Conclusion: AI是否具备创造力仍需进一步研究，但多学科方法为理解创造力提供了新视角。

Abstract: With the advancement of science and technology, the philosophy of creativity
has undergone significant reinterpretation. This paper investigates
contemporary research in the fields of psychology, cognitive neuroscience, and
the philosophy of creativity, particularly in the context of the development of
artificial intelligence (AI) techniques. It aims to address the central
question: Can AI exhibit creativity? The paper reviews the historical
perspectives on the philosophy of creativity and explores the influence of
psychological advancements on the study of creativity. Furthermore, it analyzes
various definitions of creativity and examines the responses of naturalism and
cognitive neuroscience to the concept of creativity.

</details>


### [57] [TableReasoner: Advancing Table Reasoning Framework with Large Language Models](https://arxiv.org/abs/2507.08046)
*Sishi Xiong,Dakai Wang,Yu Zhao,Jie Zhang,Changzai Pan,Haowei He,Xiangyu Li,Wenhan Chang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 论文提出了TableReasoner系统，用于解决表格问答任务中的挑战，如表格规模大、列语义不完整和实体歧义。通过结合结构和语义表示的表模式建模，以及多步模式链接计划，系统实现了高效处理和精确推理。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界表格数据在问答任务中的挑战，如规模大、语义不完整和歧义问题。

Method: 提出基于大语言模型和编程的表格推理框架TableReasoner，结合结构和语义表示建模表格，设计多步模式链接计划以聚焦查询相关信息。

Result: 系统在SemEval-2025 Task 8的两个子任务中均取得第一名。

Conclusion: TableReasoner通过结合结构和语义表示以及多步推理，有效解决了表格问答任务中的挑战，并取得了显著成果。

Abstract: The paper presents our system developed for table question answering (TQA).
TQA tasks face challenges due to the characteristics of real-world tabular
data, such as large size, incomplete column semantics, and entity ambiguity. To
address these issues, we propose a large language model (LLM)-powered and
programming-based table reasoning framework, named TableReasoner. It models a
table using the schema that combines structural and semantic representations,
enabling holistic understanding and efficient processing of large tables. We
design a multi-step schema linking plan to derive a focused table schema that
retains only query-relevant information, eliminating ambiguity and alleviating
hallucinations. This focused table schema provides precise and sufficient table
details for query refinement and programming. Furthermore, we integrate the
reasoning workflow into an iterative thinking architecture, allowing
incremental cycles of thinking, reasoning and reflection. Our system achieves
first place in both subtasks of SemEval-2025 Task 8.

</details>


### [58] [A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking](https://arxiv.org/abs/2507.08207)
*Zhengye Han,Quanyan Zhu*

Main category: cs.AI

TL;DR: 本文提出了一种动态Stackelberg博弈框架，用于建模大型语言模型（LLM）越狱攻击中的攻防交互，并引入“紫色代理”解决方案，结合对抗探索和防御策略。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在关键应用中的部署增加，越狱攻击（绕过安全机制）成为重要问题，需要有效的防御方法。

Method: 采用动态Stackelberg博弈框架，将提示-响应动态建模为序列扩展形式博弈，提出“紫色代理”结合RRT进行对抗探索和防御。

Result: 紫色代理能主动模拟攻击轨迹并干预，预防有害输出，为分析对抗动态和降低越狱风险提供理论基础。

Conclusion: 该框架为LLM安全提供了原则性方法，有效缓解越狱攻击风险。

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, the challenge of jailbreaking, where adversaries manipulate the
models to bypass safety mechanisms, has become a significant concern. This
paper presents a dynamic Stackelberg game framework to model the interactions
between attackers and defenders in the context of LLM jailbreaking. The
framework treats the prompt-response dynamics as a sequential extensive-form
game, where the defender, as the leader, commits to a strategy while
anticipating the attacker's optimal responses. We propose a novel agentic AI
solution, the "Purple Agent," which integrates adversarial exploration and
defensive strategies using Rapidly-exploring Random Trees (RRT). The Purple
Agent actively simulates potential attack trajectories and intervenes
proactively to prevent harmful outputs. This approach offers a principled
method for analyzing adversarial dynamics and provides a foundation for
mitigating the risk of jailbreaking.

</details>


### [59] [Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions](https://arxiv.org/abs/2507.08208)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: LLM-Nash框架通过游戏理论模型研究LLM驱动的决策，将推理提示作为策略空间，捕捉有限理性，与传统纳什均衡不同。


<details>
  <summary>Details</summary>
Motivation: 研究LLM在战略交互中的行为，特别是有限理性和推理过程的影响。

Method: 提出LLM-Nash框架，将推理提示作为策略空间，通过LLM推理生成行为。

Result: 推理均衡与传统纳什均衡存在差异，揭示了认知约束和心态表达的新视角。

Conclusion: LLM-Nash为LLM系统的战略交互提供了新理论基础。

Abstract: We introduce the LLM-Nash framework, a game-theoretic model where agents
select reasoning prompts to guide decision-making via Large Language Models
(LLMs). Unlike classical games that assume utility-maximizing agents with full
rationality, this framework captures bounded rationality by modeling the
reasoning process explicitly. Equilibrium is defined over the prompt space,
with actions emerging as the behavioral output of LLM inference. This approach
enables the study of cognitive constraints, mindset expressiveness, and
epistemic learning. Through illustrative examples, we show how reasoning
equilibria can diverge from classical Nash outcomes, offering a new foundation
for strategic interaction in LLM-enabled systems.

</details>


### [60] [From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration](https://arxiv.org/abs/2507.08210)
*Fryderyk Mantiuk,Hanqi Zhou,Charley M. Wu*

Main category: cs.AI

TL;DR: 论文探讨了智能体如何平衡好奇心（探索未知）与能力（控制环境），通过比较基于表格和基于内部世界模型的智能体，揭示了探索与表征学习的双向互动。


<details>
  <summary>Details</summary>
Motivation: 研究智能体如何在探索世界的同时保持对环境控制，结合认知理论和强化学习，探讨好奇心与能力的平衡。

Method: 比较两种模型：基于手工状态抽象的表格智能体（Tabular）和学习内部世界模型的智能体（Dreamer）。

Result: 表格智能体显示好奇心和能力引导探索的不同模式，而Dreamer智能体揭示了探索与表征学习的双向互动。

Conclusion: 研究形式化了适应性探索作为追求未知与可控之间的平衡，为认知理论和高效强化学习提供了见解。

Abstract: What drives an agent to explore the world while also maintaining control over
the environment? From a child at play to scientists in the lab, intelligent
agents must balance curiosity (the drive to seek knowledge) with competence
(the drive to master and control the environment). Bridging cognitive theories
of intrinsic motivation with reinforcement learning, we ask how evolving
internal representations mediate the trade-off between curiosity (novelty or
information gain) and competence (empowerment). We compare two model-based
agents using handcrafted state abstractions (Tabular) or learning an internal
world model (Dreamer). The Tabular agent shows curiosity and competence guide
exploration in distinct patterns, while prioritizing both improves exploration.
The Dreamer agent reveals a two-way interaction between exploration and
representation learning, mirroring the developmental co-evolution of curiosity
and competence. Our findings formalize adaptive exploration as a balance
between pursuing the unknown and the controllable, offering insights for
cognitive theories and efficient reinforcement learning.

</details>


### [61] [Grounding Methods for Neural-Symbolic AI](https://arxiv.org/abs/2507.08216)
*Rodrigo Castellano Ontiveros,Francesco Giannini,Marco Gori,Giuseppe Marra,Michelangelo Diligenti*

Main category: cs.AI

TL;DR: 论文提出了一种参数化的逻辑接地方法，以平衡神经符号方法的表达能力和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决传统神经符号方法中逻辑接地过程导致的组合爆炸或缺乏理论保证的问题。

Method: 提出了一种基于多跳符号推理的参数化接地方法，扩展了经典的后向链式推理。

Result: 实验表明接地标准的选择对神经符号方法的性能至关重要。

Conclusion: 参数化接地方法能够灵活控制表达能力和可扩展性的权衡。

Abstract: A large class of Neural-Symbolic (NeSy) methods employs a machine learner to
process the input entities, while relying on a reasoner based on First-Order
Logic to represent and process more complex relationships among the entities. A
fundamental role for these methods is played by the process of logic grounding,
which determines the relevant substitutions for the logic rules using a
(sub)set of entities. Some NeSy methods use an exhaustive derivation of all
possible substitutions, preserving the full expressive power of the logic
knowledge. This leads to a combinatorial explosion in the number of ground
formulas to consider and, therefore, strongly limits their scalability. Other
methods rely on heuristic-based selective derivations, which are generally more
computationally efficient, but lack a justification and provide no guarantees
of preserving the information provided to and returned by the reasoner. Taking
inspiration from multi-hop symbolic reasoning, this paper proposes a
parametrized family of grounding methods generalizing classic Backward
Chaining. Different selections within this family allow us to obtain commonly
employed grounding methods as special cases, and to control the trade-off
between expressiveness and scalability of the reasoner. The experimental
results show that the selection of the grounding criterion is often as
important as the NeSy method itself.

</details>


### [62] [Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach](https://arxiv.org/abs/2507.08217)
*Atit Pokharel,Ratun Rahman,Thomas Morris,Dinh C. Nguyen*

Main category: cs.AI

TL;DR: 提出了一种新型多模态量子联邦学习方法，利用量子纠缠实现中间融合，并引入缺失模态无关机制（MMA）以提升模型稳定性。实验显示，该方法在IID和非IID数据分布下分别提升了6.84%和7.25%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有量子联邦学习框架多为单模态，难以应对现实任务中的多模态需求，且缺失模态会降低性能。

Method: 采用多模态量子联邦学习框架，结合量子纠缠的中间融合技术，并设计MMA机制隔离未训练量子电路。

Result: 在IID和非IID数据分布下，准确率分别提升6.84%和7.25%。

Conclusion: 该方法填补了多模态量子联邦学习的空白，显著提升了模型性能和稳定性。

Abstract: Quantum federated learning (QFL) has been recently introduced to enable a
distributed privacy-preserving quantum machine learning (QML) model training
across quantum processors (clients). Despite recent research efforts, existing
QFL frameworks predominantly focus on unimodal systems, limiting their
applicability to real-world tasks that often naturally involve multiple
modalities. To fill this significant gap, we present for the first time a novel
multimodal approach specifically tailored for the QFL setting with the
intermediate fusion using quantum entanglement. Furthermore, to address a major
bottleneck in multimodal QFL, where the absence of certain modalities during
training can degrade model performance, we introduce a Missing Modality
Agnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring
stable training without corrupted states. Simulation results demonstrate that
the proposed multimodal QFL method with MMA yields an improvement in accuracy
of 6.84% in independent and identically distributed (IID) and 7.25% in non-IID
data distributions compared to the state-of-the-art methods.

</details>


### [63] [Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm](https://arxiv.org/abs/2507.08249)
*Bill Marino,Ari Juels*

Main category: cs.AI

TL;DR: 这篇立场论文探讨了赋予AI代理加密货币和智能合约访问权限可能带来的新危害，并呼吁更多技术研究以预防和缓解这些风险。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理对加密货币和智能合约的访问需求增加，研究其潜在危害变得至关重要。

Method: 分析了加密货币和智能合约的独特属性，并详细描述了可能的新危害向量。

Result: 提出了这些新危害的具体形式，强调了其潜在威胁。

Conclusion: 呼吁加强技术研究，以确保AI代理安全使用加密货币和智能合约。

Abstract: There is growing interest in giving AI agents access to cryptocurrencies as
well as to the smart contracts that transact them. But doing so, this position
paper argues, could lead to formidable new vectors of AI harm. To support this
argument, we first examine the unique properties of cryptocurrencies and smart
contracts that could lead to these new vectors of harm. Next, we describe each
of these new vectors of harm in detail. Finally, we conclude with a call for
more technical research aimed at preventing and mitigating these harms and,
thereby making it safer to endow AI agents with cryptocurrencies and smart
contracts.

</details>


### [64] [Abductive Computational Systems: Creative Abduction and Future Directions](https://arxiv.org/abs/2507.08264)
*Abhinav Sood,Kazjon Grace,Stephen Wan,Cecile Paris*

Main category: cs.AI

TL;DR: 论文综述了溯因推理在不同领域的讨论，分析了计算系统的实现，发现现有理论和计算系统未能充分支持创造性假设的生成，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨溯因推理在理论和计算系统中的不足，特别是在生成创造性假设方面的局限性。

Method: 综述了溯因推理在认识论、科学和设计中的讨论，并分析了计算系统的实现方式。

Result: 现有理论和计算系统未能有效支持创造性溯因假设的生成。

Conclusion: 提出了未来研究的具体方向，以推动计算系统中创造性溯因推理的发展。

Abstract: Abductive reasoning, reasoning for inferring explanations for observations,
is often mentioned in scientific, design-related and artistic contexts, but its
understanding varies across these domains. This paper reviews how abductive
reasoning is discussed in epistemology, science and design, and then analyses
how various computational systems use abductive reasoning. Our analysis shows
that neither theoretical accounts nor computational implementations of
abductive reasoning adequately address generating creative hypotheses.
Theoretical frameworks do not provide a straightforward model for generating
creative abductive hypotheses, computational systems largely implement
syllogistic forms of abductive reasoning. We break down abductive computational
systems into components and conclude by identifying specific directions for
future research that could advance the state of creative abductive reasoning in
computational systems.

</details>


### [65] [Agent Safety Alignment via Reinforcement Learning](https://arxiv.org/abs/2507.08270)
*Zeyang Sha,Hanling Tian,Zhuoer Xu,Shiwen Cui,Changhua Meng,Weiqiang Wang*

Main category: cs.AI

TL;DR: 本文提出了一种统一的工具使用代理安全对齐框架，通过结构化推理和沙盒强化学习处理用户和工具的双重威胁。


<details>
  <summary>Details</summary>
Motivation: 自主大型语言模型（LLM）代理的工具使用能力带来了新的安全风险，传统对话滥用无法涵盖。

Method: 提出三模态分类法（良性、恶意、敏感）和策略驱动决策模型，使用沙盒环境模拟工具执行。

Result: 在多个基准测试中，安全对齐代理显著提高了对安全威胁的抵抗能力，同时保持良性任务的高效性。

Conclusion: 安全与效能可共同优化，为自主LLM代理的可信部署奠定了基础。

Abstract: The emergence of autonomous Large Language Model (LLM) agents capable of tool
usage has introduced new safety risks that go beyond traditional conversational
misuse. These agents, empowered to execute external functions, are vulnerable
to both user-initiated threats (e.g., adversarial prompts) and tool-initiated
threats (e.g., malicious outputs from compromised tools). In this paper, we
propose the first unified safety-alignment framework for tool-using agents,
enabling models to handle both channels of threat via structured reasoning and
sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including
benign, malicious, and sensitive for both user prompts and tool responses, and
define a policy-driven decision model. Our framework employs a custom-designed
sandbox environment that simulates real-world tool execution and allows
fine-grained reward shaping. Through extensive evaluations on public and
self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we
demonstrate that our safety-aligned agents significantly improve resistance to
security threats while preserving strong utility on benign tasks. Our results
show that safety and effectiveness can be jointly optimized, laying the
groundwork for trustworthy deployment of autonomous LLM agents.

</details>


### [66] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI,:,Fudong Wang,Jiajia Liu,Jingdong Chen,Jun Zhou,Kaixiang Ji,Lixiang Ru,Qingpei Guo,Ruobing Zheng,Tianqi Li,Yi Yuan,Yifan Mao,Yuting Xiao,Ziping Ma*

Main category: cs.AI

TL;DR: M2-Reasoning-7B模型通过创新的数据管道和动态多任务训练策略，解决了MLLMs在动态空间交互上的不足，并在8个基准测试中取得SOTA表现。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在动态空间交互能力上存在不足，限制了其在实际应用中的表现。

Method: 1. 构建包含29.4万高质量样本的数据管道；2. 采用动态多任务训练策略，结合任务特定奖励。

Result: M2-Reasoning-7B在8个基准测试中表现最优，尤其在通用和空间推理领域。

Conclusion: M2-Reasoning-7B通过数据与训练策略的创新，显著提升了MLLMs的动态空间交互能力。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [67] [Multi-Agent LLMs as Ethics Advocates in AI-Based Systems](https://arxiv.org/abs/2507.08392)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.AI

TL;DR: 该研究提出了一种在多智能体LLM环境中引入伦理倡导代理的框架，用于自动生成伦理需求草案，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于时间和资源限制，手动获取伦理需求具有挑战性且优先级较低，因此需要一种自动化方法来促进伦理需求的整合。

Method: 研究提出一个框架，通过伦理倡导代理在多智能体LLM环境中对系统描述进行伦理批判和输入，生成伦理需求草案。

Result: 案例研究表明，该框架能捕捉大部分人工访谈中识别的伦理需求，并引入额外相关需求，但也存在可靠性问题。

Conclusion: 该框架有助于在需求工程中更广泛地整合伦理，但需结合人类反馈以确保可靠性。

Abstract: Incorporating ethics into the requirement elicitation process is essential
for creating ethically aligned systems. Although eliciting manual ethics
requirements is effective, it requires diverse input from multiple
stakeholders, which can be challenging due to time and resource constraints.
Moreover, it is often given a low priority in the requirements elicitation
process. This study proposes a framework for generating ethics requirements
drafts by introducing an ethics advocate agent in a multi-agent LLM setting.
This agent critiques and provides input on ethical issues based on the system
description. The proposed framework is evaluated through two case studies from
different contexts, demonstrating that it captures the majority of ethics
requirements identified by researchers during 30-minute interviews and
introduces several additional relevant requirements. However, it also
highlights reliability issues in generating ethics requirements, emphasizing
the need for human feedback in this sensitive domain. We believe this work can
facilitate the broader adoption of ethics in the requirements engineering
process, ultimately leading to more ethically aligned products.

</details>


### [68] [Why this and not that? A Logic-based Framework for Contrastive Explanations](https://arxiv.org/abs/2507.08454)
*Tobias Geibinger,Reijo Jaakkola,Antti Kuusisto,Xinghan Liu,Miikka Vilander*

Main category: cs.AI

TL;DR: 论文定义了与对比解释相关的几个典型问题，研究了它们在命题逻辑中的基本性质，并分析了计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究对比解释问题，回答“为什么是P而不是Q”这类问题，旨在明确比较P和Q的差异。

Method: 在命题逻辑中定义问题，分析其性质，计算复杂性，并使用答案集编程实现CNF公式的解决方案。

Result: 框架捕捉了文献中对比解释的基数最小版本，并提供了计算复杂性的详细分析。

Conclusion: 通过实现和示例验证了方法的实用性。

Abstract: We define several canonical problems related to contrastive explanations,
each answering a question of the form ''Why P but not Q?''. The problems
compute causes for both P and Q, explicitly comparing their differences. We
investigate the basic properties of our definitions in the setting of
propositional logic. We show, inter alia, that our framework captures a
cardinality-minimal version of existing contrastive explanations in the
literature. Furthermore, we provide an extensive analysis of the computational
complexities of the problems. We also implement the problems for CNF-formulas
using answer set programming and present several examples demonstrating how
they work in practice.

</details>


### [69] [From Language to Logic: A Bi-Level Framework for Structured Reasoning](https://arxiv.org/abs/2507.08501)
*Keying Yang,Hao Wang,Kai Yang*

Main category: cs.AI

TL;DR: 提出了一种双层框架，通过任务抽象和逻辑生成两阶段将自然语言映射到逻辑表示，显著提升了推理准确性和透明度。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言输入与形式逻辑表示之间的差距，实现模块化推理和跨领域泛化。

Method: 使用大型语言模型（LLM）分两阶段处理：高层任务抽象生成中间结构化表示，低层逻辑生成符号化工作流或可执行程序。

Result: 在多个推理基准测试中，准确率显著提升（最高40%），同时增强了透明度和错误追踪能力。

Conclusion: 双层框架为LLM的可信和系统化推理提供了有效途径。

Abstract: Structured reasoning over natural language inputs remains a core challenge in
artificial intelligence, as it requires bridging the gap between unstructured
linguistic expressions and formal logical representations. In this paper, we
propose a novel \textbf{bi-level framework} that maps language to logic through
a two-stage process: high-level task abstraction and low-level logic
generation. At the upper level, a large language model (LLM) parses natural
language queries into intermediate structured representations specifying the
problem type, objectives, decision variables, and symbolic constraints. At the
lower level, the LLM uses these representations to generate symbolic workflows
or executable reasoning programs for accurate and interpretable decision
making. The framework supports modular reasoning, enforces explicit
constraints, and generalizes across domains such as mathematical problem
solving, question answering, and logical inference. We further optimize the
framework with an end-to-end {bi-level} optimization approach that jointly
refines both the high-level abstraction and low-level logic generation stages.
Experiments on multiple realistic reasoning benchmarks demonstrate that our
approach significantly outperforms existing baselines in accuracy, with
accuracy gains reaching as high as 40\%. Moreover, the bi-level design enhances
transparency and error traceability, offering a promising step toward
trustworthy and systematic reasoning with LLMs.

</details>


### [70] [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
*Mingda Zhang,Na Zhao,Jianglong Qin,Guoyu Ye,Ruixiang Tang*

Main category: cs.AI

TL;DR: 本文提出了一种结合多粒度稀疏激活和分层知识图谱的框架，用于改善罕见病诊断，实验结果显示性能提升，接近临床阈值。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断因知识表示深度不足、概念理解有限和临床推理受限而受阻，需要更有效的解决方案。

Method: 采用多粒度稀疏激活医学概念和分层知识图谱（分类、临床特征、实例），结合四种匹配算法、多样性控制和五级回退策略。

Result: 在BioASQ罕见病QA集上，BLEU提升0.09，ROUGE提升0.05，准确率提升0.12，峰值准确率达0.89。

Conclusion: 该方法提高了信息质量、推理能力和专业表达，有望缩短罕见病患者的诊断周期。

Abstract: Despite advances from medical large language models in healthcare,
rare-disease diagnosis remains hampered by insufficient
knowledge-representation depth, limited concept understanding, and constrained
clinical reasoning. We propose a framework that couples multi-granularity
sparse activation of medical concepts with a hierarchical knowledge graph. Four
complementary matching algorithms, diversity control, and a five-level fallback
strategy enable precise concept activation, while a three-layer knowledge graph
(taxonomy, clinical features, instances) provides structured, up-to-date
context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,
ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89
approaching the 0.90 clinical threshold. Expert evaluation confirms
improvements in information quality, reasoning, and professional expression,
suggesting our approach shortens the "diagnostic odyssey" for rare-disease
patients.

</details>


### [71] [Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing](https://arxiv.org/abs/2507.08575)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.AI

TL;DR: 提出了一种利用大型多模态模型（LMM）自动地理编码生物样本记录的新方法，通过视觉上下文理解空间关系，实验结果显示其优于现有工具。


<details>
  <summary>Details</summary>
Motivation: 解决自然历史收藏中大量未地理编码样本记录的自动化处理问题，现有方法未充分利用地图信息。

Method: 采用网格化方法，结合多模态模型在零样本设置下进行地理编码，利用视觉上下文理解空间关系。

Result: 实验显示该方法平均距离误差约1公里，优于单模态语言模型和现有工具。

Conclusion: LMM在理解精细地图方面表现优异，提出了将其整合到地理编码工作流程的实用框架。

Abstract: Millions of biological sample records collected in the last few centuries
archived in natural history collections are un-georeferenced. Georeferencing
complex locality descriptions associated with these collection samples is a
highly labour-intensive task collection agencies struggle with. None of the
existing automated methods exploit maps that are an essential tool for
georeferencing complex relations. We present preliminary experiments and
results of a novel method that exploits multi-modal capabilities of recent
Large Multi-Modal Models (LMM). This method enables the model to visually
contextualize spatial relations it reads in the locality description. We use a
grid-based approach to adapt these auto-regressive models for this task in a
zero-shot setting. Our experiments conducted on a small manually annotated
dataset show impressive results for our approach ($\sim$1 km Average distance
error) compared to uni-modal georeferencing with Large Language Models and
existing georeferencing tools. The paper also discusses the findings of the
experiments in light of an LMM's ability to comprehend fine-grained maps.
Motivated by these results, a practical framework is proposed to integrate this
method into a georeferencing workflow.

</details>


### [72] [Unlocking Speech Instruction Data Potential with Query Rewriting](https://arxiv.org/abs/2507.08603)
*Yonghua Hei,Yibo Yan,Shuliang Liu,Huiyu Zhou,Linfeng Zhang,Xuming Hu*

Main category: cs.AI

TL;DR: 论文提出了一种基于多LLM知识融合的查询重写框架，用于构建高质量语音指令数据集，无需人工标注。


<details>
  <summary>Details</summary>
Motivation: 现有语音指令数据集不足且训练任务偏差大，导致语音指令跟随能力未充分发挥。

Method: 采用多LLM知识融合的查询重写框架，通过零样本重写将文本指令转换为更适合TTS模型合成的分布。

Result: 数据可用性从72%提升至93%，在复杂知识及上下文相关任务中表现优异。

Conclusion: 该方法为构建高质量语音指令数据集提供了高效且低成本的解决方案。

Abstract: End-to-end Large Speech Language Models~(\textbf{LSLMs}) demonstrate strong
potential in response latency and speech comprehension capabilities, showcasing
general intelligence across speech understanding tasks. However, the ability to
follow speech instructions has not been fully realized due to the lack of
datasets and heavily biased training tasks. Leveraging the rich ASR datasets,
previous approaches have used Large Language Models~(\textbf{LLMs}) to continue
the linguistic information of speech to construct speech instruction datasets.
Yet, due to the gap between LLM-generated results and real human responses, the
continuation methods further amplify these shortcomings. Given the high costs
of collecting and annotating speech instruction datasets by humans, using
speech synthesis to construct large-scale speech instruction datasets has
become a balanced and robust alternative. Although modern
Text-To-Speech~(\textbf{TTS}) models have achieved near-human-level synthesis
quality, it is challenging to appropriately convert out-of-distribution text
instruction to speech due to the limitations of the training data distribution
in TTS models. To address this issue, we propose a query rewriting framework
with multi-LLM knowledge fusion, employing multiple agents to annotate and
validate the synthesized speech, making it possible to construct high-quality
speech instruction datasets without relying on human annotation. Experiments
show that this method can transform text instructions into distributions more
suitable for TTS models for speech synthesis through zero-shot rewriting,
increasing data usability from 72\% to 93\%. It also demonstrates unique
advantages in rewriting tasks that require complex knowledge and
context-related abilities.

</details>


### [73] [Agentic Large Language Models for Conceptual Systems Engineering and Design](https://arxiv.org/abs/2507.08619)
*Soheyl Massoudi,Mark Fuge*

Main category: cs.AI

TL;DR: 论文评估了结构化多代理系统（MAS）在早期工程设计中的表现，相比双代理系统（2AS），MAS在生成设计细节方面更优，但在需求覆盖和代码兼容性上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 解决现有大型语言模型（LLM）工作流在早期工程设计中的任务连续性和可执行模型生成问题。

Method: 引入设计状态图（DSG）作为JSON可序列化表示，比较九角色MAS与双代理2AS的性能，使用两种LLM进行实验。

Result: MAS在生成更细粒度DSG上表现更好，但需求覆盖率低（<20%），代码兼容性平均低于50%。推理蒸馏模型显著提高了工作流完成率。

Conclusion: 结构化多代理系统能增强设计细节，但需求覆盖和代码兼容性仍需改进。

Abstract: Early-stage engineering design involves complex, iterative reasoning, yet
existing large language model (LLM) workflows struggle to maintain task
continuity and generate executable models. We evaluate whether a structured
multi-agent system (MAS) can more effectively manage requirements extraction,
functional decomposition, and simulator code generation than a simpler
two-agent system (2AS). The target application is a solar-powered water
filtration system as described in a cahier des charges. We introduce the
Design-State Graph (DSG), a JSON-serializable representation that bundles
requirements, physical embodiments, and Python-based physics models into graph
nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS
collapses the process to a Generator-Reflector loop. Both systems run a total
of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1
70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON
validity, requirement coverage, embodiment presence, code compatibility,
workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS
maintained perfect JSON integrity and embodiment tagging. Requirement coverage
remained minimal (less than 20\%). Code compatibility peaked at 100\% under
specific 2AS settings but averaged below 50\% for MAS. Only the
reasoning-distilled model reliably flagged workflow completion. Powered by
DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)
whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced
design detail. Reasoning-distilled LLM improved completion rates, yet low
requirements and fidelity gaps in coding persisted.

</details>


### [74] [Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning](https://arxiv.org/abs/2507.08649)
*Xingguang Ji,Yahui Liu,Qi Wang,Jingyuan Zhang,Yang Yue,Rui Shi,Chenxi Sun,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.AI

TL;DR: Leanabell-Prover-V2是一个7B参数的大型语言模型，用于生成Lean 4中的形式化定理证明，通过验证器集成的长链思维（CoT）和强化学习（RL）优化性能。


<details>
  <summary>Details</summary>
Motivation: 在Leanabell-Prover-V1的基础上，进一步提升模型性能，通过验证器反馈使模型能够自我感知推理过程的正确性并纠正错误。

Method: 采用验证器反馈的强化学习，结合反馈令牌掩码和简单奖励策略，优化多轮验证器交互的推理轨迹。

Result: 在MiniF2F测试集上，性能分别提升了3.2%（Kimina-Prover-Preview-Distill-7B）和2.0%（DeepSeek-Prover-V2-7B）。

Conclusion: Leanabell-Prover-V2通过验证器反馈和强化学习显著提升了形式化定理证明的性能，代码和数据已开源。

Abstract: We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that
can produce formal theorem proofs in Lean 4, with verifier-integrated Long
Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we
continual to choose to posttrain existing strong prover models for further
performance improvement. In our V2 version, we mainly upgrade the Reinforcement
Learning (RL) with feedback provided by the Lean 4 verifier. Crucially,
verifier feedback, such as indicating success or detailing specific errors,
allows the LLM to become ``self-aware'' of the correctness of its own reasoning
process and learn to reflexively correct errors. Leanabell-Prover-V2 directly
optimizes LLM reasoning trajectories with multi-turn verifier interactions,
together with feedback token masking for stable RL training and a simple reward
strategy. Experiments show that Leanabell-Prover-V2 improves performance by
3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with
DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data
and models are available at:
https://github.com/Leanabell-LM/Leanabell-Prover-V2.

</details>


### [75] [Introspection of Thought Helps AI Agents](https://arxiv.org/abs/2507.08664)
*Haoran Sun,Shaoning Zeng*

Main category: cs.AI

TL;DR: 论文提出了一种新的AI代理推理框架INoT，通过设计新的LLM-Read代码提示，减少推理成本并提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理依赖LLMs和MLLMs，但受限于其自然语言理解能力和高推理成本。

Method: 设计了INoT框架，通过LLM-Read代码提示实现程序化对话推理，减少外部迭代成本。

Result: 在六个基准测试中，INoT平均性能提升7.95%，推理成本降低58.3%。

Conclusion: INoT框架有效提升了AI代理的性能并降低了成本，同时在图像任务中展示了通用性。

Abstract: AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to
perform interpretation and inference in text and image tasks without
post-training, where LLMs and MLLMs play the most critical role and determine
the initial ability and limitations of AI Agents. Usually, AI Agents utilize
sophisticated prompt engineering and external reasoning framework to obtain a
promising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought
and Image-of-Thought. However, they are still constrained by the inherent
limitations of LLM in understanding natural language, and the iterative
reasoning process will generate a large amount of inference cost. To this end,
we propose a novel AI Agent Reasoning Framework with Introspection of Thought
(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute
programmatic dialogue reasoning processes following the code in prompt.
Therefore, self-denial and reflection occur within LLM instead of outside LLM,
which can reduce token cost effectively. Through our experiments on six
benchmarks for three different tasks, the effectiveness of INoT is verified,
with an average improvement of 7.95\% in performance, exceeding the baselines.
Furthermore, the token cost of INoT is lower on average than the best
performing method at baseline by 58.3\%. In addition, we demonstrate the
versatility of INoT in image interpretation and inference through verification
experiments.

</details>


### [76] [elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings](https://arxiv.org/abs/2507.08705)
*Philip Osborne,Danilo S. Carvalho,André Freitas*

Main category: cs.AI

TL;DR: elsciRL是一个开源的Python库，旨在将语言解决方案应用于强化学习问题，通过结合LLMs扩展了现有框架，并提供GUI支持用户输入文本生成指令，实验表明其能提升强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 加速语言解决方案在基于奖励的环境中的评估，为科学发现提供新机会。

Method: 扩展了Language Adapter with Self-Completing Instruction框架，结合LLMs，提供GUI支持用户输入文本生成指令。

Result: 实验结果表明，生成的指令可以提升强化学习代理的性能。

Conclusion: elsciRL为语言解决方案在强化学习中的应用提供了便捷工具，具有潜在的科学价值。

Abstract: We present elsciRL, an open-source Python library to facilitate the
application of language solutions on reinforcement learning problems. We
demonstrate the potential of our software by extending the Language Adapter
with Self-Completing Instruction framework defined in (Osborne, 2024) with the
use of LLMs. Our approach can be re-applied to new applications with minimal
setup requirements. We provide a novel GUI that allows a user to provide text
input for an LLM to generate instructions which it can then self-complete.
Empirical results indicate that these instructions \textit{can} improve a
reinforcement learning agent's performance. Therefore, we present this work to
accelerate the evaluation of language solutions on reward based environments to
enable new opportunities for scientific discovery.

</details>


### [77] [System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility](https://arxiv.org/abs/2507.08715)
*Paul Saves,Jasper Bussemaker,Rémi Lafage,Thierry Lefebvre,Nathalie Bartoli,Youssef Diouane,Joseph Morlier*

Main category: cs.AI

TL;DR: 论文探讨了在系统架构开发中，使用基于物理的模拟方法可能带来的计算复杂性和优化挑战，并提出基于代理的优化算法（如贝叶斯优化）作为解决方案。


<details>
  <summary>Details</summary>
Motivation: 在系统架构开发中，传统基于物理的模拟方法虽然高效，但可能导致计算复杂性和优化困难，因此需要更高效的优化方法。

Method: 提出使用基于代理的优化算法，特别是贝叶斯优化和Gaussian process模型，以应对传统方法的挑战。

Result: 基于代理的优化算法能够有效降低计算复杂性并提高优化效率。

Conclusion: 在系统架构开发中，基于代理的优化算法是一种有前景的解决方案，能够克服传统方法的局限性。

Abstract: For developing innovative systems architectures, modeling and optimization
techniques have been central to frame the architecting process and define the
optimization and modeling problems. In this context, for system-of-systems the
use of efficient dedicated approaches (often physics-based simulations) is
highly recommended to reduce the computational complexity of the targeted
applications. However, exploring novel architectures using such dedicated
approaches might pose challenges for optimization algorithms, including
increased evaluation costs and potential failures. To address these challenges,
surrogate-based optimization algorithms, such as Bayesian optimization
utilizing Gaussian process models have emerged.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [78] [Overview of the TREC 2021 deep learning track](https://arxiv.org/abs/2507.08191)
*Nick Craswell,Bhaskar Mitra,Emine Yilmaz,Daniel Campos,Jimmy Lin*

Main category: cs.IR

TL;DR: TREC Deep Learning track第三年报告，利用MS MARCO数据集进行文档和段落排序任务，数据集规模大幅增加，深度神经网络排序模型表现优于传统方法，但单阶段检索仍不及多阶段方法。


<details>
  <summary>Details</summary>
Motivation: 研究深度神经网络排序模型在文档和段落排序任务中的表现，并探讨数据集规模增加对结果的影响。

Method: 使用MS MARCO数据集进行训练和评估，对比深度神经网络模型与传统检索方法的性能。

Result: 深度神经网络模型表现优于传统方法，但单阶段检索性能仍不及多阶段方法；数据集规模增加引发对标签质量的讨论。

Conclusion: 深度神经网络在排序任务中表现优异，但数据集更新和规模扩大带来新的挑战，需进一步研究标签质量和检索方法优化。

Abstract: This is the third year of the TREC Deep Learning track. As in previous years,
we leverage the MS MARCO datasets that made hundreds of thousands of human
annotated training labels available for both passage and document ranking
tasks. In addition, this year we refreshed both the document and the passage
collections which also led to a nearly four times increase in the document
collection size and nearly $16$ times increase in the size of the passage
collection. Deep neural ranking models that employ large scale pretraininig
continued to outperform traditional retrieval methods this year. We also found
that single stage retrieval can achieve good performance on both tasks although
they still do not perform at par with multistage retrieval pipelines. Finally,
the increase in the collection size and the general data refresh raised some
questions about completeness of NIST judgments and the quality of the training
labels that were mapped to the new collections from the old ones which we
discuss in this report.

</details>


### [79] [Towards Efficient Quantity Retrieval from Text:an Approach via Description Parsing and Weak Supervision](https://arxiv.org/abs/2507.08322)
*Yixuan Cao,Zhengrong Chen,Chengxuan Xia,Kun Wu,Ping Luo*

Main category: cs.IR

TL;DR: 论文提出了一种名为“数量检索”的任务，旨在从非结构化文档中提取定量事实，并通过描述解析框架实现高效检索。


<details>
  <summary>Details</summary>
Motivation: 许多长尾定量事实埋藏在非结构化文档中，难以访问，限制了数据驱动的决策。

Method: 基于描述解析的框架，将文本转换为结构化（描述，数量）对，并利用弱监督构建大规模复述数据集以提升学习效果。

Result: 在金融年报数据集上，方法将检索准确率从30.98%提升至64.66%。

Conclusion: 该方法显著提高了定量事实的检索效率，为数据驱动决策提供了更全面的支持。

Abstract: Quantitative facts are continually generated by companies and governments,
supporting data-driven decision-making. While common facts are structured, many
long-tail quantitative facts remain buried in unstructured documents, making
them difficult to access. We propose the task of Quantity Retrieval: given a
description of a quantitative fact, the system returns the relevant value and
supporting evidence. Understanding quantity semantics in context is essential
for this task. We introduce a framework based on description parsing that
converts text into structured (description, quantity) pairs for effective
retrieval. To improve learning, we construct a large paraphrase dataset using
weak supervision based on quantity co-occurrence. We evaluate our approach on a
large corpus of financial annual reports and a newly annotated quantity
description dataset. Our method significantly improves top-1 retrieval accuracy
from 30.98 percent to 64.66 percent.

</details>


### [80] [DS@GT at LongEval: Evaluating Temporal Performance in Web Search Systems and Topics with Two-Stage Retrieval](https://arxiv.org/abs/2507.08360)
*Anthony Miyaguchi,Imran Afrulbasha,Aleksandar Pramov*

Main category: cs.IR

TL;DR: 论文分析了信息检索模型在动态网络内容中的性能退化问题，通过LongEval实验室评估了基于时间分布的网络快照的检索系统，并提出了两阶段检索方法。


<details>
  <summary>Details</summary>
Motivation: 信息检索模型通常在静态数据集上训练，容易因网络内容动态变化导致性能下降，因此需要评估和改进模型在时间分布数据上的表现。

Method: 采用两阶段检索系统，包括稀疏关键词搜索、查询扩展和文档重排序，并对Qwant网络数据集进行主题建模分析。

Result: 最佳系统在训练和测试数据集上的平均NDCG@10为0.296，在2023-05快照上达到最高分0.395。

Conclusion: 研究表明，两阶段检索系统在动态网络内容中表现良好，代码已开源。

Abstract: Information Retrieval (IR) models are often trained on static datasets,
making them vulnerable to performance degradation as web content evolves. The
DS@GT competition team participated in the Longitudinal Evaluation of Model
Performance (LongEval) lab at CLEF 2025, which evaluates IR systems across
temporally distributed web snapshots. Our analysis of the Qwant web dataset
includes exploratory data analysis with topic modeling over time. The two-phase
retrieval system employs sparse keyword searches, utilizing query expansion and
document reranking. Our best system achieves an average NDCG@10 of 0.296 across
the entire training and test dataset, with an overall best score of 0.395 on
2023-05. The accompanying source code for this paper is at
https://github.com/dsgt-arc/longeval-2025

</details>


### [81] [CUE-RAG: Towards Accurate and Cost-Efficient Graph-Based RAG via Multi-Partite Graph and Query-Driven Iterative Retrieval](https://arxiv.org/abs/2507.08445)
*Yaodong Su,Yixiang Fang,Yingli Zhou,Quanqing Xu,Chuanhui Yang*

Main category: cs.IR

TL;DR: CUE-RAG提出了一种新的检索增强生成方法，通过多粒度图索引、混合提取策略和查询驱动迭代检索，显著提升了问答性能，同时降低了索引成本。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在问答任务中因缺乏领域特定和最新知识而受限，现有图检索方法因提取不完整和查询信息利用不足导致性能不佳。

Method: CUE-RAG采用多部图索引（文本块、知识单元和实体）、混合提取策略减少LLM标记使用，以及查询驱动迭代检索策略（Q-Iter）。

Result: 在三个问答基准测试中，CUE-RAG显著优于现有方法，准确率和F1分数分别提升99.33%和113.51%，索引成本降低72.58%。

Conclusion: CUE-RAG在提升图检索性能的同时实现了成本效益，为图检索增强生成系统提供了有效解决方案。

Abstract: Despite the remarkable progress of Large Language Models (LLMs), their
performance in question answering (QA) remains limited by the lack of
domain-specific and up-to-date knowledge. Retrieval-Augmented Generation (RAG)
addresses this limitation by incorporating external information, often from
graph-structured data. However, existing graph-based RAG methods suffer from
poor graph quality due to incomplete extraction and insufficient utilization of
query information during retrieval. To overcome these limitations, we propose
CUE-RAG, a novel approach that introduces (1) a multi-partite graph index
incorporates text Chunks, knowledge Units, and Entities to capture semantic
content at multiple levels of granularity, (2) a hybrid extraction strategy
that reduces LLM token usage while still producing accurate and disambiguated
knowledge units, and (3) Q-Iter, a query-driven iterative retrieval strategy
that enhances relevance through semantic search and constrained graph
traversal. Experiments on three QA benchmarks show that CUE-RAG significantly
outperforms state-of-the-art baselines, achieving up to 99.33% higher Accuracy
and 113.51% higher F1 score while reducing indexing costs by 72.58%.
Remarkably, CUE-RAG matches or outperforms baselines even without using an LLM
for indexing. These results demonstrate the effectiveness and cost-efficiency
of CUE-RAG in advancing graph-based RAG systems.

</details>


### [82] [Improving Korean-English Cross-Lingual Retrieval: A Data-Centric Study of Language Composition and Model Merging](https://arxiv.org/abs/2507.08480)
*Youngjoon Jang,Junyoung Son,Taemin Lee,Seongtae Hong,Heuiseok Lim*

Main category: cs.IR

TL;DR: 研究了训练数据语言组成对跨语言信息检索（CLIR）和单语言信息检索（IR）性能的影响，发现特定语言对能提升CLIR但降低单语言IR性能，提出模型合并策略以优化两者表现。


<details>
  <summary>Details</summary>
Motivation: 探索训练数据语言组成对CLIR和单语言IR性能的影响，填补这一数据中心化研究的空白。

Method: 构建韩英平行数据集，训练不同语言组合的检索模型，分析性能差异。

Result: 训练数据的语言组成显著影响IR性能，CLIR和单语言IR之间存在权衡关系。

Conclusion: 模型合并能有效平衡CLIR和单语言IR性能，为优化多语言检索任务提供可行策略。

Abstract: With the increasing utilization of multilingual text information,
Cross-Lingual Information Retrieval (CLIR) has become a crucial research area.
However, the impact of training data composition on both CLIR and Mono-Lingual
Information Retrieval (IR) performance remains under-explored. To
systematically investigate this data-centric aspect, we construct
linguistically parallel Korean-English datasets and train retrieval models with
various language combinations. Our experiments reveal that the language
composition of training data significantly influences IR performance,
exhibiting important inter-lingual correlations: CLIR performance improves with
specific language pairs, while Mono-Lingual IR performance declines. Our work
demonstrates that Model Merging can effectively mitigate this trade-off,
achieving strong CLIR results while preserving Mono-Lingual IR capabilities.
Our findings underscore the effects of linguistic configuration of training
data on both CLIR and Mono-Lingual IR, and present Model Merging as a viable
strategy to optimize performance across these tasks.

</details>


### [83] [Digital gazetteers: review and prospects for place name knowledge bases](https://arxiv.org/abs/2507.08553)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.IR

TL;DR: 本文综述了数字地名录的技术现状，探讨了数据来源、组件、软件、数据管理技术、数据质量及众包数据，并强调了未来在丰富地名表示、时间演化和数据整合方法上的需求。


<details>
  <summary>Details</summary>
Motivation: 地名录在在线地理信息检索中起关键作用，但目前缺乏统一标准且功能有限，未来需提升其表示能力和整合方法。

Method: 通过综述数据来源、组件、软件、数据管理技术、数据质量和众包数据，分析地名录的现状。

Result: 当前地名录在表示地名多面性和数据整合方面存在不足。

Conclusion: 未来需关注地名丰富表示、时间演化和更有效的数据整合方法。

Abstract: Gazetteers typically store data on place names, place types and the
associated coordinates. They play an essential role in disambiguating place
names in online geographical information retrieval systems for navigation and
mapping, detecting and disambiguating place names in text, and providing
coordinates. Currently there are many gazetteers in use derived from many
sources, with no commonly accepted standard for encoding the data. Most
gazetteers are also very limited in the extent to which they represent the
multiple facets of the named places yet they have potential to assist user
search for locations with specific physical, commercial, social or cultural
characteristics. With a view to understanding digital gazetteer technologies
and advancing their future effectiveness for information retrieval, we provide
a review of data sources, components, software and data management
technologies, data quality and volunteered data, and methods for matching
sources that refer to the same real-world places. We highlight the need for
future work on richer representation of named places, the temporal evolution of
place identity and location, and the development of more effective methods for
data integration.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [84] [An Enhanced Privacy-preserving Federated Few-shot Learning Framework for Respiratory Disease Diagnosis](https://arxiv.org/abs/2507.08050)
*Ming Wang,Zhaoyang Duan,Dong Xue,Fangzhou Liu,Zhongheng Zhang*

Main category: cs.LG

TL;DR: 提出了一种联邦少样本学习框架，结合隐私保护机制，解决呼吸疾病诊断中数据标注不足和隐私保护问题。


<details>
  <summary>Details</summary>
Motivation: 医疗数据标注劳动密集且隐私敏感，导致高质量标注数据稀缺，现有方法难以兼顾数据隐私和诊断效果。

Method: 采用元随机梯度下降算法减少过拟合，并引入高斯差分隐私噪声保护梯度；通过加权平均算法聚合本地模型。

Result: 实验表明，该方法在保护隐私的同时，能有效诊断不同结构、类别和分布的呼吸疾病数据。

Conclusion: 该框架在数据有限和隐私保护需求下，为呼吸疾病诊断提供了可行方案。

Abstract: The labor-intensive nature of medical data annotation presents a significant
challenge for respiratory disease diagnosis, resulting in a scarcity of
high-quality labeled datasets in resource-constrained settings. Moreover,
patient privacy concerns complicate the direct sharing of local medical data
across institutions, and existing centralized data-driven approaches, which
rely on amounts of available data, often compromise data privacy. This study
proposes a federated few-shot learning framework with privacy-preserving
mechanisms to address the issues of limited labeled data and privacy protection
in diagnosing respiratory diseases. In particular, a meta-stochastic gradient
descent algorithm is proposed to mitigate the overfitting problem that arises
from insufficient data when employing traditional gradient descent methods for
neural network training. Furthermore, to ensure data privacy against gradient
leakage, differential privacy noise from a standard Gaussian distribution is
integrated into the gradients during the training of private models with local
data, thereby preventing the reconstruction of medical images. Given the
impracticality of centralizing respiratory disease data dispersed across
various medical institutions, a weighted average algorithm is employed to
aggregate local diagnostic models from different clients, enhancing the
adaptability of a model across diverse scenarios. Experimental results show
that the proposed method yields compelling results with the implementation of
differential privacy, while effectively diagnosing respiratory diseases using
data from different structures, categories, and distributions.

</details>


### [85] [Tree-Structured Parzen Estimator Can Solve Black-Box Combinatorial Optimization More Efficiently](https://arxiv.org/abs/2507.08053)
*Kenshin Abe,Yunzhuo Wang,Shuhei Watanabe*

Main category: cs.LG

TL;DR: 本文提出了一种针对树结构Parzen估计器（TPE）的高效组合优化算法，通过泛化分类核与数值核，并引入距离结构，改进了TPE在组合搜索空间中的表现。


<details>
  <summary>Details</summary>
Motivation: 组合优化在TPE中尚未被充分研究，但在化学和生物学等领域有重要应用。本文旨在填补这一空白。

Method: 泛化TPE中的分类核与数值核，引入距离结构，并针对大规模组合搜索空间优化核计算的时间复杂度。

Result: 在合成问题实验中，提出的方法比原始TPE以更少的评估次数找到更好的解。

Conclusion: 该方法在Optuna开源框架中可用，为组合优化提供了高效的解决方案。

Abstract: Tree-structured Parzen estimator (TPE) is a versatile hyperparameter
optimization (HPO) method supported by popular HPO tools. Since these HPO tools
have been developed in line with the trend of deep learning (DL), the problem
setups often used in the DL domain have been discussed for TPE such as
multi-objective optimization and multi-fidelity optimization. However, the
practical applications of HPO are not limited to DL, and black-box
combinatorial optimization is actively utilized in some domains, e.g.,
chemistry and biology. As combinatorial optimization has been an untouched, yet
very important, topic in TPE, we propose an efficient combinatorial
optimization algorithm for TPE. In this paper, we first generalize the
categorical kernel with the numerical kernel in TPE, enabling us to introduce a
distance structure to the categorical kernel. Then we discuss modifications for
the newly developed kernel to handle a large combinatorial search space. These
modifications reduce the time complexity of the kernel calculation with respect
to the size of a combinatorial search space. In the experiments using synthetic
problems, we verified that our proposed method identifies better solutions with
fewer evaluations than the original TPE. Our algorithm is available in Optuna,
an open-source framework for HPO.

</details>


### [86] [Quantile Reward Policy Optimization: Alignment with Pointwise Regression and Exact Partition Functions](https://arxiv.org/abs/2507.08068)
*Simon Matrenok,Skander Moalla,Caglar Gulcehre*

Main category: cs.LG

TL;DR: QRPO是一种新方法，利用分位数奖励从点式绝对奖励中学习，避免了在线算法的需求，同时保持了离线学习的简单性。


<details>
  <summary>Details</summary>
Motivation: 现有方法如DPO和REBEL只能从偏好对或相对信号中学习，而在线算法如PPO和GRPO需要在线数据。QRPO旨在填补这一空白。

Method: QRPO使用分位数奖励，通过回归到KL正则化RL目标的闭式解，避免了相对信号的需求。

Result: QRPO在聊天和编码评估中表现优于DPO、REBEL和SimPO，且能减少长度偏差。

Conclusion: QRPO为从点式绝对奖励中学习提供了一种简单且高效的离线方法。

Abstract: Aligning large language models with pointwise absolute rewards has so far
required online, on-policy algorithms such as PPO and GRPO. In contrast,
simpler methods that can leverage offline or off-policy data, such as DPO and
REBEL, are limited to learning from preference pairs or relative signals. To
bridge this gap, we introduce \emph{Quantile Reward Policy Optimization}
(QRPO), which learns from pointwise absolute rewards while preserving the
simplicity and offline applicability of DPO-like methods. QRPO uses quantile
rewards to enable regression to the closed-form solution of the KL-regularized
RL objective. This reward yields an analytically tractable partition function,
removing the need for relative signals to cancel this term. Moreover, QRPO
scales with increased compute to estimate quantile rewards, opening a new
dimension for pre-computation scaling. Empirically, QRPO consistently achieves
top performance on chat and coding evaluations -- reward model scores,
AlpacaEval 2, and LeetCode -- compared to DPO, REBEL, and SimPO across diverse
datasets and 8B-scale models. Finally, we find that training with robust
rewards instead of converting them to preferences induces less length bias.

</details>


### [87] [Low-rank Momentum Factorization for Memory Efficient Training](https://arxiv.org/abs/2507.08091)
*Pouria Mahdavinia,Mehrdad Mahdavi*

Main category: cs.LG

TL;DR: MoFaSGD是一种动态更新低秩SVD表示的内存高效微调方法，通过近似全秩动量实现内存优化，并在性能上保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 解决大型基础模型微调时因状态优化器（如AdamW）导致的高内存需求问题。

Method: 提出Momentum Factorized SGD（MoFaSGD），动态更新低秩SVD表示，近似全秩动量，并支持高效谱归一化更新。

Result: 在大型语言模型对齐基准测试中，MoFaSGD在内存减少和性能之间取得了平衡。

Conclusion: MoFaSGD是一种高效且理论收敛的微调方法，适用于内存受限的场景。

Abstract: Fine-tuning large foundation models presents significant memory challenges
due to stateful optimizers like AdamW, often requiring several times more GPU
memory than inference. While memory-efficient methods like parameter-efficient
fine-tuning (e.g., LoRA) and optimizer state compression exist, recent
approaches like GaLore bridge these by using low-rank gradient projections and
subspace moment accumulation. However, such methods may struggle with fixed
subspaces or computationally costly offline resampling (e.g., requiring
full-matrix SVDs). We propose Momentum Factorized SGD (MoFaSGD), which
maintains a dynamically updated low-rank SVD representation of the first-order
momentum, closely approximating its full-rank counterpart throughout training.
This factorization enables a memory-efficient fine-tuning method that
adaptively updates the optimization subspace at each iteration. Crucially,
MoFaSGD leverages the computed low-rank momentum factors to perform efficient
spectrally normalized updates, offering an alternative to subspace moment
accumulation. We establish theoretical convergence guarantees for MoFaSGD,
proving it achieves an optimal rate for non-convex stochastic optimization
under standard assumptions. Empirically, we demonstrate MoFaSGD's effectiveness
on large language model alignment benchmarks, achieving a competitive trade-off
between memory reduction (comparable to LoRA) and performance compared to
state-of-the-art low-rank optimization methods. Our implementation is available
at https://github.com/pmahdavi/MoFaSGD.

</details>


### [88] [PDE-aware Optimizer for Physics-informed Neural Networks](https://arxiv.org/abs/2507.08118)
*Hardik Shukla,Manurag Khullar,Vismay Churiwala*

Main category: cs.LG

TL;DR: 论文提出了一种基于PDE残差梯度方差的优化器，用于改进PINNs训练中的梯度对齐问题，相比Adam和SOAP，在多个PDE问题上表现更优。


<details>
  <summary>Details</summary>
Motivation: 标准优化器（如Adam）在处理PINNs中的多损失项时难以平衡，尤其在刚性或病态系统中表现不佳。

Method: 提出了一种PDE感知优化器，通过调整参数更新以适应PDE残差梯度的方差，避免二阶优化器的高计算成本。

Result: 在1D Burgers'、Allen-Cahn和KdV方程上，PDE感知优化器实现了更平滑的收敛和更低的绝对误差，尤其在梯度变化剧烈的区域。

Conclusion: PDE残差感知的自适应方法有效提升了PINNs训练的稳定性，但未来需进一步研究在大规模架构和硬件加速器上的扩展性。

Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework
for solving partial differential equations (PDEs) by embedding physical
constraints into the loss function. However, standard optimizers such as Adam
often struggle to balance competing loss terms, particularly in stiff or
ill-conditioned systems. In this work, we propose a PDE-aware optimizer that
adapts parameter updates based on the variance of per-sample PDE residual
gradients. This method addresses gradient misalignment without incurring the
heavy computational costs of second-order optimizers such as SOAP. We benchmark
the PDE-aware optimizer against Adam and SOAP on 1D Burgers', Allen-Cahn and
Korteweg-de Vries(KdV) equations. Across both PDEs, the PDE-aware optimizer
achieves smoother convergence and lower absolute errors, particularly in
regions with sharp gradients. Our results demonstrate the effectiveness of PDE
residual-aware adaptivity in enhancing stability in PINNs training. While
promising, further scaling on larger architectures and hardware accelerators
remains an important direction for future research.

</details>


### [89] [Quasi-Random Physics-informed Neural Networks](https://arxiv.org/abs/2507.08121)
*Tianchi Yu,Ivan Oseledets*

Main category: cs.LG

TL;DR: 论文提出QRPINNs，使用低差异序列采样替代随机采样，显著提升了PINNs在高维PDE中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs对采样点敏感，而高维问题中随机采样效率低，因此引入准蒙特卡洛方法改进采样。

Method: 采用低差异序列（准蒙特卡洛方法）进行采样，提出QRPINNs，并与自适应采样结合。

Result: QRPINNs在理论和实验上均优于传统PINNs及自适应采样方法，尤其在高维PDE中表现突出。

Conclusion: QRPINNs通过改进采样方法显著提升了PINNs的性能，结合自适应采样可进一步优化。

Abstract: Physics-informed neural networks have shown promise in solving partial
differential equations (PDEs) by integrating physical constraints into neural
network training, but their performance is sensitive to the sampling of points.
Based on the impressive performance of quasi Monte-Carlo methods in high
dimensional problems, this paper proposes Quasi-Random Physics-Informed Neural
Networks (QRPINNs), which use low-discrepancy sequences for sampling instead of
random points directly from the domain. Theoretically, QRPINNs have been proven
to have a better convergence rate than PINNs. Empirically, experiments
demonstrate that QRPINNs significantly outperform PINNs and some representative
adaptive sampling methods, especially in high-dimensional PDEs. Furthermore,
combining QRPINNs with adaptive sampling can further improve the performance.

</details>


### [90] [Physics-Informed Neural Networks with Hard Nonlinear Equality and Inequality Constraints](https://arxiv.org/abs/2507.08124)
*Ashfaq Iftakher,Rahul Golder,M. M. Faruque Hasan*

Main category: cs.LG

TL;DR: KKT-Hardnet是一种改进的PINN架构，通过KKT条件和投影技术严格满足约束，提高了模型精度和可靠性。


<details>
  <summary>Details</summary>
Motivation: 传统PINNs无法严格满足约束，影响工程系统的可靠性，需要一种新方法确保约束满足。

Method: 利用KKT条件和投影技术，通过距离最小化问题的KKT条件实现约束满足，并使用对数-指数变换重构非线性KKT条件。

Result: 在测试问题和实际化学过程模拟中，KKT-Hardnet比传统PINNs和多层感知机具有更高的精度和严格的约束满足。

Conclusion: KKT-Hardnet通过严格约束满足和领域知识集成，为复杂系统的可靠混合建模提供了新途径。

Abstract: Traditional physics-informed neural networks (PINNs) do not guarantee strict
constraint satisfaction. This is problematic in engineering systems where minor
violations of governing laws can significantly degrade the reliability and
consistency of model predictions. In this work, we develop KKT-Hardnet, a PINN
architecture that enforces both linear and nonlinear equality and inequality
constraints up to machine precision. It leverages a projection onto the
feasible region through solving Karush-Kuhn-Tucker (KKT) conditions of a
distance minimization problem. Furthermore, we reformulate the nonlinear KKT
conditions using log-exponential transformation to construct a general sparse
system with only linear and exponential terms, thereby making the projection
differentiable. We apply KKT-Hardnet on both test problems and a real-world
chemical process simulation. Compared to multilayer perceptrons and PINNs,
KKT-Hardnet achieves higher accuracy and strict constraint satisfaction. This
approach allows the integration of domain knowledge into machine learning
towards reliable hybrid modeling of complex systems.

</details>


### [91] [ALCo-FM: Adaptive Long-Context Foundation Model for Accident Prediction](https://arxiv.org/abs/2507.08153)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Rajiv Ramnath*

Main category: cs.LG

TL;DR: ALCo-FM是一种自适应长上下文基础模型，通过动态选择上下文窗口和浅层交叉注意力融合多模态数据，结合局部GAT层和全局稀疏变换器，实现了高精度的城市风险预测。


<details>
  <summary>Details</summary>
Motivation: 交通事故虽罕见但影响重大，需长上下文多模态推理进行准确风险预测。

Method: ALCo-FM通过动态选择上下文窗口、浅层交叉注意力融合数据，结合局部GAT层和全局稀疏变换器，并使用蒙特卡洛dropout提升置信度。

Result: 在15个美国城市数据上训练，ALCo-FM达到0.94准确率、0.92 F1分数和0.04 ECE，优于20多种先进基线。

Conclusion: ALCo-FM在城市风险预测中表现卓越，代码和数据集已开源。

Abstract: Traffic accidents are rare, yet high-impact events that require long-context
multimodal reasoning for accurate risk forecasting. In this paper, we introduce
ALCo-FM, a unified adaptive long-context foundation model that computes a
volatility pre-score to dynamically select context windows for input data and
encodes and fuses these multimodal data via shallow cross attention. Following
a local GAT layer and a BigBird-style sparse global transformer over H3
hexagonal grids, coupled with Monte Carlo dropout for confidence, the model
yields superior, well-calibrated predictions. Trained on data from 15 US cities
with a class-weighted loss to counter label imbalance, and fine-tuned with
minimal data on held-out cities, ALCo-FM achieves 0.94 accuracy, 0.92 F1, and
an ECE of 0.04, outperforming more than 20 state-of-the-art baselines in
large-scale urban risk prediction. Code and dataset are available at:
https://github.com/PinakiPrasad12/ALCo-FM

</details>


### [92] [Just Read the Question: Enabling Generalization to New Assessment Items with Text Awareness](https://arxiv.org/abs/2507.08154)
*Arisha Khan,Nathaniel Li,Tori Shen,Anna N. Rafferty*

Main category: cs.LG

TL;DR: Text-LENS通过扩展LENS部分变分自编码器，利用项目文本嵌入，提升了在未见项目上的预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习在教育评估中因依赖历史数据而难以处理新项目的问题。

Method: 扩展LENS部分变分自编码器，引入项目文本嵌入，并在Eedi和LLM-Sim数据集上测试。

Result: Text-LENS在已见项目上表现与LENS相当，在未见项目上表现更优。

Conclusion: Text-LENS能有效学习学生能力并预测新项目上的表现。

Abstract: Machine learning has been proposed as a way to improve educational assessment
by making fine-grained predictions about student performance and learning
relationships between items. One challenge with many machine learning
approaches is incorporating new items, as these approaches rely heavily on
historical data. We develop Text-LENS by extending the LENS partial variational
auto-encoder for educational assessment to leverage item text embeddings, and
explore the impact on predictive performance and generalization to previously
unseen items. We examine performance on two datasets: Eedi, a publicly
available dataset that includes item content, and LLM-Sim, a novel dataset with
test items produced by an LLM. We find that Text-LENS matches LENS' performance
on seen items and improves upon it in a variety of conditions involving unseen
items; it effectively learns student proficiency from and makes predictions
about student performance on new items.

</details>


### [93] [Emotion Recognition in Older Adults with Quantum Machine Learning and Wearable Sensors](https://arxiv.org/abs/2507.08175)
*Md. Saif Hassan Onim,Travis S. Humble,Himanshu Thapliyal*

Main category: cs.LG

TL;DR: 研究通过生理信号推断情绪状态，提出隐私保护的量子机器学习方法，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 探索隐私保护的情绪识别方法，替代传统面部识别技术，特别关注沟通障碍人群。

Method: 比较经典机器学习与量子机器学习（QML）方法，使用量子核SVM模型。

Result: 量子SVM在所有情绪类别中表现优于经典方法，F1分数超80%，召回率提升36%。

Conclusion: 量子机器学习结合可穿戴传感器数据，为临床和辅助生活场景提供无干扰情绪监测基础。

Abstract: We investigate the feasibility of inferring emotional states exclusively from
physiological signals, thereby presenting a privacy-preserving alternative to
conventional facial recognition techniques. We conduct a performance comparison
of classical machine learning algorithms and hybrid quantum machine learning
(QML) methods with a quantum kernel-based model. Our results indicate that the
quantum-enhanced SVM surpasses classical counterparts in classification
performance across all emotion categories, even when trained on limited
datasets. The F1 scores over all classes are over 80% with around a maximum of
36% improvement in the recall values. The integration of wearable sensor data
with quantum machine learning not only enhances accuracy and robustness but
also facilitates unobtrusive emotion recognition. This methodology holds
promise for populations with impaired communication abilities, such as
individuals with Alzheimer's Disease and Related Dementias (ADRD) and veterans
with Post-Traumatic Stress Disorder (PTSD). The findings establish an early
foundation for passive emotional monitoring in clinical and assisted living
conditions.

</details>


### [94] [Rethinking Spatio-Temporal Anomaly Detection: A Vision for Causality-Driven Cybersecurity](https://arxiv.org/abs/2507.08177)
*Arun Vignesh Malarkkan,Haoyue Bai,Xinyuan Wang,Anjali Kaushik,Dongjie Wang,Yanjie Fu*

Main category: cs.LG

TL;DR: 论文提出了一种基于因果学习的方法，用于提升空间分布式基础设施中的异常检测能力，解决了现有黑盒深度学习方法在可解释性、适应性和鲁棒性上的不足。


<details>
  <summary>Details</summary>
Motivation: 随着网络物理系统的互联性和空间分布性增强，确保其抵御不断演变的网络攻击的韧性成为关键。现有的数据驱动方法在可解释性和适应性上存在局限。

Method: 论文提出了因果学习视角下的异常检测方法，包括因果图分析、多视图融合和持续因果图学习三个方向。

Result: 通过实际案例（如供水基础设施）展示了因果模型在早期预警和根因分析上的优势。

Conclusion: 论文为未来研究提出了多模态、生成式AI驱动的可扩展自适应因果框架，旨在推动可解释、适应性强的异常检测系统的发展。

Abstract: As cyber-physical systems grow increasingly interconnected and spatially
distributed, ensuring their resilience against evolving cyberattacks has become
a critical priority. Spatio-Temporal Anomaly detection plays an important role
in ensuring system security and operational integrity. However, current
data-driven approaches, largely driven by black-box deep learning, face
challenges in interpretability, adaptability to distribution shifts, and
robustness under evolving system dynamics. In this paper, we advocate for a
causal learning perspective to advance anomaly detection in spatially
distributed infrastructures that grounds detection in structural cause-effect
relationships. We identify and formalize three key directions: causal graph
profiling, multi-view fusion, and continual causal graph learning, each
offering distinct advantages in uncovering dynamic cause-effect structures
across time and space. Drawing on real-world insights from systems such as
water treatment infrastructures, we illustrate how causal models provide early
warning signals and root cause attribution, addressing the limitations of
black-box detectors. Looking ahead, we outline the future research agenda
centered on multi-modality, generative AI-driven, and scalable adaptive causal
frameworks. Our objective is to lay a new research trajectory toward scalable,
adaptive, explainable, and spatially grounded anomaly detection systems. We
hope to inspire a paradigm shift in cybersecurity research, promoting
causality-driven approaches to address evolving threats in interconnected
infrastructures.

</details>


### [95] [CTRLS: Chain-of-Thought Reasoning via Latent State-Transition](https://arxiv.org/abs/2507.08182)
*Junda Wu,Yuxin Xiong,Xintong Li,Zhengmian Hu,Tong Yu,Rui Wang,Xiang Chen,Jingbo Shang,Julian McAuley*

Main category: cs.LG

TL;DR: CTRLS框架将链式思维（CoT）推理建模为马尔可夫决策过程（MDP），通过分布强化学习实现系统性探索，提升推理准确性、多样性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统CoT方法依赖启发式采样，缺乏对推理转换的结构化建模，限制了推理轨迹的多样性和有效性。

Method: CTRLS将CoT推理建模为MDP，利用分布强化学习建模潜在状态转换，结合ε-贪婪探索和熵正则化策略。

Result: 实验表明，CTRLS在基准推理任务中提高了准确性、多样性和探索效率。

Conclusion: CTRLS通过结构化建模和强化学习优化了CoT推理，为复杂推理任务提供了更有效的解决方案。

Abstract: Chain-of-thought (CoT) reasoning enables large language models (LLMs) to
break down complex problems into interpretable intermediate steps,
significantly enhancing model transparency and performance in reasoning tasks.
However, conventional CoT methods rely on heuristic sampling without structured
modeling of reasoning transitions, constraining their ability to systematically
explore and discover diverse and effective reasoning trajectories. In this
work, we introduce CTRLS, a framework that formulates CoT reasoning as a Markov
decision process (MDP) with latent state transitions, enabling principled and
state-aware exploration via distributional reinforcement learning. By modelling
reasoning actions as explicit probability distributions in latent space, our
approach explicitly models epistemic uncertainty, facilitating robust
exploration of the reasoning space. As part of our framework, we introduce an
on-policy reinforcement learning strategy incorporating epsilon-greedy
exploration and entropy-based regularization to iteratively refine latent state
transitions without requiring additional fine-tuning of the underlying LLM.
Theoretical analyses provide evidence lower bounds (ELBO), theoretically
grounding our transition-aware modeling of latent reasoning dynamics. Further
experiments demonstrate improvements in reasoning accuracy, diversity, and
exploration efficiency across benchmark reasoning tasks.

</details>


### [96] [EvA: Evolutionary Attacks on Graphs](https://arxiv.org/abs/2507.08212)
*Mohammad Sadegh Akhondzadeh,Soroush H. Zargarbashi,Jimin Cao,Aleksandar Bojchevski*

Main category: cs.LG

TL;DR: 论文提出了一种基于进化算法的离散优化攻击方法（EvA），显著提升了图神经网络（GNNs）的攻击效果，无需依赖梯度信息。


<details>
  <summary>Details</summary>
Motivation: 现有攻击方法依赖梯度信息，限制了攻击的适应性和效果，尤其是在非可微目标上表现不佳。

Method: 采用进化算法直接解决离散优化问题，支持黑盒模型和任意目标函数。

Result: EvA攻击平均比现有最佳攻击方法多降低11%的准确率，并展示了设计攻击的潜在能力。

Conclusion: EvA是一种高效、灵活的攻击方法，适用于多种场景，揭示了攻击设计的未开发潜力。

Abstract: Even a slight perturbation in the graph structure can cause a significant
drop in the accuracy of graph neural networks (GNNs). Most existing attacks
leverage gradient information to perturb edges. This relaxes the attack's
optimization problem from a discrete to a continuous space, resulting in
solutions far from optimal. It also restricts the adaptability of the attack to
non-differentiable objectives. Instead, we introduce a few simple yet effective
enhancements of an evolutionary-based algorithm to solve the discrete
optimization problem directly. Our Evolutionary Attack (EvA) works with any
black-box model and objective, eliminating the need for a differentiable proxy
loss. This allows us to design two novel attacks that reduce the effectiveness
of robustness certificates and break conformal sets. The memory complexity of
our attack is linear in the attack budget. Among our experiments, EvA shows
$\sim$11\% additional drop in accuracy on average compared to the best previous
attack, revealing significant untapped potential in designing attacks.

</details>


### [97] [InsightBuild: LLM-Powered Causal Reasoning in Smart Building Systems](https://arxiv.org/abs/2507.08235)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Rajiv Ramnath*

Main category: cs.LG

TL;DR: InsightBuild结合因果分析和微调的大型语言模型，为智能建筑中的异常能耗提供可读的因果解释。


<details>
  <summary>Details</summary>
Motivation: 解决设施管理者缺乏对异常能耗的清晰解释的问题。

Method: 两阶段框架：1) 轻量级因果推断模块分析建筑数据；2) 微调LLM生成可读解释。

Result: 在真实数据集上验证，结合因果发现和LLM生成清晰、精确的解释。

Conclusion: InsightBuild能有效帮助设施管理者诊断和缓解能源效率问题。

Abstract: Smart buildings generate vast streams of sensor and control data, but
facility managers often lack clear explanations for anomalous energy usage. We
propose InsightBuild, a two-stage framework that integrates causality analysis
with a fine-tuned large language model (LLM) to provide human-readable, causal
explanations of energy consumption patterns. First, a lightweight causal
inference module applies Granger causality tests and structural causal
discovery on building telemetry (e.g., temperature, HVAC settings, occupancy)
drawn from Google Smart Buildings and Berkeley Office datasets. Next, an LLM,
fine-tuned on aligned pairs of sensor-level causes and textual explanations,
receives as input the detected causal relations and generates concise,
actionable explanations. We evaluate InsightBuild on two real-world datasets
(Google: 2017-2022; Berkeley: 2018-2020), using expert-annotated ground-truth
causes for a held-out set of anomalies. Our results demonstrate that combining
explicit causal discovery with LLM-based natural language generation yields
clear, precise explanations that assist facility managers in diagnosing and
mitigating energy inefficiencies.

</details>


### [98] [Self-Supervised Learning-Based Multimodal Prediction on Prosocial Behavior Intentions](https://arxiv.org/abs/2507.08238)
*Abinay Reddy Naini,Zhaobo K. Zheng,Teruhisa Misu,Kumar Akash*

Main category: cs.LG

TL;DR: 提出了一种自监督学习方法，利用多模态数据解决亲社会行为预测中的数据稀缺问题。


<details>
  <summary>Details</summary>
Motivation: 亲社会行为预测在移动场景中研究不足，缺乏大规模标注数据集，限制了深度学习模型的应用。

Method: 采用自监督学习，利用现有生理和行为数据集进行预训练，再通过小规模标注数据微调模型。

Result: 显著提升了亲社会行为预测的性能，为解决数据稀缺问题提供了有效方法。

Conclusion: 该方法为智能车辆系统和人机交互提供了有价值的见解，并建立了更有效的预测基准。

Abstract: Human state detection and behavior prediction have seen significant
advancements with the rise of machine learning and multimodal sensing
technologies. However, predicting prosocial behavior intentions in mobility
scenarios, such as helping others on the road, is an underexplored area.
Current research faces a major limitation. There are no large, labeled datasets
available for prosocial behavior, and small-scale datasets make it difficult to
train deep-learning models effectively. To overcome this, we propose a
self-supervised learning approach that harnesses multi-modal data from existing
physiological and behavioral datasets. By pre-training our model on diverse
tasks and fine-tuning it with a smaller, manually labeled prosocial behavior
dataset, we significantly enhance its performance. This method addresses the
data scarcity issue, providing a more effective benchmark for prosocial
behavior prediction, and offering valuable insights for improving intelligent
vehicle systems and human-machine interaction.

</details>


### [99] [Data Generation without Function Estimation](https://arxiv.org/abs/2507.08239)
*Hadi Daneshmand,Ashkan Soleymani*

Main category: cs.LG

TL;DR: 提出一种无需函数估计的生成方法，通过确定性梯度下降更新点集位置，将均匀分布转换为任意数据分布。


<details>
  <summary>Details</summary>
Motivation: 避免函数估计的计算和统计挑战，探索新的生成方法。

Method: 利用确定性梯度下降更新点集位置，基于物理学中相互作用粒子的最新进展。

Result: 理论和实验证明该方法可行，能够开发新型生成方法。

Conclusion: 该方法无需函数估计、神经网络训练或噪声注入，为生成模型提供新思路。

Abstract: Estimating the score function (or other population-density-dependent
functions) is a fundamental component of most generative models. However, such
function estimation is computationally and statistically challenging. Can we
avoid function estimation for data generation? We propose an estimation-free
generative method: A set of points whose locations are deterministically
updated with (inverse) gradient descent can transport a uniform distribution to
arbitrary data distribution, in the mean field regime, without function
estimation, training neural networks, and even noise injection. The proposed
method is built upon recent advances in the physics of interacting particles.
We show, both theoretically and experimentally, that these advances can be
leveraged to develop novel generative methods.

</details>


### [100] [CoreSPECT: Enhancing Clustering Algorithms via an Interplay of Density and Geometry](https://arxiv.org/abs/2507.08243)
*Chandra Sekhar Mukherjee,Joonyoung Bae,Jiapeng Zhang*

Main category: cs.LG

TL;DR: 论文提出CoreSPECT框架，通过结合数据分布与几何特性提升K-Means和GMM的聚类性能，平均提升40%和14%。


<details>
  <summary>Details</summary>
Motivation: 传统聚类算法通常只关注数据密度或几何复杂性，而忽略了二者之间的交互作用。本文旨在利用这种交互作用提升聚类效果。

Method: 设计CoreSPECT框架，通过核心空间投影和多层传播策略优化简单聚类算法（如K-Means和GMM）。

Result: 在15个数据集上测试，K-Means的ARI提升40%，GMM提升14%，优于主流密度和几何聚类算法。

Conclusion: CoreSPECT框架有效结合分布与几何特性，显著提升聚类性能，并具备理论保障和抗噪能力。

Abstract: Density and geometry have long served as two of the fundamental guiding
principles in clustering algorithm design, with algorithm usually focusing
either on the density structure of the data (e.g., HDBSCAN and Density Peak
Clustering) or the complexity of underlying geometry (e.g., manifold clustering
algorithms).
  In this paper, we identify and formalize a recurring but often overlooked
interaction between distribution and geometry and leverage this insight to
design our clustering enhancement framework CoreSPECT (Core Space
Projection-based Enhancement of Clustering Techniques). Our framework boosts
the performance of simple algorithms like K-Means and GMM by applying them to
strategically selected regions, then extending the partial partition to a
complete partition for the dataset using a novel neighborhood graph based
multi-layer propagation procedure.
  We apply our framework on 15 datasets from three different domains and obtain
consistent and substantial gain in clustering accuracy for both K-Means and
GMM. On average, our framework improves the ARI of K-Means by 40% and of GMM by
14%, often surpassing the performance of both manifold-based and recent
density-based clustering algorithms. We further support our framework with
initial theoretical guarantees, ablation to demonstrate the usefulness of the
individual steps and with evidence of robustness to noise.

</details>


### [101] [Quantum-Accelerated Neural Imputation with Large Language Models (LLMs)](https://arxiv.org/abs/2507.08255)
*Hossein Jamali*

Main category: cs.LG

TL;DR: Quantum-UnIMP整合浅层量子电路到LLM插补架构中，利用量子特性提升混合类型数据的插补性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统嵌入方法在混合类型数据中难以捕捉复杂非线性相关性的问题。

Method: 用IQP量子电路生成量子特征映射替代传统嵌入，利用量子叠加和纠缠特性。

Result: 在基准数据集上，数值特征插补误差降低15.2%，分类特征准确率提升8.7%。

Conclusion: 量子增强表示在复杂数据插补任务中具有显著潜力。

Abstract: Missing data presents a critical challenge in real-world datasets,
significantly degrading the performance of machine learning models. While Large
Language Models (LLMs) have recently demonstrated remarkable capabilities in
tabular data imputation, exemplified by frameworks like UnIMP, their reliance
on classical embedding methods often limits their ability to capture complex,
non-linear correlations, particularly in mixed-type data scenarios encompassing
numerical, categorical, and textual features. This paper introduces
Quantum-UnIMP, a novel framework that integrates shallow quantum circuits into
an LLM-based imputation architecture. Our core innovation lies in replacing
conventional classical input embeddings with quantum feature maps generated by
an Instantaneous Quantum Polynomial (IQP) circuit. This approach enables the
model to leverage quantum phenomena such as superposition and entanglement,
thereby learning richer, more expressive representations of data and enhancing
the recovery of intricate missingness patterns. Our experiments on benchmark
mixed-type datasets demonstrate that Quantum-UnIMP reduces imputation error by
up to 15.2% for numerical features (RMSE) and improves classification accuracy
by 8.7% for categorical features (F1-Score) compared to state-of-the-art
classical and LLM-based methods. These compelling results underscore the
profound potential of quantum-enhanced representations for complex data
imputation tasks, even with near-term quantum hardware.

</details>


### [102] [A Practical Two-Stage Recipe for Mathematical LLMs: Maximizing Accuracy with SFT and Efficiency with Reinforcement Learning](https://arxiv.org/abs/2507.08267)
*Hiroshi Yoshihara,Taiki Yamaguchi,Yuichi Inoue*

Main category: cs.LG

TL;DR: 论文提出了一种结合监督微调（SFT）和在线推理强化学习（GRPO）的方法，以提升大语言模型（LLMs）的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 提升LLMs的数学推理能力是AI发展的关键挑战，但目前缺乏系统的方法来结合SFT和RL以最大化准确性和效率。

Method: 通过延长SFT阶段（最多10个epoch）提升模型准确性，再通过GRPO阶段优化解决方案长度，同时保持性能。

Result: 在严格无泄漏的AI数学奥林匹克（AIMO）等基准测试中表现优异，排名靠前。

Conclusion: 该方法为开发高准确性和高效率的数学推理模型提供了实用方案，并开源了完整框架。

Abstract: Enhancing the mathematical reasoning of Large Language Models (LLMs) is a
pivotal challenge in advancing AI capabilities. While Supervised Fine-Tuning
(SFT) and Reinforcement Learning (RL) are the dominant training paradigms, a
systematic methodology for combining them to maximize both accuracy and
efficiency remains largely unexplored. This paper introduces a practical and
effective training recipe that strategically integrates extended SFT with RL
from online inference (GRPO). We posit that these methods play complementary,
not competing, roles: a prolonged SFT phase first pushes the model's accuracy
to its limits, after which a GRPO phase dramatically improves token efficiency
while preserving this peak performance. Our experiments reveal that extending
SFT for as many as 10 epochs is crucial for performance breakthroughs, and that
the primary role of GRPO in this framework is to optimize solution length. The
efficacy of our recipe is rigorously validated through top-tier performance on
challenging benchmarks, including a high rank among over 2,200 teams in the
strictly leak-free AI Mathematical Olympiad (AIMO). This work provides the
community with a battle-tested blueprint for developing state-of-the-art
mathematical reasoners that are both exceptionally accurate and practically
efficient. To ensure full reproducibility and empower future research, we will
open-source our entire framework, including all code, model checkpoints, and
training configurations at
https://github.com/analokmaus/kaggle-aimo2-fast-math-r1.

</details>


### [103] [Data-Driven Dimensional Synthesis of Diverse Planar Four-bar Function Generation Mechanisms via Direct Parameterization](https://arxiv.org/abs/2507.08269)
*Woon Ryong Kim,Jaeheun Jung,Jeong Un Ha,Donghun Lee,Jae Kyung Shim*

Main category: cs.LG

TL;DR: 提出了一种基于数据驱动的平面四杆机构尺寸综合方法，利用监督学习替代传统方程求解和优化。


<details>
  <summary>Details</summary>
Motivation: 平面四杆机构的尺寸综合是一个复杂的逆运动学问题，传统方法效率低且对非专家不友好。

Method: 结合合成数据集、LSTM神经网络处理序列精度点，以及针对不同连杆类型的MoE架构。

Result: 实验表明，该方法能生成准确、无缺陷的连杆机构，适用于多种配置。

Conclusion: 该方法为非专家用户提供了直观高效的设计工具，并为运动学设计中的可扩展和灵活综合开辟了新途径。

Abstract: Dimensional synthesis of planar four-bar mechanisms is a challenging inverse
problem in kinematics, requiring the determination of mechanism dimensions from
desired motion specifications. We propose a data-driven framework that bypasses
traditional equation-solving and optimization by leveraging supervised
learning. Our method combines a synthetic dataset, an LSTM-based neural network
for handling sequential precision points, and a Mixture of Experts (MoE)
architecture tailored to different linkage types. Each expert model is trained
on type-specific data and guided by a type-specifying layer, enabling both
single-type and multi-type synthesis. A novel simulation metric evaluates
prediction quality by comparing desired and generated motions. Experiments show
our approach produces accurate, defect-free linkages across various
configurations. This enables intuitive and efficient mechanism design, even for
non-expert users, and opens new possibilities for scalable and flexible
synthesis in kinematic design.

</details>


### [104] [Lightweight Safety Guardrails via Synthetic Data and RL-guided Adversarial Training](https://arxiv.org/abs/2507.08284)
*Aleksei Ilin,Gor Matevosyan,Xueying Ma,Vladimir Eremin,Suhaa Dada,Muqun Li,Riyaaz Shaik,Haluk Noyan Tokgozoglu*

Main category: cs.LG

TL;DR: 提出了一种轻量级但高效的语言模型安全护栏框架，通过合成数据生成和对抗训练，使小规模语言模型在内容审核任务中表现优于大模型。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在内容审核中的计算开销大和对抗攻击脆弱性问题，提供高效且可扩展的解决方案。

Method: 采用高保真合成数据生成（基于人类种子数据增强和改写）和对抗训练（结合强化学习和GAN架构），迭代优化小模型的安全分类器。

Result: 小规模语言模型在内容审核任务中表现优于大模型，同时降低计算开销并增强对抗攻击的鲁棒性。

Conclusion: 该框架为AI系统的内容审核提供了高效、可扩展且鲁棒的解决方案。

Abstract: We introduce a lightweight yet highly effective safety guardrail framework
for language models, demonstrating that small-scale language models can
achieve, and even surpass, the performance of larger counterparts in content
moderation tasks. This is accomplished through high-fidelity synthetic data
generation and adversarial training. The synthetic data generation process
begins with human-curated seed data, which undergoes query augmentation and
paraphrasing to create diverse and contextually rich examples. This augmented
data is then subjected to multiple rounds of curation, ensuring high fidelity
and relevance. Inspired by recent advances in the Generative Adversarial
Network (GAN) architecture, our adversarial training employs reinforcement
learning to guide a generator that produces challenging synthetic examples.
These examples are used to fine-tune the safety classifier, enhancing its
ability to detect and mitigate harmful content. Additionally, we incorporate
strategies from recent research on efficient LLM training, leveraging the
capabilities of smaller models to improve the performance of larger generative
models. With iterative adversarial training and the generation of diverse,
high-quality synthetic data, our framework enables small language models (SLMs)
to serve as robust safety guardrails. This approach not only reduces
computational overhead but also enhances resilience against adversarial
attacks, offering a scalable and efficient solution for content moderation in
AI systems.

</details>


### [105] [CAS Condensed and Accelerated Silhouette: An Efficient Method for Determining the Optimal K in K-Means Clustering](https://arxiv.org/abs/2507.08311)
*Krishnendu Das,Sumit Gupta,Awadhesh Kumar*

Main category: cs.LG

TL;DR: 本文提出了一种基于Condensed Silhouette方法及多种统计技术的改进聚类方法，用于优化K-Means聚类中k值的选择，显著提升了计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 在大数据环境中，聚类精度和计算效率的平衡是主要挑战，尤其在文本和图像数据中。

Method: 结合Condensed Silhouette、Local Structures、Gap Statistics等方法，提出了一种CCR和COI算法来计算最佳k值。

Result: 实验表明，该方法在高维数据集上执行时间提升99%，同时保持精度和可扩展性。

Conclusion: 该方法适用于实时聚类或资源受限场景，显著提升了聚类效率和有效性。

Abstract: Clustering is a critical component of decision-making in todays data-driven
environments. It has been widely used in a variety of fields such as
bioinformatics, social network analysis, and image processing. However,
clustering accuracy remains a major challenge in large datasets. This paper
presents a comprehensive overview of strategies for selecting the optimal value
of k in clustering, with a focus on achieving a balance between clustering
precision and computational efficiency in complex data environments. In
addition, this paper introduces improvements to clustering techniques for text
and image data to provide insights into better computational performance and
cluster validity. The proposed approach is based on the Condensed Silhouette
method, along with statistical methods such as Local Structures, Gap
Statistics, Class Consistency Ratio, and a Cluster Overlap Index CCR and
COIbased algorithm to calculate the best value of k for K-Means clustering. The
results of comparative experiments show that the proposed approach achieves up
to 99 percent faster execution times on high-dimensional datasets while
retaining both precision and scalability, making it highly suitable for real
time clustering needs or scenarios demanding efficient clustering with minimal
resource utilization.

</details>


### [106] [A Comprehensively Adaptive Architectural Optimization-Ingrained Quantum Neural Network Model for Cloud Workloads Prediction](https://arxiv.org/abs/2507.08317)
*Jitendra Kumar,Deepika Saxena,Kishu Gupta,Satyam Kumar,Ashutosh Kumar Singh*

Main category: cs.LG

TL;DR: 提出了一种结合量子计算和结构优化的新型神经网络CA-QNN，显著提升了云工作负载预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络在处理高维动态工作负载时效率不足，需要一种更优化的方法。

Method: 采用量子计算和结构优化算法，将工作负载数据转换为量子比特，并通过量子神经元处理。

Result: CA-QNN在预测准确性上优于现有方法，误差降低高达93.40%和91.27%。

Conclusion: CA-QNN为云工作负载预测提供了一种高效且准确的解决方案。

Abstract: Accurate workload prediction and advanced resource reservation are
indispensably crucial for managing dynamic cloud services. Traditional neural
networks and deep learning models frequently encounter challenges with diverse,
high-dimensional workloads, especially during sudden resource demand changes,
leading to inefficiencies. This issue arises from their limited optimization
during training, relying only on parametric (inter-connection weights)
adjustments using conventional algorithms. To address this issue, this work
proposes a novel Comprehensively Adaptive Architectural Optimization-based
Variable Quantum Neural Network (CA-QNN), which combines the efficiency of
quantum computing with complete structural and qubit vector parametric
learning. The model converts workload data into qubits, processed through qubit
neurons with Controlled NOT-gated activation functions for intuitive pattern
recognition. In addition, a comprehensive architecture optimization algorithm
for networks is introduced to facilitate the learning and propagation of the
structure and parametric values in variable-sized QNNs. This algorithm
incorporates quantum adaptive modulation and size-adaptive recombination during
training process. The performance of CA-QNN model is thoroughly investigated
against seven state-of-the-art methods across four benchmark datasets of
heterogeneous cloud workloads. The proposed model demonstrates superior
prediction accuracy, reducing prediction errors by up to 93.40% and 91.27%
compared to existing deep learning and QNN-based approaches.

</details>


### [107] [scE$^2$TM: Toward Interpretable Single-Cell Embedding via Topic Modeling](https://arxiv.org/abs/2507.08355)
*Hegang Chen,Yuyin Lu,Zhiming Dai,Fu Lee Wang,Qing Li,Yanghui Rao*

Main category: cs.LG

TL;DR: scE2TM是一种基于外部知识的单细胞嵌入主题模型，通过引入定量评估指标和外部生物知识，显著提升了单细胞RNA测序数据的聚类性能和模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有单细胞嵌入主题模型存在解释性崩溃问题且忽视外部生物知识，限制了分析性能。

Method: 提出scE2TM模型，结合外部知识指导，并设计10个定量指标评估模型可解释性。

Result: 在20个数据集上，scE2TM的聚类性能优于7种前沿方法，且其解释性在多样性和生物信号一致性上表现优异。

Conclusion: scE2TM为单细胞RNA测序数据分析提供了高质量的嵌入和强解释性，有助于揭示潜在生物学机制。

Abstract: Recent advances in sequencing technologies have enabled researchers to
explore cellular heterogeneity at single-cell resolution. Meanwhile,
interpretability has gained prominence parallel to the rapid increase in the
complexity and performance of deep learning models. In recent years, topic
models have been widely used for interpretable single-cell embedding learning
and clustering analysis, which we refer to as single-cell embedded topic
models. However, previous studies evaluated the interpretability of the models
mainly through qualitative analysis, and these single-cell embedded topic
models suffer from the potential problem of interpretation collapse.
Furthermore, their neglect of external biological knowledge constrains
analytical performance. Here, we present scE2TM, an external knowledge-guided
single-cell embedded topic model that provides a high-quality cell embedding
and strong interpretation, contributing to comprehensive scRNA-seq data
analysis. Our comprehensive evaluation across 20 scRNA-seq datasets
demonstrates that scE2TM achieves significant clustering performance gains
compared to 7 state-of-the-art methods. In addition, we propose a new
interpretability evaluation benchmark that introduces 10 metrics to
quantitatively assess the interpretability of single-cell embedded topic
models. The results show that the interpretation provided by scE2TM performs
encouragingly in terms of diversity and consistency with the underlying
biological signals, contributing to a better revealing of the underlying
biological mechanisms.

</details>


### [108] [Leveraging Machine Learning and Enhanced Parallelism Detection for BPMN Model Generation from Text](https://arxiv.org/abs/2507.08362)
*Phuong Nam Lê,Charlotte Schneider-Depré,Alexandre Goossens,Alexander Stevens,Aurélie Leribaux,Johannes De Smedt*

Main category: cs.LG

TL;DR: 本文提出了一种自动化流程，利用机器学习和大型语言模型从文本中提取BPMN模型，并通过新标注的数据集提升训练效果。


<details>
  <summary>Details</summary>
Motivation: 将文本流程文档转换为BPMN模型的过程耗时且昂贵，现有方法难以处理写作风格和并行结构。

Method: 采用机器学习和大型语言模型构建自动化管道，并引入新标注的数据集（PET数据集增强版）以改进模型训练。

Result: 提出的方法在重构准确性上表现良好，尤其是在捕捉并行结构方面。

Conclusion: 该方法为加速BPMN模型创建提供了有前景的基础。

Abstract: Efficient planning, resource management, and consistent operations often rely
on converting textual process documents into formal Business Process Model and
Notation (BPMN) models. However, this conversion process remains time-intensive
and costly. Existing approaches, whether rule-based or machine-learning-based,
still struggle with writing styles and often fail to identify parallel
structures in process descriptions.
  This paper introduces an automated pipeline for extracting BPMN models from
text, leveraging the use of machine learning and large language models. A key
contribution of this work is the introduction of a newly annotated dataset,
which significantly enhances the training process. Specifically, we augment the
PET dataset with 15 newly annotated documents containing 32 parallel gateways
for model training, a critical feature often overlooked in existing datasets.
This addition enables models to better capture parallel structures, a common
but complex aspect of process descriptions. The proposed approach demonstrates
adequate performance in terms of reconstruction accuracy, offering a promising
foundation for organizations to accelerate BPMN model creation.

</details>


### [109] [Prediction of Lane Change Intentions of Human Drivers using an LSTM, a CNN and a Transformer](https://arxiv.org/abs/2507.08365)
*Francesco De Cristofaro,Felix Hofbaur,Aixi Yang,Arno Eichberger*

Main category: cs.LG

TL;DR: 论文研究了如何预测前车变道意图，比较了LSTM、CNN和Transformer网络的性能，发现Transformer表现最佳。


<details>
  <summary>Details</summary>
Motivation: 前车变道对自动驾驶车辆的运动规划有重大影响，预测变道意图可提升安全性和效率。现有研究多关注特定时间点的预测，而缺乏对时间区间内预测及不同架构的比较。

Method: 使用LSTM、CNN和Transformer网络预测驾驶员变道意图，基于公开数据集highD准备数据，设计网络并比较不同输入配置下的结果。

Result: Transformer网络表现最佳，准确率在82.79%至96.73%之间，且不易过拟合。

Conclusion: Transformer网络在预测变道意图任务中优于LSTM和CNN，具有较高的准确率和鲁棒性。

Abstract: Lane changes of preceding vehicles have a great impact on the motion planning
of automated vehicles especially in complex traffic situations. Predicting them
would benefit the public in terms of safety and efficiency. While many research
efforts have been made in this direction, few concentrated on predicting
maneuvers within a set time interval compared to predicting at a set prediction
time. In addition, there exist a lack of comparisons between different
architectures to try to determine the best performing one and to assess how to
correctly choose the input for such models. In this paper the structure of an
LSTM, a CNN and a Transformer network are described and implemented to predict
the intention of human drivers to perform a lane change. We show how the data
was prepared starting from a publicly available dataset (highD), which features
were used, how the networks were designed and finally we compare the results of
the three networks with different configurations of input data. We found that
transformer networks performed better than the other networks and was less
affected by overfitting. The accuracy of the method spanned from $82.79\%$ to
$96.73\%$ for different input configurations and showed overall good
performances considering also precision and recall.

</details>


### [110] [Advances in Machine Learning: Where Can Quantum Techniques Help?](https://arxiv.org/abs/2507.08379)
*Samarth Kashyap,Rohit K Ramakrishnan,Kumari Jyoti,Apoorva D Patel*

Main category: cs.LG

TL;DR: 本文综述了量子机器学习（QML）的潜力、理论基础、关键进展及挑战，强调其在特定应用中的前景及实际部署的障碍。


<details>
  <summary>Details</summary>
Motivation: 探索量子计算如何通过QML解决经典机器学习的计算瓶颈，尤其是在处理复杂数据集时的优势。

Method: 介绍QML的理论基础，包括量子数据编码、量子学习理论和优化技术，并基于数据类型和计算架构分类QML方法。

Result: QML在量子化学和传感等特定应用中具有潜力，但实际应用受限于NISQ设备的噪声、可扩展性和数据编码开销。

Conclusion: QML的未来发展需要量子原生算法、改进的纠错技术和现实基准测试，以弥合理论与实际部署之间的差距。

Abstract: Quantum Machine Learning (QML) represents a promising frontier at the
intersection of quantum computing and artificial intelligence, aiming to
leverage quantum computational advantages to enhance data-driven tasks. This
review explores the potential of QML to address the computational bottlenecks
of classical machine learning, particularly in processing complex datasets. We
introduce the theoretical foundations of QML, including quantum data encoding,
quantum learning theory and optimization techniques, while categorizing QML
approaches based on data type and computational architecture. It is
well-established that quantum computational advantages are problem-dependent,
and so potentially useful directions for QML need to be systematically
identified. Key developments, such as Quantum Principal Component Analysis,
quantum-enhanced sensing and applications in material science, are critically
evaluated for their theoretical speed-ups and practical limitations. The
challenges posed by Noisy Intermediate-Scale Quantum (NISQ) devices, including
hardware noise, scalability constraints and data encoding overheads, are
discussed in detail. We also outline future directions, emphasizing the need
for quantum-native algorithms, improved error correction, and realistic
benchmarks to bridge the gap between theoretical promise and practical
deployment. This comprehensive analysis underscores that while QML has
significant potential for specific applications such as quantum chemistry and
sensing, its broader utility in real-world scenarios remains contingent on
overcoming technological and methodological hurdles.

</details>


### [111] [Two-cluster test](https://arxiv.org/abs/2507.08382)
*Xinying Liu,Lianyu Hu,Mudi Jiang,Simen Zhang,Jun Lou,Zengyou He*

Main category: cs.LG

TL;DR: 本文提出了一种新的两簇检验方法，解决了传统两样本检验在聚类分析中导致的Type-I错误率膨胀问题。


<details>
  <summary>Details</summary>
Motivation: 传统两样本检验在聚类分析中会导致Type-I错误率显著增加，因此需要一种专门针对两簇检验的新方法。

Method: 基于两个子集之间的边界点，提出了一种新的两簇检验方法，用于计算显著性p值。

Result: 实验表明，该方法能显著降低Type-I错误率，优于传统两样本检验方法。

Conclusion: 该方法在树状可解释聚类和基于显著性的层次聚类中验证了其实际应用价值。

Abstract: Cluster analysis is a fundamental research issue in statistics and machine
learning. In many modern clustering methods, we need to determine whether two
subsets of samples come from the same cluster. Since these subsets are usually
generated by certain clustering procedures, the deployment of classic
two-sample tests in this context would yield extremely smaller p-values,
leading to inflated Type-I error rate. To overcome this bias, we formally
introduce the two-cluster test issue and argue that it is a totally different
significance testing issue from conventional two-sample test. Meanwhile, we
present a new method based on the boundary points between two subsets to derive
an analytical p-value for the purpose of significance quantification.
Experiments on both synthetic and real data sets show that the proposed test is
able to significantly reduce the Type-I error rate, in comparison with several
classic two-sample testing methods. More importantly, the practical usage of
such two-cluster test is further verified through its applications in
tree-based interpretable clustering and significance-based hierarchical
clustering.

</details>


### [112] [Online Pre-Training for Offline-to-Online Reinforcement Learning](https://arxiv.org/abs/2507.08387)
*Yongjae Shin,Jeonghye Kim,Whiyoung Jung,Sunghoon Hong,Deunsol Yoon,Youngsoo Jang,Geonhyeong Kim,Jongseong Chae,Youngchul Sung,Kanghoon Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: 论文提出了一种新方法OPT，通过在线预训练解决离线预训练代理在线微调时的价值估计不准确问题，性能提升30%。


<details>
  <summary>Details</summary>
Motivation: 离线预训练代理在线微调时因分布偏移导致价值估计不准确，影响性能，甚至不如随机初始化。

Method: 引入在线预训练阶段，训练专门用于在线微调的新价值函数。

Result: 在TD3和SPOT上实现，D4RL环境中性能平均提升30%。

Conclusion: OPT有效解决了离线到在线强化学习中的价值估计问题，显著提升性能。

Abstract: Offline-to-online reinforcement learning (RL) aims to integrate the
complementary strengths of offline and online RL by pre-training an agent
offline and subsequently fine-tuning it through online interactions. However,
recent studies reveal that offline pre-trained agents often underperform during
online fine-tuning due to inaccurate value estimation caused by distribution
shift, with random initialization proving more effective in certain cases. In
this work, we propose a novel method, Online Pre-Training for Offline-to-Online
RL (OPT), explicitly designed to address the issue of inaccurate value
estimation in offline pre-trained agents. OPT introduces a new learning phase,
Online Pre-Training, which allows the training of a new value function tailored
specifically for effective online fine-tuning. Implementation of OPT on TD3 and
SPOT demonstrates an average 30% improvement in performance across a wide range
of D4RL environments, including MuJoCo, Antmaze, and Adroit.

</details>


### [113] [Inference-Time Scaling of Diffusion Language Models with Particle Gibbs Sampling](https://arxiv.org/abs/2507.08390)
*Meihua Dang,Jiaqi Han,Minkai Xu,Kai Xu,Akash Srivastava,Stefano Ermon*

Main category: cs.LG

TL;DR: 本文提出了一种基于粒子Gibbs采样的离散扩散模型推理时间扩展方法，用于奖励引导的文本生成任务，显著提升了生成质量。


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在语言建模中表现出色，但其推理时间扩展尚未充分探索，尤其是在奖励引导的文本生成任务中。

Method: 采用粒子Gibbs采样算法，通过条件序贯蒙特卡洛机制迭代优化扩散轨迹，逐步接近奖励加权目标分布。

Result: 实验表明，该方法在奖励引导的文本生成任务中优于现有推理时间扩展策略，显著提高了准确性。

Conclusion: 提出的方法在固定计算预算下，通过多轨迹迭代优化，实现了更高质量的文本生成。

Abstract: Discrete diffusion models have emerged as a powerful paradigm for language
modeling, rivaling auto-regressive models by training-time scaling. However,
inference-time scaling in discrete diffusion models remains relatively
under-explored. In this work, we study sampling-based approaches for achieving
high-quality text generation from discrete diffusion models in reward-guided
settings. We introduce a novel inference-time scaling approach based on
particle Gibbs sampling for discrete diffusion models. The particle Gibbs
sampling algorithm iteratively refines full diffusion trajectories using
conditional Sequential Monte Carlo as its transition mechanism. This process
ensures that the updated samples progressively improve and move closer to the
reward-weighted target distribution. Unlike existing inference-time scaling
methods, which are often limited to single diffusion trajectories, our approach
leverages iterative refinement across multiple trajectories. Within this
framework, we further analyze the trade-offs between four key axes for
inference-time scaling under fixed compute budgets: particle Gibbs iterations,
particle count, denoising steps, and reward estimation cost. Empirically, our
method consistently outperforms prior inference-time strategies on
reward-guided text generation tasks, achieving significant improvement in
accuracy under varying compute budgets.

</details>


### [114] [RTNinja: a generalized machine learning framework for analyzing random telegraph noise signals in nanoelectronic devices](https://arxiv.org/abs/2507.08424)
*Anirudh Varanasi,Robin Degraeve,Philippe Roussel,Clement Merckling*

Main category: cs.LG

TL;DR: RTNinja是一个全自动机器学习框架，用于无监督分析随机电报噪声信号，无需系统先验知识即可识别隐藏源的特性。


<details>
  <summary>Details</summary>
Motivation: 随机电报噪声影响纳米电子器件的可靠性和性能，传统分析方法受限，需要自动化解决方案。

Method: RTNinja包含LevelsExtractor（贝叶斯推断去噪）和SourcesMapper（概率聚类优化），通过蒙特卡洛模拟验证性能。

Result: 在7000个数据集中，RTNinja实现了高保真信号重构和准确提取源振幅与活动模式。

Conclusion: RTNinja为随机电报噪声分析提供了稳健、可扩展的工具，适用于下一代纳米电子技术。

Abstract: Random telegraph noise is a prevalent variability phenomenon in
nanoelectronic devices, arising from stochastic carrier exchange at defect
sites and critically impacting device reliability and performance. Conventional
analysis techniques often rely on restrictive assumptions or manual
interventions, limiting their applicability to complex, noisy datasets. Here,
we introduce RTNinja, a generalized, fully automated machine learning framework
for the unsupervised analysis of random telegraph noise signals. RTNinja
deconvolves complex signals to identify the number and characteristics of
hidden individual sources, without requiring prior knowledge of the system. The
framework comprises two modular components: LevelsExtractor, which uses
Bayesian inference and model selection to denoise and discretize the signal;
and SourcesMapper, which infers source configurations through probabilistic
clustering and optimization. To evaluate performance, we developed a Monte
Carlo simulator that generates labeled datasets spanning broad signal-to-noise
ratios and source complexities; across 7000 such datasets, RTNinja consistently
demonstrated high-fidelity signal reconstruction and accurate extraction of
source amplitudes and activity patterns. Our results demonstrate that RTNinja
offers a robust, scalable, and device-agnostic tool for random telegraph noise
characterization, enabling large-scale statistical benchmarking,
reliability-centric technology qualification, predictive failure modeling, and
device physics exploration in next-generation nanoelectronics.

</details>


### [115] [KGRAG-Ex: Explainable Retrieval-Augmented Generation with Knowledge Graph-based Perturbations](https://arxiv.org/abs/2507.08443)
*Georgios Balanos,Evangelos Chasanis,Konstantinos Skianis,Evaggelia Pitoura*

Main category: cs.LG

TL;DR: KGRAG-Ex 是一个基于知识图谱的 RAG 系统，通过结构化检索路径和伪段落生成提升事实基础和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决传统 RAG 在非结构化文本检索中缺乏可解释性的问题。

Method: 利用领域特定知识图谱，通过提示信息提取构建图谱，生成伪段落指导检索，并引入扰动方法评估图谱组件对答案的影响。

Result: 实验分析了系统对不同扰动方法的敏感性、图谱组件重要性与其结构位置的关系、语义节点类型的影响，以及图谱指标与解释过程中组件影响力的对应关系。

Conclusion: KGRAG-Ex 通过结构化知识图谱显著提升了 RAG 的事实基础和可解释性。

Abstract: Retrieval-Augmented Generation (RAG) enhances language models by grounding
responses in external information, yet explainability remains a critical
challenge, particularly when retrieval relies on unstructured text. Knowledge
graphs (KGs) offer a solution by introducing structured, semantically rich
representations of entities and their relationships, enabling transparent
retrieval paths and interpretable reasoning. In this work, we present KGRAG-Ex,
a RAG system that improves both factual grounding and explainability by
leveraging a domain-specific KG constructed via prompt-based information
extraction. Given a user query, KGRAG-Ex identifies relevant entities and
semantic paths in the graph, which are then transformed into pseudo-paragraphs:
natural language representations of graph substructures that guide corpus
retrieval. To improve interpretability and support reasoning transparency, we
incorporate perturbation-based explanation methods that assess the influence of
specific KG-derived components on the generated answers. We conduct a series of
experiments to analyze the sensitivity of the system to different perturbation
methods, the relationship between graph component importance and their
structural positions, the influence of semantic node types, and how graph
metrics correspond to the influence of components within the explanations
process.

</details>


### [116] [Space filling positionality and the Spiroformer](https://arxiv.org/abs/2507.08456)
*M. Maurin,M. Á. Evangelista-Alvarado,P. Suárez-Serrato*

Main category: cs.LG

TL;DR: 提出了一种基于空间填充曲线的注意力机制，用于在几何域（如流形）上推广Transformer模型，并以2-球面上的极坐标螺旋为例验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: Transformer在处理序列数据时表现出色，但在几何域（如流形）上缺乏全局顺序定义，限制了其应用。

Method: 采用空间填充曲线作为注意力头的路径，提出Spiroformer模型，以2-球面上的极坐标螺旋为例进行实验。

Result: Spiroformer展示了在几何域上应用Transformer的可行性。

Conclusion: 通过空间填充曲线，可以在缺乏全局顺序的几何域上有效推广Transformer模型。

Abstract: Transformers excel when dealing with sequential data. Generalizing
transformer models to geometric domains, such as manifolds, we encounter the
problem of not having a well-defined global order. We propose a solution with
attention heads following a space-filling curve. As a first experimental
example, we present the Spiroformer, a transformer that follows a polar spiral
on the $2$-sphere.

</details>


### [117] [Ranked Set Sampling-Based Multilayer Perceptron: Improving Generalization via Variance-Based Bounds](https://arxiv.org/abs/2507.08465)
*Feijiang Li,Liuya Zhang,Jieting Wang,Tao Yan,Yuhua Qian*

Main category: cs.LG

TL;DR: 本文提出了一种基于Rank Set Sampling (RSS)的MLP方法（RSS-MLP），通过减少经验损失的方差来提高模型的泛化能力，理论证明和实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统bagging方法使用简单随机抽样（SRS）生成训练数据，随机性较高，本文旨在通过引入有序结构的RSS进一步减少损失方差。

Method: 提出RSS-MLP方法，利用RSS对训练数据集进行有序采样，减少经验损失的方差。

Result: 理论证明RSS估计的经验指数损失和逻辑损失的方差小于SRS；实验在12个基准数据集上验证了RSS-MLP的优越性。

Conclusion: RSS-MLP通过减少方差显著提升了MLP的泛化能力，实验和理论分析均支持其有效性和合理性。

Abstract: Multilayer perceptron (MLP), one of the most fundamental neural networks, is
extensively utilized for classification and regression tasks. In this paper, we
establish a new generalization error bound, which reveals how the variance of
empirical loss influences the generalization ability of the learning model.
Inspired by this learning bound, we advocate to reduce the variance of
empirical loss to enhance the ability of MLP. As is well-known, bagging is a
popular ensemble method to realize variance reduction. However, bagging
produces the base training data sets by the Simple Random Sampling (SRS)
method, which exhibits a high degree of randomness. To handle this issue, we
introduce an ordered structure in the training data set by Rank Set Sampling
(RSS) to further reduce the variance of loss and develop a RSS-MLP method.
Theoretical results show that the variance of empirical exponential loss and
the logistic loss estimated by RSS are smaller than those estimated by SRS,
respectively. To validate the performance of RSS-MLP, we conduct comparison
experiments on twelve benchmark data sets in terms of the two convex loss
functions under two fusion methods. Extensive experimental results and analysis
illustrate the effectiveness and rationality of the propose method.

</details>


### [118] [Pre-Training LLMs on a budget: A comparison of three optimizers](https://arxiv.org/abs/2507.08472)
*Joel Schlotthauer,Christian Kroos,Chris Hinze,Viktor Hangya,Luzian Hahn,Fabian Küch*

Main category: cs.LG

TL;DR: 比较三种优化器（AdamW、Lion、Sophia）在LLM预训练中的表现，发现Sophia损失最低，Lion最快，AdamW下游任务表现最佳。


<details>
  <summary>Details</summary>
Motivation: 研究优化器对LLM预训练时间和模型性能的影响。

Method: 使用两种基础架构和单/多轮方法，通过Maximal Update Parametrization调参，比较三种优化器。

Result: Sophia损失最低，Lion训练最快，AdamW下游任务表现最好。

Conclusion: 不同优化器各有优势，选择需权衡速度和性能。

Abstract: Optimizers play a decisive role in reducing pre-training times for LLMs and
achieving better-performing models. In this study, we compare three major
variants: the de-facto standard AdamW, the simpler Lion, developed through an
evolutionary search, and the second-order optimizer Sophia. For better
generalization, we train with two different base architectures and use a
single- and a multiple-epoch approach while keeping the number of tokens
constant. Using the Maximal Update Parametrization and smaller proxy models, we
tune relevant hyperparameters separately for each combination of base
architecture and optimizer. We found that while the results from all three
optimizers were in approximately the same range, Sophia exhibited the lowest
training and validation loss, Lion was fastest in terms of training GPU hours
but AdamW led to the best downstream evaluation results.

</details>


### [119] [Evaluating SAE interpretability without explanations](https://arxiv.org/abs/2507.08473)
*Gonçalo Paulo,Nora Belrose*

Main category: cs.LG

TL;DR: 本文提出了一种无需生成自然语言解释的直接评估稀疏编码器可解释性的方法，并与人类评估结果对比，为改进评估方法提供建议。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器和转码器的可解释性评估缺乏共识，现有方法依赖自然语言解释，难以区分解释生成与潜在特征的实际可解释性。

Method: 采用无需自然语言解释的现有方法，直接评估稀疏编码器的可解释性，并与人类评估结果对比。

Result: 提出了一种更直接、标准化的可解释性评估方法，并与人类评估结果一致。

Conclusion: 该方法为稀疏编码器的可解释性评估提供了更直接的途径，并建议社区改进评估技术。

Abstract: Sparse autoencoders (SAEs) and transcoders have become important tools for
machine learning interpretability. However, measuring how interpretable they
are remains challenging, with weak consensus about which benchmarks to use.
Most evaluation procedures start by producing a single-sentence explanation for
each latent. These explanations are then evaluated based on how well they
enable an LLM to predict the activation of a latent in new contexts. This
method makes it difficult to disentangle the explanation generation and
evaluation process from the actual interpretability of the latents discovered.
In this work, we adapt existing methods to assess the interpretability of
sparse coders, with the advantage that they do not require generating natural
language explanations as an intermediate step. This enables a more direct and
potentially standardized assessment of interpretability. Furthermore, we
compare the scores produced by our interpretability metrics with human
evaluations across similar tasks and varying setups, offering suggestions for
the community on improving the evaluation of these techniques.

</details>


### [120] [SynBridge: Bridging Reaction States via Discrete Flow for Bidirectional Reaction Prediction](https://arxiv.org/abs/2507.08475)
*Haitao Lin,Junjie Wang,Zhifeng Gao,Xiaohong Ji,Rong Zhu,Linfeng Zhang,Guolin Ke,Weinan E*

Main category: cs.LG

TL;DR: SynBridge是一种基于双向流的生成模型，用于多任务反应预测，通过图到图转换和离散流桥实现化学反应的双向建模。


<details>
  <summary>Details</summary>
Motivation: 化学反应的本质是电子的重新分布和重组，通常表现为电子转移或电子对的迁移。这些变化在物理世界中是离散和突变的，需要一种模型来捕捉这种状态转换。

Method: 提出SynBridge，利用图到图转换网络架构和离散流桥，通过原子和键的离散状态实现反应物和产物图之间的双向化学转换。

Result: 在三个基准数据集（USPTO-50K、USPTO-MIT、Pistachio）上进行了广泛实验，证明了方法的有效性，并在正向和逆向合成任务中达到了最先进的性能。

Conclusion: 通过消融研究和噪声调度分析，揭示了离散空间上的结构化扩散对反应预测的益处。

Abstract: The essence of a chemical reaction lies in the redistribution and
reorganization of electrons, which is often manifested through electron
transfer or the migration of electron pairs. These changes are inherently
discrete and abrupt in the physical world, such as alterations in the charge
states of atoms or the formation and breaking of chemical bonds. To model the
transition of states, we propose SynBridge, a bidirectional flow-based
generative model to achieve multi-task reaction prediction. By leveraging a
graph-to-graph transformer network architecture and discrete flow bridges
between any two discrete distributions, SynBridge captures bidirectional
chemical transformations between graphs of reactants and products through the
bonds' and atoms' discrete states. We further demonstrate the effectiveness of
our method through extensive experiments on three benchmark datasets
(USPTO-50K, USPTO-MIT, Pistachio), achieving state-of-the-art performance in
both forward and retrosynthesis tasks. Our ablation studies and noise
scheduling analysis reveal the benefits of structured diffusion over discrete
spaces for reaction prediction.

</details>


### [121] [Efficient Deployment of Vision-Language Models on Mobile Devices: A Case Study on OnePlus 13R](https://arxiv.org/abs/2507.08505)
*Pablo Robin Guerrero,Yueyang Pan,Sanidhya Kashyap*

Main category: cs.LG

TL;DR: 本文调查了移动设备上视觉语言模型（VLM）的部署框架，评估了llama.cpp、MLC-Imp和mllm在OnePlus 13R上运行LLaVA-1.5 7B、MobileVLM-3B和Imp-v1.5 3B的性能，发现CPU过度使用而GPU和NPU利用不足。


<details>
  <summary>Details</summary>
Motivation: 移动设备部署VLM面临计算和能效挑战，需评估现有框架的性能瓶颈。

Method: 在OnePlus 13R上测试三种框架运行三种VLM，测量CPU、GPU、NPU利用率、温度、推理时间、功耗和用户体验。

Result: CPU资源在生成令牌时过度使用，GPU和NPU利用不足或饱和，导致设备响应性下降。

Conclusion: 当前部署框架存在CPU过度使用和GPU/NPU利用不稳定的问题，需优化硬件资源分配。

Abstract: Vision-Language Models (VLMs) offer promising capabilities for mobile
devices, but their deployment faces significant challenges due to computational
limitations and energy inefficiency, especially for real-time applications.
This study provides a comprehensive survey of deployment frameworks for VLMs on
mobile devices, evaluating llama.cpp, MLC-Imp, and mllm in the context of
running LLaVA-1.5 7B, MobileVLM-3B, and Imp-v1.5 3B as representative workloads
on a OnePlus 13R. Each deployment framework was evaluated on the OnePlus 13R
while running VLMs, with measurements covering CPU, GPU, and NPU utilization,
temperature, inference time, power consumption, and user experience.
Benchmarking revealed critical performance bottlenecks across frameworks: CPU
resources were consistently over-utilized during token generation, while GPU
and NPU accelerators were largely unused. When the GPU was used, primarily for
image feature extraction, it was saturated, leading to degraded device
responsiveness. The study contributes framework-level benchmarks, practical
profiling tools, and an in-depth analysis of hardware utilization bottlenecks,
highlighting the consistent overuse of CPUs and the ineffective or unstable use
of GPUs and NPUs in current deployment frameworks.

</details>


### [122] [SFedKD: Sequential Federated Learning with Discrepancy-Aware Multi-Teacher Knowledge Distillation](https://arxiv.org/abs/2507.08508)
*Haotian Xu,Jinrui Zhou,Xichong Zhang,Mingjun Xiao,He Sun,Yin Xu*

Main category: cs.LG

TL;DR: SFedKD通过多教师知识蒸馏解决了SFL中的灾难性遗忘问题，提升了模型训练效果。


<details>
  <summary>Details</summary>
Motivation: 解决Sequential Federated Learning（SFL）在异构环境中严重的灾难性遗忘问题。

Method: 提出SFedKD框架，采用基于分布差异的多教师知识蒸馏，并通过贪心策略选择互补教师模型。

Result: SFedKD有效缓解了灾难性遗忘，并优于现有联邦学习方法。

Conclusion: SFedKD为SFL提供了一种高效的解决方案，显著提升了模型性能。

Abstract: Federated Learning (FL) is a distributed machine learning paradigm which
coordinates multiple clients to collaboratively train a global model via a
central server. Sequential Federated Learning (SFL) is a newly-emerging FL
training framework where the global model is trained in a sequential manner
across clients. Since SFL can provide strong convergence guarantees under data
heterogeneity, it has attracted significant research attention in recent years.
However, experiments show that SFL suffers from severe catastrophic forgetting
in heterogeneous environments, meaning that the model tends to forget knowledge
learned from previous clients. To address this issue, we propose an SFL
framework with discrepancy-aware multi-teacher knowledge distillation, called
SFedKD, which selects multiple models from the previous round to guide the
current round of training. In SFedKD, we extend the single-teacher Decoupled
Knowledge Distillation approach to our multi-teacher setting and assign
distinct weights to teachers' target-class and non-target-class knowledge based
on the class distributional discrepancy between teacher and student data.
Through this fine-grained weighting strategy, SFedKD can enhance model training
efficacy while mitigating catastrophic forgetting. Additionally, to prevent
knowledge dilution, we eliminate redundant teachers for the knowledge
distillation and formalize it as a variant of the maximum coverage problem.
Based on the greedy strategy, we design a complementary-based teacher selection
mechanism to ensure that the selected teachers achieve comprehensive knowledge
space coverage while reducing communication and computational costs. Extensive
experiments show that SFedKD effectively overcomes catastrophic forgetting in
SFL and outperforms state-of-the-art FL methods.

</details>


### [123] [Recursive Reward Aggregation](https://arxiv.org/abs/2507.08537)
*Yuting Tang,Yivan Zhang,Johannes Ackermann,Yu-Jie Zhang,Soichiro Nishimori,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 提出一种无需修改奖励函数的强化学习行为对齐方法，通过选择适当的奖励聚合函数实现目标优化。


<details>
  <summary>Details</summary>
Motivation: 在强化学习中，复杂目标的行为对齐通常需要精心设计奖励函数，这具有挑战性。

Method: 引入MDP的代数视角，通过递归生成和聚合奖励，推广标准折扣和到其他递归聚合形式（如折扣最大值和夏普比率）。

Result: 实验证明该方法能有效优化多样化目标，适用于确定性和随机环境，并与基于值和演员-评论家算法无缝集成。

Conclusion: 该方法展示了灵活性和实际应用的潜力。

Abstract: In reinforcement learning (RL), aligning agent behavior with specific
objectives typically requires careful design of the reward function, which can
be challenging when the desired objectives are complex. In this work, we
propose an alternative approach for flexible behavior alignment that eliminates
the need to modify the reward function by selecting appropriate reward
aggregation functions. By introducing an algebraic perspective on Markov
decision processes (MDPs), we show that the Bellman equations naturally emerge
from the recursive generation and aggregation of rewards, allowing for the
generalization of the standard discounted sum to other recursive aggregations,
such as discounted max and Sharpe ratio. Our approach applies to both
deterministic and stochastic settings and integrates seamlessly with
value-based and actor-critic algorithms. Experimental results demonstrate that
our approach effectively optimizes diverse objectives, highlighting its
versatility and potential for real-world applications.

</details>


### [124] [CircFormerMoE: An End-to-End Deep Learning Framework for Circular RNA Splice Site Detection and Pairing in Plant Genomes](https://arxiv.org/abs/2507.08542)
*Tianyou Jiang*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer和混合专家的深度学习框架CircFormerMoE，直接从植物基因组DNA预测circRNA，解决了现有方法依赖RNA实验数据、计算成本高和效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有circRNA识别方法依赖RNA-seq数据，计算成本高且效率低，尤其在植物中缺乏高效预测工具。

Method: CircFormerMoE框架包含剪接位点检测（SSD）和剪接位点配对（SSP）两个子任务，基于Transformer和混合专家模型。

Result: 在10种植物基因数据上验证了模型有效性，能发现未注释的circRNA，并进行了可解释性分析。

Conclusion: 该框架为植物大规模circRNA发现提供了快速准确的计算方法，为植物功能基因组学和非编码RNA注释研究奠定了基础。

Abstract: Circular RNAs (circRNAs) are important components of the non-coding RNA
regulatory network. Previous circRNA identification primarily relies on
high-throughput RNA sequencing (RNA-seq) data combined with alignment-based
algorithms that detect back-splicing signals. However, these methods face
several limitations: they can't predict circRNAs directly from genomic DNA
sequences and relies heavily on RNA experimental data; they involve high
computational costs due to complex alignment and filtering steps; and they are
inefficient for large-scale or genome-wide circRNA prediction. The challenge is
even greater in plants, where plant circRNA splice sites often lack the
canonical GT-AG motif seen in human mRNA splicing, and no efficient deep
learning model with strong generalization capability currently exists.
Furthermore, the number of currently identified plant circRNAs is likely far
lower than their true abundance. In this paper, we propose a deep learning
framework named CircFormerMoE based on transformers and mixture-of experts for
predicting circRNAs directly from plant genomic DNA. Our framework consists of
two subtasks known as splicing site detection (SSD) and splicing site pairing
(SSP). The model's effectiveness has been validated on gene data of 10 plant
species. Trained on known circRNA instances, it is also capable of discovering
previously unannotated circRNAs. In addition, we performed interpretability
analyses on the trained model to investigate the sequence patterns contributing
to its predictions. Our framework provides a fast and accurate computational
method and tool for large-scale circRNA discovery in plants, laying a
foundation for future research in plant functional genomics and non-coding RNA
annotation.

</details>


### [125] [STRAP: Spatial-Temporal Risk-Attentive Vehicle Trajectory Prediction for Autonomous Driving](https://arxiv.org/abs/2507.08563)
*Xinyi Ning,Zilin Bian,Kaan Ozbay,Semiha Ergan*

Main category: cs.LG

TL;DR: 提出了一种新颖的空间-时间风险感知轨迹预测框架，通过风险势场评估周围车辆行为的不确定性或攻击性，提升自动驾驶系统的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注运动模式和车辆交互，但忽略了周围车辆不确定或攻击性行为带来的潜在风险。

Method: 结合空间-时间编码器和风险感知特征融合解码器，将风险势场嵌入空间-时间特征表示中，并设计风险缩放损失函数。

Result: 在NGSIM和HighD数据集上，预测误差分别减少4.8%和31.2%，尤其在高风险场景表现更优。

Conclusion: 该框架提供了可解释的风险感知预测，有助于提升自动驾驶系统的决策鲁棒性。

Abstract: Accurate vehicle trajectory prediction is essential for ensuring safety and
efficiency in fully autonomous driving systems. While existing methods
primarily focus on modeling observed motion patterns and interactions with
other vehicles, they often neglect the potential risks posed by the uncertain
or aggressive behaviors of surrounding vehicles. In this paper, we propose a
novel spatial-temporal risk-attentive trajectory prediction framework that
incorporates a risk potential field to assess perceived risks arising from
behaviors of nearby vehicles. The framework leverages a spatial-temporal
encoder and a risk-attentive feature fusion decoder to embed the risk potential
field into the extracted spatial-temporal feature representations for
trajectory prediction. A risk-scaled loss function is further designed to
improve the prediction accuracy of high-risk scenarios, such as short relative
spacing. Experiments on the widely used NGSIM and HighD datasets demonstrate
that our method reduces average prediction errors by 4.8% and 31.2%
respectively compared to state-of-the-art approaches, especially in high-risk
scenarios. The proposed framework provides interpretable, risk-aware
predictions, contributing to more robust decision-making for autonomous driving
systems.

</details>


### [126] [AbbIE: Autoregressive Block-Based Iterative Encoder for Efficient Sequence Modeling](https://arxiv.org/abs/2507.08567)
*Preslav Aleksandrov,Meghdad Kurmanji,Fernando Garcia Redondo,David O'Shea,William Shen,Alex Iacob,Lorenzo Sani,Xinchi Qiu,Nicola Cancedda,Nicholas D. Lane*

Main category: cs.LG

TL;DR: AbbIE是一种新型递归编码器，优于标准Transformer，支持动态计算资源扩展，提升零样本学习和语言困惑度。


<details>
  <summary>Details</summary>
Motivation: 通过递归方法扩展Transformer性能，避免依赖参数和标记数量的增加。

Method: 采用递归潜在空间迭代，无需特殊数据集或训练协议，仅需2次训练迭代。

Result: 在零样本学习中提升12%，语言困惑度提升5%，支持动态计算扩展。

Conclusion: AbbIE为Transformer性能扩展提供了新途径。

Abstract: We introduce the Autoregressive Block-Based Iterative Encoder (AbbIE), a
novel recursive generalization of the encoder-only Transformer architecture,
which achieves better perplexity than a standard Transformer and allows for the
dynamic scaling of compute resources at test time. This simple, recursive
approach is a complement to scaling large language model (LLM) performance
through parameter and token counts. AbbIE performs its iterations in latent
space, but unlike latent reasoning models, does not require a specialized
dataset or training protocol. We show that AbbIE upward generalizes (ability to
generalize to arbitrary iteration lengths) at test time by only using 2
iterations during train time, far outperforming alternative iterative methods.
AbbIE's ability to scale its computational expenditure based on the complexity
of the task gives it an up to \textbf{12\%} improvement in zero-shot in-context
learning tasks versus other iterative and standard methods and up to 5\%
improvement in language perplexity. The results from this study open a new
avenue to Transformer performance scaling. We perform all of our evaluations on
model sizes up to 350M parameters.

</details>


### [127] [ADAPT: A Pseudo-labeling Approach to Combat Concept Drift in Malware Detection](https://arxiv.org/abs/2507.08597)
*Md Tanvirul Alam,Aritran Piplai,Nidhi Rastogi*

Main category: cs.LG

TL;DR: 论文提出了一种名为ADAPT的半监督伪标签算法，用于解决恶意软件检测中的概念漂移问题，实验表明其性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在恶意软件分类中因概念漂移导致性能下降，而频繁更新需要高成本的真实标注。半监督学习利用未标注数据的方法在恶意软件检测中尚未充分探索。

Method: 提出ADAPT，一种模型无关的半监督伪标签算法，适用于多种机器学习模型（如神经网络和树基算法）。

Result: 在五个不同的恶意软件检测数据集上实验，ADAPT性能优于基线模型和竞争基准。

Conclusion: ADAPT为恶意软件检测中概念漂移的适应提供了更有效的方法。

Abstract: Machine learning models are commonly used for malware classification;
however, they suffer from performance degradation over time due to concept
drift. Adapting these models to changing data distributions requires frequent
updates, which rely on costly ground truth annotations. While active learning
can reduce the annotation burden, leveraging unlabeled data through
semi-supervised learning remains a relatively underexplored approach in the
context of malware detection. In this research, we introduce \texttt{ADAPT}, a
novel pseudo-labeling semi-supervised algorithm for addressing concept drift.
Our model-agnostic method can be applied to various machine learning models,
including neural networks and tree-based algorithms. We conduct extensive
experiments on five diverse malware detection datasets spanning Android,
Windows, and PDF domains. The results demonstrate that our method consistently
outperforms baseline models and competitive benchmarks. This work paves the way
for more effective adaptation of machine learning models to concept drift in
malware detection.

</details>


### [128] [Remote Sensing Reveals Adoption of Sustainable Rice Farming Practices Across Punjab, India](https://arxiv.org/abs/2507.08605)
*Ando Shah,Rajveer Singh,Akram Zaytar,Girmaw Abebe Tadesse,Caleb Robinson,Negar Tafti,Stephen A. Wood,Rahul Dodhia,Juan M. Lavista Ferres*

Main category: cs.LG

TL;DR: 该研究开发了一种遥感框架，用于监测印度旁遮普邦的可持续水稻灌溉实践（如DSR和AWD），以解决水资源管理挑战。


<details>
  <summary>Details</summary>
Motivation: 水稻种植消耗大量淡水，而可持续灌溉实践的采用数据不足，阻碍了政策制定和资源分配。

Method: 利用Sentinel-1卫星影像和地面真实数据，开发分类系统，区分不同灌溉方式。

Result: 分类系统F1得分为78%，并能在大规模（300万农田）上准确预测DSR采用情况。

Conclusion: 该框架为政策制定者提供了监测可持续水资源管理实践的有效工具。

Abstract: Rice cultivation consumes 24-30% of global freshwater, creating critical
water management challenges in major rice-producing regions. Sustainable
irrigation practices like direct seeded rice (DSR) and alternate wetting and
drying (AWD) can reduce water use by 20-40% while maintaining yields, helping
secure long-term agricultural productivity as water scarcity intensifies - a
key component of the Zero Hunger Sustainable Development Goal. However, limited
data on adoption rates of these practices prevents evidence-based policymaking
and targeted resource allocation. We developed a novel remote sensing framework
to monitor sustainable water management practices at scale in Punjab, India - a
region facing severe groundwater depletion of 41.6 cm/year. To collect
essential ground truth data, we partnered with the Nature Conservancy's
Promoting Regenerative and No-burn Agriculture (PRANA) program, which trained
approximately 1,400 farmers on water-saving techniques while documenting their
field-level practices. Using this data, we created a classification system with
Sentinel-1 satellite imagery that separates water management along sowing and
irrigation dimensions. Our approach achieved a 78% F1-score in distinguishing
DSR from traditional puddled transplanted rice without requiring prior
knowledge of planting dates. We demonstrated scalability by mapping DSR
adoption across approximately 3 million agricultural plots in Punjab, with
district-level predictions showing strong correlation (Pearson=0.77, RBO= 0.77)
with government records. This study provides policymakers with a powerful tool
to track sustainable water management adoption, target interventions, and
measure program impacts at scale.

</details>


### [129] [Emergent Natural Language with Communication Games for Improving Image Captioning Capabilities without Additional Data](https://arxiv.org/abs/2507.08610)
*Parag Dutta,Ambedkar Dukkipati*

Main category: cs.LG

TL;DR: 论文提出了一种名为LoGIC的多智能体强化学习方法，通过‘说话者’和‘倾听者’两个代理的无监督学习，提升了图像描述任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有标注数据集已被大型视觉语言模型（VLMs）充分利用，改进性能面临挑战，因此探索无监督图像描述成为必要。

Method: 采用多智能体强化学习游戏（LoGIC），使用GRPO算法训练‘说话者’和‘倾听者’代理，结合预训练VLMs和LLMs。

Result: 无监督微调后BLEU得分46，比基线高2分；轻量级组件组合在无监督设置下BLEU得分31，优于现有方法10分。

Conclusion: LoGIC方法在无监督图像描述任务中表现优异，为未来研究提供了新方向。

Abstract: Image captioning is an important problem in developing various AI systems,
and these tasks require large volumes of annotated images to train the models.
Since all existing labelled datasets are already used for training the large
Vision Language Models (VLMs), it becomes challenging to improve the
performance of the same. Considering this, it is essential to consider the
unsupervised image captioning performance, which remains relatively
under-explored. To that end, we propose LoGIC (Lewis Communication Game for
Image Captioning), a Multi-agent Reinforcement Learning game. The proposed
method consists of two agents, a 'speaker' and a 'listener', with the objective
of learning a strategy for communicating in natural language. We train agents
in the cooperative common-reward setting using the GRPO algorithm and show that
improvement in image captioning performance emerges as a consequence of the
agents learning to play the game. We show that using pre-trained VLMs as the
'speaker' and Large Language Model (LLM) for language understanding in the
'listener', we achieved a $46$ BLEU score after fine-tuning using LoGIC without
additional labels, a $2$ units advantage in absolute metrics compared to the
$44$ BLEU score of the vanilla VLM. Additionally, we replace the VLM from the
'speaker' with lightweight components: (i) a ViT for image perception and (ii)
a GPT2 language generation, and train them from scratch using LoGIC, obtaining
a $31$ BLEU score in the unsupervised setting, a $10$ points advantage over
existing unsupervised image-captioning methods.

</details>


### [130] [Towards Collaborative Fairness in Federated Learning Under Imbalanced Covariate Shift](https://arxiv.org/abs/2507.08617)
*Tianrun Yu,Jiaqi Wang,Haoyu Wang,Mingquan Lin,Han Liu,Nelson S. Yee,Fenglong Ma*

Main category: cs.LG

TL;DR: FedAKD提出了一种解决联邦学习中协作公平性的方法，通过异步知识蒸馏处理不平衡协变量偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了不平衡协变量偏移这一实际但复杂的异构性问题，FedAKD旨在平衡预测准确性和协作公平性。

Method: FedAKD包括客户端和服务器更新：客户端使用异步知识蒸馏策略，服务器聚合模型并更新全局模型。

Result: 实验表明，FedAKD在FashionMNIST、CIFAR10和真实EHR数据集上显著提高了协作公平性和预测准确性。

Conclusion: FedAKD是一种简单有效的方法，适用于高度异构数据分布下的联邦学习。

Abstract: Collaborative fairness is a crucial challenge in federated learning. However,
existing approaches often overlook a practical yet complex form of
heterogeneity: imbalanced covariate shift. We provide a theoretical analysis of
this setting, which motivates the design of FedAKD (Federated Asynchronous
Knowledge Distillation)- simple yet effective approach that balances accurate
prediction with collaborative fairness. FedAKD consists of client and server
updates. In the client update, we introduce a novel asynchronous knowledge
distillation strategy based on our preliminary analysis, which reveals that
while correctly predicted samples exhibit similar feature distributions across
clients, incorrectly predicted samples show significant variability. This
suggests that imbalanced covariate shift primarily arises from misclassified
samples. Leveraging this insight, our approach first applies traditional
knowledge distillation to update client models while keeping the global model
fixed. Next, we select correctly predicted high-confidence samples and update
the global model using these samples while keeping client models fixed. The
server update simply aggregates all client models. We further provide a
theoretical proof of FedAKD's convergence. Experimental results on public
datasets (FashionMNIST and CIFAR10) and a real-world Electronic Health Records
(EHR) dataset demonstrate that FedAKD significantly improves collaborative
fairness, enhances predictive accuracy, and fosters client participation even
under highly heterogeneous data distributions.

</details>


### [131] [Scaling Attention to Very Long Sequences in Linear Time with Wavelet-Enhanced Random Spectral Attention (WERSA)](https://arxiv.org/abs/2507.08637)
*Vincenzo Dentamaro*

Main category: cs.LG

TL;DR: WERSA是一种线性时间复杂度的注意力机制，显著降低长序列处理的计算成本，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统注意力机制在长序列上计算复杂度高的问题，提升效率。

Method: 结合内容自适应的随机频谱特征、多分辨率Haar小波和可学习参数，选择性关注数据的有信息尺度。

Result: 在多个基准测试中表现最佳，显著减少训练时间和计算资源，同时提升准确率。

Conclusion: WERSA为长上下文模型提供了更高效、可持续的解决方案。

Abstract: Transformer models are computationally costly on long sequences since regular
attention has quadratic $O(n^2)$ time complexity. We introduce Wavelet-Enhanced
Random Spectral Attention (WERSA), a novel mechanism of linear $O(n)$ time
complexity that is pivotal to enable successful long-sequence processing
without the performance trade-off. WERSA merges content-adaptive random
spectral features together with multi-resolution Haar wavelets and learnable
parameters to selectively attend to informative scales of data while preserving
linear efficiency.
  Large-scale comparisons \textbf{on single GPU} and across various benchmarks
(vision, NLP, hierarchical reasoning) and various attention mechanisms (like
Multiheaded Attention, Flash-Attention-2, FNet, Linformer, Performer,
Waveformer), reveal uniform advantages of WERSA. It achieves best accuracy in
all tests. On ArXiv classification, WERSA improves accuracy over vanilla
attention by 1.2\% (86.2\% vs 85.0\%) while cutting training time by 81\% (296s
vs 1554s) and FLOPS by 73.4\% (26.2G vs 98.4G). Significantly, WERSA excels
where vanilla and FlashAttention-2 fail: on ArXiv-128k's extremely lengthy
sequences, it achieves best accuracy (79.1\%) and AUC (0.979) among viable
methods, operating on data that gives Out-Of-Memory errors to quadratic methods
while being \textbf{twice as fast} as Waveformer, its next-best competitor.
  By significantly reducing computational loads without compromising accuracy,
WERSA makes possible more practical, more affordable, long-context models, in
particular on low-resource hardware, for more sustainable and more scalable AI
development.

</details>


### [132] [Forget Me Not: Fighting Local Overfitting with Knowledge Fusion and Distillation](https://arxiv.org/abs/2507.08686)
*Uri Stern,Eli Corn,Daphna Weinshall*

Main category: cs.LG

TL;DR: 论文提出了一种衡量深度学习模型局部过拟合的新方法，并通过两阶段知识融合与蒸馏提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究深度学习模型中局部过拟合现象，解决传统理论预测与实际观察不一致的问题。

Method: 引入局部过拟合评分，提出两阶段方法：先集成检查点，再蒸馏为单一模型。

Result: 在多种数据集和架构上验证了方法的有效性，尤其在标签噪声下表现优异。

Conclusion: 该方法在降低训练和推理复杂度的同时，显著提升了模型性能。

Abstract: Overfitting in deep neural networks occurs less frequently than expected.
This is a puzzling observation, as theory predicts that greater model capacity
should eventually lead to overfitting -- yet this is rarely seen in practice.
But what if overfitting does occur, not globally, but in specific sub-regions
of the data space? In this work, we introduce a novel score that measures the
forgetting rate of deep models on validation data, capturing what we term local
overfitting: a performance degradation confined to certain regions of the input
space. We demonstrate that local overfitting can arise even without
conventional overfitting, and is closely linked to the double descent
phenomenon.
  Building on these insights, we introduce a two-stage approach that leverages
the training history of a single model to recover and retain forgotten
knowledge: first, by aggregating checkpoints into an ensemble, and then by
distilling it into a single model of the original size, thus enhancing
performance without added inference cost.
  Extensive experiments across multiple datasets, modern architectures, and
training regimes validate the effectiveness of our approach. Notably, in the
presence of label noise, our method -- Knowledge Fusion followed by Knowledge
Distillation -- outperforms both the original model and independently trained
ensembles, achieving a rare win-win scenario: reduced training and inference
complexity.

</details>


### [133] [Domain-Informed Operation Excellence of Gas Turbine System with Machine Learning](https://arxiv.org/abs/2507.08697)
*Waqar Muhammad Ashraf,Amir H. Keshavarzzadeh,Abdulelah S. Alshehri,Abdulrahman bin Jumah,Ramit Debnath,Vivek Dua*

Main category: cs.LG

TL;DR: 提出了一种基于马氏距离的优化框架（MAD-OPT），将领域知识融入数据驱动的分析中，以提升热电厂中AI的应用效果。


<details>
  <summary>Details</summary>
Motivation: 热电厂中AI应用率低，主要因为AI算法的黑盒特性及传统数据分析中领域知识不足。

Method: 开发了MAD-OPT框架，结合马氏距离约束，将领域知识引入数据分析，并应用于395 MW燃气轮机系统。

Result: MAD-OPT能估计不同环境条件下的最优工艺参数，结果稳健且与电厂实际数据一致。

Conclusion: 数据驱动的优化需结合领域知识，否则可能产生无效解。MAD-OPT为热电厂安全应用AI提供了新途径。

Abstract: The domain-consistent adoption of artificial intelligence (AI) remains low in
thermal power plants due to the black-box nature of AI algorithms and low
representation of domain knowledge in conventional data-centric analytics. In
this paper, we develop a MAhalanobis Distance-based OPTimization (MAD-OPT)
framework that incorporates the Mahalanobis distance-based constraint to
introduce domain knowledge into data-centric analytics. The developed MAD-OPT
framework is applied to maximize thermal efficiency and minimize turbine heat
rate for a 395 MW capacity gas turbine system. We demonstrate that the MAD-OPT
framework can estimate domain-informed optimal process conditions under
different ambient conditions, and the optimal solutions are found to be robust
as evaluated by Monte Carlo simulations. We also apply the MAD-OPT framework to
estimate optimal process conditions beyond the design power generation limit of
the gas turbine system, and have found comparable results with the actual data
of the power plant. We demonstrate that implementing data-centric optimization
analytics without incorporating domain-informed constraints may provide
ineffective solutions that may not be implementable in the real operation of
the gas turbine system. This research advances the integration of the
data-driven domain knowledge into machine learning-powered analytics that
enhances the domain-informed operation excellence and paves the way for safe AI
adoption in thermal power systems.

</details>


### [134] [SPLASH! Sample-efficient Preference-based inverse reinforcement learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical demonstrations](https://arxiv.org/abs/2507.08707)
*Peter Crowley,Zachary Serlin,Tyler Paine,Makai Mann,Michael Benjamin,Calin Belta*

Main category: cs.LG

TL;DR: SPLASH是一种基于偏好的逆向强化学习方法，专注于从次优演示中学习长时域和对抗性任务，显著优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 现有逆向强化学习方法假设专家演示可用，但实际中往往缺乏，且不适用于长时域或对抗性任务。

Method: 提出SPLASH方法，结合偏好学习和分层演示，适用于次优演示的长时域和对抗性任务。

Result: 在模拟和真实实验中验证了SPLASH的优越性，特别是在奖励学习方面。

Conclusion: SPLASH填补了逆向强化学习在长时域和对抗性任务中的不足，具有实际应用潜力。

Abstract: Inverse Reinforcement Learning (IRL) presents a powerful paradigm for
learning complex robotic tasks from human demonstrations. However, most
approaches make the assumption that expert demonstrations are available, which
is often not the case. Those that allow for suboptimality in the demonstrations
are not designed for long-horizon goals or adversarial tasks. Many desirable
robot capabilities fall into one or both of these categories, thus highlighting
a critical shortcoming in the ability of IRL to produce field-ready robotic
agents. We introduce Sample-efficient Preference-based inverse reinforcement
learning for Long-horizon Adversarial tasks from Suboptimal Hierarchical
demonstrations (SPLASH), which advances the state-of-the-art in learning from
suboptimal demonstrations to long-horizon and adversarial settings. We
empirically validate SPLASH on a maritime capture-the-flag task in simulation,
and demonstrate real-world applicability with sim-to-real translation
experiments on autonomous unmanned surface vehicles. We show that our proposed
methods allow SPLASH to significantly outperform the state-of-the-art in reward
learning from suboptimal demonstrations.

</details>


### [135] [On the Effect of Regularization in Policy Mirror Descent](https://arxiv.org/abs/2507.08718)
*Jan Felix Kleuker,Aske Plaat,Thomas Moerland*

Main category: cs.LG

TL;DR: PMD框架通过结合两种正则化技术（距离项和MDP正则化器）在RL中实现稳定和鲁棒的策略更新，实证研究表明其组合对性能至关重要。


<details>
  <summary>Details</summary>
Motivation: 研究PMD框架中两种正则化技术的相互作用，填补理论丰富但实证不足的空白。

Method: 在大规模实验中运行超过50万次训练种子，分析两种正则化技术的组合效果。

Result: 两种正则化技术可以部分替代，但精确组合对鲁棒性能至关重要。

Conclusion: 研究为开发更鲁棒的RL算法提供了方向，尤其是针对超参数敏感性。

Abstract: Policy Mirror Descent (PMD) has emerged as a unifying framework in
reinforcement learning (RL) by linking policy gradient methods with a
first-order optimization method known as mirror descent. At its core, PMD
incorporates two key regularization components: (i) a distance term that
enforces a trust region for stable policy updates and (ii) an MDP regularizer
that augments the reward function to promote structure and robustness. While
PMD has been extensively studied in theory, empirical investigations remain
scarce. This work provides a large-scale empirical analysis of the interplay
between these two regularization techniques, running over 500k training seeds
on small RL environments. Our results demonstrate that, although the two
regularizers can partially substitute each other, their precise combination is
critical for achieving robust performance. These findings highlight the
potential for advancing research on more robust algorithms in RL, particularly
with respect to hyperparameter sensitivity.

</details>


### [136] [Monitoring Risks in Test-Time Adaptation](https://arxiv.org/abs/2507.08721)
*Mona Schirmer,Metod Jazbec,Christian A. Naesseth,Eric Nalisnick*

Main category: cs.LG

TL;DR: 论文提出了一种结合测试时适应（TTA）和风险监控框架的方法，以检测模型性能下降的临界点。


<details>
  <summary>Details</summary>
Motivation: 测试时数据分布偏移是常见问题，TTA虽能暂时适应，但模型最终可能失效。需要一种方法来监控性能并预警。

Method: 扩展了基于顺序测试和置信序列的监控工具，适用于无标签测试数据且模型动态更新的场景。

Result: 在多种数据集、分布偏移类型和TTA方法中验证了框架的有效性。

Conclusion: 提出的TTA监控框架为模型性能退化提供了统计严谨的监控手段。

Abstract: Encountering shifted data at test time is a ubiquitous challenge when
deploying predictive models. Test-time adaptation (TTA) methods address this
issue by continuously adapting a deployed model using only unlabeled test data.
While TTA can extend the model's lifespan, it is only a temporary solution.
Eventually the model might degrade to the point that it must be taken offline
and retrained. To detect such points of ultimate failure, we propose pairing
TTA with risk monitoring frameworks that track predictive performance and raise
alerts when predefined performance criteria are violated. Specifically, we
extend existing monitoring tools based on sequential testing with confidence
sequences to accommodate scenarios in which the model is updated at test time
and no test labels are available to estimate the performance metrics of
interest. Our extensions unlock the application of rigorous statistical risk
monitoring to TTA, and we demonstrate the effectiveness of our proposed TTA
monitoring framework across a representative set of datasets, distribution
shift types, and TTA methods.

</details>


### [137] [Catastrophic Forgetting Mitigation Through Plateau Phase Activity Profiling](https://arxiv.org/abs/2507.08736)
*Idan Mashiach,Oren Glickman,Tom Tirer*

Main category: cs.LG

TL;DR: 论文提出一种新方法，通过跟踪参数在训练末期的活动来缓解深度神经网络中的灾难性遗忘问题。


<details>
  <summary>Details</summary>
Motivation: 灾难性遗忘是深度神经网络学习新任务时性能下降的问题，现有正则化方法试图约束重要参数以保留旧知识。

Method: 提出在训练末期的高原阶段跟踪参数活动，认为高活性的参数揭示了损失景观中平坦的方向，适合适应新任务同时保留旧知识。

Result: 实验表明该方法在平衡灾难性遗忘缓解和新任务性能方面表现优越。

Conclusion: 跟踪训练末期高原阶段的参数活动是一种有效的缓解灾难性遗忘的方法。

Abstract: Catastrophic forgetting in deep neural networks occurs when learning new
tasks degrades performance on previously learned tasks due to knowledge
overwriting. Among the approaches to mitigate this issue, regularization
techniques aim to identify and constrain "important" parameters to preserve
previous knowledge. In the highly nonconvex optimization landscape of deep
learning, we propose a novel perspective: tracking parameters during the final
training plateau is more effective than monitoring them throughout the entire
training process. We argue that parameters that exhibit higher activity
(movement and variability) during this plateau reveal directions in the loss
landscape that are relatively flat, making them suitable for adaptation to new
tasks while preserving knowledge from previous ones. Our comprehensive
experiments demonstrate that this approach achieves superior performance in
balancing catastrophic forgetting mitigation with strong performance on newly
learned tasks.

</details>


### [138] [Adaptive Nonlinear Vector Autoregression: Robust Forecasting for Noisy Chaotic Time Series](https://arxiv.org/abs/2507.08738)
*Azimov Sherkhon,Susana Lopez-Moreno,Eric Dolores-Cuenca,Sieun Lee,Sangil Kim*

Main category: cs.LG

TL;DR: 提出了一种结合延迟嵌入线性输入和可学习多层感知机（MLP）的自适应NVAR模型，优于标准NVAR，在噪声条件下表现更稳健。


<details>
  <summary>Details</summary>
Motivation: 标准NVAR和RC在混沌系统预测中表现良好，但依赖固定非线性且在高维场景下扩展性差。

Method: 结合延迟嵌入线性输入和可学习MLP，通过梯度优化联合训练，避免网格搜索。

Result: 在混沌系统中，自适应模型预测精度更高，噪声条件下表现更稳健。

Conclusion: 自适应NVAR模型在预测精度和扩展性上优于标准方法。

Abstract: Nonlinear vector autoregression (NVAR) and reservoir computing (RC) have
shown promise in forecasting chaotic dynamical systems, such as the Lorenz-63
model and El Nino-Southern Oscillation. However, their reliance on fixed
nonlinearities - polynomial expansions in NVAR or random feature maps in RC -
limits their adaptability to high noise or real-world data. These methods also
scale poorly in high-dimensional settings due to costly matrix inversion during
readout computation. We propose an adaptive NVAR model that combines
delay-embedded linear inputs with features generated by a shallow, learnable
multi-layer perceptron (MLP). The MLP and linear readout are jointly trained
using gradient-based optimization, enabling the model to learn data-driven
nonlinearities while preserving a simple readout structure. Unlike standard
NVAR, our approach avoids the need for an exhaustive and sensitive grid search
over ridge and delay parameters. Instead, tuning is restricted to neural
network hyperparameters, improving scalability. Initial experiments on chaotic
systems tested under noise-free and synthetically noisy conditions showed that
the adaptive model outperformed the standard NVAR in predictive accuracy and
showed robust forecasting under noisy conditions with a lower observation
frequency.

</details>


### [139] [Partitioned Hybrid Quantum Fourier Neural Operators for Scientific Quantum Machine Learning](https://arxiv.org/abs/2507.08746)
*Paolo Marcandelli,Yuanchun He,Stefano Mariani,Martina Siena,Stefano Markidis*

Main category: cs.LG

TL;DR: PHQFNO是一种量子-经典混合的傅里叶神经算子，通过分区计算实现资源优化，在科学机器学习中表现优于经典方法。


<details>
  <summary>Details</summary>
Motivation: 扩展量子傅里叶神经算子（QFNO）的能力，通过量子-经典混合计算提高效率和精度，适用于高维问题和分布式执行。

Method: 采用分区计算框架，结合经典和量子资源，使用单热编码将数据转换为量子态，并通过变分优化量子电路参数。

Result: 在Burgers方程和Navier-Stokes方程中，PHQFNO达到经典FNO的精度，并在不可压缩Navier-Stokes中表现更优，且对输入噪声具有更好的稳定性。

Conclusion: PHQFNO展示了量子-经典混合方法在科学机器学习中的潜力，特别是在高维问题和噪声环境下的优越性。

Abstract: We introduce the Partitioned Hybrid Quantum Fourier Neural Operator (PHQFNO),
a generalization of the Quantum Fourier Neural Operator (QFNO) for scientific
machine learning. PHQFNO partitions the Fourier operator computation across
classical and quantum resources, enabling tunable quantum-classical
hybridization and distributed execution across quantum and classical devices.
The method extends QFNOs to higher dimensions and incorporates a
message-passing framework to distribute data across different partitions. Input
data are encoded into quantum states using unary encoding, and quantum circuit
parameters are optimized using a variational scheme. We implement PHQFNO using
PennyLane with PyTorch integration and evaluate it on Burgers' equation,
incompressible and compressible Navier-Stokes equations. We show that PHQFNO
recovers classical FNO accuracy. On incompressible Navier-Stokes, PHQFNO
achieves higher accuracy than its classical counterparts. Finally, we perform a
sensitivity analysis under input noise, confirming improved stability of PHQFNO
over classical baselines.

</details>


### [140] [Modeling Partially Observed Nonlinear Dynamical Systems and Efficient Data Assimilation via Discrete-Time Conditional Gaussian Koopman Network](https://arxiv.org/abs/2507.08749)
*Chuanqi Chen,Zhongrui Wang,Nan Chen,Jin-Long Wu*

Main category: cs.LG

TL;DR: 开发了一种离散时间条件高斯Koopman网络（CGKN），用于高效状态预测和数据同化（DA），适用于高维复杂动力系统，如非线性偏微分方程（PDEs）系统。


<details>
  <summary>Details</summary>
Motivation: 针对工程和地球科学中常见的非线性部分观测系统，探索Koopman嵌入以发现未观测系统状态的潜在表示，使潜在状态的动态条件线性化。

Method: 利用Koopman嵌入构建条件高斯系统，通过解析公式高效评估潜在状态的后验分布，将DA性能纳入系统学习过程。

Result: 在多个非线性PDEs问题（如Burgers方程、Kuramoto-Sivashinsky方程和2-D Navier-Stokes方程）中，CGKN框架在状态预测和数据同化方面表现优异。

Conclusion: CGKN框架展示了科学机器学习（SciML）与数据同化的统一，并可作为其他外环应用（如设计优化和最优控制）的范例。

Abstract: A discrete-time conditional Gaussian Koopman network (CGKN) is developed in
this work to learn surrogate models that can perform efficient state forecast
and data assimilation (DA) for high-dimensional complex dynamical systems,
e.g., systems governed by nonlinear partial differential equations (PDEs).
Focusing on nonlinear partially observed systems that are common in many
engineering and earth science applications, this work exploits Koopman
embedding to discover a proper latent representation of the unobserved system
states, such that the dynamics of the latent states are conditional linear,
i.e., linear with the given observed system states. The modeled system of the
observed and latent states then becomes a conditional Gaussian system, for
which the posterior distribution of the latent states is Gaussian and can be
efficiently evaluated via analytical formulae. The analytical formulae of DA
facilitate the incorporation of DA performance into the learning process of the
modeled system, which leads to a framework that unifies scientific machine
learning (SciML) and data assimilation. The performance of discrete-time CGKN
is demonstrated on several canonical problems governed by nonlinear PDEs with
intermittency and turbulent features, including the viscous Burgers' equation,
the Kuramoto-Sivashinsky equation, and the 2-D Navier-Stokes equations, with
which we show that the discrete-time CGKN framework achieves comparable
performance as the state-of-the-art SciML methods in state forecast and
provides efficient and accurate DA results. The discrete-time CGKN framework
also serves as an example to illustrate unifying the development of SciML
models and their other outer-loop applications such as design optimization,
inverse problems, and optimal control.

</details>


### [141] [ML-Based Automata Simplification for Symbolic Accelerators](https://arxiv.org/abs/2507.08751)
*Tiffany Yu,Rye Stahle-Smith,Darssan Eswaramoorthi,Rasha Karakchi*

Main category: cs.LG

TL;DR: AutoSlim是一种基于机器学习的图简化框架，用于降低符号加速器的复杂性，通过随机森林分类修剪低影响力转换，显著减少自动机图密度，同时保持语义正确性。


<details>
  <summary>Details</summary>
Motivation: 符号加速器在基因组学、NLP和网络安全等领域广泛应用，但面临内存使用和路由复杂性的可扩展性问题。

Method: AutoSlim利用随机森林分类，基于边缘分数和结构特征修剪低影响力转换，支持加权转换的自动化简化。

Result: 在NAPOLY+上评估，AutoSlim实现了FPGA LUT减少40%和转换修剪超过30%，并能扩展到比现有基准大一个数量级的图。

Conclusion: AutoSlim通过优化硬件互联和资源使用，显著提升了符号加速器的可扩展性和效率。

Abstract: Symbolic accelerators are increasingly used for symbolic data processing in
domains such as genomics, NLP, and cybersecurity. However, these accelerators
face scalability issues due to excessive memory use and routing complexity,
especially when targeting a large set. We present AutoSlim, a machine
learning-based graph simplification framework designed to reduce the complexity
of symbolic accelerators built on Non-deterministic Finite Automata (NFA)
deployed on FPGA-based overlays such as NAPOLY+. AutoSlim uses Random Forest
classification to prune low-impact transitions based on edge scores and
structural features, significantly reducing automata graph density while
preserving semantic correctness. Unlike prior tools, AutoSlim targets automated
score-aware simplification with weighted transitions, enabling efficient
ranking-based sequence analysis. We evaluated data sets (1K to 64K nodes) in
NAPOLY+ and conducted performance measurements including latency, throughput,
and resource usage. AutoSlim achieves up to 40 percent reduction in FPGA LUTs
and over 30 percent pruning in transitions, while scaling to graphs an order of
magnitude larger than existing benchmarks. Our results also demonstrate how
hardware interconnection (fanout) heavily influences hardware cost and that
AutoSlim's pruning mitigates resource blowup.

</details>


### [142] [Penalizing Infeasible Actions and Reward Scaling in Reinforcement Learning with Offline Data](https://arxiv.org/abs/2507.08761)
*Jeonghye Kim,Yongjae Shin,Whiyoung Jung,Sunghoon Hong,Deunsol Yoon,Youngchul Sung,Kanghoon Lee,Woohyung Lim*

Main category: cs.LG

TL;DR: 论文提出PARS算法，通过奖励缩放与层归一化（RS-LN）及不可行动作惩罚（PA）解决离线数据强化学习中的Q值外推问题，在D4RL基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 离线数据强化学习中Q值外推误差问题显著，尤其是线性外推时。

Method: 结合RS-LN和PA，逐步降低数据范围外的Q值，开发PARS算法。

Result: 在D4RL基准测试中，PARS在离线训练和在线微调中均优于现有算法，尤其在AntMaze Ultra任务中表现突出。

Conclusion: PARS算法有效解决了Q值外推问题，提升了离线强化学习的性能。

Abstract: Reinforcement learning with offline data suffers from Q-value extrapolation
errors. To address this issue, we first demonstrate that linear extrapolation
of the Q-function beyond the data range is particularly problematic. To
mitigate this, we propose guiding the gradual decrease of Q-values outside the
data range, which is achieved through reward scaling with layer normalization
(RS-LN) and a penalization mechanism for infeasible actions (PA). By combining
RS-LN and PA, we develop a new algorithm called PARS. We evaluate PARS across a
range of tasks, demonstrating superior performance compared to state-of-the-art
algorithms in both offline training and online fine-tuning on the D4RL
benchmark, with notable success in the challenging AntMaze Ultra task.

</details>


### [143] [BlockFFN: Towards End-Side Acceleration-Friendly Mixture-of-Experts with Chunk-Level Activation Sparsity](https://arxiv.org/abs/2507.08771)
*Chenyang Song,Weilin Zhao,Xu Han,Chaojun Xiao,Yingfa Chen,Yuxuan Li,Zhiyuan Liu,Maosong Sun*

Main category: cs.LG

TL;DR: 论文提出了一种名为BlockFFN的新型MoE架构，通过改进路由机制和设计CLS感知训练目标，提高了模型的稀疏性和加速性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统MoE架构中路由机制不可微分和灵活性不足的问题，以及稀疏激活架构在低资源条件下的加速挑战。

Method: 使用结合ReLU激活和RMSNorm的路由器，设计CLS感知训练目标，并实现高效加速内核。

Result: BlockFFN在实验中表现优于其他MoE基线，实现了80%的TLS和70%的8-token CLS，加速内核在终端设备上实现了3.67倍的速度提升。

Conclusion: BlockFFN通过改进路由和稀疏性设计，显著提升了MoE模型的性能和加速效率，适用于低资源环境。

Abstract: To alleviate the computational burden of large language models (LLMs),
architectures with activation sparsity, represented by mixture-of-experts
(MoE), have attracted increasing attention. However, the non-differentiable and
inflexible routing of vanilla MoE hurts model performance. Moreover, while each
token activates only a few parameters, these sparsely-activated architectures
exhibit low chunk-level sparsity, indicating that the union of multiple
consecutive tokens activates a large ratio of parameters. Such a sparsity
pattern is unfriendly for acceleration under low-resource conditions (e.g.,
end-side devices) and incompatible with mainstream acceleration techniques
(e.g., speculative decoding). To address these challenges, we introduce a novel
MoE architecture, BlockFFN, as well as its efficient training and deployment
techniques. Specifically, we use a router integrating ReLU activation and
RMSNorm for differentiable and flexible routing. Next, to promote both
token-level sparsity (TLS) and chunk-level sparsity (CLS), CLS-aware training
objectives are designed, making BlockFFN more acceleration-friendly. Finally,
we implement efficient acceleration kernels, combining activation sparsity and
speculative decoding for the first time. The experimental results demonstrate
the superior performance of BlockFFN over other MoE baselines, achieving over
80% TLS and 70% 8-token CLS. Our kernels achieve up to 3.67$\times$ speedup on
real end-side devices than dense models. All codes and checkpoints are
available publicly (https://github.com/thunlp/BlockFFN).

</details>


### [144] [Greedy Low-Rank Gradient Compression for Distributed Learning with Convergence Guarantees](https://arxiv.org/abs/2507.08784)
*Chuyan Chen,Yutong He,Pengrui Li,Weichen Jia,Kun Yuan*

Main category: cs.LG

TL;DR: GreedyLore是一种贪婪低秩梯度压缩算法，用于分布式学习，具有严格的收敛保证，通过误差反馈和半懒惰子空间更新实现线性加速收敛。


<details>
  <summary>Details</summary>
Motivation: 分布式优化在大规模信号处理和机器学习中至关重要，但通信开销是主要瓶颈。现有低秩梯度压缩方法在随机化和贪婪策略之间存在性能与收敛性之间的权衡。

Method: 提出GreedyLore算法，结合误差反馈和半懒惰子空间更新，确保压缩操作在所有迭代中保持收缩性。

Result: GreedyLore在MSGD和Adam等标准优化器下实现了$\mathcal{O}(\sigma/\sqrt{NT} + 1/T)$的收敛速率，首次为低秩梯度压缩提供了线性加速收敛保证。

Conclusion: GreedyLore填补了贪婪压缩方法缺乏收敛保证的空白，并通过实验验证了其理论优势。

Abstract: Distributed optimization is pivotal for large-scale signal processing and
machine learning, yet communication overhead remains a major bottleneck.
Low-rank gradient compression, in which the transmitted gradients are
approximated by low-rank matrices to reduce communication, offers a promising
remedy. Existing methods typically adopt either randomized or greedy
compression strategies: randomized approaches project gradients onto randomly
chosen subspaces, introducing high variance and degrading empirical
performance; greedy methods select the most informative subspaces, achieving
strong empirical results but lacking convergence guarantees. To address this
gap, we propose GreedyLore--the first Greedy Low-Rank gradient compression
algorithm for distributed learning with rigorous convergence guarantees.
GreedyLore incorporates error feedback to correct the bias introduced by greedy
compression and introduces a semi-lazy subspace update that ensures the
compression operator remains contractive throughout all iterations. With these
techniques, we prove that GreedyLore achieves a convergence rate of
$\mathcal{O}(\sigma/\sqrt{NT} + 1/T)$ under standard optimizers such as MSGD
and Adam--marking the first linear speedup convergence rate for low-rank
gradient compression. Extensive experiments are conducted to validate our
theoretical findings.

</details>


### [145] [Optimistic Exploration for Risk-Averse Constrained Reinforcement Learning](https://arxiv.org/abs/2507.08793)
*James McCarthy,Radu Marinescu,Elizabeth Daly,Ivana Dusparic*

Main category: cs.LG

TL;DR: 论文提出了一种乐观风险规避的强化学习方法（ORAC），通过平衡奖励和成本的不确定性来优化策略，避免保守探索导致的次优解。


<details>
  <summary>Details</summary>
Motivation: 传统风险规避强化学习（RaCRL）容易因保守探索而收敛到次优策略，无法有效平衡奖励与安全约束。

Method: ORAC方法通过最大化奖励值函数的上界和最小化成本值函数的下界，动态调整成本权重，鼓励探索不确定区域。

Result: 实验表明，ORAC在Safety-Gymnasium和CityLearn等任务中显著改善了奖励与成本的权衡，避免了次优策略。

Conclusion: ORAC通过乐观探索机制有效解决了RaCRL的保守性问题，提升了策略性能。

Abstract: Risk-averse Constrained Reinforcement Learning (RaCRL) aims to learn policies
that minimise the likelihood of rare and catastrophic constraint violations
caused by an environment's inherent randomness. In general, risk-aversion leads
to conservative exploration of the environment which typically results in
converging to sub-optimal policies that fail to adequately maximise reward or,
in some cases, fail to achieve the goal. In this paper, we propose an
exploration-based approach for RaCRL called Optimistic Risk-averse Actor Critic
(ORAC), which constructs an exploratory policy by maximising a local upper
confidence bound of the state-action reward value function whilst minimising a
local lower confidence bound of the risk-averse state-action cost value
function. Specifically, at each step, the weighting assigned to the cost value
is increased or decreased if it exceeds or falls below the safety constraint
value. This way the policy is encouraged to explore uncertain regions of the
environment to discover high reward states whilst still satisfying the safety
constraints. Our experimental results demonstrate that the ORAC approach
prevents convergence to sub-optimal policies and improves significantly the
reward-cost trade-off in various continuous control tasks such as
Safety-Gymnasium and a complex building energy management environment
CityLearn.

</details>


### [146] [One Token to Fool LLM-as-a-Judge](https://arxiv.org/abs/2507.08794)
*Yulai Zhao,Haolin Liu,Dian Yu,S. Y. Kung,Haitao Mi,Dong Yu*

Main category: cs.LG

TL;DR: 生成奖励模型（LLMs-as-judges）在复杂任务中广泛使用，但易受表面操作影响，导致错误奖励。作者提出数据增强策略改进模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究生成奖励模型在评估答案质量时的脆弱性，尤其是对非单词符号和推理开头的敏感性。

Method: 通过实验展示脆弱性，并提出数据增强策略训练更鲁棒的生成奖励模型。

Result: 发现生成奖励模型普遍存在脆弱性，改进后的模型显著提升了鲁棒性。

Conclusion: 强调需要更可靠的LLM评估方法，并发布了改进的奖励模型和训练数据。

Abstract: Generative reward models (also known as LLMs-as-judges), which use large
language models (LLMs) to evaluate answer quality, are increasingly adopted in
reinforcement learning with verifiable rewards (RLVR). They are often preferred
over rigid rule-based metrics, especially for complex reasoning tasks involving
free-form outputs. In this paradigm, an LLM is typically prompted to compare a
candidate answer against a ground-truth reference and assign a binary reward
indicating correctness. Despite the seeming simplicity of this comparison task,
we find that generative reward models exhibit surprising vulnerabilities to
superficial manipulations: non-word symbols (e.g., ":" or ".") or reasoning
openers like "Thought process:" and "Let's solve this problem step by step."
can often lead to false positive rewards. We demonstrate that this weakness is
widespread across LLMs, datasets, and prompt formats, posing a serious threat
for core algorithmic paradigms that rely on generative reward models, such as
rejection sampling, preference optimization, and RLVR. To mitigate this issue,
we introduce a simple yet effective data augmentation strategy and train a new
generative reward model with substantially improved robustness. Our findings
highlight the urgent need for more reliable LLM-based evaluation methods. We
release our robust, general-domain reward model and its synthetic training data
at https://huggingface.co/sarosavo/Master-RM and
https://huggingface.co/datasets/sarosavo/Master-RM.

</details>


### [147] [The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?](https://arxiv.org/abs/2507.08802)
*Denis Sutter,Julian Minder,Thomas Hofmann,Tiago Pimentel*

Main category: cs.LG

TL;DR: 论文探讨了因果抽象的概念，指出无限制的映射会导致其失去意义，并提出了非线性表示的困境。


<details>
  <summary>Details</summary>
Motivation: 研究旨在批判性地分析因果抽象的概念，特别是无限制对齐映射的影响。

Method: 通过理论和实验证明，使用任意强大的对齐映射可以将任何神经网络映射到任何算法。

Result: 研究发现无限制的因果抽象是空洞的，并提出了非线性表示的困境。

Conclusion: 因果抽象不足以支持机制可解释性，未来需研究信息编码假设与因果抽象的关系。

Abstract: The concept of causal abstraction got recently popularised to demystify the
opaque decision-making processes of machine learning models; in short, a neural
network can be abstracted as a higher-level algorithm if there exists a
function which allows us to map between them. Notably, most interpretability
papers implement these maps as linear functions, motivated by the linear
representation hypothesis: the idea that features are encoded linearly in a
model's representations. However, this linearity constraint is not required by
the definition of causal abstraction. In this work, we critically examine the
concept of causal abstraction by considering arbitrarily powerful alignment
maps. In particular, we prove that under reasonable assumptions, any neural
network can be mapped to any algorithm, rendering this unrestricted notion of
causal abstraction trivial and uninformative. We complement these theoretical
findings with empirical evidence, demonstrating that it is possible to
perfectly map models to algorithms even when these models are incapable of
solving the actual task; e.g., on an experiment using randomly initialised
language models, our alignment maps reach 100% interchange-intervention
accuracy on the indirect object identification task. This raises the non-linear
representation dilemma: if we lift the linearity constraint imposed to
alignment maps in causal abstraction analyses, we are left with no principled
way to balance the inherent trade-off between these maps' complexity and
accuracy. Together, these results suggest an answer to our title's question:
causal abstraction is not enough for mechanistic interpretability, as it
becomes vacuous without assumptions about how models encode information.
Studying the connection between this information-encoding assumption and causal
abstraction should lead to exciting future work.

</details>


<div id='physics.flu-dyn'></div>

# physics.flu-dyn [[Back]](#toc)

### [148] [Predicting Flow Dynamics using Diffusion Models](https://arxiv.org/abs/2507.08106)
*Yannick Gachnang,Vismay Churiwala*

Main category: physics.flu-dyn

TL;DR: 本文旨在复现并扩展DiffFluid论文的结果，验证其方法在其他流体模拟类型（如Lattice Boltzmann方法）中的适用性。


<details>
  <summary>Details</summary>
Motivation: 验证DiffFluid模型的复现性，并探索其作为通用流体动力学求解器的潜力。

Method: 使用去噪扩散概率模型（DDPM）框架处理Navier-Stokes和Darcy流方程，并测试其在Lattice Boltzmann方法中的适用性。

Result: 结果表明扩散模型在复杂流体动力学问题中具有潜力，但也面临挑战。

Conclusion: 研究强调了优化计算效率和扩展模型应用范围的未来研究方向。

Abstract: In this work, we aimed to replicate and extend the results presented in the
DiffFluid paper[1]. The DiffFluid model showed that diffusion models combined
with Transformers are capable of predicting fluid dynamics. It uses a denoising
diffusion probabilistic model (DDPM) framework to tackle Navier-Stokes and
Darcy flow equations. Our goal was to validate the reproducibility of the
methods in the DiffFluid paper while testing its viability for other simulation
types, particularly the Lattice Boltzmann method. Despite our computational
limitations and time constraints, this work provides evidence of the
flexibility and potential of the model as a general-purpose solver for fluid
dynamics. Our results show both the potential and challenges of applying
diffusion models to complex fluid dynamics problems. This work highlights the
opportunities for future research in optimizing the computational efficiency
and scaling such models in broader domains.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [149] [Generating Proto-Personas through Prompt Engineering: A Case Study on Efficiency, Effectiveness and Empathy](https://arxiv.org/abs/2507.08594)
*Fernando Ayach,Vitor Lameirão,Raul Leão,Jerfferson Felizardo,Rafael Sobrinho,Vanessa Borges,Patrícia Matsubara,Awdren Fontão*

Main category: cs.SE

TL;DR: 论文提出了一种基于提示工程的生成式AI方法，用于快速生成原型人物角色，提高了效率和质量，但存在泛化和领域特异性问题。


<details>
  <summary>Details</summary>
Motivation: 手动创建原型人物角色耗时、认知负担重且易受偏见影响，因此探索生成式AI的替代方案。

Method: 采用提示工程和生成式AI生成原型人物角色，并通过案例研究（19名参与者）进行定性和定量评估。

Result: 方法显著减少了时间和精力，提高了人物角色的质量和可重用性，但泛化和情感共鸣存在差异。

Conclusion: 生成式AI可有效整合到产品发现实践中，但需解决泛化和情感共鸣的挑战。

Abstract: Proto-personas are commonly used during early-stage Product Discovery, such
as Lean Inception, to guide product definition and stakeholder alignment.
However, the manual creation of proto-personas is often time-consuming,
cognitively demanding, and prone to bias. In this paper, we propose and
empirically investigate a prompt engineering-based approach to generate
proto-personas with the support of Generative AI (GenAI). Our goal is to
evaluate the approach in terms of efficiency, effectiveness, user acceptance,
and the empathy elicited by the generated personas. We conducted a case study
with 19 participants embedded in a real Lean Inception, employing a qualitative
and quantitative methods design. The results reveal the approach's efficiency
by reducing time and effort and improving the quality and reusability of
personas in later discovery phases, such as Minimum Viable Product (MVP)
scoping and feature refinement. While acceptance was generally high, especially
regarding perceived usefulness and ease of use, participants noted limitations
related to generalization and domain specificity. Furthermore, although
cognitive empathy was strongly supported, affective and behavioral empathy
varied significantly across participants. These results contribute novel
empirical evidence on how GenAI can be effectively integrated into software
Product Discovery practices, while also identifying key challenges to be
addressed in future iterations of such hybrid design processes.

</details>


### [150] [Dually Hierarchical Drift Adaptation for Online Configuration Performance Learning](https://arxiv.org/abs/2507.08730)
*Zezhen Xiang,Jingzhi Gong,Tao Chen*

Main category: cs.SE

TL;DR: DHDA是一个在线配置性能学习框架，旨在通过双重层次适应机制处理动态环境中的全局和局部概念漂移，显著提升准确性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现代可配置软件系统在动态环境中运行时，工作负载变化、硬件变更和系统更新会引入全局和局部概念漂移，现有离线学习和迁移学习方法难以实时适应这些变化。

Method: DHDA采用双重层次适应：上层通过重新划分数据并局部重训练处理全局漂移；下层通过局部模型异步检测和适应局部漂移，结合增量更新和定期完全重训练以平衡效率。

Result: 在八个软件系统中评估，DHDA显著优于现有方法，准确性提升高达2倍，同时保持合理开销。

Conclusion: DHDA能有效适应动态环境中的概念漂移，提升配置性能学习的准确性和适应性。

Abstract: Modern configurable software systems need to learn models that correlate
configuration and performance. However, when the system operates in dynamic
environments, the workload variations, hardware changes, and system updates
will inevitably introduce concept drifts at different levels - global drifts,
which reshape the performance landscape of the entire configuration space; and
local drifts, which only affect certain sub-regions of that space. As such,
existing offline and transfer learning approaches can struggle to adapt to
these implicit and unpredictable changes in real-time, rendering configuration
performance learning challenging. To address this, we propose DHDA, an online
configuration performance learning framework designed to capture and adapt to
these drifts at different levels. The key idea is that DHDA adapts to both the
local and global drifts using dually hierarchical adaptation: at the upper
level, we redivide the data into different divisions, within each of which the
local model is retrained, to handle global drifts only when necessary. At the
lower level, the local models of the divisions can detect local drifts and
adapt themselves asynchronously. To balance responsiveness and efficiency, DHDA
combines incremental updates with periodic full retraining to minimize
redundant computation when no drifts are detected. Through evaluating eight
software systems and against state-of-the-art approaches, we show that DHDA
achieves considerably better accuracy and can effectively adapt to drifts with
up to 2x improvements, while incurring reasonable overhead and is able to
improve different local models in handling concept drift.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [151] [KP-A: A Unified Network Knowledge Plane for Catalyzing Agentic Network Intelligence](https://arxiv.org/abs/2507.08164)
*Yun Tang,Mengbang Zou,Zeinab Nezami,Syed Ali Raza Zaidi,Weisi Guo*

Main category: cs.NI

TL;DR: KP-A是一个统一的网络知识平面，旨在简化6G网络中智能代理的开发与维护，通过解耦知识获取与管理，提升互操作性。


<details>
  <summary>Details</summary>
Motivation: 当前6G网络中智能任务的实现需要独立的知识检索流程，导致数据冗余和解释不一致，KP-A旨在解决这一问题。

Method: 提出KP-A，一个统一的网络知识平面，解耦知识获取与管理与智能逻辑，提供一致的知识接口。

Result: 在实时网络知识问答和边缘AI服务编排两个任务中验证了KP-A的有效性，并开源实现以支持标准化。

Conclusion: KP-A通过统一知识管理简化了智能代理的开发，提升了互操作性，为6G网络的智能化提供了可行方案。

Abstract: The emergence of large language models (LLMs) and agentic systems is enabling
autonomous 6G networks with advanced intelligence, including
self-configuration, self-optimization, and self-healing. However, the current
implementation of individual intelligence tasks necessitates isolated knowledge
retrieval pipelines, resulting in redundant data flows and inconsistent
interpretations. Inspired by the service model unification effort in Open-RAN
(to support interoperability and vendor diversity), we propose KP-A: a unified
Network Knowledge Plane specifically designed for Agentic network intelligence.
By decoupling network knowledge acquisition and management from intelligence
logic, KP-A streamlines development and reduces maintenance complexity for
intelligence engineers. By offering an intuitive and consistent knowledge
interface, KP-A also enhances interoperability for the network intelligence
agents. We demonstrate KP-A in two representative intelligence tasks: live
network knowledge Q&A and edge AI service orchestration. All implementation
artifacts have been open-sourced to support reproducibility and future
standardization efforts.

</details>


### [152] [Towards AI-Native RAN: An Operator's Perspective of 6G Day 1 Standardization](https://arxiv.org/abs/2507.08403)
*Nan Li,Qi Sun,Lehan Wang,Xiaofei Xu,Jinri Huang,Chunhui Liu,Jing Gao,Yuhong Huang,Chih-Lin I*

Main category: cs.NI

TL;DR: 本文探讨了6G移动网络中AI原生无线接入网络（RAN）的设计与标准化原则，提出了Day 1架构及其关键功能，并通过大规模试验验证了其性能提升。


<details>
  <summary>Details</summary>
Motivation: 6G需要从设计之初就原生集成AI，以应对复杂性和支持广泛的AI应用，而5G中AI仅为附加功能。

Method: 基于从2G到5G的网络运营和标准化经验，提出了AI-Native RAN的框架及其三大能力：AI驱动的RAN处理/优化/自动化、可靠的AI生命周期管理和AI即服务（AIaaS）。

Result: 通过5000多个5G-A基站的试验，验证了架构在降低延迟、故障定位和能耗方面的显著改进。

Conclusion: 本文为6G AI-Native RAN的标准化设计提供了Day 1框架，平衡了技术创新与实际部署需求。

Abstract: Artificial Intelligence/Machine Learning (AI/ML) has become the most certain
and prominent feature of 6G mobile networks. Unlike 5G, where AI/ML was not
natively integrated but rather an add-on feature over existing architecture, 6G
shall incorporate AI from the onset to address its complexity and support
ubiquitous AI applications. Based on our extensive mobile network operation and
standardization experience from 2G to 5G, this paper explores the design and
standardization principles of AI-Native radio access networks (RAN) for 6G,
with a particular focus on its critical Day 1 architecture, functionalities and
capabilities. We investigate the framework of AI-Native RAN and present its
three essential capabilities to shed some light on the standardization
direction; namely, AI-driven RAN processing/optimization/automation, reliable
AI lifecycle management (LCM), and AI-as-a-Service (AIaaS) provisioning. The
standardization of AI-Native RAN, in particular the Day 1 features, including
an AI-Native 6G RAN architecture, were proposed. For validation, a large-scale
field trial with over 5000 5G-A base stations have been built and delivered
significant improvements in average air interface latency, root cause
identification, and network energy consumption with the proposed architecture
and the supporting AI functions. This paper aims to provide a Day 1 framework
for 6G AI-Native RAN standardization design, balancing technical innovation
with practical deployment.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [153] [Filter Equivariant Functions: A symmetric account of length-general extrapolation on lists](https://arxiv.org/abs/2507.08796)
*Owen Lewis,Neil Ghani,Andrew Dudzik,Christos Perivolaropoulos,Razvan Pascanu,Petar Veličković*

Main category: cs.PL

TL;DR: 论文探讨了如何设计能够外推已知输入/输出示例的函数，提出了“过滤等变函数”这一新语义类，证明了其基本定理，并展示了其与几何结构的对应关系。


<details>
  <summary>Details</summary>
Motivation: 研究函数在外推时的行为规则，特别是通过过滤操作实现可预测性。

Method: 引入过滤等变函数类，证明其性质，并与映射等变函数类关联，提出几何解释和构造算法。

Result: 证明了过滤等变函数类的存在性和性质，提出了构造外推函数的算法。

Conclusion: 过滤等变函数为外推问题提供了新的理论框架和实用算法。

Abstract: What should a function that extrapolates beyond known input/output examples
look like? This is a tricky question to answer in general, as any function
matching the outputs on those examples can in principle be a correct
extrapolant. We argue that a "good" extrapolant should follow certain kinds of
rules, and here we study a particularly appealing criterion for rule-following
in list functions: that the function should behave predictably even when
certain elements are removed. In functional programming, a standard way to
express such removal operations is by using a filter function. Accordingly, our
paper introduces a new semantic class of functions -- the filter equivariant
functions. We show that this class contains interesting examples, prove some
basic theorems about it, and relate it to the well-known class of map
equivariant functions. We also present a geometric account of filter
equivariants, showing how they correspond naturally to certain simplicial
structures. Our highlight result is the amalgamation algorithm, which
constructs any filter-equivariant function's output by first studying how it
behaves on sublists of the input, in a way that extrapolates perfectly.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [154] [Safe Deep Reinforcement Learning for Resource Allocation with Peak Age of Information Violation Guarantees](https://arxiv.org/abs/2507.08653)
*Berire Gunes Reyhan,Sinem Coleri*

Main category: eess.SP

TL;DR: 提出了一种基于优化理论的安全深度强化学习框架，用于超可靠无线网络控制系统，首次在文献中实现约束满足与性能优化的平衡。


<details>
  <summary>Details</summary>
Motivation: 无线网络控制系统中，控制与通信系统需协同设计，但现有方法难以同时满足约束条件和优化性能。

Method: 采用两阶段方法：优化理论阶段推导最优条件，简化问题；安全深度强化学习阶段通过师生框架指导智能体。

Result: 仿真表明，该框架优于基于规则和其他优化理论的深度强化学习方法，收敛更快、奖励更高、稳定性更强。

Conclusion: 该框架为超可靠无线网络控制系统提供了一种高效且安全的解决方案。

Abstract: In Wireless Networked Control Systems (WNCSs), control and communication
systems must be co-designed due to their strong interdependence. This paper
presents a novel optimization theory-based safe deep reinforcement learning
(DRL) framework for ultra-reliable WNCSs, ensuring constraint satisfaction
while optimizing performance, for the first time in the literature. The
approach minimizes power consumption under key constraints, including Peak Age
of Information (PAoI) violation probability, transmit power, and schedulability
in the finite blocklength regime. PAoI violation probability is uniquely
derived by combining stochastic maximum allowable transfer interval (MATI) and
maximum allowable packet delay (MAD) constraints in a multi-sensor network. The
framework consists of two stages: optimization theory and safe DRL. The first
stage derives optimality conditions to establish mathematical relationships
among variables, simplifying and decomposing the problem. The second stage
employs a safe DRL model where a teacher-student framework guides the DRL agent
(student). The control mechanism (teacher) evaluates compliance with system
constraints and suggests the nearest feasible action when needed. Extensive
simulations show that the proposed framework outperforms rule-based and other
optimization theory based DRL benchmarks, achieving faster convergence, higher
rewards, and greater stability.

</details>


<div id='q-fin.RM'></div>

# q-fin.RM [[Back]](#toc)

### [155] [Entity-Specific Cyber Risk Assessment using InsurTech Empowered Risk Factors](https://arxiv.org/abs/2507.08193)
*Jiayi Guo,Zhiyun Quan,Linfeng Zhang*

Main category: q-fin.RM

TL;DR: 论文提出了一种InsurTech框架，通过丰富网络事件数据中的实体特定属性，利用机器学习模型预测网络事件类型及其频率，提升风险评估的透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量的网络事件公开数据，且企业不愿披露可能损害声誉的事件，导致网络风险评估的实证研究和预测建模受限。

Method: 提出InsurTech框架，结合多标签分类和多输出回归模型预测网络事件类型及频率，并应用可解释的ML技术验证风险因素。

Result: InsurTech增强的特征提升了预测的稳健性，框架生成了透明的、实体特定的网络风险画像。

Conclusion: 该框架为保险公司和组织提供了数据驱动的决策支持，支持定制化承保和主动的网络风险缓解。

Abstract: The lack of high-quality public cyber incident data limits empirical research
and predictive modeling for cyber risk assessment. This challenge persists due
to the reluctance of companies to disclose incidents that could damage their
reputation or investor confidence. Therefore, from an actuarial perspective,
potential resolutions conclude two aspects: the enhancement of existing cyber
incident datasets and the implementation of advanced modeling techniques to
optimize the use of the available data. A review of existing data-driven
methods highlights a significant lack of entity-specific organizational
features in publicly available datasets. To address this gap, we propose a
novel InsurTech framework that enriches cyber incident data with
entity-specific attributes. We develop various machine learning (ML) models: a
multilabel classification model to predict the occurrence of cyber incident
types (e.g., Privacy Violation, Data Breach, Fraud and Extortion, IT Error, and
Others) and a multioutput regression model to estimate their annual
frequencies. While classifier and regressor chains are implemented to explore
dependencies among cyber incident types as well, no significant correlations
are observed in our datasets. Besides, we apply multiple interpretable ML
techniques to identify and cross-validate potential risk factors developed by
InsurTech across ML models. We find that InsurTech empowered features enhance
prediction occurrence and frequency estimation robustness compared to only
using conventional risk factors. The framework generates transparent,
entity-specific cyber risk profiles, supporting customized underwriting and
proactive cyber risk mitigation. It provides insurers and organizations with
data-driven insights to support decision-making and compliance planning.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [156] [SPINT: Spatial Permutation-Invariant Neural Transformer for Consistent Intracortical Motor Decoding](https://arxiv.org/abs/2507.08402)
*Trung Le,Hao Fang,Jingyuan Li,Tung Nguyen,Lu Mi,Amy Orsborn,Uygar Sümbül,Eli Shlizerman*

Main category: q-bio.NC

TL;DR: SPINT是一种新型的神经解码框架，通过动态推断神经单元身份和动态通道丢弃技术，解决了iBCI中神经记录的非平稳性问题，实现了跨会话的稳健解码。


<details>
  <summary>Details</summary>
Motivation: 解决iBCI中神经记录的非平稳性问题，避免现有方法对固定神经身份和测试时标签的依赖。

Method: 提出SPINT框架，使用上下文相关的位置嵌入和动态通道丢弃技术，支持无序神经单元集的解码和少量无标签数据的适应。

Result: 在FALCON Benchmark的三个多会话数据集上，SPINT表现出优于现有零样本和少样本无监督基线的性能。

Conclusion: SPINT为长期iBCI应用提供了一个稳健且可扩展的神经解码框架。

Abstract: Intracortical Brain-Computer Interfaces (iBCI) aim to decode behavior from
neural population activity, enabling individuals with motor impairments to
regain motor functions and communication abilities. A key challenge in
long-term iBCI is the nonstationarity of neural recordings, where the
composition and tuning profiles of the recorded populations are unstable across
recording sessions. Existing methods attempt to address this issue by explicit
alignment techniques; however, they rely on fixed neural identities and require
test-time labels or parameter updates, limiting their generalization across
sessions and imposing additional computational burden during deployment. In
this work, we introduce SPINT - a Spatial Permutation-Invariant Neural
Transformer framework for behavioral decoding that operates directly on
unordered sets of neural units. Central to our approach is a novel
context-dependent positional embedding scheme that dynamically infers
unit-specific identities, enabling flexible generalization across recording
sessions. SPINT supports inference on variable-size populations and allows
few-shot, gradient-free adaptation using a small amount of unlabeled data from
the test session. To further promote model robustness to population
variability, we introduce dynamic channel dropout, a regularization method for
iBCI that simulates shifts in population composition during training. We
evaluate SPINT on three multi-session datasets from the FALCON Benchmark,
covering continuous motor decoding tasks in human and non-human primates. SPINT
demonstrates robust cross-session generalization, outperforming existing
zero-shot and few-shot unsupervised baselines while eliminating the need for
test-time alignment and fine-tuning. Our work contributes an initial step
toward a robust and scalable neural decoding framework for long-term iBCI
applications.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [157] [Energy Management for Renewable-Colocated Artificial Intelligence Data Centers](https://arxiv.org/abs/2507.08011)
*Siying Li,Lang Tong,Timothy D. Mount*

Main category: math.OC

TL;DR: 开发了一种用于AI数据中心的能源管理系统（EMS），通过优化AI工作负载调度、可再生能源利用和电力市场参与，实现利润最大化。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过可再生能源与AI数据中心的协同优化，提高经济收益。

Method: 在批发和零售市场参与模式下，优化AI工作负载调度、可再生能源利用和电力市场参与。

Result: 实证评估显示，可再生能源与AI数据中心的协同显著提升了利润。

Conclusion: 可再生能源与AI数据中心的协同优化能有效提高经济收益。

Abstract: We develop an energy management system (EMS) for artificial intelligence (AI)
data centers with colocated renewable generation. Under a profit-maximizing
framework, the EMS of renewable-colocated data center (RCDC) co-optimizes AI
workload scheduling, on-site renewable utilization, and electricity market
participation. Within both wholesale and retail market participation models,
the economic benefit of the RCDC operation is maximized. Empirical evaluations
using real-world traces of electricity prices, data center power consumption,
and renewable generation demonstrate significant profit gains from renewable
and AI data center colocations.

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [158] [Consciousness as a Jamming Phase](https://arxiv.org/abs/2507.08197)
*Kaichen Ouyang*

Main category: cond-mat.dis-nn

TL;DR: 论文通过类比颗粒物质和其他复杂系统中的堵塞相变，提出了一个神经堵塞相图，将大语言模型中的意识涌现解释为高维无序系统中的临界现象。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型中意识涌现的物理机制，揭示其与堵塞相变的相似性。

Method: 建立神经堵塞相图，类比堵塞相变的控制参数（温度、体积分数、应力），分析神经网络中的临界行为。

Result: 理论为人工智能中的经验缩放定律提供了统一的物理解释，表明计算冷却、密度优化和噪声减少共同推动系统达到临界堵塞表面，从而涌现广义智能。

Conclusion: 意识可能是通过长程关联连接知识组分的堵塞相，神经语言模型的临界缩放行为可通过堵塞物理学解释。

Abstract: This paper develops a neural jamming phase diagram that interprets the
emergence of consciousness in large language models as a critical phenomenon in
high-dimensional disordered systems.By establishing analogies with jamming
transitions in granular matter and other complex systems, we identify three
fundamental control parameters governing the phase behavior of neural networks:
temperature, volume fraction, and stress.The theory provides a unified physical
explanation for empirical scaling laws in artificial intelligence,
demonstrating how computational cooling, density optimization, and noise
reduction collectively drive systems toward a critical jamming surface where
generalized intelligence emerges. Remarkably, the same thermodynamic principles
that describe conventional jamming transitions appear to underlie the emergence
of consciousness in neural networks, evidenced by shared critical signatures
including divergent correlation lengths and scaling exponents.Our work explains
neural language models' critical scaling through jamming physics, suggesting
consciousness is a jamming phase that intrinsically connects knowledge
components via long-range correlations.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [159] [To Trade or Not to Trade: An Agentic Approach to Estimating Market Risk Improves Trading Decisions](https://arxiv.org/abs/2507.08584)
*Dimitrios Emmanoulopoulos,Ollie Olby,Justin Lyon,Namid R. Stillman*

Main category: q-fin.ST

TL;DR: 论文提出了一种基于大语言模型（LLMs）的代理系统，用于迭代发现金融时间序列的随机微分方程，以改进交易决策。


<details>
  <summary>Details</summary>
Motivation: 现有代理框架通常缺乏原则性的建模步骤，依赖情感或趋势分析，无法有效支持金融决策。

Method: 开发了一种代理系统，利用LLMs迭代发现随机微分方程，生成风险指标以指导交易决策。

Result: 模型驱动的交易策略在传统回测和市场模拟器中表现优于标准LLM代理，提高了多只股票的夏普比率。

Conclusion: 结合LLMs与代理模型发现可以增强市场风险估计，实现更有利可图的交易决策。

Abstract: Large language models (LLMs) are increasingly deployed in agentic frameworks,
in which prompts trigger complex tool-based analysis in pursuit of a goal.
While these frameworks have shown promise across multiple domains including in
finance, they typically lack a principled model-building step, relying instead
on sentiment- or trend-based analysis. We address this gap by developing an
agentic system that uses LLMs to iteratively discover stochastic differential
equations for financial time series. These models generate risk metrics which
inform daily trading decisions. We evaluate our system in both traditional
backtests and using a market simulator, which introduces synthetic but causally
plausible price paths and news events. We find that model-informed trading
strategies outperform standard LLM-based agents, improving Sharpe ratios across
multiple equities. Our results show that combining LLMs with agentic model
discovery enhances market risk estimation and enables more profitable trading
decisions.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [160] [Unraveling the Potential of Diffusion Models in Small Molecule Generation](https://arxiv.org/abs/2507.08005)
*Peining Zhang,Daniel Baker,Minghu Song,Jinbo Bi*

Main category: q-bio.BM

TL;DR: 综述了扩散模型（DMs）在分子生成中的最新进展与应用，涵盖理论、方法分类、性能评估及未来挑战。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在药物研发中的潜力，促进化学空间的广泛探索。

Method: 介绍DMs理论，分类基于数学和化学应用的分子生成方法，评估模型性能。

Result: 总结了现有3D方法的生成性能，并指出其优势与不足。

Conclusion: 强调当前挑战，提出未来研究方向以充分发挥DMs在药物发现中的潜力。

Abstract: Generative AI presents chemists with novel ideas for drug design and
facilitates the exploration of vast chemical spaces. Diffusion models (DMs), an
emerging tool, have recently attracted great attention in drug R\&D. This paper
comprehensively reviews the latest advancements and applications of DMs in
molecular generation. It begins by introducing the theoretical principles of
DMs. Subsequently, it categorizes various DM-based molecular generation methods
according to their mathematical and chemical applications. The review further
examines the performance of these models on benchmark datasets, with a
particular focus on comparing the generation performance of existing 3D
methods. Finally, it concludes by emphasizing current challenges and suggesting
future research directions to fully exploit the potential of DMs in drug
discovery.

</details>


### [161] [AmpLyze: A Deep Learning Model for Predicting the Hemolytic Concentration](https://arxiv.org/abs/2507.08162)
*Peng Qiu,Hanqi Feng,Barnabas Poczos*

Main category: q-bio.BM

TL;DR: AmpLyze模型通过序列预测红血球溶解（HC50）值，并解释驱动毒性的残基，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有模型仅能判断抗菌肽（AMP）的毒性或非毒性，无法量化HC50值，AmpLyze填补了这一空白。

Method: 结合残基级ProtT5/ESM2嵌入和序列级描述符，通过交叉注意力模块对齐，使用log-cosh损失函数训练。

Result: AmpLyze模型PCC为0.756，MSE为0.987，优于传统回归器和现有最佳方法。

Conclusion: AmpLyze为AMP设计提供了定量、序列化和可解释的毒性预测工具。

Abstract: Red-blood-cell lysis (HC50) is the principal safety barrier for
antimicrobial-peptide (AMP) therapeutics, yet existing models only say "toxic"
or "non-toxic." AmpLyze closes this gap by predicting the actual HC50 value
from sequence alone and explaining the residues that drive toxicity. The model
couples residue-level ProtT5/ESM2 embeddings with sequence-level descriptors in
dual local and global branches, aligned by a cross-attention module and trained
with log-cosh loss for robustness to assay noise. The optimal AmpLyze model
reaches a PCC of 0.756 and an MSE of 0.987, outperforming classical regressors
and the state-of-the-art. Ablations confirm that both branches are essential,
and cross-attention adds a further 1% PCC and 3% MSE improvement.
Expected-Gradients attributions reveal known toxicity hotspots and suggest
safer substitutions. By turning hemolysis assessment into a quantitative,
sequence-based, and interpretable prediction, AmpLyze facilitates AMP design
and offers a practical tool for early-stage toxicity screening.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [162] [VideoConviction: A Multimodal Benchmark for Human Conviction and Stock Market Recommendations](https://arxiv.org/abs/2507.08104)
*Michael Galarnyk,Veer Kejriwal,Agam Shah,Yash Bhardwaj,Nicholas Meyer,Anand Krishnan,Sudheer Chava*

Main category: cs.MM

TL;DR: 论文研究了社交媒体上金融影响者（finfluencers）的多模态信号对股票推荐的影响，并提出了VideoConviction数据集来评估多模态和文本大模型的表现。


<details>
  <summary>Details</summary>
Motivation: 理解金融影响者通过多模态信号（如语气、表达方式和面部表情）传达的股票推荐信息，超越传统文本分析。

Method: 引入VideoConviction数据集，包含6000+专家标注，用于评估多模态和文本大模型在金融话语中的表现。

Result: 多模态输入提高了股票代码提取的准确性，但模型难以区分投资行为和信心强度；逆策略（反其道而行）表现优于S&P 500但风险更高。

Conclusion: VideoConviction为多模态金融研究提供了基准，推动了多模态任务的深入发展。

Abstract: Social media has amplified the reach of financial influencers known as
"finfluencers," who share stock recommendations on platforms like YouTube.
Understanding their influence requires analyzing multimodal signals like tone,
delivery style, and facial expressions, which extend beyond text-based
financial analysis. We introduce VideoConviction, a multimodal dataset with
6,000+ expert annotations, produced through 457 hours of human effort, to
benchmark multimodal large language models (MLLMs) and text-based large
language models (LLMs) in financial discourse. Our results show that while
multimodal inputs improve stock ticker extraction (e.g., extracting Apple's
ticker AAPL), both MLLMs and LLMs struggle to distinguish investment actions
and conviction--the strength of belief conveyed through confident delivery and
detailed reasoning--often misclassifying general commentary as definitive
recommendations. While high-conviction recommendations perform better than
low-conviction ones, they still underperform the popular S\&P 500 index fund.
An inverse strategy--betting against finfluencer recommendations--outperforms
the S\&P 500 by 6.8\% in annual returns but carries greater risk (Sharpe ratio
of 0.41 vs. 0.65). Our benchmark enables a diverse evaluation of multimodal
tasks, comparing model performance on both full video and segmented video
inputs. This enables deeper advancements in multimodal financial research. Our
code, dataset, and evaluation leaderboard are available under the CC BY-NC 4.0
license.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [163] [Audio Flamingo 3: Advancing Audio Intelligence with Fully Open Large Audio Language Models](https://arxiv.org/abs/2507.08128)
*Arushi Goel,Sreyan Ghosh,Jaehyeon Kim,Sonal Kumar,Zhifeng Kong,Sang-gil Lee,Chao-Han Huck Yang,Ramani Duraiswami,Dinesh Manocha,Rafael Valle,Bryan Catanzaro*

Main category: cs.SD

TL;DR: AF3是一个开源的先进音频-语言模型，支持语音、声音和音乐的多模态理解和推理，具备链式思维、多轮对话和长音频处理能力。


<details>
  <summary>Details</summary>
Motivation: 提升跨语音、声音和音乐的推理和理解能力，填补现有模型的不足。

Method: 引入AF-Whisper统一编码器、链式思维推理、多轮对话和长音频处理技术，采用五阶段课程训练策略。

Result: 在20多个音频理解和推理基准测试中取得SOTA结果，超越闭源和大数据集训练的模型。

Conclusion: AF3展示了开源模型在多模态音频理解和推理任务中的强大潜力。

Abstract: We present Audio Flamingo 3 (AF3), a fully open state-of-the-art (SOTA) large
audio-language model that advances reasoning and understanding across speech,
sound, and music. AF3 introduces: (i) AF-Whisper, a unified audio encoder
trained using a novel strategy for joint representation learning across all 3
modalities of speech, sound, and music; (ii) flexible, on-demand thinking,
allowing the model to do chain-of-thought-type reasoning before answering;
(iii) multi-turn, multi-audio chat; (iv) long audio understanding and reasoning
(including speech) up to 10 minutes; and (v) voice-to-voice interaction. To
enable these capabilities, we propose several large-scale training datasets
curated using novel strategies, including AudioSkills-XL, LongAudio-XL,
AF-Think, and AF-Chat, and train AF3 with a novel five-stage curriculum-based
training strategy. Trained on only open-source audio data, AF3 achieves new
SOTA results on over 20+ (long) audio understanding and reasoning benchmarks,
surpassing both open-weight and closed-source models trained on much larger
datasets.

</details>


### [164] [On Barriers to Archival Audio Processing](https://arxiv.org/abs/2507.08768)
*Peter Sullivan,Muhammad Abdul-Mageed*

Main category: cs.SD

TL;DR: 研究利用UNESCO的20世纪中期广播录音，测试现代语言识别（LID）和说话人识别（SR）方法的鲁棒性，发现LID系统表现较好，但SR系统存在偏见问题。


<details>
  <summary>Details</summary>
Motivation: 探究现代LID和SR方法在处理多语言说话人和跨年龄录音时的表现。

Method: 利用UNESCO的广播录音数据集测试LID和SR方法。

Result: LID系统（如Whisper）能较好处理第二语言和带口音语音，但SR系统易受频道、年龄和语言偏见影响。

Conclusion: SR方法在档案索引应用中需克服偏见问题。

Abstract: In this study, we leverage a unique UNESCO collection of mid-20th century
radio recordings to probe the robustness of modern off-the-shelf language
identification (LID) and speaker recognition (SR) methods, especially with
respect to the impact of multilingual speakers and cross-age recordings. Our
findings suggest that LID systems, such as Whisper, are increasingly adept at
handling second-language and accented speech. However, speaker embeddings
remain a fragile component of speech processing pipelines that is prone to
biases related to the channel, age, and language. Issues which will need to be
overcome should archives aim to employ SR methods for speaker indexing.

</details>


### [165] [Audio Inpanting using Discrete Diffusion Model](https://arxiv.org/abs/2507.08333)
*Tali Dror,Iftach Shoham,Moshe Buchris,Oren Gal,Haim Permuter,Gilad Katz,Eliya Nachmani*

Main category: cs.SD

TL;DR: 提出了一种基于离散扩散模型的音频修复方法，适用于长缺失段（最长500毫秒），性能优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现有方法在修复长缺失段（超过100毫秒）时质量下降，需要更稳定的解决方案。

Method: 使用预训练的音频标记器生成离散潜在空间，并在该空间内直接建模生成过程。

Result: 在MusicNet和MTG数据集上，该方法在长达500毫秒的缺失段中表现优于基线。

Conclusion: 该方法为长缺失段的音频修复提供了鲁棒且语义连贯的解决方案。

Abstract: Audio inpainting refers to the task of reconstructing missing segments in
corrupted audio recordings. While prior approaches-including waveform and
spectrogram-based diffusion models-have shown promising results for short gaps,
they often degrade in quality when gaps exceed 100 milliseconds (ms). In this
work, we introduce a novel inpainting method based on discrete diffusion
modeling, which operates over tokenized audio representations produced by a
pre-trained audio tokenizer. Our approach models the generative process
directly in the discrete latent space, enabling stable and semantically
coherent reconstruction of missing audio. We evaluate the method on the
MusicNet dataset using both objective and perceptual metrics across gap
durations up to 300 ms. We further evaluated our approach on the MTG dataset,
extending the gap duration to 500 ms. Experimental results demonstrate that our
method achieves competitive or superior performance compared to existing
baselines, particularly for longer gaps, offering a robust solution for
restoring degraded musical recordings. Audio examples of our proposed method
can be found at https://iftach21.github.io/

</details>


### [166] [MIDI-VALLE: Improving Expressive Piano Performance Synthesis Through Neural Codec Language Modelling](https://arxiv.org/abs/2507.08530)
*Jingjing Tang,Xin Wang,Zhe Zhang,Junichi Yamagishi,Geraint Wiggins,George Fazekas*

Main category: cs.SD

TL;DR: MIDI-VALLE是一种基于VALLE框架的神经编解码语言模型，用于从音乐乐谱生成富有表现力的音频表演，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 传统音乐表演合成模型在多样化的MIDI来源、音乐风格和录音环境中泛化能力不足，需要改进。

Method: 改进VALLE框架，引入参考音频和MIDI作为条件，将MIDI和音频编码为离散标记，增强模型对钢琴表演的一致性建模。

Result: 在ATEPP和Maestro数据集上，MIDI-VALLE的Frechet Audio Distance降低了75%以上，听众测试中获得了更高的投票数。

Conclusion: MIDI-VALLE在音乐表演合成中表现出卓越的泛化能力和合成质量。

Abstract: Generating expressive audio performances from music scores requires models to
capture both instrument acoustics and human interpretation. Traditional music
performance synthesis pipelines follow a two-stage approach, first generating
expressive performance MIDI from a score, then synthesising the MIDI into
audio. However, the synthesis models often struggle to generalise across
diverse MIDI sources, musical styles, and recording environments. To address
these challenges, we propose MIDI-VALLE, a neural codec language model adapted
from the VALLE framework, which was originally designed for zero-shot
personalised text-to-speech (TTS) synthesis. For performance MIDI-to-audio
synthesis, we improve the architecture to condition on a reference audio
performance and its corresponding MIDI. Unlike previous TTS-based systems that
rely on piano rolls, MIDI-VALLE encodes both MIDI and audio as discrete tokens,
facilitating a more consistent and robust modelling of piano performances.
Furthermore, the model's generalisation ability is enhanced by training on an
extensive and diverse piano performance dataset. Evaluation results show that
MIDI-VALLE significantly outperforms a state-of-the-art baseline, achieving
over 75% lower Frechet Audio Distance on the ATEPP and Maestro datasets. In the
listening test, MIDI-VALLE received 202 votes compared to 58 for the baseline,
demonstrating improved synthesis quality and generalisation across diverse
performance MIDI inputs.

</details>


### [167] [FreeAudio: Training-Free Timing Planning for Controllable Long-Form Text-to-Audio Generation](https://arxiv.org/abs/2507.08557)
*Yuxuan Jiang,Zehua Chen,Zeqian Ju,Chang Li,Weibei Dou,Jun Zhu*

Main category: cs.SD

TL;DR: FreeAudio提出了一种无需训练的时序控制文本到音频生成框架，支持长文本提示，如精确时间控制的音频生成。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音频生成方法因数据质量与数量限制，难以处理复杂的时间控制提示。

Method: 使用LLM规划时间窗口并重新描述文本，结合解耦聚合注意力控制和上下文潜在组合技术。

Result: FreeAudio在无需训练的方法中达到最优性能，与基于训练的方法相当，支持长时序控制音频生成。

Conclusion: FreeAudio为时序控制长文本音频合成开辟了新途径，性能接近领先的基于训练方法。

Abstract: Text-to-audio (T2A) generation has achieved promising results with the recent
advances in generative models. However, because of the limited quality and
quantity of temporally-aligned audio-text pairs, existing T2A methods struggle
to handle the complex text prompts that contain precise timing control, e.g.,
"owl hooted at 2.4s-5.2s". Recent works have explored data augmentation
techniques or introduced timing conditions as model inputs to enable
timing-conditioned 10-second T2A generation, while their synthesis quality is
still limited. In this work, we propose a novel training-free timing-controlled
T2A framework, FreeAudio, making the first attempt to enable timing-controlled
long-form T2A generation, e.g., "owl hooted at 2.4s-5.2s and crickets chirping
at 0s-24s". Specifically, we first employ an LLM to plan non-overlapping time
windows and recaption each with a refined natural language description, based
on the input text and timing prompts. Then we introduce: 1) Decoupling and
Aggregating Attention Control for precise timing control; 2) Contextual Latent
Composition for local smoothness and Reference Guidance for global consistency.
Extensive experiments show that: 1) FreeAudio achieves state-of-the-art
timing-conditioned T2A synthesis quality among training-free methods and is
comparable to leading training-based methods; 2) FreeAudio demonstrates
comparable long-form generation quality with training-based Stable Audio and
paves the way for timing-controlled long-form T2A synthesis. Demo samples are
available at: https://freeaudio.github.io/FreeAudio/

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [168] [Mallows Model with Learned Distance Metrics: Sampling and Maximum Likelihood Estimation](https://arxiv.org/abs/2507.08108)
*Yeganeh Alimohammadi,Kiana Asgari*

Main category: stat.ML

TL;DR: 本文提出了一种广义的Mallows模型，通过学习数据中的距离度量来适应不同场景下的排名数据，并开发了高效的采样和最大似然估计算法。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要使用固定距离度量（如Kendall's τ距离），缺乏从数据中学习距离度量的方法。实际应用中，不同场景的排名行为差异较大，需要自适应的方法。

Method: 提出广义Mallows模型，使用Lα距离度量，开发了FPTAS采样算法和高效的MLE算法，联合估计中心排名、分散参数和最优距离度量。

Result: 证明了估计器的强一致性，并通过体育排名数据集验证了方法的有效性。

Conclusion: 广义Mallows模型能够灵活适应不同场景的排名数据，为实际应用提供了更强大的工具。

Abstract: \textit{Mallows model} is a widely-used probabilistic framework for learning
from ranking data, with applications ranging from recommendation systems and
voting to aligning language models with human
preferences~\cite{chen2024mallows, kleinberg2021algorithmic,
rafailov2024direct}. Under this model, observed rankings are noisy
perturbations of a central ranking $\sigma$, with likelihood decaying
exponentially in distance from $\sigma$, i.e, $P (\pi) \propto \exp\big(-\beta
\cdot d(\pi, \sigma)\big),$ where $\beta > 0$ controls dispersion and $d$ is a
distance function.
  Existing methods mainly focus on fixed distances (such as Kendall's $\tau$
distance), with no principled approach to learning the distance metric directly
from data. In practice, however, rankings naturally vary by context; for
instance, in some sports we regularly see long-range swaps (a low-rank team
beating a high-rank one), while in others such events are rare. Motivated by
this, we propose a generalization of Mallows model that learns the distance
metric directly from data. Specifically, we focus on $L_\alpha$ distances:
$d_\alpha(\pi,\sigma):=\sum_{i=1} |\pi(i)-\sigma(i)|^\alpha$.
  For any $\alpha\geq 1$ and $\beta>0$, we develop a Fully Polynomial-Time
Approximation Scheme (FPTAS) to efficiently generate samples that are
$\epsilon$- close (in total variation distance) to the true distribution. Even
in the special cases of $L_1$ and $L_2$, this generalizes prior results that
required vanishing dispersion ($\beta\to0$). Using this sampling algorithm, we
propose an efficient Maximum Likelihood Estimation (MLE) algorithm that jointly
estimates the central ranking, the dispersion parameter, and the optimal
distance metric. We prove strong consistency results for our estimators (for
any values of $\alpha$ and $\beta$), and we validate our approach empirically
using datasets from sports rankings.

</details>


### [169] [CLEAR: Calibrated Learning for Epistemic and Aleatoric Risk](https://arxiv.org/abs/2507.08150)
*Ilia Azizi,Juraj Bodik,Jakob Heiss,Bin Yu*

Main category: stat.ML

TL;DR: CLEAR是一种校准方法，通过结合两种不确定性（偶然性和认知性）来改进预测建模的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常单独处理偶然性或认知性不确定性，但未能平衡两者。

Method: CLEAR使用两个参数（γ₁和γ₂）结合偶然性和认知性不确定性，兼容多种估计器。

Result: 在17个数据集上，CLEAR平均将区间宽度改善了28.2%和17.4%，同时保持覆盖率。

Conclusion: CLEAR在不确定性主导的场景中表现优异，显著提升了预测建模的可靠性。

Abstract: Accurate uncertainty quantification is critical for reliable predictive
modeling, especially in regression tasks. Existing methods typically address
either aleatoric uncertainty from measurement noise or epistemic uncertainty
from limited data, but not necessarily both in a balanced way. We propose
CLEAR, a calibration method with two distinct parameters, $\gamma_1$ and
$\gamma_2$, to combine the two uncertainty components for improved conditional
coverage. CLEAR is compatible with any pair of aleatoric and epistemic
estimators; we show how it can be used with (i) quantile regression for
aleatoric uncertainty and (ii) ensembles drawn from the
Predictability-Computability-Stability (PCS) framework for epistemic
uncertainty. Across 17 diverse real-world datasets, CLEAR achieves an average
improvement of 28.2% and 17.4% in the interval width compared to the two
individually calibrated baselines while maintaining nominal coverage. This
improvement can be particularly evident in scenarios dominated by either high
epistemic or high aleatoric uncertainty.

</details>


### [170] [Admissibility of Stein Shrinkage for Batch Normalization in the Presence of Adversarial Attacks](https://arxiv.org/abs/2507.08261)
*Sofia Ivolgina,P. Thomas Fletcher,Baba C. Vemuri*

Main category: stat.ML

TL;DR: 论文提出在批归一化（BN）中使用Stein收缩估计器替代样本均值和方差，以在对抗攻击下提升性能。


<details>
  <summary>Details</summary>
Motivation: 批归一化在深度学习中广泛使用，但其样本统计量在对抗攻击下表现不佳，Stein收缩估计器能提供更稳健的估计。

Method: 通过Stein收缩估计器改进BN中的均值和方差估计，并在分类和分割任务中验证其效果。

Result: 在CIFAR-10、PPMI和Cityscape数据集上，改进后的BN在对抗攻击下表现优于传统BN。

Conclusion: Stein收缩估计器显著提升了BN在对抗攻击下的鲁棒性，适用于多种任务。

Abstract: Batch normalization (BN) is a ubiquitous operation in deep neural networks
used primarily to achieve stability and regularization during network training.
BN involves feature map centering and scaling using sample means and variances,
respectively. Since these statistics are being estimated across the feature
maps within a batch, this problem is ideally suited for the application of
Stein's shrinkage estimation, which leads to a better, in the
mean-squared-error sense, estimate of the mean and variance of the batch. In
this paper, we prove that the Stein shrinkage estimator for the mean and
variance dominates over the sample mean and variance estimators in the presence
of adversarial attacks when modeling these attacks using sub-Gaussian
distributions. This facilitates and justifies the application of Stein
shrinkage to estimate the mean and variance parameters in BN and use it in
image classification (segmentation) tasks with and without adversarial attacks.
We present SOTA performance results using this Stein corrected batch norm in a
standard ResNet architecture applied to the task of image classification using
CIFAR-10 data, 3D CNN on PPMI (neuroimaging) data and image segmentation using
HRNet on Cityscape data with and without adversarial attacks.

</details>


### [171] [MIRRAMS: Towards Training Models Robust to Missingness Distribution Shifts](https://arxiv.org/abs/2507.08280)
*Jihye Lee,Minseo Kang,Dongha Kim*

Main category: stat.ML

TL;DR: 提出了一种名为MIRRAMS的深度学习框架，用于解决训练和测试数据中缺失分布偏移的问题，通过互信息条件提升模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实数据分析中，训练和测试数据的缺失分布偏移常见且影响预测性能，需开发鲁棒方法应对。

Method: 引入互信息条件（MI robustness conditions），设计损失项并构建目标函数MIRRAMS，确保模型对缺失模式不变。

Result: MIRRAMS在多个基准数据集上优于现有方法，且在无缺失数据时也表现优异，适用于半监督学习。

Conclusion: MIRRAMS是一种通用且强大的学习框架，能有效应对缺失分布偏移，并扩展至半监督任务。

Abstract: In real-world data analysis, missingness distributional shifts between
training and test input datasets frequently occur, posing a significant
challenge to achieving robust prediction performance. In this study, we propose
a novel deep learning framework designed to address such shifts in missingness
distributions. We begin by introducing a set of mutual information-based
conditions, called MI robustness conditions, which guide a prediction model to
extract label-relevant information while remaining invariant to diverse
missingness patterns, thereby enhancing robustness to unseen missingness
scenarios at test-time. To make these conditions practical, we propose simple
yet effective techniques to derive loss terms corresponding to each and
formulate a final objective function, termed MIRRAMS(Mutual Information
Regularization for Robustness Against Missingness Shifts). As a by-product, our
analysis provides a theoretical interpretation of the principles underlying
consistency regularization-based semi-supervised learning methods, such as
FixMatch. Extensive experiments across various benchmark datasets show that
MIRRAMS consistently outperforms existing baselines and maintains stable
performance across diverse missingness scenarios. Moreover, our approach
achieves state-of-the-art performance even without missing data and can be
naturally extended to address semi-supervised learning tasks, highlighting
MIRRAMS as a powerful, off-the-shelf framework for general-purpose learning.

</details>


### [172] [Optimal and Practical Batched Linear Bandit Algorithm](https://arxiv.org/abs/2507.08438)
*Sanghoon Yu,Min-hwan Oh*

Main category: stat.ML

TL;DR: 论文提出了一种名为BLAE的新算法，解决了批量线性老虎机问题，首次在所有情况下实现了最小最大最优遗憾，同时仅需O(log log T)批次，并在实践中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有方法虽然在理论上可以实现接近最优的遗憾，但计算成本高或实际表现不佳。

Method: BLAE算法结合了臂消除和正则化G-最优设计，引入了新的批次最优设计技术和改进的浓度边界。

Result: BLAE在所有情况下首次实现了最小最大最优遗憾（在T的对数因子内），计算开销低，并在实验中优于现有方法。

Conclusion: BLAE是首个在所有情况下兼具理论最优性和实际优越性的批量线性老虎机算法。

Abstract: We study the linear bandit problem under limited adaptivity, known as the
batched linear bandit. While existing approaches can achieve near-optimal
regret in theory, they are often computationally prohibitive or underperform in
practice. We propose \texttt{BLAE}, a novel batched algorithm that integrates
arm elimination with regularized G-optimal design, achieving the minimax
optimal regret (up to logarithmic factors in $T$) in both large-$K$ and
small-$K$ regimes for the first time, while using only $O(\log\log T)$ batches.
Our analysis introduces new techniques for batch-wise optimal design and
refined concentration bounds. Crucially, \texttt{BLAE} demonstrates low
computational overhead and strong empirical performance, outperforming
state-of-the-art methods in extensive numerical evaluations. Thus,
\texttt{BLAE} is the first algorithm to combine provable minimax-optimality in
all regimes and practical superiority in batched linear bandits.

</details>


### [173] [Data Depth as a Risk](https://arxiv.org/abs/2507.08518)
*Arturo Castellanos,Pavlo Mozharovskyi*

Main category: stat.ML

TL;DR: 论文提出了一种新的数据深度框架“损失深度”，通过最小化分类器损失来定义数据深度，扩展了半空间深度的概念，并适用于高维数据。


<details>
  <summary>Details</summary>
Motivation: 传统数据深度方法（如半空间深度）在泛化性和计算效率上存在局限，需要一种更灵活且高效的方法来适应高维数据和复杂分类器。

Method: 将数据深度定义为分类器的最小损失，通过改变损失函数或分类器集合，提出“损失深度”框架，支持SVM、逻辑回归等分类器。

Result: 新框架继承了机器学习算法的计算效率和统计收敛速度，适用于高维数据，并在异常检测中表现出色。

Conclusion: “损失深度”不仅易于解释，还能有效平衡分类器的复杂度与数据特性，为数据深度领域提供了新的视角和工具。

Abstract: Data depths are score functions that quantify in an unsupervised fashion how
central is a point inside a distribution, with numerous applications such as
anomaly detection, multivariate or functional data analysis, arising across
various fields. The halfspace depth was the first depth to aim at generalising
the notion of quantile beyond the univariate case. Among the existing variety
of depth definitions, it remains one of the most used notions of data depth.
Taking a different angle from the quantile point of view, we show that the
halfspace depth can also be regarded as the minimum loss of a set of
classifiers for a specific labelling of the points. By changing the loss or the
set of classifiers considered, this new angle naturally leads to a family of
"loss depths", extending to well-studied classifiers such as, e.g., SVM or
logistic regression, among others. This framework directly inherits
computational efficiency of existing machine learning algorithms as well as
their fast statistical convergence rates, and opens the data depth realm to the
high-dimensional setting. Furthermore, the new loss depths highlight a
connection between the dataset and the right amount of complexity or simplicity
of the classifiers. The simplicity of classifiers as well as the interpretation
as a risk makes our new kind of data depth easy to explain, yet efficient for
anomaly detection, as is shown by experiments.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [174] [Dual-Attention U-Net++ with Class-Specific Ensembles and Bayesian Hyperparameter Optimization for Precise Wound and Scale Marker Segmentation](https://arxiv.org/abs/2507.05314)
*Daniel Cieślak,Miriam Reca,Olena Onyshchenko,Jacek Rumiński*

Main category: eess.IV

TL;DR: 提出了一种双注意力U-Net++架构，用于临床图像中伤口和比例标记的精确分割，通过集成通道和空间注意力机制解决类别不平衡问题，最终在基准测试中取得了高F1分数。


<details>
  <summary>Details</summary>
Motivation: 临床图像中伤口和比例标记的精确分割对伤口管理和自动化评估至关重要，但现有方法面临类别不平衡和图像变异性的挑战。

Method: 采用双注意力U-Net++架构，结合SCSE和空间注意力机制，通过EfficientNet-B7作为编码器骨干，使用数据增强和贝叶斯超参数调优，最终通过模型集成和测试时间增强提升性能。

Result: 在NBC 2025 & PCBBE 2025竞赛的基准数据集上，加权F1分数达到0.8640。

Conclusion: 提出的方法在复杂医学分割任务中表现出色，验证了其有效性。

Abstract: Accurate segmentation of wounds and scale markers in clinical images remainsa
significant challenge, crucial for effective wound management and
automatedassessment. In this study, we propose a novel dual-attention U-Net++
archi-tecture, integrating channel-wise (SCSE) and spatial attention mechanisms
toaddress severe class imbalance and variability in medical images
effectively.Initially, extensive benchmarking across diverse architectures and
encoders via 5-fold cross-validation identified EfficientNet-B7 as the optimal
encoder backbone.Subsequently, we independently trained two class-specific
models with tailoredpreprocessing, extensive data augmentation, and Bayesian
hyperparameter tun-ing (WandB sweeps). The final model ensemble utilized Test
Time Augmentationto further enhance prediction reliability. Our approach was
evaluated on a bench-mark dataset from the NBC 2025 & PCBBE 2025 competition.
Segmentationperformance was quantified using a weighted F1-score (75% wounds,
25% scalemarkers), calculated externally by competition organizers on
undisclosed hard-ware. The proposed approach achieved an F1-score of 0.8640,
underscoring itseffectiveness for complex medical segmentation tasks.

</details>


### [175] [Raptor: Scalable Train-Free Embeddings for 3D Medical Volumes Leveraging Pretrained 2D Foundation Models](https://arxiv.org/abs/2507.08254)
*Ulzee An,Moonseong Jeong,Simon A. Lee,Aditya Gorla,Yuzhe Yang,Sriram Sankararaman*

Main category: eess.IV

TL;DR: Raptor是一种无需训练的方法，通过随机平面张量降维生成体积数据的语义丰富嵌入，显著降低计算复杂度并保持语义信息。


<details>
  <summary>Details</summary>
Motivation: 解决体积成像数据（如MRI）基础模型开发中的计算复杂性和数据不足问题。

Method: 利用预训练的2D基础模型提取医学体积数据的视觉标记，并通过随机投影进行空间压缩。

Result: 在十项医学体积任务中表现优于现有方法（性能提升3%-14%），且无需昂贵训练。

Conclusion: Raptor是推进医学体积深度学习方法的高效且多功能基础。

Abstract: Current challenges in developing foundational models for volumetric imaging
data, such as magnetic resonance imaging (MRI), stem from the computational
complexity of training state-of-the-art architectures in high dimensions and
curating sufficiently large datasets of volumes. To address these challenges,
we introduce Raptor (Random Planar Tensor Reduction), a train-free method for
generating semantically rich embeddings for volumetric data. Raptor leverages a
frozen 2D foundation model, pretrained on natural images, to extract visual
tokens from individual cross-sections of medical volumes. These tokens are then
spatially compressed using random projections, significantly reducing
computational complexity while retaining semantic information. Extensive
experiments on ten diverse medical volume tasks verify the superior performance
of Raptor over state-of-the-art methods, including those pretrained exclusively
on medical volumes (+3% SuPreM, +6% MISFM, +10% Merlin, +13% VoCo, and +14%
SLIViT), while entirely bypassing the need for costly training. Our results
highlight the effectiveness and versatility of Raptor as a foundation for
advancing deep learning-based methods for medical volumes.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [176] [EP-GAT: Energy-based Parallel Graph Attention Neural Network for Stock Trend Classification](https://arxiv.org/abs/2507.08184)
*Zhuodong Jiang,Pengju Zhang,Peter Martin*

Main category: cs.CE

TL;DR: 提出了一种基于能量差异和Boltzmann分布的动态股票图生成方法，结合并行图注意力机制，用于预测多只股票的未来走势。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态或手动定义的因子建模股票间依赖关系，且难以保留股票内的层次特征。

Method: 通过能量差异和Boltzmann分布生成动态股票图，并设计并行图注意力机制保留层次特征。

Result: 在五个真实数据集上验证，EP-GAT在多种指标上优于五个基线方法。

Conclusion: EP-GAT有效解决了动态依赖和层次特征保留问题，实验验证了其优越性。

Abstract: Graph neural networks have shown remarkable performance in forecasting stock
movements, which arises from learning complex inter-dependencies between stocks
and intra-dynamics of stocks. Existing approaches based on graph neural
networks typically rely on static or manually defined factors to model changing
inter-dependencies between stocks. Furthermore, these works often struggle to
preserve hierarchical features within stocks. To bridge these gaps, this work
presents the Energy-based Parallel Graph Attention Neural Network, a novel
approach for predicting future movements for multiple stocks. First, it
generates a dynamic stock graph with the energy difference between stocks and
Boltzmann distribution, capturing evolving inter-dependencies between stocks.
Then, a parallel graph attention mechanism is proposed to preserve the
hierarchical intra-stock dynamics. Extensive experiments on five real-world
datasets are conducted to validate the proposed approach, spanning from the US
stock markets (NASDAQ, NYSE, SP) and UK stock markets (FTSE, LSE). The
experimental results demonstrate that EP-GAT consistently outperforms
competitive five baselines on test periods across various metrics. The ablation
studies and hyperparameter sensitivity analysis further validate the
effectiveness of each module in the proposed method.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [177] [Robust Semi-Supervised CT Radiomics for Lung Cancer Prognosis: Cost-Effective Learning with Limited Labels and SHAP Interpretation](https://arxiv.org/abs/2507.08189)
*Mohammad R. Salmanpour,Amir Hossein Pouria,Sonia Falahati,Shahram Taeb,Somayeh Sadat Mehrnia,Ali Fathi Jouzdani,Mehrdad Oveisi,Ilker Hacihaliloglu,Arman Rahmim*

Main category: physics.med-ph

TL;DR: 提出了一种半监督学习框架，用于肺癌CT影像的生存预测，显著优于监督学习，并提高了模型的稳定性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 监督学习在肺癌CT影像分析中需要大量标注数据，限制了其在标注稀缺场景的应用。

Method: 通过提取1218个放射组学特征，结合56种特征选择算法和27种分类器，构建半监督学习框架，利用伪标签技术整合未标注数据。

Result: 半监督学习比监督学习性能提升17%，最佳模型在交叉验证和外部测试中分别达到0.90和0.88的准确率，且在仅10%标注数据下表现稳定。

Conclusion: 该半监督学习框架具有成本效益、稳定性和可解释性，提升了肺癌生存预测的性能和临床适用性。

Abstract: Background: CT imaging is vital for lung cancer management, offering detailed
visualization for AI-based prognosis. However, supervised learning SL models
require large labeled datasets, limiting their real-world application in
settings with scarce annotations.
  Methods: We analyzed CT scans from 977 patients across 12 datasets extracting
1218 radiomics features using Laplacian of Gaussian and wavelet filters via
PyRadiomics Dimensionality reduction was applied with 56 feature selection and
extraction algorithms and 27 classifiers were benchmarked A semi supervised
learning SSL framework with pseudo labeling utilized 478 unlabeled and 499
labeled cases Model sensitivity was tested in three scenarios varying labeled
data in SL increasing unlabeled data in SSL and scaling both from 10 percent to
100 percent SHAP analysis was used to interpret predictions Cross validation
and external testing in two cohorts were performed.
  Results: SSL outperformed SL, improving overall survival prediction by up to
17 percent. The top SSL model, Random Forest plus XGBoost classifier, achieved
0.90 accuracy in cross-validation and 0.88 externally. SHAP analysis revealed
enhanced feature discriminability in both SSL and SL, especially for Class 1
survival greater than 4 years. SSL showed strong performance with only 10
percent labeled data, with more stable results compared to SL and lower
variance across external testing, highlighting SSL's robustness and cost
effectiveness.
  Conclusion: We introduced a cost-effective, stable, and interpretable SSL
framework for CT-based survival prediction in lung cancer, improving
performance, generalizability, and clinical readiness by integrating SHAP
explainability and leveraging unlabeled data.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [178] [CL3R: 3D Reconstruction and Contrastive Learning for Enhanced Robotic Manipulation Representations](https://arxiv.org/abs/2507.08262)
*Wenbo Cui,Chengyang Zhao,Yuhui Chen,Haoran Li,Zhizheng Zhang,Dongbin Zhao,He Wang*

Main category: cs.RO

TL;DR: CL3R是一种新型3D预训练框架，通过结合空间感知和语义理解，提升机器人操作策略的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练的2D基础模型，但缺乏3D空间信息和对多视角的泛化能力，限制了策略在精细操作中的效果。

Method: CL3R采用点云掩码自编码器学习3D表示，并通过对比学习结合2D模型的语义知识，同时提出多视角点云随机融合以提升泛化能力。

Result: 实验表明，CL3R在仿真和真实场景中均表现出色，显著提升了机器人操作的感知能力。

Conclusion: CL3R通过统一的3D预训练框架，有效解决了视角模糊问题，为机器人操作策略提供了更鲁棒的感知模块。

Abstract: Building a robust perception module is crucial for visuomotor policy
learning. While recent methods incorporate pre-trained 2D foundation models
into robotic perception modules to leverage their strong semantic
understanding, they struggle to capture 3D spatial information and generalize
across diverse camera viewpoints. These limitations hinder the policy's
effectiveness, especially in fine-grained robotic manipulation scenarios. To
address these challenges, we propose CL3R, a novel 3D pre-training framework
designed to enhance robotic manipulation policies. Our method integrates both
spatial awareness and semantic understanding by employing a point cloud Masked
Autoencoder to learn rich 3D representations while leveraging pre-trained 2D
foundation models through contrastive learning for efficient semantic knowledge
transfer. Additionally, we propose a 3D visual representation pre-training
framework for robotic tasks. By unifying coordinate systems across datasets and
introducing random fusion of multi-view point clouds, we mitigate camera view
ambiguity and improve generalization, enabling robust perception from novel
viewpoints at test time. Extensive experiments in both simulation and the real
world demonstrate the superiority of our method, highlighting its effectiveness
in visuomotor policy learning for robotic manipulation.

</details>


### [179] [Intelligent Control of Spacecraft Reaction Wheel Attitude Using Deep Reinforcement Learning](https://arxiv.org/abs/2507.08366)
*Ghaith El-Dalahmeh,Mohammad Reza Jabbarpour,Bao Quoc Vo,Ryszard Kowalczyk*

Main category: cs.RO

TL;DR: 提出了一种基于深度强化学习（DRL）的卫星姿态控制策略TD3-HD，结合了Twin Delayed Deep Deterministic Policy Gradient (TD3)、Hindsight Experience Replay (HER)和Dimension Wise Clipping (DWC)，显著提升了卫星在反应轮故障下的适应性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 卫星在动态和不确定环境中自主运行时，传统PD控制器和现有DRL算法（如TD3、PPO、A2C）在实时适应性和容错性方面表现不足，需要一种更强大的控制策略。

Method: 提出TD3-HD方法，结合TD3、HER和DWC，优化稀疏奖励环境下的学习能力，并在反应轮故障时保持卫星稳定性。

Result: 实验表明，TD3-HD在姿态误差、角速度调节和故障条件下的稳定性方面显著优于PD控制器和其他DRL算法。

Conclusion: TD3-HD是一种强大的、具有容错能力的机载AI解决方案，适用于自主卫星姿态控制。

Abstract: Reliable satellite attitude control is essential for the success of space
missions, particularly as satellites increasingly operate autonomously in
dynamic and uncertain environments. Reaction wheels (RWs) play a pivotal role
in attitude control, and maintaining control resilience during RW faults is
critical to preserving mission objectives and system stability. However,
traditional Proportional Derivative (PD) controllers and existing deep
reinforcement learning (DRL) algorithms such as TD3, PPO, and A2C often fall
short in providing the real time adaptability and fault tolerance required for
autonomous satellite operations. This study introduces a DRL-based control
strategy designed to improve satellite resilience and adaptability under fault
conditions. Specifically, the proposed method integrates Twin Delayed Deep
Deterministic Policy Gradient (TD3) with Hindsight Experience Replay (HER) and
Dimension Wise Clipping (DWC) referred to as TD3-HD to enhance learning in
sparse reward environments and maintain satellite stability during RW failures.
The proposed approach is benchmarked against PD control and leading DRL
algorithms. Experimental results show that TD3-HD achieves significantly lower
attitude error, improved angular velocity regulation, and enhanced stability
under fault conditions. These findings underscore the proposed method potential
as a powerful, fault tolerant, onboard AI solution for autonomous satellite
attitude control.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [180] [AgentsNet: Coordination and Collaborative Reasoning in Multi-Agent LLMs](https://arxiv.org/abs/2507.08616)
*Florian Grötschla,Luis Müller,Jan Tönshoff,Mikhail Galkin,Bryan Perozzi*

Main category: cs.MA

TL;DR: AgentsNet是一个新的多智能体推理基准，用于评估智能体系统在协作、自组织和通信方面的能力，并测试其在不同网络规模下的表现。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在解决复杂问题方面表现出强大能力，但其自组织和协作能力尚不明确，需要新的评估方法。

Method: 通过借鉴分布式系统和图论中的经典问题，设计AgentsNet基准，评估智能体系统在协作策略、自组织和通信方面的表现。

Result: 前沿大语言模型在小规模网络中表现良好，但随着网络规模扩大性能下降。AgentsNet支持无限扩展，测试了多达100个智能体的系统。

Conclusion: AgentsNet为多智能体系统的协作能力提供了新的评估工具，揭示了现有模型在规模扩展时的局限性。

Abstract: Large-language models (LLMs) have demonstrated powerful problem-solving
capabilities, in particular when organized in multi-agent systems. However, the
advent of such systems also raises several questions on the ability of a
complex network of agents to effectively self-organize and collaborate. While
measuring performance on standard reasoning benchmarks indicates how well
multi-agent systems can solve reasoning tasks, it is unclear whether these
systems are able to leverage their topology effectively. Here, we propose
AgentsNet, a new benchmark for multi-agent reasoning. By drawing inspiration
from classical problems in distributed systems and graph theory, AgentsNet
measures the ability of multi-agent systems to collaboratively form strategies
for problem-solving, self-organization, and effective communication given a
network topology. We evaluate a variety of baseline methods on AgentsNet
including homogeneous networks of agents which first have to agree on basic
protocols for organization and communication. We find that some frontier LLMs
are already demonstrating strong performance for small networks but begin to
fall off once the size of the network scales. While existing multi-agent
benchmarks cover at most 2-5 agents, AgentsNet is practically unlimited in size
and can scale with new generations of LLMs. As such, we also probe frontier
models in a setup with up to 100 agents.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [181] [Generative AI in Science: Applications, Challenges, and Emerging Questions](https://arxiv.org/abs/2507.08310)
*Ryan Harries,Cornelia Lawson,Philip Shapira*

Main category: cs.CY

TL;DR: 本文通过定性文献综述探讨了生成式人工智能（GenAI）对科学实践的影响，分析了其应用、益处和挑战。


<details>
  <summary>Details</summary>
Motivation: 研究GenAI在科学领域的快速应用及其潜在影响，以填补当前对其长期意义和治理的不确定性。

Method: 基于OpenAlex数据库，采用布尔搜索方法筛选并定性编码39篇高引用论文和评论。

Result: GenAI在科学、科学写作、医疗实践和教育培训中应用广泛，但其长期影响和治理仍不明确。

Conclusion: 研究提供了GenAI在科学中作用的早期见解，并提出了未来研究方向。

Abstract: This paper examines the impact of Generative Artificial Intelligence (GenAI)
on scientific practices, conducting a qualitative review of selected literature
to explore its applications, benefits, and challenges. The review draws on the
OpenAlex publication database, using a Boolean search approach to identify
scientific literature related to GenAI (including large language models and
ChatGPT). Thirty-nine highly cited papers and commentaries are reviewed and
qualitatively coded. Results are categorized by GenAI applications in science,
scientific writing, medical practice, and education and training. The analysis
finds that while there is a rapid adoption of GenAI in science and science
practice, its long-term implications remain unclear, with ongoing uncertainties
about its use and governance. The study provides early insights into GenAI's
growing role in science and identifies questions for future research in this
evolving field.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [182] [A Versatile Dataset of Mouse and Eye Movements on Search Engine Results Pages](https://arxiv.org/abs/2507.08003)
*Kayhan Latifzadeh,Jacek Gwizdka,Luis A. Leiva*

Main category: cs.HC

TL;DR: 论文贡献了一个研究用户注意力和购买行为的综合数据集，使用眼动仪构建客观的视觉注意力数据，弥补了以往依赖鼠标移动和自我报告标签的不足。


<details>
  <summary>Details</summary>
Motivation: 解决以往研究中依赖鼠标移动和自我报告标签的不准确性和偏差问题，提供更客观的用户注意力数据。

Method: 使用眼动仪收集47名参与者在Google搜索结果页面上的2,776次交易查询数据，包括HTML、截图、眼动和鼠标移动数据等。

Result: 构建了一个包含多种数据类型的数据集，并提供了基线实验（分类任务）以启发未来研究。

Conclusion: 该数据集为研究用户注意力和购买行为提供了更客观的基础，并展示了未来研究的多种可能性。

Abstract: We contribute a comprehensive dataset to study user attention and purchasing
behavior on Search Engine Result Pages (SERPs). Previous work has relied on
mouse movements as a low-cost large-scale behavioral proxy but also has relied
on self-reported ground-truth labels, collected at post-task, which can be
inaccurate and prone to biases. To address this limitation, we use an eye
tracker to construct an objective ground-truth of continuous visual attention.
Our dataset comprises 2,776 transactional queries on Google SERPs, collected
from 47 participants, and includes: (1) HTML source files, with CSS and images;
(2) rendered SERP screenshots; (3) eye movement data; (4) mouse movement data;
(5) bounding boxes of direct display and organic advertisements; and (6)
scripts for further preprocessing the data. In this paper we provide an
overview of the dataset and baseline experiments (classification tasks) that
can inspire researchers about the different possibilities for future work.

</details>


### [183] [Human vs. LLM-Based Thematic Analysis for Digital Mental Health Research: Proof-of-Concept Comparative Study](https://arxiv.org/abs/2507.08002)
*Karisa Parkington,Bazen G. Teferra,Marianne Rouleau-Tang,Argyrios Perivolaris,Alice Rueda,Adam Dubrowski,Bill Kapralos,Reza Samavi,Andrew Greenshaw,Yanbo Zhang,Bo Cao,Yuqi Wu,Sirisha Rambhatla,Sridhar Krishnan,Venkat Bhat*

Main category: cs.HC

TL;DR: 研究比较了基于大语言模型（LLM）和传统人工方法的主题分析在心理健康访谈中的应用，发现LLM更具成本效益但缺乏深度。


<details>
  <summary>Details</summary>
Motivation: 主题分析在大型医疗研究中因资源密集而受限，LLM可规模化分析文本，但其在心理健康访谈中的应用需与传统方法对比。

Method: 使用OpenAI的GPT-4o模型和RISEN提示框架，与传统人工分析（Dedoose）对比，评估编码、饱和点和主题合成。

Result: LLM的RISEN框架生成与人类相似的演绎父代码，但人类在归纳子代码和主题合成上更优。知识型LLM编码饱和所需转录本更少。

Conclusion: LLM在心理健康研究中可结合人工监督，平衡参与者视角和研究资源，但需补充人类分析的深度。

Abstract: Thematic analysis provides valuable insights into participants' experiences
through coding and theme development, but its resource-intensive nature limits
its use in large healthcare studies. Large language models (LLMs) can analyze
text at scale and identify key content automatically, potentially addressing
these challenges. However, their application in mental health interviews needs
comparison with traditional human analysis. This study evaluates out-of-the-box
and knowledge-base LLM-based thematic analysis against traditional methods
using transcripts from a stress-reduction trial with healthcare workers.
OpenAI's GPT-4o model was used along with the Role, Instructions, Steps,
End-Goal, Narrowing (RISEN) prompt engineering framework and compared to human
analysis in Dedoose. Each approach developed codes, noted saturation points,
applied codes to excerpts for a subset of participants (n = 20), and
synthesized data into themes. Outputs and performance metrics were compared
directly. LLMs using the RISEN framework developed deductive parent codes
similar to human codes, but humans excelled in inductive child code development
and theme synthesis. Knowledge-based LLMs reached coding saturation with fewer
transcripts (10-15) than the out-of-the-box model (15-20) and humans (90-99).
The out-of-the-box LLM identified a comparable number of excerpts to human
researchers, showing strong inter-rater reliability (K = 0.84), though the
knowledge-based LLM produced fewer excerpts. Human excerpts were longer and
involved multiple codes per excerpt, while LLMs typically applied one code.
Overall, LLM-based thematic analysis proved more cost-effective but lacked the
depth of human analysis. LLMs can transform qualitative analysis in mental
healthcare and clinical research when combined with human oversight to balance
participant perspectives and research resources.

</details>


### [184] [SSSUMO: Real-Time Semi-Supervised Submovement Decomposition](https://arxiv.org/abs/2507.08028)
*Evgenii Rudakov,Jonathan Shock,Otto Lappi,Benjamin Ultan Cowley*

Main category: cs.HC

TL;DR: SSSUMO是一种半监督深度学习方法，用于子运动分解，在精度和速度上达到最优水平。


<details>
  <summary>Details</summary>
Motivation: 现有方法在重建精度、计算成本和验证方面存在困难，主要由于手工标记数据难以获取。

Method: 采用半监督学习框架，从基于最小抖动原理生成的合成数据开始，逐步适应未标记的人类运动数据。

Result: 在合成和多样化人类运动数据集上显著优于现有方法，且能在高噪声条件下实时运行。

Conclusion: 该方法在多个领域（如人机交互、康复医学）具有广泛应用潜力。

Abstract: This paper introduces a SSSUMO, semi-supervised deep learning approach for
submovement decomposition that achieves state-of-the-art accuracy and speed.
While submovement analysis offers valuable insights into motor control,
existing methods struggle with reconstruction accuracy, computational cost, and
validation, due to the difficulty of obtaining hand-labeled data. We address
these challenges using a semi-supervised learning framework. This framework
learns from synthetic data, initially generated from minimum-jerk principles
and then iteratively refined through adaptation to unlabeled human movement
data. Our fully convolutional architecture with differentiable reconstruction
significantly surpasses existing methods on both synthetic and diverse human
motion datasets, demonstrating robustness even in high-noise conditions.
Crucially, the model operates in real-time (less than a millisecond per input
second), a substantial improvement over optimization-based techniques. This
enhanced performance facilitates new applications in human-computer
interaction, rehabilitation medicine, and motor control studies. We demonstrate
the model's effectiveness across diverse human-performed tasks such as
steering, rotation, pointing, object moving, handwriting, and mouse-controlled
gaming, showing notable improvements particularly on challenging datasets where
traditional methods largely fail. Training and benchmarking source code, along
with pre-trained model weights, are made publicly available at
https://github.com/dolphin-in-a-coma/sssumo.

</details>


### [185] [Emotion Detection in Older Adults Using Physiological Signals from Wearable Sensors](https://arxiv.org/abs/2507.08167)
*Md. Saif Hassan Onim,Andrew M. Kiselica,Himanshu Thapliyal*

Main category: cs.HC

TL;DR: 本文研究了基于边缘计算的非侵入性情绪识别方法，仅使用可穿戴传感器获取的生理信号，避免了摄像头或面部分析的需求。


<details>
  <summary>Details</summary>
Motivation: 了解老年人的认知和情感状态对其福祉至关重要，尤其是在医院和辅助生活环境中。

Method: 使用Empatica E4和Shimmer3 GSR+腕带获取生理信号，结合iMotion的面部表情分析模块记录情绪状态，通过经典机器学习模型预测情绪强度。

Result: 在回归任务中取得了最高的0.782 R2分数和最低的0.0006 MSE。

Conclusion: 该方法为阿尔茨海默病及相关痴呆症患者以及创伤后应激障碍患者提供了一种隐私保护且高效的情绪识别解决方案。

Abstract: Emotion detection in older adults is crucial for understanding their
cognitive and emotional well-being, especially in hospital and assisted living
environments. In this work, we investigate an edge-based, non-obtrusive
approach to emotion identification that uses only physiological signals
obtained via wearable sensors. Our dataset includes data from 40 older
individuals. Emotional states were obtained using physiological signals from
the Empatica E4 and Shimmer3 GSR+ wristband and facial expressions were
recorded using camera-based emotion recognition with the iMotion's Facial
Expression Analysis (FEA) module. The dataset also contains twelve emotion
categories in terms of relative intensities. We aim to study how well emotion
recognition can be accomplished using simply physiological sensor data, without
the requirement for cameras or intrusive facial analysis. By leveraging
classical machine learning models, we predict the intensity of emotional
responses based on physiological signals. We achieved the highest 0.782 r2
score with the lowest 0.0006 MSE on the regression task. This method has
significant implications for individuals with Alzheimer's Disease and Related
Dementia (ADRD), as well as veterans coping with Post-Traumatic Stress Disorder
(PTSD) or other cognitive impairments. Our results across multiple classical
regression models validate the feasibility of this method, paving the way for
privacy-preserving and efficient emotion recognition systems in real-world
settings.

</details>


### [186] [Adaptive Framework for Ambient Intelligence in Rehabilitation Assistance](https://arxiv.org/abs/2507.08624)
*Gábor Baranyi,Zsolt Csibi,Kristian Fenech,Áron Fóthi,Zsófia Gaál,Joul Skaf,András Lőrincz*

Main category: cs.HC

TL;DR: AIRS框架是一种基于AI的家庭康复支持系统，整合了实时3D重建、智能导航和视觉语言模型，为膝关节置换术后康复提供机器引导。


<details>
  <summary>Details</summary>
Motivation: 解决家庭康复环境中缺乏专业指导和隐私保护的问题，同时适应视觉和听觉障碍患者的需求。

Method: 结合RT-3DR、智能导航和VLMs，通过智能手机生成3D重建和虚拟化身，提供视觉和语言反馈。

Result: 系统成功应用于263个视频记录的评估，优化了运动配置并解决了隐私问题。

Conclusion: AIRS是一种模块化、适应性强的康复支持框架，具有广泛的应用潜力。

Abstract: This paper introduces the Ambient Intelligence Rehabilitation Support (AIRS)
framework, an advanced artificial intelligence-based solution tailored for home
rehabilitation environments. AIRS integrates cutting-edge technologies,
including Real-Time 3D Reconstruction (RT-3DR), intelligent navigation, and
large Vision-Language Models (VLMs), to create a comprehensive system for
machine-guided physical rehabilitation. The general AIRS framework is
demonstrated in rehabilitation scenarios following total knee replacement
(TKR), utilizing a database of 263 video recordings for evaluation. A
smartphone is employed within AIRS to perform RT-3DR of living spaces and has a
body-matched avatar to provide visual feedback about the excercise. This avatar
is necessary in (a) optimizing exercise configurations, including camera
placement, patient positioning, and initial poses, and (b) addressing privacy
concerns and promoting compliance with the AI Act. The system guides users
through the recording process to ensure the collection of properly recorded
videos. AIRS employs two feedback mechanisms: (i) visual 3D feedback, enabling
direct comparisons between prerecorded clinical exercises and patient home
recordings and (ii) VLM-generated feedback, providing detailed explanations and
corrections for exercise errors. The framework also supports people with visual
and hearing impairments. It also features a modular design that can be adapted
to broader rehabilitation contexts. AIRS software components are available for
further use and customization.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [187] [Transfer Learning and Mixup for Fine-Grained Few-Shot Fungi Classification](https://arxiv.org/abs/2507.08248)
*Jason Kahei Tam,Murilo Gustineli,Anthony Miyaguchi*

Main category: cs.CV

TL;DR: 本文介绍了团队DS@GT在FungiCLEF 2025竞赛中采用的基于视觉Transformer的方法，用于真菌物种的细粒度视觉分类。


<details>
  <summary>Details</summary>
Motivation: 真菌物种识别在计算机视觉中具有挑战性，因物种间差异细微而物种内差异显著。

Method: 实验了多种视觉Transformer模型、数据增强、加权采样及文本信息融合，并探索了生成式AI的零样本分类。

Result: 最终模型表现优于竞赛基线，但生成式AI表现不佳。团队排名35/74，表明在元数据选择和跨模态学习上仍有改进空间。

Conclusion: 领域特定预训练和平衡采样策略有效，未来需优化元数据选择和跨模态学习。

Abstract: Accurate identification of fungi species presents a unique challenge in
computer vision due to fine-grained inter-species variation and high
intra-species variation. This paper presents our approach for the FungiCLEF
2025 competition, which focuses on few-shot fine-grained visual categorization
(FGVC) using the FungiTastic Few-Shot dataset. Our team (DS@GT) experimented
with multiple vision transformer models, data augmentation, weighted sampling,
and incorporating textual information. We also explored generative AI models
for zero-shot classification using structured prompting but found them to
significantly underperform relative to vision-based models. Our final model
outperformed both competition baselines and highlighted the effectiveness of
domain specific pretraining and balanced sampling strategies. Our approach
ranked 35/74 on the private test set in post-completion evaluation, this
suggests additional work can be done on metadata selection and domain-adapted
multi-modal learning. Our code is available at
https://github.com/dsgt-arc/fungiclef-2025.

</details>


### [188] [Towards Evaluating Robustness of Prompt Adherence in Text to Image Models](https://arxiv.org/abs/2507.08039)
*Sujith Vemishetty,Advitiya Arora,Anupama Sharma*

Main category: cs.CV

TL;DR: 本文提出了一种评估Text-to-Image模型的新框架，重点关注其对输入提示的遵循能力，并发现现有模型在生成简单二元图像时表现不佳。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在多领域表现出色，但多模态LLMs和Text-to-Image模型的可靠性研究不足，需要更全面的评估方法。

Method: 创建新数据集评估模型鲁棒性，利用gpt-4o生成文本描述作为基准，通过比较生成图像与基准描述差异来评估模型性能。

Result: 模型在生成简单二元图像（如几何形状和位置）时表现不佳，且无法遵循输入数据分布。

Conclusion: 现有Text-to-Image模型在遵循提示和生成特定分布图像方面存在显著局限性，需进一步改进。

Abstract: The advancements in the domain of LLMs in recent years have surprised many,
showcasing their remarkable capabilities and diverse applications. Their
potential applications in various real-world scenarios have led to significant
research on their reliability and effectiveness. On the other hand, multimodal
LLMs and Text-to-Image models have only recently gained prominence, especially
when compared to text-only LLMs. Their reliability remains constrained due to
insufficient research on assessing their performance and robustness. This paper
aims to establish a comprehensive evaluation framework for Text-to-Image
models, concentrating particularly on their adherence to prompts. We created a
novel dataset that aimed to assess the robustness of these models in generating
images that conform to the specified factors of variation in the input text
prompts. Our evaluation studies present findings on three variants of Stable
Diffusion models: Stable Diffusion 3 Medium, Stable Diffusion 3.5 Large, and
Stable Diffusion 3.5 Large Turbo, and two variants of Janus models: Janus Pro
1B and Janus Pro 7B. We introduce a pipeline that leverages text descriptions
generated by the gpt-4o model for our ground-truth images, which are then used
to generate artificial images by passing these descriptions to the
Text-to-Image models. We then pass these generated images again through gpt-4o
using the same system prompt and compare the variation between the two
descriptions. Our results reveal that these models struggle to create simple
binary images with only two factors of variation: a simple geometric shape and
its location. We also show, using pre-trained VAEs on our dataset, that they
fail to generate images that follow our input dataset distribution.

</details>


### [189] [ConsNoTrainLoRA: Data-driven Weight Initialization of Low-rank Adapters using Constraints](https://arxiv.org/abs/2507.08044)
*Debasmit Das,Hyoungwoo Park,Munawar Hayat,Seokeon Choi,Sungrack Yun,Fatih Porikli*

Main category: cs.CV

TL;DR: 提出了一种名为CNTLoRA的数据驱动权重初始化方法，用于改进LoRA微调的收敛性和性能。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA权重矩阵随机初始化且固定秩，限制了性能提升。

Method: 将LoRA初始化视为域偏移问题，利用预训练和微调激活的约束关系，提出无需训练的闭式权重估计方法，并支持可变秩初始化。

Result: 在图像生成、分类和理解等任务中，CNTLoRA优于标准及数据驱动初始化方法。

Conclusion: CNTLoRA提供了更快的收敛和更好的性能，并通过分析验证了其设计选择的合理性。

Abstract: Foundation models are pre-trained on large-scale datasets and subsequently
fine-tuned on small-scale datasets using parameter-efficient fine-tuning (PEFT)
techniques like low-rank adapters (LoRA). In most previous works, LoRA weight
matrices are randomly initialized with a fixed rank across all attachment
points. In this paper, we improve convergence and final performance of LoRA
fine-tuning, using our proposed data-driven weight initialization method,
ConsNoTrainLoRA (CNTLoRA). We express LoRA initialization as a domain shift
problem where we use multiple constraints relating the pre-training and
fine-tuning activations. By reformulating these constraints, we obtain a
closed-form estimate of LoRA weights that depends on pre-training weights and
fine-tuning activation vectors and hence requires no training during
initialization. This weight estimate is decomposed to initialize the up and
down matrices with proposed flexibility of variable ranks. With the proposed
initialization method, we fine-tune on downstream tasks such as image
generation, image classification and image understanding. Both quantitative and
qualitative results demonstrate that CNTLoRA outperforms standard and
data-driven weight initialization methods. Extensive analyses and ablations
further elucidate the design choices of our framework, providing an optimal
recipe for faster convergence and enhanced performance.

</details>


### [190] [An Object-Based Deep Learning Approach for Building Height Estimation from Single SAR Images](https://arxiv.org/abs/2507.08096)
*Babak Memar,Luigi Russo,Silvia Liberata Ullo,Paolo Gamba*

Main category: cs.CV

TL;DR: 本文提出了一种基于深度学习的自动化方法，利用单张高分辨率SAR图像估计建筑物高度，并在多洲数据集上验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 高分辨率SAR图像在城市应用中至关重要，但现有方法在跨城市和跨洲泛化能力上存在不足。

Method: 采用基于边界框检测和目标回归的深度学习模型，利用多洲数据集进行训练和交叉验证。

Result: 模型在欧洲城市表现优异（MAE约2.20米），但在亚洲城市泛化能力稍弱。

Conclusion: 深度学习在单张SAR图像的建筑物高度估计中具有强大的跨洲泛化潜力。

Abstract: Accurate estimation of building heights using very high resolution (VHR)
synthetic aperture radar (SAR) imagery is crucial for various urban
applications. This paper introduces a Deep Learning (DL)-based methodology for
automated building height estimation from single VHR COSMO-SkyMed images: an
object-based regression approach based on bounding box detection followed by
height estimation. This model was trained and evaluated on a unique
multi-continental dataset comprising eight geographically diverse cities across
Europe, North and South America, and Asia, employing a cross-validation
strategy to explicitly assess out-of-distribution (OOD) generalization. The
results demonstrate highly promising performance, particularly on European
cities where the model achieves a Mean Absolute Error (MAE) of approximately
one building story (2.20 m in Munich), significantly outperforming recent
state-of-the-art methods in similar OOD scenarios. Despite the increased
variability observed when generalizing to cities in other continents,
particularly in Asia with its distinct urban typologies and prevalence of
high-rise structures, this study underscores the significant potential of DL
for robust cross-city and cross-continental transfer learning in building
height estimation from single VHR SAR data.

</details>


### [191] [Temporally Consistent Amodal Completion for 3D Human-Object Interaction Reconstruction](https://arxiv.org/abs/2507.08137)
*Hyungjun Doh,Dong In Lee,Seunggeun Chi,Pin-Hao Huang,Kwonjoon Lee,Sangpil Kim,Karthik Ramani*

Main category: cs.CV

TL;DR: 提出了一种从单目视频重建动态人-物交互的新框架，解决遮挡和时间不一致性问题。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建方法假设静态物体或完全可见的动态主体，在遮挡场景中性能下降。

Method: 利用模态补全推断遮挡区域，结合时间上下文增强视频序列一致性，无需预定义模板。

Result: 在挑战性单目视频上验证，3D高斯泼溅显示处理遮挡和时间稳定性优于现有技术。

Conclusion: 该框架显著提升了动态场景中复杂细节的恢复能力。

Abstract: We introduce a novel framework for reconstructing dynamic human-object
interactions from monocular video that overcomes challenges associated with
occlusions and temporal inconsistencies. Traditional 3D reconstruction methods
typically assume static objects or full visibility of dynamic subjects, leading
to degraded performance when these assumptions are violated-particularly in
scenarios where mutual occlusions occur. To address this, our framework
leverages amodal completion to infer the complete structure of partially
obscured regions. Unlike conventional approaches that operate on individual
frames, our method integrates temporal context, enforcing coherence across
video sequences to incrementally refine and stabilize reconstructions. This
template-free strategy adapts to varying conditions without relying on
predefined models, significantly enhancing the recovery of intricate details in
dynamic scenes. We validate our approach using 3D Gaussian Splatting on
challenging monocular videos, demonstrating superior precision in handling
occlusions and maintaining temporal stability compared to existing techniques.

</details>


### [192] [NeuralOS: Towards Simulating Operating Systems via Neural Generative Models](https://arxiv.org/abs/2507.08800)
*Luke Rivard,Sun Sun,Hongyu Guo,Wenhu Chen,Yuntian Deng*

Main category: cs.CV

TL;DR: NeuralOS是一个神经框架，通过直接预测屏幕帧来模拟操作系统的图形用户界面（GUI），结合RNN和扩散渲染器，训练于Ubuntu XFCE数据集，能生成逼真的GUI序列。


<details>
  <summary>Details</summary>
Motivation: 为未来人机交互系统创建完全自适应的生成神经界面。

Method: 结合RNN跟踪计算机状态和扩散渲染器生成屏幕图像，训练于Ubuntu XFCE数据集。

Result: 成功渲染逼真GUI序列，准确捕捉鼠标交互，可靠预测状态转换（如应用启动）。

Conclusion: 尽管精确建模键盘交互仍有挑战，但NeuralOS为未来自适应神经界面迈出一步。

Abstract: We introduce NeuralOS, a neural framework that simulates graphical user
interfaces (GUIs) of operating systems by directly predicting screen frames in
response to user inputs such as mouse movements, clicks, and keyboard events.
NeuralOS combines a recurrent neural network (RNN), which tracks computer
state, with a diffusion-based neural renderer that generates screen images. The
model is trained on a large-scale dataset of Ubuntu XFCE recordings, which
include both randomly generated interactions and realistic interactions
produced by AI agents. Experiments show that NeuralOS successfully renders
realistic GUI sequences, accurately captures mouse interactions, and reliably
predicts state transitions like application launches. Although modeling
fine-grained keyboard interactions precisely remains challenging, NeuralOS
offers a step toward creating fully adaptive, generative neural interfaces for
future human-computer interaction systems.

</details>


### [193] [Interpretability-Aware Pruning for Efficient Medical Image Analysis](https://arxiv.org/abs/2507.08330)
*Nikita Malik,Pratinav Seth,Neeraj Kumar Singh,Chintan Chitroda,Vinay Kumar Sankarapu*

Main category: cs.CV

TL;DR: 提出了一种基于可解释性指导的剪枝框架，减少模型复杂度同时保持预测性能和透明度。


<details>
  <summary>Details</summary>
Motivation: 深度学习在医学图像分析中取得显著进展，但模型的大规模和缺乏透明度限制了其临床应用。

Method: 通过选择性保留每层最相关部分，实现有针对性的压缩，保持临床有意义的表示。

Result: 在多个医学图像分类基准测试中，该方法实现了高压缩率和最小精度损失。

Conclusion: 为医疗环境中轻量级、可解释模型的部署铺平了道路。

Abstract: Deep learning has driven significant advances in medical image analysis, yet
its adoption in clinical practice remains constrained by the large size and
lack of transparency in modern models. Advances in interpretability techniques
such as DL-Backtrace, Layer-wise Relevance Propagation, and Integrated
Gradients make it possible to assess the contribution of individual components
within neural networks trained on medical imaging tasks. In this work, we
introduce an interpretability-guided pruning framework that reduces model
complexity while preserving both predictive performance and transparency. By
selectively retaining only the most relevant parts of each layer, our method
enables targeted compression that maintains clinically meaningful
representations. Experiments across multiple medical image classification
benchmarks demonstrate that this approach achieves high compression rates with
minimal loss in accuracy, paving the way for lightweight, interpretable models
suited for real-world deployment in healthcare settings.

</details>


### [194] [CoCo-Bot: Energy-based Composable Concept Bottlenecks for Interpretable Generative Models](https://arxiv.org/abs/2507.08334)
*Sangwon Kim,In-su Jang,Pyongkun Kim,Kwang-Ju Kim*

Main category: cs.CV

TL;DR: CoCo-Bot是一种后处理、可组合的概念瓶颈生成模型，通过显式概念传递所有信息，无需辅助线索，提高了可控性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统生成概念瓶颈模型依赖辅助视觉线索补偿信息损失，损害了可解释性和组合性。

Method: 使用基于扩散的能量函数引导，支持后处理干预（如概念组合和否定）。

Result: 在CelebA-HQ数据集上实验显示，CoCo-Bot在保持视觉质量的同时，提高了概念级可控性和可解释性。

Conclusion: CoCo-Bot通过显式概念传递信息，消除了辅助线索的需求，提升了生成模型的可解释性和组合性。

Abstract: Concept Bottleneck Models (CBMs) provide interpretable and controllable
generative modeling by routing generation through explicit,
human-understandable concepts. However, previous generative CBMs often rely on
auxiliary visual cues at the bottleneck to compensate for information not
captured by the concepts, which undermines interpretability and
compositionality. We propose CoCo-Bot, a post-hoc, composable concept
bottleneck generative model that eliminates the need for auxiliary cues by
transmitting all information solely through explicit concepts. Guided by
diffusion-based energy functions, CoCo-Bot supports robust post-hoc
interventions-such as concept composition and negation-across arbitrary
concepts. Experiments using StyleGAN2 pre-trained on CelebA-HQ show that
CoCo-Bot improves concept-level controllability and interpretability, while
maintaining competitive visual quality.

</details>


### [195] [Single-Domain Generalization for Multimodal Cross-Cancer Prognosis via Dirac Rebalancer and Distribution Entanglement](https://arxiv.org/abs/2507.08340)
*Jia-Xuan Jiang,Jiashuai Liu,Hongtao Wu,Yifeng Wu,Zhong Wang,Qi Bi,Yefeng Zheng*

Main category: cs.CV

TL;DR: 本文揭示了多模态预后模型在跨癌症场景中泛化能力较差的问题，并提出了一种新的任务和两个模块（SDIR和CADE）来提升泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有多模态方法主要针对单一癌症类型，忽视了跨癌症泛化的挑战，而临床实践中需要模型的鲁棒性。

Method: 提出两个模块：SDIR通过稀疏化和稳定化增强弱模态信号；CADE通过合成目标域分布融合形态和基因表达特征。

Result: 在四种癌症类型的基准测试中表现出优越的泛化能力。

Conclusion: 为跨癌症多模态预后提供了实用的鲁棒性基础。

Abstract: Deep learning has shown remarkable performance in integrating multimodal data
for survival prediction. However, existing multimodal methods mainly focus on
single cancer types and overlook the challenge of generalization across
cancers. In this work, we are the first to reveal that multimodal prognosis
models often generalize worse than unimodal ones in cross-cancer scenarios,
despite the critical need for such robustness in clinical practice. To address
this, we propose a new task: Cross-Cancer Single Domain Generalization for
Multimodal Prognosis, which evaluates whether models trained on a single cancer
type can generalize to unseen cancers. We identify two key challenges: degraded
features from weaker modalities and ineffective multimodal integration. To
tackle these, we introduce two plug-and-play modules: Sparse Dirac Information
Rebalancer (SDIR) and Cancer-aware Distribution Entanglement (CADE). SDIR
mitigates the dominance of strong features by applying Bernoulli-based
sparsification and Dirac-inspired stabilization to enhance weaker modality
signals. CADE, designed to synthesize the target domain distribution, fuses
local morphological cues and global gene expression in latent space.
Experiments on a four-cancer-type benchmark demonstrate superior
generalization, laying the foundation for practical, robust cross-cancer
multimodal prognosis. Code is available at
https://github.com/HopkinsKwong/MCCSDG

</details>


### [196] [PanMatch: Unleashing the Potential of Large Vision Models for Unified Matching Models](https://arxiv.org/abs/2507.08400)
*Yongjian Zhang,Longguang Wang,Kunhong Li,Ye Zhang,Yun Wang,Liang Lin,Yulan Guo*

Main category: cs.CV

TL;DR: PanMatch是一个通用的基础模型，用于鲁棒的对应匹配，通过统一的2D位移估计框架支持多任务，无需任务特定架构或微调。


<details>
  <summary>Details</summary>
Motivation: 传统方法需要任务特定架构和领域微调，PanMatch旨在通过统一框架实现多任务集成，提升泛化能力。

Method: 提出基于2D位移估计的统一框架，利用大型视觉模型的特征提取器和跨域数据集（180万样本）预训练。

Result: PanMatch在跨任务评估中优于UniMatch和Flow-Anything，在任务特定基准上表现媲美SOTA，并在异常场景中展现零样本能力。

Conclusion: PanMatch通过统一框架和强大特征提取器，实现了多任务鲁棒匹配，尤其在异常场景中表现突出。

Abstract: This work presents PanMatch, a versatile foundation model for robust
correspondence matching. Unlike previous methods that rely on task-specific
architectures and domain-specific fine-tuning to support tasks like stereo
matching, optical flow or feature matching, our key insight is that any
two-frame correspondence matching task can be addressed within a 2D
displacement estimation framework using the same model weights. Such a
formulation eliminates the need for designing specialized unified architectures
or task-specific ensemble models. Instead, it achieves multi-task integration
by endowing displacement estimation algorithms with unprecedented
generalization capabilities. To this end, we highlight the importance of a
robust feature extractor applicable across multiple domains and tasks, and
propose the feature transformation pipeline that leverage all-purpose features
from Large Vision Models to endow matching baselines with zero-shot cross-view
matching capabilities. Furthermore, we assemble a cross-domain dataset with
near 1.8 million samples from stereo matching, optical flow, and feature
matching domains to pretrain PanMatch. We demonstrate the versatility of
PanMatch across a wide range of domains and downstream tasks using the same
model weights. Our model outperforms UniMatch and Flow-Anything on cross-task
evaluations, and achieves comparable performance to most state-of-the-art
task-specific algorithms on task-oriented benchmarks. Additionally, PanMatch
presents unprecedented zero-shot performance in abnormal scenarios, such as
rainy day and satellite imagery, where most existing robust algorithms fail to
yield meaningful results.

</details>


### [197] [Deep Hashing with Semantic Hash Centers for Image Retrieval](https://arxiv.org/abs/2507.08404)
*Li Chen,Rui Liu,Yuxiang Zhou,Xudong Ma,Yong Chen,Dell Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种基于语义哈希中心（SHC）的深度哈希方法，通过数据依赖的相似性计算和优化算法生成哈希中心，显著提升了大规模图像检索的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用数据无关算法生成哈希中心，忽略了类间语义关系，可能降低检索性能。SHC旨在通过语义哈希中心保留语义结构，提升检索效果。

Method: 1. 使用分类网络计算类间语义相似性；2. 通过优化算法生成语义哈希中心；3. 训练深度哈希网络生成二进制哈希码。

Result: 在多个公共数据集上，SHC在MAP@100、MAP@1000和MAP@ALL指标上分别平均提升了7.26%、7.62%和11.71%。

Conclusion: SHC通过语义哈希中心有效提升了大规模图像检索的性能，验证了语义关系在哈希码生成中的重要性。

Abstract: Deep hashing is an effective approach for large-scale image retrieval.
Current methods are typically classified by their supervision types:
point-wise, pair-wise, and list-wise. Recent point-wise techniques (e.g., CSQ,
MDS) have improved retrieval performance by pre-assigning a hash center to each
class, enhancing the discriminability of hash codes across various datasets.
However, these methods rely on data-independent algorithms to generate hash
centers, which neglect the semantic relationships between classes and may
degrade retrieval performance.
  This paper introduces the concept of semantic hash centers, building on the
idea of traditional hash centers. We hypothesize that hash centers of
semantically related classes should have closer Hamming distances, while those
of unrelated classes should be more distant. To this end, we propose a
three-stage framework, SHC, to generate hash codes that preserve semantic
structure.
  First, we develop a classification network to identify semantic similarities
between classes using a data-dependent similarity calculation that adapts to
varying data distributions. Second, we introduce an optimization algorithm to
generate semantic hash centers, preserving semantic relatedness while enforcing
a minimum distance between centers to avoid excessively similar hash codes.
Finally, a deep hashing network is trained using these semantic centers to
convert images into binary hash codes.
  Experimental results on large-scale retrieval tasks across several public
datasets show that SHC significantly improves retrieval performance.
Specifically, SHC achieves average improvements of +7.26%, +7.62%, and +11.71%
in MAP@100, MAP@1000, and MAP@ALL metrics, respectively, over state-of-the-art
methods.

</details>


### [198] [A Hybrid Multilayer Extreme Learning Machine for Image Classification with an Application to Quadcopters](https://arxiv.org/abs/2507.08047)
*Rolando A. Hernandez-Hernandez,Adrian Rubio-Solis*

Main category: cs.CV

TL;DR: 本文提出了一种基于ELM-AE和区间类型2模糊逻辑理论的混合多层极限学习机（HML-ELM），用于无人机图像分类。该方法通过无监督特征提取和监督分类两阶段实现高效分类，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有多层极限学习机（ML-ELM）及其变体在自然信号分类中表现良好，但需要进一步提升图像分类效率，尤其是在无人机应用中。

Method: HML-ELM采用两阶段框架：1）通过堆叠ELM-AE实现无监督特征提取；2）使用改进的SIT2-FELM进行分类。

Result: 实验表明，HML-ELM在基准图像分类和无人机实际应用中均优于ML-ELM、ML-FELM和ELM。

Conclusion: HML-ELM是一种高效的图像分类方法，特别适用于无人机应用。

Abstract: Multilayer Extreme Learning Machine (ML-ELM) and its variants have proven to
be an effective technique for the classification of different natural signals
such as audio, video, acoustic and images. In this paper, a Hybrid Multilayer
Extreme Learning Machine (HML-ELM) that is based on ELM-based autoencoder
(ELM-AE) and an Interval Type-2 fuzzy Logic theory is suggested for active
image classification and applied to Unmanned Aerial Vehicles (UAVs). The
proposed methodology is a hierarchical ELM learning framework that consists of
two main phases: 1) self-taught feature extraction and 2) supervised feature
classification. First, unsupervised multilayer feature encoding is achieved by
stacking a number of ELM-AEs, in which input data is projected into a number of
high-level representations. At the second phase, the final features are
classified using a novel Simplified Interval Type-2 Fuzzy ELM (SIT2-FELM) with
a fast output reduction layer based on the SC algorithm; an improved version of
the algorithm Center of Sets Type Reducer without Sorting Requirement
(COSTRWSR). To validate the efficiency of the HML-ELM, two types of experiments
for the classification of images are suggested. First, the HML-ELM is applied
to solve a number of benchmark problems for image classification. Secondly, a
number of real experiments to the active classification and transport of four
different objects between two predefined locations using a UAV is implemented.
Experiments demonstrate that the proposed HML-ELM delivers a superior
efficiency compared to other similar methodologies such as ML-ELM, Multilayer
Fuzzy Extreme Learning Machine (ML-FELM) and ELM.

</details>


### [199] [Lightweight Cloud Masking Models for On-Board Inference in Hyperspectral Imaging](https://arxiv.org/abs/2507.08052)
*Mazen Ali,António Pereira,Fabio Gentile,Aser Cortines,Sam Mugel,Román Orús,Stelios P. Neophytides,Michalis Mavrovouniotis*

Main category: cs.CV

TL;DR: 该研究评估了多种机器学习方法在云和云阴影掩模中的应用，发现CNN结合特征缩减在精度、存储和计算效率上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 云和云阴影掩模是高光谱卫星影像预处理的关键步骤，需要高效且准确的模型以支持实时处理。

Method: 研究比较了梯度提升方法（如XGBoost、LightGBM）和卷积神经网络（CNN），并测试了不同版本的CNN模型。

Result: 所有模型精度超过93%，其中特征缩减的CNN在精度、存储和计算效率上表现最优。

Conclusion: 轻量级AI模型（如特征缩减的CNN）适合实时高光谱影像处理，支持卫星AI系统的开发。

Abstract: Cloud and cloud shadow masking is a crucial preprocessing step in
hyperspectral satellite imaging, enabling the extraction of high-quality,
analysis-ready data. This study evaluates various machine learning approaches,
including gradient boosting methods such as XGBoost and LightGBM as well as
convolutional neural networks (CNNs). All boosting and CNN models achieved
accuracies exceeding 93%. Among the investigated models, the CNN with feature
reduction emerged as the most efficient, offering a balance of high accuracy,
low storage requirements, and rapid inference times on both CPUs and GPUs.
Variations of this version, with only up to 597 trainable parameters,
demonstrated the best trade-off in terms of deployment feasibility, accuracy,
and computational efficiency. These results demonstrate the potential of
lightweight artificial intelligence (AI) models for real-time hyperspectral
image processing, supporting the development of on-board satellite AI systems
for space-based applications.

</details>


### [200] [Vision Foundation Models as Effective Visual Tokenizers for Autoregressive Image Generation](https://arxiv.org/abs/2507.08441)
*Anlin Zheng,Xin Wen,Xuanyang Zhang,Chuofan Ma,Tiancai Wang,Gang Yu,Xiangyu Zhang,Xiaojuan Qi*

Main category: cs.CV

TL;DR: 利用预训练视觉基础模型构建图像分词器VFMTok，通过区域自适应量化和语义重建目标提升性能，显著改善图像重建与生成质量。


<details>
  <summary>Details</summary>
Motivation: 探索预训练视觉基础模型的新应用方向，构建高效图像分词器，填补该领域的研究空白。

Method: 采用冻结的视觉基础模型作为编码器，引入区域自适应量化框架和语义重建目标。

Result: VFMTok在图像重建和生成质量上显著提升，gFID达2.07，加速模型收敛三倍，无需CFG即可实现高保真类别条件合成。

Conclusion: VFMTok为图像分词器设计提供了高效解决方案，代码将公开以促进社区发展。

Abstract: Leveraging the powerful representations of pre-trained vision foundation
models -- traditionally used for visual comprehension -- we explore a novel
direction: building an image tokenizer directly atop such models, a largely
underexplored area. Specifically, we employ a frozen vision foundation model as
the encoder of our tokenizer. To enhance its effectiveness, we introduce two
key components: (1) a region-adaptive quantization framework that reduces
redundancy in the pre-trained features on regular 2D grids, and (2) a semantic
reconstruction objective that aligns the tokenizer's outputs with the
foundation model's representations to preserve semantic fidelity. Based on
these designs, our proposed image tokenizer, VFMTok, achieves substantial
improvements in image reconstruction and generation quality, while also
enhancing token efficiency. It further boosts autoregressive (AR) generation --
achieving a gFID of 2.07 on ImageNet benchmarks, while accelerating model
convergence by three times, and enabling high-fidelity class-conditional
synthesis without the need for classifier-free guidance (CFG). The code will be
released publicly to benefit the community.

</details>


### [201] [Review of Feed-forward 3D Reconstruction: From DUSt3R to VGGT](https://arxiv.org/abs/2507.08448)
*Wei Zhang,Yihang Wu,Songhua Li,Wenjie Ma,Xin Ma,Qiang Li,Qi Wang*

Main category: cs.CV

TL;DR: 该论文综述了基于深度学习的3D重建新范式，特别是DUSt3R等前馈模型，对比了传统方法和早期学习方法的局限性，并探讨了未来挑战。


<details>
  <summary>Details</summary>
Motivation: 传统3D重建方法（如SfM和MVS）存在计算成本高、鲁棒性差等问题，深度学习为这一领域带来了新的解决方案。

Method: 采用前馈模型（如DUSt3R），通过统一深度网络直接从图像中联合推断相机位姿和密集几何结构。

Result: 新方法在效率和鲁棒性上优于传统方法，但仍面临精度和动态场景处理的挑战。

Conclusion: 深度学习驱动的3D重建具有广阔应用前景，未来需解决模型精度、可扩展性和动态场景处理等问题。

Abstract: 3D reconstruction, which aims to recover the dense three-dimensional
structure of a scene, is a cornerstone technology for numerous applications,
including augmented/virtual reality, autonomous driving, and robotics. While
traditional pipelines like Structure from Motion (SfM) and Multi-View Stereo
(MVS) achieve high precision through iterative optimization, they are limited
by complex workflows, high computational cost, and poor robustness in
challenging scenarios like texture-less regions. Recently, deep learning has
catalyzed a paradigm shift in 3D reconstruction. A new family of models,
exemplified by DUSt3R, has pioneered a feed-forward approach. These models
employ a unified deep network to jointly infer camera poses and dense geometry
directly from an Unconstrained set of images in a single forward pass. This
survey provides a systematic review of this emerging domain. We begin by
dissecting the technical framework of these feed-forward models, including
their Transformer-based correspondence modeling, joint pose and geometry
regression mechanisms, and strategies for scaling from two-view to multi-view
scenarios. To highlight the disruptive nature of this new paradigm, we contrast
it with both traditional pipelines and earlier learning-based methods like
MVSNet. Furthermore, we provide an overview of relevant datasets and evaluation
metrics. Finally, we discuss the technology's broad application prospects and
identify key future challenges and opportunities, such as model accuracy and
scalability, and handling dynamic scenes.

</details>


### [202] [Adaptive Diffusion Denoised Smoothing : Certified Robustness via Randomized Smoothing with Differentially Private Guided Denoising Diffusion](https://arxiv.org/abs/2507.08163)
*Frederick Shpilevskiy,Saiyue Lyu,Krishnamurthy Dj Dvijotham,Mathias Lécuyer,Pierre-André Noël*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We propose Adaptive Diffusion Denoised Smoothing, a method for certifying the
predictions of a vision model against adversarial examples, while adapting to
the input. Our key insight is to reinterpret a guided denoising diffusion model
as a long sequence of adaptive Gaussian Differentially Private (GDP) mechanisms
refining a pure noise sample into an image. We show that these adaptive
mechanisms can be composed through a GDP privacy filter to analyze the
end-to-end robustness of the guided denoising process, yielding a provable
certification that extends the adaptive randomized smoothing analysis. We
demonstrate that our design, under a specific guiding strategy, can improve
both certified accuracy and standard accuracy on ImageNet for an $\ell_2$
threat model.

</details>


### [203] [A document is worth a structured record: Principled inductive bias design for document recognition](https://arxiv.org/abs/2507.08458)
*Benjamin Meyer,Lukas Tuggener,Sascha Hänzi,Daniel Schmid,Erdal Ayfer,Benjamin F. Grewe,Ahmed Abdulkadir,Thilo Stadelmann*

Main category: cs.CV

TL;DR: 论文提出了一种将文档识别视为转录任务的新视角，利用文档固有结构设计归纳偏置，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有文档识别方法忽视文档类型的固有结构，导致对复杂或低频文档类型的识别效果不佳。

Method: 提出基于文档固有结构的归纳偏置设计方法，并采用基础Transformer架构适配不同结构。

Result: 实验证明该方法在单音乐谱、形状图纸和简化工程图纸等复杂结构上有效，首次实现了工程图纸的端到端转录。

Conclusion: 该方法为未来文档基础模型的设计提供了统一框架，尤其适用于非标准文档类型的识别。

Abstract: Many document types use intrinsic, convention-driven structures that serve to
encode precise and structured information, such as the conventions governing
engineering drawings. However, state-of-the-art approaches treat document
recognition as a mere computer vision problem, neglecting these underlying
document-type-specific structural properties, making them dependent on
sub-optimal heuristic post-processing and rendering many less frequent or more
complicated document types inaccessible to modern document recognition. We
suggest a novel perspective that frames document recognition as a transcription
task from a document to a record. This implies a natural grouping of documents
based on the intrinsic structure inherent in their transcription, where related
document types can be treated (and learned) similarly. We propose a method to
design structure-specific inductive biases for the underlying machine-learned
end-to-end document recognition systems, and a respective base transformer
architecture that we successfully adapt to different structures. We demonstrate
the effectiveness of the so-found inductive biases in extensive experiments
with progressively complex record structures from monophonic sheet music, shape
drawings, and simplified engineering drawings. By integrating an inductive bias
for unrestricted graph structures, we train the first-ever successful
end-to-end model to transcribe engineering drawings to their inherently
interlinked information. Our approach is relevant to inform the design of
document recognition systems for document types that are less well understood
than standard OCR, OMR, etc., and serves as a guide to unify the design of
future document foundation models.

</details>


### [204] [RadiomicsRetrieval: A Customizable Framework for Medical Image Retrieval Using Radiomics Features](https://arxiv.org/abs/2507.08546)
*Inye Na,Nejung Rue,Jiwon Chung,Hyunjin Park*

Main category: cs.CV

TL;DR: RadiomicsRetrieval是一个基于3D内容的医学图像检索框架，结合手工放射组学特征和深度学习嵌入，支持灵活查询。


<details>
  <summary>Details</summary>
Motivation: 解决现有2D医学图像检索方法对全标注查询的依赖和临床灵活性的限制。

Method: 利用可提示分割模型（如SAM）生成肿瘤特异性嵌入，通过对比学习与放射组学特征对齐，并加入解剖位置嵌入（APE）。

Result: 在肺CT和脑MRI数据集上验证，放射组学特征提高了检索特异性，APE支持基于位置的搜索。

Conclusion: RadiomicsRetrieval支持灵活查询，减少标注负担，适用于临床和研究场景。

Abstract: Medical image retrieval is a valuable field for supporting clinical
decision-making, yet current methods primarily support 2D images and require
fully annotated queries, limiting clinical flexibility. To address this, we
propose RadiomicsRetrieval, a 3D content-based retrieval framework bridging
handcrafted radiomics descriptors with deep learning-based embeddings at the
tumor level. Unlike existing 2D approaches, RadiomicsRetrieval fully exploits
volumetric data to leverage richer spatial context in medical images. We employ
a promptable segmentation model (e.g., SAM) to derive tumor-specific image
embeddings, which are aligned with radiomics features extracted from the same
tumor via contrastive learning. These representations are further enriched by
anatomical positional embedding (APE). As a result, RadiomicsRetrieval enables
flexible querying based on shape, location, or partial feature sets. Extensive
experiments on both lung CT and brain MRI public datasets demonstrate that
radiomics features significantly enhance retrieval specificity, while APE
provides global anatomical context essential for location-based searches.
Notably, our framework requires only minimal user prompts (e.g., a single
point), minimizing segmentation overhead and supporting diverse clinical
scenarios. The capability to query using either image embeddings or selected
radiomics attributes highlights its adaptability, potentially benefiting
diagnosis, treatment planning, and research on large-scale medical imaging
repositories. Our code is available at
https://github.com/nainye/RadiomicsRetrieval.

</details>


### [205] [A Multi-Modal Fusion Framework for Brain Tumor Segmentation Based on 3D Spatial-Language-Vision Integration and Bidirectional Interactive Attention Mechanism](https://arxiv.org/abs/2507.08574)
*Mingda Zhang,Kaiwen Pan*

Main category: cs.CV

TL;DR: 提出了一种结合空间-语言-视觉信息的多模态融合框架，通过双向交互注意力机制提升脑肿瘤分割精度和边界划分。


<details>
  <summary>Details</summary>
Motivation: 现有方法在脑肿瘤分割中难以充分利用多模态信息，尤其是临床文本与影像的结合。

Method: 提出多模态语义融合适配器（MSFA）和双向交互视觉语义注意力（BIVA），结合3D MRI与临床文本，通过分层语义解耦和迭代信息交换实现融合。

Result: 在BraTS 2020数据集上，平均Dice系数为0.8505，95% Hausdorff距离为2.8256mm，优于现有方法。

Conclusion: 多模态语义融合与双向交互注意力显著提升分割性能，为临床知识融入医学影像分析提供了新范式。

Abstract: This study aims to develop a novel multi-modal fusion framework for brain
tumor segmentation that integrates spatial-language-vision information through
bidirectional interactive attention mechanisms to improve segmentation accuracy
and boundary delineation. Methods: We propose two core components: Multi-modal
Semantic Fusion Adapter (MSFA) integrating 3D MRI data with clinical text
descriptions through hierarchical semantic decoupling, and Bidirectional
Interactive Visual-semantic Attention (BIVA) enabling iterative information
exchange between modalities. The framework was evaluated on BraTS 2020 dataset
comprising 369 multi-institutional MRI scans. Results: The proposed method
achieved average Dice coefficient of 0.8505 and 95% Hausdorff distance of
2.8256mm across enhancing tumor, tumor core, and whole tumor regions,
outperforming state-of-the-art methods including SCAU-Net, CA-Net, and 3D
U-Net. Ablation studies confirmed critical contributions of semantic and
spatial modules to boundary precision. Conclusion: Multi-modal semantic fusion
combined with bidirectional interactive attention significantly enhances brain
tumor segmentation performance, establishing new paradigms for integrating
clinical knowledge into medical image analysis.

</details>


### [206] [Normalized vs Diplomatic Annotation: A Case Study of Automatic Information Extraction from Handwritten Uruguayan Birth Certificates](https://arxiv.org/abs/2507.08636)
*Natalia Bottaioli,Solène Tarride,Jérémy Anger,Seginus Mowlavi,Marina Gardella,Antoine Tadros,Gabriele Facciolo,Rafael Grompone von Gioi,Christopher Kermorvant,Jean-Michel Morel,Javier Preciozzi*

Main category: cs.CV

TL;DR: 评估Document Attention Network（DAN）在乌拉圭西班牙语手写出生证明中提取键值信息的有效性，比较两种标注策略的效果。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过最小化训练数据和标注工作量，高效转录手写文档。

Method: 使用两种标注策略（标准化标注和外交标注）对DAN进行微调，并在两个数据集上进行实验。

Result: 标准化标注适用于可标准化字段（如日期和出生地），外交标注更适合不可标准化字段（如姓名）。

Conclusion: 不同标注策略适用于不同类型的字段，需根据字段特性选择合适的标注方法。

Abstract: This study evaluates the recently proposed Document Attention Network (DAN)
for extracting key-value information from Uruguayan birth certificates,
handwritten in Spanish. We investigate two annotation strategies for
automatically transcribing handwritten documents, fine-tuning DAN with minimal
training data and annotation effort. Experiments were conducted on two datasets
containing the same images (201 scans of birth certificates written by more
than 15 different writers) but with different annotation methods. Our findings
indicate that normalized annotation is more effective for fields that can be
standardized, such as dates and places of birth, whereas diplomatic annotation
performs much better for fields containing names and surnames, which can not be
standardized.

</details>


### [207] [DatasetAgent: A Novel Multi-Agent System for Auto-Constructing Datasets from Real-World Images](https://arxiv.org/abs/2507.08648)
*Haoran Sun,Haoyu Bian,Shaoning Zeng,Yunbo Rao,Xu Xu,Lin Mei,Jianping Gou*

Main category: cs.CV

TL;DR: 提出了一种名为DatasetAgent的多智能体协作系统，通过协调四个配备多模态大语言模型（MLLMs）的智能体，自动从真实世界图像构建高质量数据集。


<details>
  <summary>Details</summary>
Motivation: 手动收集和标注图像数据集耗时且低效，而AI生成的数据不如真实数据有价值，因此需要一种自动构建真实世界图像数据集的方法。

Method: 使用多智能体协作系统（DatasetAgent），结合MLLMs和图像优化工具包，根据用户需求构建数据集。

Result: 通过扩展现有数据集和从头创建新数据集两种实验，验证了DatasetAgent构建的数据集可用于训练多种视觉模型（分类、检测、分割）。

Conclusion: DatasetAgent能高效构建高质量的真实世界图像数据集，为视觉任务提供支持。

Abstract: Common knowledge indicates that the process of constructing image datasets
usually depends on the time-intensive and inefficient method of manual
collection and annotation. Large models offer a solution via data generation.
Nonetheless, real-world data are obviously more valuable comparing to
artificially intelligence generated data, particularly in constructing image
datasets. For this reason, we propose a novel method for auto-constructing
datasets from real-world images by a multiagent collaborative system, named as
DatasetAgent. By coordinating four different agents equipped with Multi-modal
Large Language Models (MLLMs), as well as a tool package for image
optimization, DatasetAgent is able to construct high-quality image datasets
according to user-specified requirements. In particular, two types of
experiments are conducted, including expanding existing datasets and creating
new ones from scratch, on a variety of open-source datasets. In both cases,
multiple image datasets constructed by DatasetAgent are used to train various
vision models for image classification, object detection, and image
segmentation.

</details>


### [208] [MoSAiC: Multi-Modal Multi-Label Supervision-Aware Contrastive Learning for Remote Sensing](https://arxiv.org/abs/2507.08683)
*Debashis Gupta,Aditi Golder,Rongkhun Zhu,Kangning Cui,Wei Tang,Fan Yang,Ovidiu Csillik,Sarra Alaqahtani,V. Paul Pauca*

Main category: cs.CV

TL;DR: MoSAiC是一个针对多模态卫星图像的对比学习框架，通过联合优化模态内和模态间的对比学习，提升语义解耦和表示学习能力。


<details>
  <summary>Details</summary>
Motivation: 地球系统观测（ESO）中多模态卫星图像的高类间相似性和复杂场景使得表示学习困难，现有对比学习框架缺乏多标签对齐和跨模态语义精确性。

Method: 提出MoSAiC框架，结合模态内和模态间对比学习，并引入多标签监督对比损失。

Result: 在BigEarthNet V2.0和Sent12MS数据集上，MoSAiC在低标签和高类重叠场景中优于全监督和自监督基线。

Conclusion: MoSAiC在多模态卫星图像中实现了更精细的语义解耦和鲁棒的表示学习。

Abstract: Contrastive learning (CL) has emerged as a powerful paradigm for learning
transferable representations without the reliance on large labeled datasets.
Its ability to capture intrinsic similarities and differences among data
samples has led to state-of-the-art results in computer vision tasks. These
strengths make CL particularly well-suited for Earth System Observation (ESO),
where diverse satellite modalities such as optical and SAR imagery offer
naturally aligned views of the same geospatial regions. However, ESO presents
unique challenges, including high inter-class similarity, scene clutter, and
ambiguous boundaries, which complicate representation learning -- especially in
low-label, multi-label settings. Existing CL frameworks often focus on
intra-modality self-supervision or lack mechanisms for multi-label alignment
and semantic precision across modalities. In this work, we introduce MoSAiC, a
unified framework that jointly optimizes intra- and inter-modality contrastive
learning with a multi-label supervised contrastive loss. Designed specifically
for multi-modal satellite imagery, MoSAiC enables finer semantic
disentanglement and more robust representation learning across spectrally
similar and spatially complex classes. Experiments on two benchmark datasets,
BigEarthNet V2.0 and Sent12MS, show that MoSAiC consistently outperforms both
fully supervised and self-supervised baselines in terms of accuracy, cluster
coherence, and generalization in low-label and high-class-overlap scenarios.

</details>


### [209] [SAM2RL: Towards Reinforcement Learning Memory Control in Segment Anything Model 2](https://arxiv.org/abs/2507.08548)
*Alen Adamyan,Tomáš Čížek,Matej Straka,Klara Janouskova,Martin Schmid*

Main category: cs.CV

TL;DR: SAM 2在目标分割任务中表现优异，成为视觉目标跟踪的SOTA。本文提出用强化学习优化其内存更新，替代手工规则，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖手工规则处理SAM 2的内存更新，限制了性能提升潜力。

Method: 将内存控制建模为序列决策问题，采用强化学习优化内存更新。

Result: 在单视频过拟合实验中，性能提升超过现有启发式方法的三倍。

Conclusion: 强化学习是优化内存控制的有效替代方案，揭示了内存银行的未开发潜力。

Abstract: Segment Anything Model 2 (SAM 2) has demonstrated strong performance in
object segmentation tasks and has become the state-of-the-art for visual object
tracking. The model stores information from previous frames in a memory bank,
enabling temporal consistency across video sequences. Recent methods augment
SAM 2 with hand-crafted update rules to better handle distractors, occlusions,
and object motion. We propose a fundamentally different approach using
reinforcement learning for optimizing memory updates in SAM 2 by framing memory
control as a sequential decision-making problem. In an overfitting setup with a
separate agent per video, our method achieves a relative improvement over SAM 2
that exceeds by more than three times the gains of existing heuristics. These
results reveal the untapped potential of the memory bank and highlight
reinforcement learning as a powerful alternative to hand-crafted update rules
for memory control in visual object tracking.

</details>


### [210] [A Hybrid Multi-Well Hopfield-CNN with Feature Extraction and K-Means for MNIST Classification](https://arxiv.org/abs/2507.08766)
*Ahmed Farooq*

Main category: cs.CV

TL;DR: 提出了一种结合CNN和多井Hopfield网络的混合模型，用于MNIST手写数字分类，测试准确率达99.2%。


<details>
  <summary>Details</summary>
Motivation: 解决手写数字分类中的类内变异性问题，同时提供可解释的基于能量的决策框架。

Method: 使用CNN提取特征，k-means聚类生成类特定原型，Hopfield网络通过能量最小化进行分类。

Result: 在10,000张MNIST图像上达到99.2%的测试准确率。

Conclusion: 模型展示了深度特征提取和原型覆盖对高性能的重要性，适用于更广泛的模式识别任务。

Abstract: This study presents a hybrid model for classifying handwritten digits in the
MNIST dataset, combining convolutional neural networks (CNNs) with a multi-well
Hopfield network. The approach employs a CNN to extract high-dimensional
features from input images, which are then clustered into class-specific
prototypes using k-means clustering. These prototypes serve as attractors in a
multi-well energy landscape, where a Hopfield network performs classification
by minimizing an energy function that balances feature similarity and class
assignment.The model's design enables robust handling of intraclass
variability, such as diverse handwriting styles, while providing an
interpretable framework through its energy-based decision process. Through
systematic optimization of the CNN architecture and the number of wells, the
model achieves a high test accuracy of 99.2% on 10,000 MNIST images,
demonstrating its effectiveness for image classification tasks. The findings
highlight the critical role of deep feature extraction and sufficient prototype
coverage in achieving high performance, with potential for broader applications
in pattern recognition.

</details>


### [211] [Geo-ORBIT: A Federated Digital Twin Framework for Scene-Adaptive Lane Geometry Detection](https://arxiv.org/abs/2507.08743)
*Rei Tamaru,Pei Li,Bin Ran*

Main category: cs.CV

TL;DR: Geo-ORBIT框架结合实时车道检测、数字孪生同步和联邦元学习，解决了交通管理中动态几何感知的挑战，提升了隐私保护和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态地图或高成本传感器，限制了可扩展性和适应性，同时大规模数字孪生面临隐私、通信和计算效率问题。

Method: 提出Geo-ORBIT框架，包括轻量级车道检测模型GeoLane、个性化参数学习的Meta-GeoLane，以及联邦学习策略FedMeta-GeoLane。

Result: FedMeta-GeoLane在多样城市场景中表现优于基线方法，几何误差更低，泛化能力更强，同时显著减少通信开销。

Conclusion: Geo-ORBIT为数字孪生中的灵活、上下文感知基础设施建模奠定了基础。

Abstract: Digital Twins (DT) have the potential to transform traffic management and
operations by creating dynamic, virtual representations of transportation
systems that sense conditions, analyze operations, and support decision-making.
A key component for DT of the transportation system is dynamic roadway geometry
sensing. However, existing approaches often rely on static maps or costly
sensors, limiting scalability and adaptability. Additionally, large-scale DTs
that collect and analyze data from multiple sources face challenges in privacy,
communication, and computational efficiency. To address these challenges, we
introduce Geo-ORBIT (Geometrical Operational Roadway Blueprint with Integrated
Twin), a unified framework that combines real-time lane detection, DT
synchronization, and federated meta-learning. At the core of Geo-ORBIT is
GeoLane, a lightweight lane detection model that learns lane geometries from
vehicle trajectory data using roadside cameras. We extend this model through
Meta-GeoLane, which learns to personalize detection parameters for local
entities, and FedMeta-GeoLane, a federated learning strategy that ensures
scalable and privacy-preserving adaptation across roadside deployments. Our
system is integrated with CARLA and SUMO to create a high-fidelity DT that
renders highway scenarios and captures traffic flows in real-time. Extensive
experiments across diverse urban scenes show that FedMeta-GeoLane consistently
outperforms baseline and meta-learning approaches, achieving lower geometric
error and stronger generalization to unseen locations while drastically
reducing communication overhead. This work lays the foundation for flexible,
context-aware infrastructure modeling in DTs. The framework is publicly
available at https://github.com/raynbowy23/FedMeta-GeoLane.git.

</details>


### [212] [Compress Any Segment Anything Model (SAM)](https://arxiv.org/abs/2507.08765)
*Juntong Fan,Zhiwei Hao,Jianqiang Shen,Shang-Ling Jui,Yi Zhang,Jing-Xiao Liao,Feng-Lei Fan*

Main category: cs.CV

TL;DR: Birkhoff是一种无需数据的压缩算法，用于Segment Anything Model（SAM）及其变体，具有高压缩比和快速推理速度。


<details>
  <summary>Details</summary>
Motivation: 由于SAM及其变体在零样本分割中的优异表现，压缩这些模型成为迫切需求。

Method: 提出Hyper-Compression算法和HyperLinear算子，将高维参数向量转为低维标量，并加速推理。

Result: 在多个数据集上验证，Birkhoff压缩比达5.17x，性能下降小于1%，压缩时间短。

Conclusion: Birkhoff是一种高效、通用的压缩方法，适用于SAM及其变体。

Abstract: Due to the excellent performance in yielding high-quality, zero-shot
segmentation, Segment Anything Model (SAM) and its variants have been widely
applied in diverse scenarios such as healthcare and intelligent manufacturing.
Therefore, effectively compressing SAMs has become an increasingly pressing
practical need. In this study, we propose Birkhoff, a novel data-free
compression algorithm for SAM and its variants. Unlike quantization, pruning,
distillation, and other compression methods, Birkhoff embodies versatility
across model types, agility in deployment, faithfulness to the original model,
and compactness in model size. Specifically, Birkhoff introduces a novel
compression algorithm: Hyper-Compression, whose core principle is to find a
dense trajectory to turn a high-dimensional parameter vector into a
low-dimensional scalar. Furthermore, Birkhoff designs a dedicated linear layer
operator, HyperLinear, to fuse decompression and matrix multiplication to
significantly accelerate inference of the compressed SAMs. Extensive
experiments on 18 SAMs in the COCO, LVIS, and SA-1B datasets show that Birkhoff
performs consistently and competitively in compression time, compression ratio,
post-compression performance, and inference speed. For example, Birkhoff can
achieve a compression ratio of 5.17x on SAM2-B, with less than 1% performance
drop without using any fine-tuning data. Moreover, the compression is finished
within 60 seconds for all models.

</details>


### [213] [Lumos-1: On Autoregressive Video Generation from a Unified Model Perspective](https://arxiv.org/abs/2507.08801)
*Hangjie Yuan,Weihua Chen,Jun Cen,Hu Yu,Jingyun Liang,Shuning Chang,Zhihui Lin,Tao Feng,Pengwei Liu,Jiazheng Xing,Hao Luo,Jiasheng Tang,Fan Wang,Yi Yang*

Main category: cs.CV

TL;DR: Lumos-1是一种基于LLM架构的自回归视频生成器，通过3D RoPE和MM-RoPE技术增强时空相关性，并提出AR-DF解决帧间损失不平衡问题，性能优异且训练高效。


<details>
  <summary>Details</summary>
Motivation: 现有自回归视频生成器存在架构偏离、依赖外部编码器或延迟高的问题，Lumos-1旨在保留标准LLM架构并提升性能。

Method: 采用3D RoPE和MM-RoPE增强时空建模，提出AR-DF解决帧间损失不平衡，并优化训练策略。

Result: 在48个GPU上预训练，性能与EMU3、COSMOS-Video2World和OpenSoraPlan相当。

Conclusion: Lumos-1通过高效架构和优化策略，实现了高性能的自回归视频生成。

Abstract: Autoregressive large language models (LLMs) have unified a vast range of
language tasks, inspiring preliminary efforts in autoregressive video
generation. Existing autoregressive video generators either diverge from
standard LLM architectures, depend on bulky external text encoders, or incur
prohibitive latency due to next-token decoding. In this paper, we introduce
Lumos-1, an autoregressive video generator that retains the LLM architecture
with minimal architectural modifications. To inject spatiotemporal correlations
in LLMs, we identify the efficacy of incorporating 3D RoPE and diagnose its
imbalanced frequency spectrum ranges. Therefore, we propose MM-RoPE, a RoPE
scheme that preserves the original textual RoPE while providing comprehensive
frequency spectra and scaled 3D positions for modeling multimodal
spatiotemporal data. Moreover, Lumos-1 resorts to a token dependency strategy
that obeys intra-frame bidirectionality and inter-frame temporal causality.
Based on this dependency strategy, we identify the issue of frame-wise loss
imbalance caused by spatial information redundancy and solve it by proposing
Autoregressive Discrete Diffusion Forcing (AR-DF). AR-DF introduces temporal
tube masking during training with a compatible inference-time masking policy to
avoid quality degradation. By using memory-efficient training techniques, we
pre-train Lumos-1 on only 48 GPUs, achieving performance comparable to EMU3 on
GenEval, COSMOS-Video2World on VBench-I2V, and OpenSoraPlan on VBench-T2V. Code
and models are available at https://github.com/alibaba-damo-academy/Lumos.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [214] [xpSHACL: Explainable SHACL Validation using Retrieval-Augmented Generation and Large Language Models](https://arxiv.org/abs/2507.08432)
*Gustavo Correa Publio,José Emilio Labra Gayo*

Main category: cs.DB

TL;DR: xpSHACL是一个可解释的SHACL验证系统，通过结合规则驱动的解释树、检索增强生成（RAG）和大语言模型（LLMs），为约束违规提供多语言、易读的解释。


<details>
  <summary>Details</summary>
Motivation: 传统SHACL验证引擎生成的报告通常难以被非技术用户理解，因此需要一种更易解释的验证系统。

Method: xpSHACL结合规则驱动的解释树、RAG和LLMs，并利用Violation KG缓存和重用解释。

Result: 系统能够生成详细的多语言解释，提高了效率和一致性。

Conclusion: xpSHACL为SHACL验证提供了更易理解的解释，适合非技术用户。

Abstract: Shapes Constraint Language (SHACL) is a powerful language for validating RDF
data. Given the recent industry attention to Knowledge Graphs (KGs), more users
need to validate linked data properly. However, traditional SHACL validation
engines often provide terse reports in English that are difficult for
non-technical users to interpret and act upon. This paper presents xpSHACL, an
explainable SHACL validation system that addresses this issue by combining
rule-based justification trees with retrieval-augmented generation (RAG) and
large language models (LLMs) to produce detailed, multilanguage, human-readable
explanations for constraint violations. A key feature of xpSHACL is its usage
of a Violation KG to cache and reuse explanations, improving efficiency and
consistency.

</details>


### [215] [ONION: A Multi-Layered Framework for Participatory ER Design](https://arxiv.org/abs/2507.08702)
*Viktoriia Makovska,George Fletcher,Julia Stoyanovich*

Main category: cs.DB

TL;DR: ONION是一个多层框架，用于参与式实体关系建模，结合了设计正义、参与式AI和概念建模的见解。


<details>
  <summary>Details</summary>
Motivation: 减少设计者偏见，促进包容性参与，并在建模过程中提高透明度。

Method: 采用五阶段方法：观察、培养、整合、优化、规范化，支持从非结构化利益相关者输入到结构化ER图的逐步抽象。

Result: 通过乌克兰的实际研讨会评估，展示了多样利益相关者参与如何丰富数据模型并加深相互理解。

Conclusion: ONION在早期数据建模中具有多样性潜力，但需解决扩展和优化框架的挑战。

Abstract: We present ONION, a multi-layered framework for participatory
Entity-Relationship (ER) modeling that integrates insights from design justice,
participatory AI, and conceptual modeling. ONION introduces a five-stage
methodology: Observe, Nurture, Integrate, Optimize, Normalize. It supports
progressive abstraction from unstructured stakeholder input to structured ER
diagrams.
  Our approach aims to reduce designer bias, promote inclusive participation,
and increase transparency through the modeling process. We evaluate ONION
through real-world workshops focused on sociotechnical systems in Ukraine,
highlighting how diverse stakeholder engagement leads to richer data models and
deeper mutual understanding. Early results demonstrate ONION's potential to
host diversity in early-stage data modeling. We conclude with lessons learned,
limitations and challenges involved in scaling and refining the framework for
broader adoption.

</details>


### [216] [Hashing for Fast Pattern Set Selection](https://arxiv.org/abs/2507.08745)
*Maiju Karjalainen,Pauli Miettinen*

Main category: cs.DB

TL;DR: 本文提出了一种基于bottom-k哈希的高效模式集挖掘方法，用于降低重构误差并提高效率。


<details>
  <summary>Details</summary>
Motivation: 模式集挖掘是数据挖掘中的核心问题，但现有方法在效率和准确性上存在不足。本文旨在通过重构误差作为衡量标准，提出一种更高效的解决方案。

Method: 采用bottom-k哈希技术高效选择模式集，并扩展方法以处理模式在数据中近似出现的情况。

Result: 在合成和真实数据集上，哈希方法比标准贪婪算法显著更快，且结果质量相近。

Conclusion: 该方法在多种应用中具有潜力，如数据库平铺、布尔矩阵分解和重描述挖掘。

Abstract: Pattern set mining, which is the task of finding a good set of patterns
instead of all patterns, is a fundamental problem in data mining. Many
different definitions of what constitutes a good set have been proposed in
recent years. In this paper, we consider the reconstruction error as a proxy
measure for the goodness of the set, and concentrate on the adjacent problem of
how to find a good set efficiently. We propose a method based on bottom-k
hashing for efficiently selecting the set and extend the method for the common
case where the patterns might only appear in approximate form in the data. Our
approach has applications in tiling databases, Boolean matrix factorization,
and redescription mining, among others. We show that our hashing-based approach
is significantly faster than the standard greedy algorithm while obtaining
almost equally good results in both synthetic and real-world data sets.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [217] [A Personalised Formal Verification Framework for Monitoring Activities of Daily Living of Older Adults Living Independently in Their Homes](https://arxiv.org/abs/2507.08701)
*Ricardo Contreras,Filip Smola,Nuša Farič,Jiawei Zheng,Jane Hillston,Jacques D. Fleuriot*

Main category: cs.LO

TL;DR: 提出了一种框架，用于建模和验证独立生活的老年人的日常活动，结合传感器数据和个人偏好，通过形式化方法提升安全性。


<details>
  <summary>Details</summary>
Motivation: 为独立生活的老年人提供个性化解决方案，改善其生活质量。

Method: 整合传感器数据和上下文信息，构建个性化形式化模型，使用线性时序逻辑验证需求。

Result: 框架可推广至不同参与者，生成违反需求的反馈，提升安全性和幸福感。

Conclusion: 该框架具有潜力，能有效支持老年人独立生活的安全与福祉。

Abstract: There is an imperative need to provide quality of life to a growing
population of older adults living independently. Personalised solutions that
focus on the person and take into consideration their preferences and context
are key. In this work, we introduce a framework for representing and reasoning
about the Activities of Daily Living of older adults living independently at
home. The framework integrates data from sensors and contextual information
that aggregates semi-structured interviews, home layouts and sociological
observations from the participants. We use these data to create formal models,
personalised for each participant according to their preferences and context.
We formulate requirements that are specific to each individual as properties
encoded in Linear Temporal Logic and use a model checker to verify whether each
property is satisfied by the model. When a property is violated, a
counterexample is generated giving the cause of the violation. We demonstrate
the framework's generalisability by applying it to different participants,
highlighting its potential to enhance the safety and well-being of older adults
ageing in place.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [218] [Quantum Properties Trojans (QuPTs) for Attacking Quantum Neural Networks](https://arxiv.org/abs/2507.08202)
*Sounak Bhowmik,Travis S. Humble,Himanshu Thapliyal*

Main category: quant-ph

TL;DR: 本文提出了一种基于量子计算特性的新型特洛伊攻击（QuPTs），针对量子神经网络（QNN）的二元分类器，展示了其隐蔽性和对性能的显著影响。


<details>
  <summary>Details</summary>
Motivation: 量子神经网络（QNN）在量子机器学习（QML）中潜力巨大，但其安全性和鲁棒性尚未充分研究。

Method: 利用量子门的酉特性和Hadamard门叠加特性，设计特洛伊攻击（QuPTs），插入噪声并攻击QNN。

Result: 实验表明，QuPTs隐蔽性强，显著影响量子电路性能，最大导致QNN准确率下降23%。

Conclusion: 这是首个针对纯量子神经网络的特洛伊攻击研究，独立于任何经典-量子混合架构。

Abstract: Quantum neural networks (QNN) hold immense potential for the future of
quantum machine learning (QML). However, QNN security and robustness remain
largely unexplored. In this work, we proposed novel Trojan attacks based on the
quantum computing properties in a QNN-based binary classifier. Our proposed
Quantum Properties Trojans (QuPTs) are based on the unitary property of quantum
gates to insert noise and Hadamard gates to enable superposition to develop
Trojans and attack QNNs. We showed that the proposed QuPTs are significantly
stealthier and heavily impact the quantum circuits' performance, specifically
QNNs. The most impactful QuPT caused a deterioration of 23% accuracy of the
compromised QNN under the experimental setup. To the best of our knowledge,
this is the first work on the Trojan attack on a fully quantum neural network
independent of any hybrid classical-quantum architecture.

</details>


### [219] [Parametrized Quantum Circuit Learning for Quantum Chemical Applications](https://arxiv.org/abs/2507.08183)
*Grier M. Jones,Viki Kumar Prasad,Ulrich Fekl,Hans-Arno Jacobsen*

Main category: quant-ph

TL;DR: 研究了参数化量子电路（PQCs）在量子化学数据集上的表现，发现其在化学相关问题上仍面临挑战。


<details>
  <summary>Details</summary>
Motivation: 探索PQCs在量子化学数据集上的潜力与局限性，填补相关研究空白。

Method: 构建168种PQCs，结合14种数据编码策略和12种变分ansätze，在5和16量子比特电路上评估性能。

Result: PQCs在化学问题上表现不如经典机器学习方法，电路结构和训练集大小对性能有显著影响。

Conclusion: PQCs在化学相关问题上仍面临挑战，需进一步优化。

Abstract: In the field of quantum machine learning (QML), parametrized quantum circuits
(PQCs) -- constructed using a combination of fixed and tunable quantum gates --
provide a promising hybrid framework for tackling complex machine learning
problems. Despite numerous proposed applications, there remains limited
exploration of datasets relevant to quantum chemistry. In this study, we
investigate the potential benefits and limitations of PQCs on two chemically
meaningful datasets: (1) the BSE49 dataset, containing bond separation energies
for 49 different classes of chemical bonds, and (2) a dataset of water
conformations, where coupled-cluster singles and doubles (CCSD) wavefunctions
are predicted from lower-level electronic structure methods using the
data-driven coupled-cluster (DDCC) approach. We construct a comprehensive set
of 168 PQCs by combining 14 data encoding strategies with 12 variational
ans{\"a}tze, and evaluate their performance on circuits with 5 and 16 qubits.
Our initial analysis examines the impact of circuit structure on model
performance using state-vector simulations. We then explore how circuit depth
and training set size influence model performance. Finally, we assess the
performance of the best-performing PQCs on current quantum hardware, using both
noisy simulations ("fake" backends) and real quantum devices. Our findings
underscore the challenges of applying PQCs to chemically relevant problems that
are straightforward for classical machine learning methods but remain
non-trivial for quantum approaches.

</details>


### [220] [Quantum Algorithms for Projection-Free Sparse Convex Optimization](https://arxiv.org/abs/2507.08543)
*Jianhao He,John C. S. Lui*

Main category: quant-ph

TL;DR: 本文研究了向量和矩阵域的无投影稀疏凸优化问题，提出了量子算法，显著降低了查询和时间复杂度，优于经典算法。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习和数据科学中稀疏凸优化问题的计算效率问题，利用量子计算的优势提升性能。

Method: 针对向量域和矩阵域分别提出两种量子算法，利用函数值预言机减少查询和时间复杂度。

Result: 量子算法在查询和时间复杂度上优于经典算法，向量域减少O(√d)和O(d)，矩阵域减少至少O(√d)。

Conclusion: 量子算法在稀疏凸优化问题中展现出优于经典方法的维度依赖性优势。

Abstract: This paper considers the projection-free sparse convex optimization problem
for the vector domain and the matrix domain, which covers a large number of
important applications in machine learning and data science. For the vector
domain $\mathcal{D} \subset \mathbb{R}^d$, we propose two quantum algorithms
for sparse constraints that finds a $\varepsilon$-optimal solution with the
query complexity of $O(\sqrt{d}/\varepsilon)$ and $O(1/\varepsilon)$ by using
the function value oracle, reducing a factor of $O(\sqrt{d})$ and $O(d)$ over
the best classical algorithm, respectively, where $d$ is the dimension. For the
matrix domain $\mathcal{D} \subset \mathbb{R}^{d\times d}$, we propose two
quantum algorithms for nuclear norm constraints that improve the time
complexity to $\tilde{O}(rd/\varepsilon^2)$ and
$\tilde{O}(\sqrt{r}d/\varepsilon^3)$ for computing the update step, reducing at
least a factor of $O(\sqrt{d})$ over the best classical algorithm, where $r$ is
the rank of the gradient matrix. Our algorithms show quantum advantages in
projection-free sparse convex optimization problems as they outperform the
optimal classical methods in dependence on the dimension $d$.

</details>


### [221] [Entangled Threats: A Unified Kill Chain Model for Quantum Machine Learning Security](https://arxiv.org/abs/2507.08623)
*Pascal Debus,Maximilian Wendlinger,Kilian Tscharke,Daniel Herr,Cedric Brügmann,Daniel Ohl de Mello,Juris Ulmanis,Alexander Erhard,Arthur Schmidt,Fabian Petsch*

Main category: quant-ph

TL;DR: 该论文提出了一种结构化建模量子机器学习（QML）攻击面的方法，借鉴了经典IT中的杀伤链模型，以更全面地分析QML中的安全威胁。


<details>
  <summary>Details</summary>
Motivation: QML系统继承了经典机器学习的漏洞，并引入了新的量子计算攻击面，但目前研究多孤立分析，缺乏整体防御策略。

Method: 论文提出将经典IT中的杀伤链模型应用于QML，构建了一个量子感知的杀伤链框架，并详细分类了攻击向量及其关系。

Result: 提出了一个详细的QML攻击向量分类法，映射到杀伤链各阶段，揭示了物理层、数据算法层和隐私攻击之间的相互依赖。

Conclusion: 该研究为QML的威胁建模和深度安全设计提供了基础，推动了这一新兴领域的安全性发展。

Abstract: Quantum Machine Learning (QML) systems inherit vulnerabilities from classical
machine learning while introducing new attack surfaces rooted in the physical
and algorithmic layers of quantum computing. Despite a growing body of research
on individual attack vectors - ranging from adversarial poisoning and evasion
to circuit-level backdoors, side-channel leakage, and model extraction - these
threats are often analyzed in isolation, with unrealistic assumptions about
attacker capabilities and system environments. This fragmentation hampers the
development of effective, holistic defense strategies. In this work, we argue
that QML security requires more structured modeling of the attack surface,
capturing not only individual techniques but also their relationships,
prerequisites, and potential impact across the QML pipeline. We propose
adapting kill chain models, widely used in classical IT and cybersecurity, to
the quantum machine learning context. Such models allow for structured
reasoning about attacker objectives, capabilities, and possible multi-stage
attack paths - spanning reconnaissance, initial access, manipulation,
persistence, and exfiltration. Based on extensive literature analysis, we
present a detailed taxonomy of QML attack vectors mapped to corresponding
stages in a quantum-aware kill chain framework that is inspired by the MITRE
ATLAS for classical machine learning. We highlight interdependencies between
physical-level threats (like side-channel leakage and crosstalk faults), data
and algorithm manipulation (such as poisoning or circuit backdoors), and
privacy attacks (including model extraction and training data inference). This
work provides a foundation for more realistic threat modeling and proactive
security-in-depth design in the emerging field of quantum machine learning.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [222] [Invariant-based Robust Weights Watermark for Large Language Models](https://arxiv.org/abs/2507.08288)
*Qingxiao Guo,Xinjie Zhu,Yilong Ma,Hui Jin,Yunhao Wang,Weifeng Zhang,Xiaobing Guo*

Main category: cs.CR

TL;DR: 本文提出了一种无需重新训练或微调的鲁棒水印方案，用于保护大型语言模型的IP，通过线性约束和噪声机制实现多用户场景下的抗共谋攻击。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在边缘设备上的广泛部署，保护知识产权免受恶意用户窃取的需求日益增长。

Method: 方案为每个用户生成唯一密钥，并通过模型不变性构建线性约束来推导稳定的水印值，同时利用噪声机制隐藏水印位置以抵御共谋攻击。

Result: 在三种流行模型（Llama3、Phi3、Gemma）上的实验表明，该方案对多种攻击方法（如微调、剪枝、量化等）具有强鲁棒性。

Conclusion: 该水印技术能有效保护模型IP，适用于多用户场景，且无需额外训练开销。

Abstract: Watermarking technology has gained significant attention due to the
increasing importance of intellectual property (IP) rights, particularly with
the growing deployment of large language models (LLMs) on billions
resource-constrained edge devices. To counter the potential threats of IP theft
by malicious users, this paper introduces a robust watermarking scheme without
retraining or fine-tuning for transformer models. The scheme generates a unique
key for each user and derives a stable watermark value by solving linear
constraints constructed from model invariants. Moreover, this technology
utilizes noise mechanism to hide watermark locations in multi-user scenarios
against collusion attack. This paper evaluates the approach on three popular
models (Llama3, Phi3, Gemma), and the experimental results confirm the strong
robustness across a range of attack methods (fine-tuning, pruning,
quantization, permutation, scaling, reversible matrix and collusion attacks).

</details>


### [223] [White-Basilisk: A Hybrid Model for Code Vulnerability Detection](https://arxiv.org/abs/2507.08540)
*Ioannis Lamprou,Alexander Shevtsov,Ioannis Arapakis,Sotiris Ioannidis*

Main category: cs.CR

TL;DR: White-Basilisk是一种新型漏洞检测方法，通过创新的架构（Mamba层、线性自注意力和专家混合框架）实现了卓越性能，仅用2亿参数就达到了最先进水平。


<details>
  <summary>Details</summary>
Motivation: 软件漏洞的激增对网络安全构成重大挑战，需要更有效的检测方法。

Method: 采用Mamba层、线性自注意力和专家混合框架的创新架构，能够处理超长序列代码。

Result: 在漏洞检测任务中表现优异，处理大规模代码库时超越现有大型语言模型的限制。

Conclusion: 研究表明，紧凑高效的模型在特定任务中可以超越大型模型，可能重新定义AI开发的优化策略。

Abstract: The proliferation of software vulnerabilities presents a significant
challenge to cybersecurity, necessitating more effective detection
methodologies. We introduce White-Basilisk, a novel approach to vulnerability
detection that demonstrates superior performance while challenging prevailing
assumptions in AI model scaling. Utilizing an innovative architecture that
integrates Mamba layers, linear self-attention, and a Mixture of Experts
framework, White-Basilisk achieves state-of-the-art results in vulnerability
detection tasks with a parameter count of only 200M. The model's capacity to
process sequences of unprecedented length enables comprehensive analysis of
extensive codebases in a single pass, surpassing the context limitations of
current Large Language Models (LLMs). White-Basilisk exhibits robust
performance on imbalanced, real-world datasets, while maintaining computational
efficiency that facilitates deployment across diverse organizational scales.
This research not only establishes new benchmarks in code security but also
provides empirical evidence that compact, efficiently designed models can
outperform larger counterparts in specialized tasks, potentially redefining
optimization strategies in AI development for domain-specific applications.

</details>
