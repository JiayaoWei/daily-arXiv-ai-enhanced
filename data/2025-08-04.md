<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 46]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.IR](#cs.IR) [Total: 6]
- [cs.LG](#cs.LG) [Total: 62]
- [eess.AS](#eess.AS) [Total: 2]
- [cs.RO](#cs.RO) [Total: 4]
- [math.PR](#math.PR) [Total: 1]
- [cs.HC](#cs.HC) [Total: 8]
- [cs.LO](#cs.LO) [Total: 3]
- [math.NA](#math.NA) [Total: 1]
- [hep-ph](#hep-ph) [Total: 1]
- [eess.SY](#eess.SY) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [math.ST](#math.ST) [Total: 1]
- [eess.IV](#eess.IV) [Total: 3]
- [stat.ML](#stat.ML) [Total: 2]
- [math.OC](#math.OC) [Total: 3]
- [cs.SE](#cs.SE) [Total: 5]
- [cs.NI](#cs.NI) [Total: 7]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]
- [cs.CR](#cs.CR) [Total: 5]
- [cs.PL](#cs.PL) [Total: 3]
- [cs.IT](#cs.IT) [Total: 2]
- [quant-ph](#quant-ph) [Total: 4]
- [cs.OS](#cs.OS) [Total: 1]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.CV](#cs.CV) [Total: 27]
- [physics.ed-ph](#physics.ed-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [PhysicsEval: Inference-Time Techniques to Improve the Reasoning Proficiency of Large Language Models on Physics Problems](https://arxiv.org/abs/2508.00079)
*Oshayer Siddique,J. M Areeb Uzair Alam,Md Jobayer Rahman Rafy,Syed Rifat Raiyan,Hasan Mahmud,Md Kamrul Hasan*

Main category: cs.CL

TL;DR: 论文评估前沿大语言模型（LLM）在解决物理问题（数学和描述性）中的表现，并采用多代理框架和推理技术提升性能，同时引入新的评估基准PHYSICSEVAL。


<details>
  <summary>Details</summary>
Motivation: 物理问题是自然语言推理的重要领域，但现有模型在此任务上的表现尚不明确，需通过技术改进和基准测试来推动进展。

Method: 使用多代理框架和推理技术（如验证解决方案的累积方法）提升LLM性能，并构建包含19,609个物理问题的评估基准PHYSICSEVAL。

Result: 多代理框架显著改善了模型在初始表现较差的问题上的性能。

Conclusion: 论文提出了一种有效的多代理框架和新的评估基准，为LLM在物理问题解决中的性能提升提供了方向。

Abstract: The discipline of physics stands as a cornerstone of human intellect, driving
the evolution of technology and deepening our understanding of the fundamental
principles of the cosmos. Contemporary literature includes some works centered
on the task of solving physics problems - a crucial domain of natural language
reasoning. In this paper, we evaluate the performance of frontier LLMs in
solving physics problems, both mathematical and descriptive. We also employ a
plethora of inference-time techniques and agentic frameworks to improve the
performance of the models. This includes the verification of proposed solutions
in a cumulative fashion by other, smaller LLM agents, and we perform a
comparative analysis of the performance that the techniques entail. There are
significant improvements when the multi-agent framework is applied to problems
that the models initially perform poorly on. Furthermore, we introduce a new
evaluation benchmark for physics problems, ${\rm P{\small HYSICS}E{\small
VAL}}$, consisting of 19,609 problems sourced from various physics textbooks
and their corresponding correct solutions scraped from physics forums and
educational websites. Our code and data are publicly available at
https://github.com/areebuzair/PhysicsEval.

</details>


### [2] [Do LLMs produce texts with "human-like" lexical diversity?](https://arxiv.org/abs/2508.00086)
*Kelly Kendro,Jeffrey Maloney,Scott Jarvis*

Main category: cs.CL

TL;DR: LLM生成的文本在词汇多样性上与人类写作有显著差异，新版模型（如ChatGPT-4.5）差异更大。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM生成的文本在词汇多样性上是否与人类写作相似。

Method: 比较四种ChatGPT模型与240名L1/L2英语参与者的文本，测量六种词汇多样性维度。

Result: LLM文本与人类文本在词汇多样性上显著不同，新版模型差异更大。

Conclusion: LLM无法生成词汇多样性上类似人类的文本，新版模型更不相似。

Abstract: The degree to which LLMs produce writing that is truly human-like remains
unclear despite the extensive empirical attention that this question has
received. The present study addresses this question from the perspective of
lexical diversity. Specifically, the study investigates patterns of lexical
diversity in LLM-generated texts from four ChatGPT models (-3.5, -4, -o4 mini,
and -4.5) in comparison with texts written by L1 and L2 English participants (n
= 240) across four education levels. Six dimensions of lexical diversity were
measured in each text: volume, abundance, variety-repetition, evenness,
disparity, and dispersion. Results from one-way MANOVAs, one-way ANOVAS, and
Support Vector Machines revealed that the LLM-generated texts differed
significantly from human-written texts for each variable, with ChatGPT-o4 mini
and -4.5 differing the most. Within these two groups, ChatGPT-4.5 demonstrated
higher levels of lexical diversity despite producing fewer tokens. The human
writers' lexical diversity did not differ across subgroups (i.e., education,
language status). Altogether, the results indicate that LLMs do not produce
human-like texts in relation to lexical diversity, and the newer LLMs produce
less human-like texts than older models. We discuss the implications of these
results for language pedagogy and related applications.

</details>


### [3] [Semiotic Complexity and Its Epistemological Implications for Modeling Culture](https://arxiv.org/abs/2508.00095)
*Zachary K. Stine,James E. Deitrick*

Main category: cs.CL

TL;DR: 论文呼吁在计算人文学科中加强方法的理论化，以提升领域的成熟度。作者将建模工作视为文化、语言领域与计算、数学领域之间的翻译过程，并强调理论化翻译过程的重要性以避免错误和提升透明度。


<details>
  <summary>Details</summary>
Motivation: 计算人文学科需要更多方法的理论化，以实现认识论和解释的清晰性，推动领域的成熟。

Method: 将建模工作视为翻译过程，提出“符号复杂性”概念，指出当前建模实践中的翻译错误。

Result: 当前建模实践常将符号复杂的数据视为符号简单的数据，导致翻译错误。

Conclusion: 论文提出建议，帮助研究者更好地处理认识论问题，避免翻译错误。

Abstract: Greater theorizing of methods in the computational humanities is needed for
epistemological and interpretive clarity, and therefore the maturation of the
field. In this paper, we frame such modeling work as engaging in translation
work from a cultural, linguistic domain into a computational, mathematical
domain, and back again. Translators benefit from articulating the theory of
their translation process, and so do computational humanists in their work --
to ensure internal consistency, avoid subtle yet consequential translation
errors, and facilitate interpretive transparency. Our contribution in this
paper is to lay out a particularly consequential dimension of the lack of
theorizing and the sorts of translation errors that emerge in our modeling
practices as a result. Along these lines we introduce the idea of semiotic
complexity as the degree to which the meaning of some text may vary across
interpretive lenses, and make the case that dominant modeling practices --
especially around evaluation -- commit a translation error by treating
semiotically complex data as semiotically simple when it seems
epistemologically convenient by conferring superficial clarity. We then lay out
several recommendations for researchers to better account for these
epistemological issues in their own work.

</details>


### [4] [FACTORY: A Challenging Human-Verified Prompt Set for Long-Form Factuality](https://arxiv.org/abs/2508.00109)
*Mingda Chen,Yang Li,Xilun Chen,Adina Williams,Gargi Ghosh,Scott Yih*

Main category: cs.CL

TL;DR: FACTORY是一个大规模、人工验证的提示集，用于评估模型生成准确、全面回答的能力。相比现有基准，FACTORY更具挑战性，约40%的SOTA模型回答存在不准确内容。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏人工验证，可能导致质量问题，因此需要更可靠的评估工具。

Method: 采用模型循环开发方法，结合人工优化，构建FACTORY提示集，并对6个SOTA模型进行人工评估。

Result: FACTORY显示约40%的SOTA模型回答不准确，而其他数据集仅为10%。

Conclusion: FACTORY是一个可靠且具有挑战性的基准，强调模型需具备长尾事实推理能力。

Abstract: Long-form factuality evaluation assesses the ability of models to generate
accurate, comprehensive responses to short prompts. Existing benchmarks often
lack human verification, leading to potential quality issues. To address this
limitation, we introduce FACTORY, a large-scale, human-verified prompt set.
Developed using a model-in-the-loop approach and refined by humans, FACTORY
includes challenging prompts that are fact-seeking, answerable, and
unambiguous. We conduct human evaluations on 6 state-of-the-art language models
using FACTORY and existing datasets. Our results show that FACTORY is a
challenging benchmark: approximately 40% of the claims made in the responses of
SOTA models are not factual, compared to only 10% for other datasets. Our
analysis identifies the strengths of FACTORY over prior benchmarks, emphasizing
its reliability and the necessity for models to reason across long-tailed
facts.

</details>


### [5] [Is neural semantic parsing good at ellipsis resolution, or isn't it?](https://arxiv.org/abs/2508.00121)
*Xiao Zhang,Johan bos*

Main category: cs.CL

TL;DR: 神经语义解析器在多种语言现象中表现良好，但在强上下文敏感现象（如英语动词短语省略）中表现不佳。


<details>
  <summary>Details</summary>
Motivation: 研究神经语义解析器是否能处理强上下文敏感的语言现象，尤其是动词短语省略。

Method: 构建包含120个省略案例及其完整语义表示的语料库，作为挑战集测试多种神经语义解析器。

Result: 解析器在标准测试集上表现优异，但在省略案例中失败。

Conclusion: 数据增强可能是解决这一问题的潜在方法。

Abstract: Neural semantic parsers have shown good overall performance for a variety of
linguistic phenomena, reaching semantic matching scores of more than 90%. But
how do such parsers perform on strongly context-sensitive phenomena, where
large pieces of semantic information need to be duplicated to form a meaningful
semantic representation? A case in point is English verb phrase ellipsis, a
construct where entire verb phrases can be abbreviated by a single auxiliary
verb. Are the otherwise known as powerful semantic parsers able to deal with
ellipsis or aren't they? We constructed a corpus of 120 cases of ellipsis with
their fully resolved meaning representation and used this as a challenge set
for a large battery of neural semantic parsers. Although these parsers
performed very well on the standard test set, they failed in the instances with
ellipsis. Data augmentation

</details>


### [6] [Comparison of Large Language Models for Deployment Requirements](https://arxiv.org/abs/2508.00185)
*Alper Yaman,Jannik Schwab,Christof Nitsche,Abhirup Sinha,Marco Huber*

Main category: cs.CL

TL;DR: 本文比较了大型语言模型（LLMs）的基础和领域特定模型，帮助研究者和公司根据许可和硬件需求选择最佳模型。


<details>
  <summary>Details</summary>
Motivation: 由于开源模型数量激增，选择合适的大型语言模型变得复杂，需要一种系统化的比较方法。

Method: 通过整理和比较不同模型的特征（如发布年份、许可和硬件需求），创建并持续更新一个公开的模型列表。

Result: 提供了一个在GitLab上发布的模型比较列表，便于用户快速了解和选择适合的LLM。

Conclusion: 该列表为研究者和公司提供了实用的工具，以应对快速发展的LLM领域中的选择挑战。

Abstract: Large Language Models (LLMs), such as Generative Pre-trained Transformers
(GPTs) are revolutionizing the generation of human-like text, producing
contextually relevant and syntactically correct content. Despite challenges
like biases and hallucinations, these Artificial Intelligence (AI) models excel
in tasks, such as content creation, translation, and code generation.
Fine-tuning and novel architectures, such as Mixture of Experts (MoE), address
these issues. Over the past two years, numerous open-source foundational and
fine-tuned models have been introduced, complicating the selection of the
optimal LLM for researchers and companies regarding licensing and hardware
requirements. To navigate the rapidly evolving LLM landscape and facilitate LLM
selection, we present a comparative list of foundational and domain-specific
models, focusing on features, such as release year, licensing, and hardware
requirements. This list is published on GitLab and will be continuously
updated.

</details>


### [7] [Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges](https://arxiv.org/abs/2508.00217)
*Xiaofeng Wu,Alan Ritter,Wei Xu*

Main category: cs.CL

TL;DR: 本文探讨了表格在大型语言模型（LLMs）和多模态大型语言模型（MLLMs）中的重要性，提出了表格理解的分类和任务，并指出了当前研究中的关键问题。


<details>
  <summary>Details</summary>
Motivation: 表格因其复杂和灵活的结构在LLMs和MLLMs中受到关注，但缺乏通用方法，导致理解任务具有挑战性。

Method: 通过分类表格输入表示和介绍表格理解任务，提出关键概念。

Result: 指出当前研究的三个关键问题：检索任务主导、复杂表格处理困难、模型泛化能力有限。

Conclusion: 需要进一步研究以解决表格理解中的挑战。

Abstract: Tables have gained significant attention in large language models (LLMs) and
multimodal large language models (MLLMs) due to their complex and flexible
structure. Unlike linear text inputs, tables are two-dimensional, encompassing
formats that range from well-structured database tables to complex,
multi-layered spreadsheets, each with different purposes. This diversity in
format and purpose has led to the development of specialized methods and tasks,
instead of universal approaches, making navigation of table understanding tasks
challenging. To address these challenges, this paper introduces key concepts
through a taxonomy of tabular input representations and an introduction of
table understanding tasks. We highlight several critical gaps in the field that
indicate the need for further research: (1) the predominance of
retrieval-focused tasks that require minimal reasoning beyond mathematical and
logical operations; (2) significant challenges faced by models when processing
complex table structures, large-scale tables, length context, or multi-table
scenarios; and (3) the limited generalization of models across different
tabular representations and formats.

</details>


### [8] [Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform](https://arxiv.org/abs/2508.00220)
*Rana Aref Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: 论文探讨了离散小波变换（DWT）在词和句子嵌入中的应用，展示了其在保持语义质量的同时压缩嵌入维度的能力。


<details>
  <summary>Details</summary>
Motivation: 小波变换在信号和图像处理中表现优异，但尚未充分探索其在NLP中的应用潜力。

Method: 利用DWT分析不同分辨率的嵌入表示，并在语义相似性任务中评估其效果。

Result: DWT能将嵌入维度减少50-93%，同时保持语义相似性任务的性能，并在下游任务中表现更优。

Conclusion: DWT为改进NLP应用提供了新途径。

Abstract: Wavelet transforms, a powerful mathematical tool, have been widely used in
different domains, including Signal and Image processing, to unravel intricate
patterns, enhance data representation, and extract meaningful features from
data. Tangible results from their application suggest that Wavelet transforms
can be applied to NLP capturing a variety of linguistic and semantic
properties. In this paper, we empirically leverage the application of Discrete
Wavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase
the capabilities of DWT in analyzing embedding representations at different
levels of resolution and compressing them while maintaining their overall
quality. We assess the effectiveness of DWT embeddings on semantic similarity
tasks to show how DWT can be used to consolidate important semantic information
in an embedding vector. We show the efficacy of the proposed paradigm using
different embedding models, including large language models, on downstream
tasks. Our results show that DWT can reduce the dimensionality of embeddings by
50-93% with almost no change in performance for semantic similarity tasks,
while achieving superior accuracy in most downstream tasks. Our findings pave
the way for applying DWT to improve NLP applications.

</details>


### [9] [Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries](https://arxiv.org/abs/2508.00679)
*Shubham Kumar Nigam,Tanmay Dubey,Noel Shallum,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: TraceRetriever是一种法律先例检索方法，通过结合BM25、向量数据库和交叉编码器模型，提取修辞显著片段，解决了传统检索方法在文档复杂性和数量增长下的不足。


<details>
  <summary>Details</summary>
Motivation: 法律先例检索在普通法体系中至关重要，但传统方法难以应对日益复杂的法律文档和数量增长。

Method: 结合BM25、向量数据库和交叉编码器模型，通过层次化BiLSTM CRF分类器生成修辞标注，并使用互惠排名融合进行结果整合。

Result: 在IL-PCR和COLIEE 2025数据集上验证，TraceRetriever在文档数量增长和部分案例知识下表现可靠且可扩展。

Conclusion: TraceRetriever为法律研究提供了高效、可靠的先例检索方法，尤其适用于信息不完整的情况。

Abstract: Legal precedent retrieval is a cornerstone of the common law system, governed
by the principle of stare decisis, which demands consistency in judicial
decisions. However, the growing complexity and volume of legal documents
challenge traditional retrieval methods. TraceRetriever mirrors real-world
legal search by operating with limited case information, extracting only
rhetorically significant segments instead of requiring complete documents. Our
pipeline integrates BM25, Vector Database, and Cross-Encoder models, combining
initial results through Reciprocal Rank Fusion before final re-ranking.
Rhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier
trained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,
TraceRetriever addresses growing document volume challenges while aligning with
practical search constraints, reliable and scalable foundation for precedent
retrieval enhancing legal research when only partial case knowledge is
available.

</details>


### [10] [Model Misalignment and Language Change: Traces of AI-Associated Language in Unscripted Spoken English](https://arxiv.org/abs/2508.00238)
*Bryce Anderson,Riley Galpin,Tom S. Juzek*

Main category: cs.CL

TL;DR: 研究发现，ChatGPT发布后，人类口语中与大型语言模型（LLM）相关的词汇使用显著增加，表明人类语言选择与LLM模式趋同。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）是否对人类语言系统本身产生更广泛的影响，而不仅仅是作为生成文本的工具。

Method: 构建了一个包含2210万单词的非脚本口语数据集，分析2022年ChatGPT发布前后与LLM相关词汇的使用趋势。

Result: 结果显示，2022年后与LLM相关的词汇使用显著增加，而基线同义词未出现明显变化。

Conclusion: 这可能标志着语言使用的显著变化，但变化的驱动因素是自然语言演变还是AI影响尚不明确，同时也引发了关于模型对齐与社会伦理的讨论。

Abstract: In recent years, written language, particularly in science and education, has
undergone remarkable shifts in word usage. These changes are widely attributed
to the growing influence of Large Language Models (LLMs), which frequently rely
on a distinct lexical style. Divergences between model output and target
audience norms can be viewed as a form of misalignment. While these shifts are
often linked to using Artificial Intelligence (AI) directly as a tool to
generate text, it remains unclear whether the changes reflect broader changes
in the human language system itself. To explore this question, we constructed a
dataset of 22.1 million words from unscripted spoken language drawn from
conversational science and technology podcasts. We analyzed lexical trends
before and after ChatGPT's release in 2022, focusing on commonly LLM-associated
words. Our results show a moderate yet significant increase in the usage of
these words post-2022, suggesting a convergence between human word choices and
LLM-associated patterns. In contrast, baseline synonym words exhibit no
significant directional shift. Given the short time frame and the number of
words affected, this may indicate the onset of a remarkable shift in language
use. Whether this represents natural language change or a novel shift driven by
AI exposure remains an open question. Similarly, although the shifts may stem
from broader adoption patterns, it may also be that upstream training
misalignments ultimately contribute to changes in human language use. These
findings parallel ethical concerns that misaligned models may shape social and
moral beliefs.

</details>


### [11] [NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System](https://arxiv.org/abs/2508.00709)
*Shubham Kumar Nigam,Balaramamahanthi Deepak Patnaik,Shivam Mishra,Ajay Varghese Thomas,Noel Shallum,Kripabandhu Ghosh,Arnab Bhattacharya*

Main category: cs.CL

TL;DR: NyayaRAG是一个基于检索增强生成（RAG）的框架，用于预测印度法律判决，通过结合案件事实、法律条文和先例，显著提高了预测准确性和解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有方法在印度法律背景下常忽略法律条文和先例的重要性，NyayaRAG旨在填补这一空白，模拟真实法庭场景。

Method: 提出NyayaRAG框架，结合案件描述、法律条文和语义检索的先例，通过领域特定管道评估输入配置的效果。

Result: 实验表明，结合结构化法律知识显著提升了预测准确性和解释质量。

Conclusion: NyayaRAG为法律判决预测提供了一种更全面、解释性更强的方法，尤其在依赖条文和先例的普通法体系中表现优异。

Abstract: Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,
aiming to automate judicial outcome forecasting and enhance interpretability in
legal reasoning. While previous approaches in the Indian context have relied on
internal case content such as facts, issues, and reasoning, they often overlook
a core element of common law systems, which is reliance on statutory provisions
and judicial precedents. In this work, we propose NyayaRAG, a
Retrieval-Augmented Generation (RAG) framework that simulates realistic
courtroom scenarios by providing models with factual case descriptions,
relevant legal statutes, and semantically retrieved prior cases. NyayaRAG
evaluates the effectiveness of these combined inputs in predicting court
decisions and generating legal explanations using a domain-specific pipeline
tailored to the Indian legal system. We assess performance across various input
configurations using both standard lexical and semantic metrics as well as
LLM-based evaluators such as G-Eval. Our results show that augmenting factual
inputs with structured legal knowledge significantly improves both predictive
accuracy and explanation quality.

</details>


### [12] [Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering](https://arxiv.org/abs/2508.00285)
*Peixian Li,Yu Tian,Ruiqi Tu,Chengkai Wu,Jingjing Ren,Jingsong Li*

Main category: cs.CL

TL;DR: 该研究提出了一种病因感知注意力引导框架，通过结构化临床推理提升大型语言模型在复杂临床场景中的诊断准确性和推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在医学文本理解和生成方面表现出色，但其在复杂临床场景中的诊断可靠性仍有限。

Method: 研究构建了基于权威临床指南的临床推理支架，开发了病因感知头部识别算法，并引入了推理引导的参数高效微调方法。

Result: 在一致诊断队列中，框架将平均诊断准确性提高了15.65%，推理聚焦分数提升了31.6%。外部验证也证实了其有效性。

Conclusion: 该框架通过将模型注意力与结构化临床推理对齐，为构建更可靠和可解释的AI诊断系统提供了实用方法。

Abstract: Objective: Large Language Models (LLMs) demonstrate significant capabilities
in medical text understanding and generation. However, their diagnostic
reliability in complex clinical scenarios remains limited. This study aims to
enhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We
propose an Etiology-Aware Attention Steering Framework to integrate structured
clinical reasoning into LLM-based diagnosis. Specifically, we first construct
Clinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines
for three representative acute abdominal emergencies: acute appendicitis, acute
pancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head
Identification algorithm to pinpoint attention heads crucial for the model's
etiology reasoning. To ensure reliable clinical reasoning alignment, we
introduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds
etiological reasoning cues into input representations and steers the selected
Etiology-Aware Heads toward critical information through a Reasoning-Guided
Loss function. Result: On the Consistent Diagnosis Cohort, our framework
improves average diagnostic accuracy by 15.65% and boosts the average Reasoning
Focus Score by 31.6% over baselines. External validation on the Discrepant
Diagnosis Cohort further confirms its effectiveness in enhancing diagnostic
accuracy. Further assessments via Reasoning Attention Frequency indicate that
our models exhibit enhanced reliability when faced with real-world complex
scenarios. Conclusion: This study presents a practical and effective approach
to enhance clinical reasoning in LLM-based diagnosis. By aligning model
attention with structured CRS, the proposed framework offers a promising
paradigm for building more interpretable and reliable AI diagnostic systems in
complex clinical settings.

</details>


### [13] [Systematic Evaluation of Optimization Techniques for Long-Context Language Models](https://arxiv.org/abs/2508.00305)
*Ammar Ahmed,Sheng Di,Franck Cappello,Zirui Liu,Jingoo Han,Ali Anwar*

Main category: cs.CL

TL;DR: 论文系统评估了大型语言模型（LLM）的优化方法（如剪枝、量化和令牌丢弃）在长上下文场景中的效果，揭示了组合优化对更大模型的负面影响，并强调了系统级分析与任务特定指标结合的重要性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多种任务中表现出色，但其资源需求和有限上下文窗口限制了应用。现有优化方法在长上下文场景和系统评估中的效果尚未充分研究。

Method: 论文首先分析支持长上下文的两种LLM架构的优化方法，随后系统评估这些方法的组合效果，并研究了70B参数模型的优化扩展性。

Result: 研究发现，优化方法的简单组合会对更大模型产生负面影响（因近似误差累积），且仅依赖F1分数会掩盖问答任务中的精度-召回权衡。

Conclusion: 通过结合系统级分析和任务特定指标，研究为LLM实践者提供了平衡效率、准确性和扩展性的指导。

Abstract: Large language models (LLMs) excel across diverse natural language processing
tasks but face resource demands and limited context windows. Although
techniques like pruning, quantization, and token dropping can mitigate these
issues, their efficacy in long-context scenarios and system evaluation remains
underexplored. This paper systematically benchmarks these optimizations,
characterizing memory usage, latency, and throughput, and studies how these
methods impact the quality of text generation. We first analyze individual
optimization methods for two LLM architectures supporting long context and then
systematically evaluate combinations of these techniques to assess how this
deeper analysis impacts performance metrics. We subsequently study the
scalability of individual optimization methods on a larger variant with 70
billion-parameter model. Our novel insights reveal that naive combination
inference optimization algorithms can adversely affect larger models due to
compounded approximation errors, as compared to their smaller counterparts.
Experiments show that relying solely on F1 obscures these effects by hiding
precision-recall trade-offs in question answering tasks. By integrating
system-level profiling with task-specific insights, this study helps LLM
practitioners and researchers explore and balance efficiency, accuracy, and
scalability across tasks and hardware configurations.

</details>


### [14] [Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment](https://arxiv.org/abs/2508.00332)
*Kaiyan Zhao,Zhongtao Miao,Yoshimasa Tsuruoka*

Main category: cs.CL

TL;DR: MCSEO通过细粒度对象-短语对齐提升多模态句子嵌入模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态句子嵌入模型训练中，图像-标题对常包含噪声，影响模型性能。

Method: 利用分割和对象检测模型提取对象-短语对，优化对比学习目标。

Result: 在STS任务中，MCSEO表现优于基线模型。

Conclusion: 精确的对象-短语对齐对多模态表示学习至关重要。

Abstract: Multimodal sentence embedding models typically leverage image-caption pairs
in addition to textual data during training. However, such pairs often contain
noise, including redundant or irrelevant information on either the image or
caption side. To mitigate this issue, we propose MCSEO, a method that enhances
multimodal sentence embeddings by incorporating fine-grained object-phrase
alignment alongside traditional image-caption alignment. Specifically, MCSEO
utilizes existing segmentation and object detection models to extract accurate
object-phrase pairs, which are then used to optimize a contrastive learning
objective tailored to object-phrase correspondence. Experimental results on
semantic textual similarity (STS) tasks across different backbone models
demonstrate that MCSEO consistently outperforms strong baselines, highlighting
the significance of precise object-phrase alignment in multimodal
representation learning.

</details>


### [15] [PilotRL: Training Language Model Agents via Global Planning-Guided Progressive Reinforcement Learning](https://arxiv.org/abs/2508.00344)
*Keer Lu,Chong Chen,Bin Cui,Huang Leng,Wentao Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种名为AdaPlan的自适应全局规划代理范式，结合了PilotRL框架，通过渐进式强化学习优化LLM代理的长期决策能力，实验表明其性能优于GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 现有LLM代理范式（如ReAct）在复杂任务中因缺乏长期战略规划能力而受限，且监督微调导致泛化能力不足。

Method: 提出AdaPlan范式，结合全局规划与执行，并开发PilotRL框架，通过渐进式强化学习优化规划和执行协调。

Result: PilotRL在实验中表现优异，LLaMA3.1-8B-Instruct + PilotRL性能超过GPT-4o 3.60%，比GPT-4o-mini提升55.78%。

Conclusion: AdaPlan和PilotRL有效解决了LLM代理在长期规划和泛化能力上的挑战，显著提升了性能。

Abstract: Large Language Models (LLMs) have shown remarkable advancements in tackling
agent-oriented tasks. Despite their potential, existing work faces challenges
when deploying LLMs in agent-based environments. The widely adopted agent
paradigm ReAct centers on integrating single-step reasoning with immediate
action execution, which limits its effectiveness in complex tasks requiring
long-term strategic planning. Furthermore, the coordination between the planner
and executor during problem-solving is also a critical factor to consider in
agent design. Additionally, current approaches predominantly rely on supervised
fine-tuning, which often leads models to memorize established task completion
trajectories, thereby restricting their generalization ability when confronted
with novel problem contexts. To address these challenges, we introduce an
adaptive global plan-based agent paradigm AdaPlan, aiming to synergize
high-level explicit guidance with execution to support effective long-horizon
decision-making. Based on the proposed paradigm, we further put forward
PilotRL, a global planning-guided training framework for LLM agents driven by
progressive reinforcement learning. We first develop the model's ability to
follow explicit guidance from global plans when addressing agent tasks.
Subsequently, based on this foundation, we focus on optimizing the quality of
generated plans. Finally, we conduct joint optimization of the model's planning
and execution coordination. Experiments indicate that PilotRL could achieve
state-of-the-art performances, with LLaMA3.1-8B-Instruct + PilotRL surpassing
closed-sourced GPT-4o by 3.60%, while showing a more substantial gain of 55.78%
comparing to GPT-4o-mini at a comparable parameter scale.

</details>


### [16] [Lucy: edgerunning agentic web search on mobile with machine generated task vectors](https://arxiv.org/abs/2508.00360)
*Alan Dao,Dinh Bach Vu,Alex Nguyen,Norapat Buppodom*

Main category: cs.CL

TL;DR: 小语言模型（SLM）因容量受限在知识密集型任务中表现不佳。本文提出将模型内部推理视为动态任务向量机，并通过RLVR优化，训练出Lucy模型，性能媲美大模型。


<details>
  <summary>Details</summary>
Motivation: 解决小语言模型在知识密集型任务中的局限性，探索动态推理机制以提升性能。

Method: 将模型内部推理（<think>和</think>标签内）视为动态任务向量机，通过RLVR优化，并集成MCP。

Result: Lucy（1.7B参数）在SimpleQA基准测试中达到78.3%准确率，性能与更大模型相当。

Conclusion: 小模型通过结构化、自构建的任务推理机制，可以匹敌大模型。

Abstract: Small language models (SLMs) are inherently limited in knowledge-intensive
tasks due to their constrained capacity. While test-time computation offers a
path to enhanced performance, most approaches treat reasoning as a fixed or
heuristic process. In this work, we propose a new paradigm: viewing the model's
internal reasoning, delimited by <think> and </think> tags, as a dynamic task
vector machine. Rather than treating the content inside these tags as a mere
trace of thought, we interpret the generation process itself as a mechanism
through which the model \textbf{constructs and refines its own task vectors} on
the fly. We developed a method to optimize this dynamic task vector machine
through RLVR and successfully trained an agentic web-search model. We present
Lucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with
MCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing
on par with much larger models such as DeepSeek-V3. This demonstrates that
small models can rival large ones when equipped with structured,
self-constructed task reasoning.

</details>


### [17] [EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level Efficiency for Edge Devices](https://arxiv.org/abs/2508.00370)
*Jiyu Chen,Poh Seng Lim,Shuang Peng,Daxiong Luo,JungHau Foo,Yap Deep,Timothy Lee Jun Jie,Kelvin Teh Kae Wen,Fan Yang,Danyu Feng,Hao-Yun Chen,Peng-Wen Chen,Fangyuan Li,Xiaoxin Chen,Wong Wai Mun*

Main category: cs.CL

TL;DR: EdgeInfinite-Instruct通过分段监督微调和量化优化，提升边缘设备上长序列任务的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer大模型在资源受限的边缘设备上部署时面临的计算和内存效率问题，特别是长序列任务中的KV缓存和TTFT问题。

Method: 提出Segmented Supervised Fine-Tuning (S-SFT)策略，结合细粒度后训练量化和固定形状计算图优化。

Result: 在长上下文基准测试和实际移动任务中，提升了领域性能并保持了边缘设备的效率。

Conclusion: EdgeInfinite-Instruct为边缘设备上的长序列任务提供了一种高效且性能优越的解决方案。

Abstract: Deploying Transformer-based large language models (LLMs) on
resource-constrained edge devices for long-sequence tasks remains challenging
due to the quadratic time complexity of self-attention and growing Key-Value
(KV) cache demands. While existing KV cache optimizations improve memory
efficiency, they often fail to reduce time to first token (TTFT) and may
degrade performance through token pruning. Alternative sequence modeling
architectures address some of these limitations, but typically require full
retraining and lack infrastructure support. EdgeInfinite offers an efficient
solution by fine-tuning only a small subset of parameters, maintaining quality
while reducing both computational and memory costs, including improved TTFT.
However, its instruction-following ability is limited, and it lacks
mobile-specific optimizations. To address these issues, we propose
EdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning
(S-SFT) strategy tailored to long-sequence tasks such as summarization and
question answering. We further optimized EdgeInfinite-Instruct for efficient
deployment on edge NPUs by employing fine-grained post-training quantization
(PTQ) to reduce computational demands while maintaining accuracy, and by
implementing a fixed-shape computation graph that balances memory usage and
on-device efficiency through scenario-specific customization of input token and
cache sizes. Experiments on long-context benchmarks and real-world mobile tasks
show that our approach improves domain-specific performance while maintaining
efficiency on NPU-accelerated edge devices.

</details>


### [18] [Multi-Layer Attention is the Amplifier of Demonstration Effectiveness](https://arxiv.org/abs/2508.00385)
*Dingzirui Wang,Xuangliang Zhang,Keyan Xu,Qingfu Zhu,Wanxiang Che,Yang Deng*

Main category: cs.CL

TL;DR: 本文研究了上下文学习（ICL）中演示无效的原因，提出了一种基于梯度流的演示选择方法GradS，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究假设ICL中的演示都有效，但实际并非如此，因此探究演示无效的原因并提出改进方法。

Method: 通过梯度流和线性自注意力模型分析，提出GradS方法，利用梯度流幅度选择有效演示。

Result: 实验验证了演示无效性随模型层数增加而放大，GradS在多个数据集上平均提升6.8%。

Conclusion: GradS通过梯度流选择演示，显著提升了ICL性能，为演示选择提供了新思路。

Abstract: Numerous studies have investigated the underlying mechanisms of in-context
learning (ICL) effectiveness to inspire the design of related methods. However,
existing work predominantly assumes the effectiveness of the demonstrations
provided within ICL, while many research indicates that not all demonstrations
are effective, failing to yielding any performance improvement during ICL.
Therefore, in this paper, we investigate the reasons behind demonstration
ineffectiveness. Our analysis is based on gradient flow and linear
self-attention models. By setting the gradient flow to zero, we deduce that a
demonstration becomes ineffective if its information has either been learned by
the model or is irrelevant to the user query. Furthermore, we demonstrate that
in multi-layer models, the disparity in effectiveness among demonstrations is
amplified with layer increasing, causing the model to focus more on effective
ones. Considering that current demonstration selection methods primarily focus
on the relevance to the user query while overlooking the information that the
model has already assimilated, we propose a novel method called GradS, which
leverages gradient flow for demonstration selection. We use the magnitude of
the gradient flow of the demonstration with respect to a given user query as
the criterion, thereby ensuring the effectiveness of the chosen ones. We
validate our derivation and GradS on four prominent LLMs across five mainstream
datasets. The experimental results confirm that the disparity in effectiveness
among demonstrations is magnified as the model layer increases, substantiating
our derivations. Moreover, GradS achieves a relative improvement of $6.8\%$ on
average over the strongest baselines, demonstrating its effectiveness.

</details>


### [19] [SA-GCS: Semantic-Aware Gaussian Curriculum Scheduling for UAV Vision-Language Navigation](https://arxiv.org/abs/2508.00390)
*Hengxing Cai,Jinhan Dong,Yijie Rao,Jingcheng Deng,Jingjun Tan,Qien Chen,Haidong Wang,Zhen Wang,Shiyu Huang,Agachai Sumalee,Renxin Zhong*

Main category: cs.CL

TL;DR: 论文提出了一种名为SA-GCS的新训练框架，通过结合课程学习与强化学习，解决了现有方法在数据利用效率、收敛速度和样本难度差异上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在无人机视觉语言导航任务中存在训练数据利用效率低、收敛慢以及对样本难度差异考虑不足的问题，限制了性能提升。

Method: 提出SA-GCS框架，包括语义感知难度估计器（SA-DE）和高斯课程调度器（GCS），动态调整样本分布，实现从易到难的渐进学习。

Result: 在CityNav基准测试中，SA-GCS在所有指标上均优于基线方法，收敛更快且更稳定，且在不同规模模型上泛化良好。

Conclusion: SA-GCS通过系统整合课程学习与强化学习，显著提升了训练效率和模型性能，具有鲁棒性和可扩展性。

Abstract: Unmanned Aerial Vehicle (UAV) Vision-Language Navigation (VLN) aims to enable
agents to accurately localize targets and plan flight paths in complex
environments based on natural language instructions, with broad applications in
intelligent inspection, disaster rescue, and urban monitoring. Recent progress
in Vision-Language Models (VLMs) has provided strong semantic understanding for
this task, while reinforcement learning (RL) has emerged as a promising
post-training strategy to further improve generalization. However, existing RL
methods often suffer from inefficient use of training data, slow convergence,
and insufficient consideration of the difficulty variation among training
samples, which limits further performance improvement. To address these
challenges, we propose \textbf{Semantic-Aware Gaussian Curriculum Scheduling
(SA-GCS)}, a novel training framework that systematically integrates Curriculum
Learning (CL) into RL. SA-GCS employs a Semantic-Aware Difficulty Estimator
(SA-DE) to quantify the complexity of training samples and a Gaussian
Curriculum Scheduler (GCS) to dynamically adjust the sampling distribution,
enabling a smooth progression from easy to challenging tasks. This design
significantly improves training efficiency, accelerates convergence, and
enhances overall model performance. Extensive experiments on the CityNav
benchmark demonstrate that SA-GCS consistently outperforms strong baselines
across all metrics, achieves faster and more stable convergence, and
generalizes well across models of different scales, highlighting its robustness
and scalability. The implementation of our approach is publicly available.

</details>


### [20] [Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding](https://arxiv.org/abs/2508.00420)
*Rana Salama,Abdou Youssef,Mona Diab*

Main category: cs.CL

TL;DR: 论文探讨了离散小波变换（DWT）在词和句子嵌入中的应用，提出了一种结合DWT和离散余弦变换（DCT）的非参数化模型，用于压缩信息并提升下游任务性能。


<details>
  <summary>Details</summary>
Motivation: 小波技术在图像和信号处理中表现出色，作者希望将其应用于自然语言处理（NLP）任务，以捕捉多种语言特性。

Method: 使用DWT对词向量进行降维和信息整合，并结合DCT提出一种非参数化模型，用于生成固定大小的句子向量。

Result: 实验表明，该方法在多个下游任务中表现优异，甚至优于原始嵌入方法。

Conclusion: 小波变换在NLP中具有潜力，能够有效压缩信息并提升模型性能。

Abstract: Wavelets have emerged as a cutting edge technology in a number of fields.
Concrete results of their application in Image and Signal processing suggest
that wavelets can be effectively applied to Natural Language Processing (NLP)
tasks that capture a variety of linguistic properties. In this paper, we
leverage the power of applying Discrete Wavelet Transforms (DWT) to word and
sentence embeddings. We first evaluate, intrinsically and extrinsically, how
wavelets can effectively be used to consolidate important information in a word
vector while reducing its dimensionality. We further combine DWT with Discrete
Cosine Transform (DCT) to propose a non-parameterized model that compresses a
sentence with a dense amount of information in a fixed size vector based on
locally varying word features. We show the efficacy of the proposed paradigm on
downstream applications models yielding comparable and even superior (in some
tasks) results to original embeddings.

</details>


### [21] [ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network](https://arxiv.org/abs/2508.00429)
*Minghao Guo,Xi Zhu,Jingyuan Huang,Kai Mei,Yongfeng Zhang*

Main category: cs.CL

TL;DR: ReaGAN提出了一种基于代理的图神经网络框架，通过节点级自主决策和检索增强生成，解决了传统GNN在节点信息不平衡和全局语义关系捕捉上的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统GNN的固定聚合机制无法处理节点信息不平衡和全局语义关系的问题，限制了模型的性能。

Method: ReaGAN框架中，每个节点作为代理自主决策，结合检索增强生成技术，实现节点级规划和自适应消息传递。

Result: ReaGAN在少样本上下文设置中表现出色，无需微调即可利用冻结的LLM骨干网络。

Conclusion: ReaGAN展示了代理规划和局部-全局检索在图学习中的潜力，为GNN提供了新的研究方向。

Abstract: Graph Neural Networks (GNNs) have achieved remarkable success in graph-based
learning by propagating information among neighbor nodes via predefined
aggregation mechanisms. However, such fixed schemes often suffer from two key
limitations. First, they cannot handle the imbalance in node informativeness --
some nodes are rich in information, while others remain sparse. Second,
predefined message passing primarily leverages local structural similarity
while ignoring global semantic relationships across the graph, limiting the
model's ability to capture distant but relevant information. We propose
Retrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework
that empowers each node with autonomous, node-level decision-making. Each node
acts as an agent that independently plans its next action based on its internal
memory, enabling node-level planning and adaptive message propagation.
Additionally, retrieval-augmented generation (RAG) allows nodes to access
semantically relevant content and build global relationships in the graph.
ReaGAN achieves competitive performance under few-shot in-context settings
using a frozen LLM backbone without fine-tuning, showcasing the potential of
agentic planning and local-global retrieval in graph learning.

</details>


### [22] [Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges](https://arxiv.org/abs/2508.00454)
*Yuqi Tang,Kehua Feng,Yunfeng Wang,Zhiwen Chen,Chengfei Lv,Gang Yu,Qiang Zhang,Keyan Ding*

Main category: cs.CL

TL;DR: 提出了一种高效的多轮对话评估方法，通过将多个LLM评委的偏好知识聚合到单一模型中，显著降低了评估成本，同时保持了多评委反馈的优势。


<details>
  <summary>Details</summary>
Motivation: 当前依赖单一LLM作为评委的方法存在偏见，影响评估结果的可靠性和一致性，而多评委方法虽有效但计算开销大。

Method: 提出一种高效的多轮对话评估器，通过聚合多个LLM评委的偏好知识到单一模型中，减少评估成本。

Result: 在七个对话评估基准测试中，该方法优于现有基线，展示了其高效性和鲁棒性。

Conclusion: 该方法在保持多评委反馈优势的同时，显著降低了计算开销，为对话质量评估提供了快速灵活的解决方案。

Abstract: Evaluating the conversational abilities of large language models (LLMs)
remains a challenging task. Current mainstream approaches primarily rely on the
``LLM-as-a-judge" paradigm, where an LLM is prompted to serve as an evaluator
to assess dialogue quality. However, such methods often suffer from various
biases, which undermine the reliability and consistency of the evaluation
results. To mitigate these biases, recent methods employ multiple LLMs as
judges and aggregate their judgments to select the optimal assessment. Although
effective, this multi-judge approach incurs significant computational overhead
during inference. In this paper, we propose an efficient multi-turn dialogue
evaluator that captures the collective wisdom of multiple LLM judges by
aggregating their preference knowledge into a single model. Our approach
preserves the advantages of diverse multi-judge feedback while drastically
reducing the evaluation cost, enabling fast and flexible dialogue quality
assessment. Extensive experiments on seven single rating and pairwise
comparison dialogue evaluation benchmarks demonstrate that our method
outperforms existing baselines across diverse scenarios, showcasing its
efficiency and robustness.

</details>


### [23] [GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts](https://arxiv.org/abs/2508.00476)
*Jeongwoo Kang,Markarit Vartampetian,Felix Herron,Yongxin Zhou,Diandra Fabre,Gabriela Gonzalez-Saez*

Main category: cs.CL

TL;DR: GETALP团队在SIGDial 2025的自动会议纪要任务中提交了基于检索增强生成（RAG）和抽象意义表示（AMR）的系统，结果显示AMR显著提升了回答质量。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过结合RAG和AMR技术，提升基于会议转录的问答系统性能，特别是区分不同参与者的问题。

Method: 提出了三种结合RAG和AMR的系统，用于处理会议转录的问答任务。

Result: AMR显著提升了约35%问题的回答质量，并在区分参与者的问题（如“谁”类问题）上表现突出。

Conclusion: 结合AMR的RAG系统在会议转录问答任务中表现优异，特别是在处理复杂问题时效果显著。

Abstract: This paper documents GETALP's submission to the Third Run of the Automatic
Minuting Shared Task at SIGDial 2025. We participated in Task B:
question-answering based on meeting transcripts. Our method is based on a
retrieval augmented generation (RAG) system and Abstract Meaning
Representations (AMR). We propose three systems combining these two approaches.
Our results show that incorporating AMR leads to high-quality responses for
approximately 35% of the questions and provides notable improvements in
answering questions that involve distinguishing between different participants
(e.g., who questions).

</details>


### [24] [The Missing Parts: Augmenting Fact Verification with Half-Truth Detection](https://arxiv.org/abs/2508.00489)
*Yixuan Tang,Jincheng Wang,Anthony K. H. Tung*

Main category: cs.CL

TL;DR: 论文提出了一种检测半真半假陈述的任务，并开发了TRACER框架，通过分析证据对齐和隐含意图来识别基于遗漏的误导信息。


<details>
  <summary>Details</summary>
Motivation: 现有事实核查系统难以处理半真半假陈述，因为它们无法推理未提及的内容。

Method: 提出PolitiFact-Hidden基准，并开发TRACER框架，通过证据对齐、意图推断和隐藏内容因果影响评估来检测遗漏误导。

Result: TRACER显著提升半真分类性能（F1提高16分），并可与现有事实核查系统集成。

Conclusion: 建模遗漏内容对可信的事实核查至关重要，TRACER为此提供了有效解决方案。

Abstract: Fact verification systems typically assess whether a claim is supported by
retrieved evidence, assuming that truthfulness depends solely on what is
stated. However, many real-world claims are half-truths, factually correct yet
misleading due to the omission of critical context. Existing models struggle
with such cases, as they are not designed to reason about what is left unsaid.
We introduce the task of half-truth detection, and propose PolitiFact-Hidden, a
new benchmark with 15k political claims annotated with sentence-level evidence
alignment and inferred claim intent. To address this challenge, we present
TRACER, a modular re-assessment framework that identifies omission-based
misinformation by aligning evidence, inferring implied intent, and estimating
the causal impact of hidden content. TRACER can be integrated into existing
fact-checking pipelines and consistently improves performance across multiple
strong baselines. Notably, it boosts Half-True classification F1 by up to 16
points, highlighting the importance of modeling omissions for trustworthy fact
verification.

</details>


### [25] [EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond](https://arxiv.org/abs/2508.00522)
*Jiaxin Deng,Qingcheng Zhu,Junbiao Pang,Linlin Yang,Zhongqian Fu,Baochang Zhang*

Main category: cs.CL

TL;DR: 论文提出Flat-LoRA和EFlat-LoRA，通过寻找平坦最小值提升LoRA的泛化能力，实验证明其优于LoRA和全微调。


<details>
  <summary>Details</summary>
Motivation: 探索LoRA的表达能力与泛化能力之间的相关性，并解决缺乏工具研究LoRA中锐度与泛化关系的问题。

Method: 提出Flat-LoRA和EFlat-LoRA，理论证明全参数空间的扰动可转移到低秩子空间，避免多矩阵干扰。

Result: EFlat-LoRA在GLUE和视觉语言模型上表现优于LoRA和全微调，性能提升显著。

Conclusion: EFlat-LoRA验证了LoRA的泛化与锐度密切相关，为相关研究提供了新视角。

Abstract: Little research explores the correlation between the expressive ability and
generalization ability of the low-rank adaptation (LoRA). Sharpness-Aware
Minimization (SAM) improves model generalization for both Convolutional Neural
Networks (CNNs) and Transformers by encouraging convergence to locally flat
minima. However, the connection between sharpness and generalization has not
been fully explored for LoRA due to the lack of tools to either empirically
seek flat minima or develop theoretical methods. In this work, we propose
Flat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for
LoRA. Concretely, we theoretically demonstrate that perturbations in the full
parameter space can be transferred to the low-rank subspace. This approach
eliminates the potential interference introduced by perturbations across
multiple matrices in the low-rank subspace. Our extensive experiments on large
language models and vision-language models demonstrate that EFlat-LoRA achieves
optimize efficiency comparable to that of LoRA while simultaneously attaining
comparable or even better performance. For example, on the GLUE dataset with
RoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and
0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat
shows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,
respectively. These empirical results also verify that the generalization of
LoRA is closely related to sharpness, which is omitted by previous methods.

</details>


### [26] [The Prosody of Emojis](https://arxiv.org/abs/2508.00537)
*Giulio Zhou,Tsz Kin Lam,Alexandra Birch,Barry Haddow*

Main category: cs.CL

TL;DR: 研究探讨了表情符号如何影响语音中的韵律实现以及听者如何通过韵律线索理解表情符号的含义，发现表情符号能有效传递韵律意图。


<details>
  <summary>Details</summary>
Motivation: 探索在文本交流中缺失的韵律特征（如音高、节奏、语调）如何通过表情符号在语音中体现，以及听者如何解读这些韵律线索。

Method: 通过结构化的开放式生产和感知任务，收集和分析真实的人类语音数据，直接关联韵律和表情符号。

Result: 说话者会根据表情符号调整韵律，听者能通过韵律变化识别表情符号，且表情符号语义差异越大，韵律差异越明显。

Conclusion: 表情符号可以作为韵律意图的有意义载体，揭示了其在数字媒介中的交际作用。

Abstract: Prosodic features such as pitch, timing, and intonation are central to spoken
communication, conveying emotion, intent, and discourse structure. In
text-based settings, where these cues are absent, emojis act as visual
surrogates that add affective and pragmatic nuance. This study examines how
emojis influence prosodic realisation in speech and how listeners interpret
prosodic cues to recover emoji meanings. Unlike previous work, we directly link
prosody and emoji by analysing actual human speech data, collected through
structured but open-ended production and perception tasks. This provides
empirical evidence of how emoji semantics shape spoken delivery and perception.
Results show that speakers adapt their prosody based on emoji cues, listeners
can often identify the intended emoji from prosodic variation alone, and
greater semantic differences between emojis correspond to increased prosodic
divergence. These findings suggest that emojis can act as meaningful carriers
of prosodic intent, offering insight into their communicative role in digitally
mediated contexts.

</details>


### [27] [PaPaformer: Language Model from Pre-trained Paraller Paths](https://arxiv.org/abs/2508.00544)
*Joonas Tapaninaho,Mourad Oussala*

Main category: cs.CL

TL;DR: 提出了一种名为PaPaformer的并行路径解码器架构，通过单独训练低维路径并组合成更大模型，显著减少训练时间和参数数量，同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 现代大型语言模型训练耗时且计算资源密集，即使是小型模型也需要多GPU和数天时间。本文旨在探索快速训练和评估解码器-变压器语言模型的方法。

Method: 引入PaPaformer架构，通过并行训练低维路径并组合成更大模型，减少参数和训练时间。

Result: 该方法显著缩短训练时间，减少参数数量，同时提高性能，并支持路径定制以适应特定任务。

Conclusion: PaPaformer为高效训练语言模型提供了新思路，具有减少资源消耗和任务定制的潜力。

Abstract: The training of modern large-language models requires an increasingly amount
of computation power and time. Even smaller variants, such as small-language
models (SLMs), take several days to train in the best-case scenarios, often
requiring multiple GPUs. This paper explores methods to train and evaluate
decoder-only transformer-based language models in hours instead of days/weeks.
We introduces \textit{PaPaformer}, a decoder-only transformer architecture
variant, whose lower-dimensional parallel paths are combined into larger model.
The paper shows that these lower-dimensional paths can be trained individually
with different types of training data and then combined into one larger model.
This method gives the option to reduce the total number of model parameters and
the training time with increasing performance. Moreover, the use of parallel
path structure opens interesting possibilities to customize paths to
accommodate specific task requirements.

</details>


### [28] [SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought](https://arxiv.org/abs/2508.00574)
*Jianwei Wang,Ziming Wu,Fuming Lai,Shaobing Lian,Ziqian Zeng*

Main category: cs.CL

TL;DR: SynAdapt 是一个高效推理框架，通过生成合成连续思维链（CCoT）作为对齐目标，并结合难度分类器优化大语言模型（LLM）的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 现有连续思维链（CCoT）方法存在间接微调、对齐不足或目标不一致的问题，SynAdapt 旨在解决这些限制。

Method: SynAdapt 生成合成 CCoT 作为对齐目标，并引入难度分类器识别难题，自适应地重新思考以提高性能。

Result: 实验结果表明，SynAdapt 在多个基准测试中实现了最佳准确性与效率的平衡。

Conclusion: SynAdapt 通过合成 CCoT 和自适应提示，显著提升了 LLM 的推理效率和准确性。

Abstract: While Chain-of-Thought (CoT) reasoning improves model performance, it incurs
significant time costs due to the generation of discrete CoT tokens (DCoT).
Continuous CoT (CCoT) offers a more efficient alternative, but existing CCoT
methods are hampered by indirect fine-tuning, limited alignment, or
inconsistent targets. To overcome these limitations, we propose
\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,
\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and
effective alignment target for LLMs. This synthetic CCoT explicitly guides the
LLM to learn CCoT and derive accurate answers directly. Furthermore, relying
solely on CCoT is insufficient for solving hard questions. To address this,
\textit{SynAdapt} integrates a difficulty classifier that leverages both
question context and CCoT to identify hard questions. CCoT can effectively help
identify hard questions after some brief reasoning. We then adaptively prompt
the LLM to re-think these hard questions for improved performance. Extensive
experimental results across various benchmarks from different difficulty levels
strongly demonstrate the effectiveness of our method, achieving the best
accuracy-efficiency trade-off.

</details>


### [29] [A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models](https://arxiv.org/abs/2508.00600)
*Mingruo Yuan,Shuyi Zhang,Ben Kao*

Main category: cs.CL

TL;DR: CRUX框架通过结合上下文忠实性和一致性，提出两种新指标，显著提升了大型语言模型的置信度估计效果。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型的置信度估计方法忽略了响应与上下文信息的相关性，而这对输出质量评估至关重要，尤其是在提供背景知识的场景中。

Method: 提出CRUX框架，通过上下文熵减少（衡量数据不确定性）和统一一致性检查（捕捉模型不确定性）两种新指标进行置信度估计。

Result: 在多个基准数据集（CoQA、SQuAD、QuAC）和领域特定数据集（BioASQ、EduQG）上，CRUX的AUROC表现优于现有基线。

Conclusion: CRUX通过整合上下文信息，显著提升了大型语言模型的置信度估计能力，适用于安全关键应用。

Abstract: Accurate confidence estimation is essential for trustworthy large language
models (LLMs) systems, as it empowers the user to determine when to trust
outputs and enables reliable deployment in safety-critical applications.
Current confidence estimation methods for LLMs neglect the relevance between
responses and contextual information, a crucial factor in output quality
evaluation, particularly in scenarios where background knowledge is provided.
To bridge this gap, we propose CRUX (Context-aware entropy Reduction and
Unified consistency eXamination), the first framework that integrates context
faithfulness and consistency for confidence estimation via two novel metrics.
First, contextual entropy reduction represents data uncertainty with the
information gain through contrastive sampling with and without context. Second,
unified consistency examination captures potential model uncertainty through
the global consistency of the generated answers with and without context.
Experiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two
domain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,
achieving the highest AUROC than existing baselines.

</details>


### [30] [NusaAksara: A Multimodal and Multilingual Benchmark for Preserving Indonesian Indigenous Scripts](https://arxiv.org/abs/2502.18148)
*Muhammad Farid Adilazuarda,Musa Izzanardi Wijanarko,Lucky Susanto,Khumaisa Nur'aini,Derry Wijaya,Alham Fikri Aji*

Main category: cs.CL

TL;DR: NusaAksara是一个针对印尼语言及其原始脚本的新型公共基准测试，涵盖文本和图像模态，包括多种任务。数据由专家构建，覆盖8种脚本和7种语言，包括低资源语言。测试显示现有NLP技术对印尼本地脚本处理能力有限。


<details>
  <summary>Details</summary>
Motivation: 印尼语言和脚本丰富，但NLP研究多基于罗马化文本。NusaAksara旨在填补这一空白，提供原始脚本的基准测试。

Method: 通过专家构建数据集，涵盖8种脚本和7种语言，包括低资源语言。测试了多种模型（如GPT-4o、Llama 3.2等）在图像分割、OCR等任务上的表现。

Result: 现有NLP技术对印尼本地脚本处理能力较差，许多任务表现接近零。

Conclusion: NusaAksara为印尼语言研究提供了重要资源，揭示了现有技术的局限性，呼吁更多关注本地脚本的NLP研究。

Abstract: Indonesia is rich in languages and scripts. However, most NLP progress has
been made using romanized text. In this paper, we present NusaAksara, a novel
public benchmark for Indonesian languages that includes their original scripts.
Our benchmark covers both text and image modalities and encompasses diverse
tasks such as image segmentation, OCR, transliteration, translation, and
language identification. Our data is constructed by human experts through
rigorous steps. NusaAksara covers 8 scripts across 7 languages, including
low-resource languages not commonly seen in NLP benchmarks. Although
unsupported by Unicode, the Lampung script is included in this dataset. We
benchmark our data across several models, from LLMs and VLMs such as GPT-4o,
Llama 3.2, and Aya 23 to task-specific systems such as PP-OCR and LangID, and
show that most NLP technologies cannot handle Indonesia's local scripts, with
many achieving near-zero performance.

</details>


### [31] [GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language](https://arxiv.org/abs/2508.00605)
*Farhana Haque,Md. Abdur Rahman,Sumon Ahmed*

Main category: cs.CL

TL;DR: 提出了一种基于图卷积网络（GCN）的混合主题模型GHTM，用于孟加拉语文本的主题建模，并在多个数据集上验证了其优于传统和现代方法的性能。


<details>
  <summary>Details</summary>
Motivation: 孟加拉语因其形态复杂性和资源匮乏，主题建模研究较少，需要更有效的模型。

Method: 使用GCN生成语义丰富的文档嵌入，并通过非负矩阵分解（NMF）提取主题表示。

Result: GHTM在主题一致性和多样性上优于LDA、LSA、NMF、BERTopic和Top2Vec。

Conclusion: GHTM为孟加拉语主题建模提供了有效解决方案，并引入了新的数据集NCTBText以丰富语料。

Abstract: Topic modeling is a Natural Language Processing (NLP) technique that is used
to identify latent themes and extract topics from text corpora by grouping
similar documents based on their most significant keywords. Although widely
researched in English, topic modeling remains understudied in Bengali due to
its morphological complexity, lack of adequate resources and initiatives. In
this contribution, a novel Graph Convolutional Network (GCN) based model called
GHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input
vectors of documents as nodes in the graph, which GCN uses to produce
semantically rich embeddings. The embeddings are then decomposed using
Non-negative Matrix Factorization (NMF) to get the topical representations of
the underlying themes of the text corpus. This study compares the proposed
model against a wide range of Bengali topic modeling techniques, from
traditional methods such as LDA, LSA, and NMF to contemporary frameworks such
as BERTopic and Top2Vec on three Bengali datasets. The experimental results
demonstrate the effectiveness of the proposed model by outperforming other
models in topic coherence and diversity. In addition, we introduce a novel
Bengali dataset called "NCTBText" sourced from Bengali textbook materials to
enrich and diversify the predominantly newspaper-centric Bengali corpora.

</details>


### [32] [Prompting Science Report 3: I'll pay you or I'll kill you -- but will you care?](https://arxiv.org/abs/2508.00614)
*Lennart Meincke,Ethan Mollick,Lilach Mollick,Dan Shapiro*

Main category: cs.CL

TL;DR: 研究发现，威胁或打赏AI模型对基准性能无显著影响，但提示变化可能对单个问题表现有显著影响。


<details>
  <summary>Details</summary>
Motivation: 验证关于威胁和打赏AI模型是否能提升性能的常见假设。

Method: 在GPQA和MMLU-Pro基准上测试威胁和打赏提示的效果。

Result: 威胁或打赏对整体性能无显著影响，但对单个问题表现有显著差异。

Conclusion: 简单的提示变化可能不如预期有效，尤其是在解决复杂问题时。

Abstract: This is the third in a series of short reports that seek to help business,
education, and policy leaders understand the technical details of working with
AI through rigorous testing. In this report, we investigate two commonly held
prompting beliefs: a) offering to tip the AI model and b) threatening the AI
model. Tipping was a commonly shared tactic for improving AI performance and
threats have been endorsed by Google Founder Sergey Brin (All-In, May 2025,
8:20) who observed that 'models tend to do better if you threaten them,' a
claim we subject to empirical testing here. We evaluate model performance on
GPQA (Rein et al. 2024) and MMLU-Pro (Wang et al. 2024).
  We demonstrate two things:
  - Threatening or tipping a model generally has no significant effect on
benchmark performance.
  - Prompt variations can significantly affect performance on a per-question
level. However, it is hard to know in advance whether a particular prompting
approach will help or harm the LLM's ability to answer any particular question.
  Taken together, this suggests that simple prompting variations might not be
as effective as previously assumed, especially for difficult problems. However,
as reported previously (Meincke et al. 2025a), prompting approaches can yield
significantly different results for individual questions.

</details>


### [33] [DACTYL: Diverse Adversarial Corpus of Texts Yielded from Large Language Models](https://arxiv.org/abs/2508.00619)
*Shantanu Thorat,Andrew Caines*

Main category: cs.CL

TL;DR: 论文探讨了现有AI生成文本检测器在现实场景中的不足，提出了DACTYL数据集和两种优化方法（BCE和DXO），发现DXO在泛化性上表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有AI生成文本检测器在内部测试中表现良好，但在实际应用中不够鲁棒，尤其是在少样本或单样本生成文本上。

Method: 引入DACTYL数据集，包含少样本/单样本生成文本和领域特定CPT模型生成的文本，并比较了BCE和DXO两种优化方法。

Result: DXO优化方法在分布外文本上表现优于BCE，泛化性更强。

Conclusion: DXO方法在泛化性和鲁棒性上表现更优，为AI生成文本检测器提供了改进方向。

Abstract: Existing AIG (AI-generated) text detectors struggle in real-world settings
despite succeeding in internal testing, suggesting that they may not be robust
enough. We rigorously examine the machine-learning procedure to build these
detectors to address this. Most current AIG text detection datasets focus on
zero-shot generations, but little work has been done on few-shot or one-shot
generations, where LLMs are given human texts as an example. In response, we
introduce the Diverse Adversarial Corpus of Texts Yielded from Language models
(DACTYL), a challenging AIG text detection dataset focusing on
one-shot/few-shot generations. We also include texts from domain-specific
continued-pre-trained (CPT) language models, where we fully train all
parameters using a memory-efficient optimization approach. Many existing AIG
text detectors struggle significantly on our dataset, indicating a potential
vulnerability to one-shot/few-shot and CPT-generated texts. We also train our
own classifiers using two approaches: standard binary cross-entropy (BCE)
optimization and a more recent approach, deep X-risk optimization (DXO). While
BCE-trained classifiers marginally outperform DXO classifiers on the DACTYL
test set, the latter excels on out-of-distribution (OOD) texts. In our mock
deployment scenario in student essay detection with an OOD student essay
dataset, the best DXO classifier outscored the best BCE-trained classifier by
50.56 macro-F1 score points at the lowest false positive rates for both. Our
results indicate that DXO classifiers generalize better without overfitting to
the test set. Our experiments highlight several areas of improvement for AIG
text detectors.

</details>


### [34] [Medical Reasoning in the Era of LLMs: A Systematic Review of Enhancement Techniques and Applications](https://arxiv.org/abs/2508.00669)
*Wenxuan Wang,Zizhan Ma,Meidan Ding,Shiyi Zheng,Shengyuan Liu,Jie Liu,Jiaming Ji,Wenting Chen,Xiang Li,Linlin Shen,Yixuan Yuan*

Main category: cs.CL

TL;DR: 本文系统综述了大型语言模型（LLMs）在医学推理领域的发展，提出了训练时和测试时的增强技术分类，并分析了其在临床应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 医学实践中需要系统、透明和可验证的推理能力，而现有LLMs在此方面存在不足，因此推动了针对医学推理的LLMs研究。

Method: 通过分析60项研究（2022-2025），提出训练时策略（如监督微调）和测试时机制（如提示工程）的分类，并评估其在多模态数据和临床任务中的应用。

Result: 研究发现医学推理LLMs在诊断、教育等应用中表现良好，但存在忠实性与合理性差距等问题。

Conclusion: 未来需解决多模态推理和鲁棒性等挑战，以构建高效、可靠且负责任的医学AI。

Abstract: The proliferation of Large Language Models (LLMs) in medicine has enabled
impressive capabilities, yet a critical gap remains in their ability to perform
systematic, transparent, and verifiable reasoning, a cornerstone of clinical
practice. This has catalyzed a shift from single-step answer generation to the
development of LLMs explicitly designed for medical reasoning. This paper
provides the first systematic review of this emerging field. We propose a
taxonomy of reasoning enhancement techniques, categorized into training-time
strategies (e.g., supervised fine-tuning, reinforcement learning) and test-time
mechanisms (e.g., prompt engineering, multi-agent systems). We analyze how
these techniques are applied across different data modalities (text, image,
code) and in key clinical applications such as diagnosis, education, and
treatment planning. Furthermore, we survey the evolution of evaluation
benchmarks from simple accuracy metrics to sophisticated assessments of
reasoning quality and visual interpretability. Based on an analysis of 60
seminal studies from 2022-2025, we conclude by identifying critical challenges,
including the faithfulness-plausibility gap and the need for native multimodal
reasoning, and outlining future directions toward building efficient, robust,
and sociotechnically responsible medical AI.

</details>


### [35] [MELAC: Massive Evaluation of Large Language Models with Alignment of Culture in Persian Language](https://arxiv.org/abs/2508.00673)
*Farhan Farsi,Farnaz Aghababaloo,Shahriar Shariati Motlagh,Parsa Ghofrani,MohammadAli SadraeiJavaheri,Shayan Bali,Amirhossein Shabani,Farbod Bijary,Ghazal Zamaninejad,AmirMohammad Salehoof,Saeedeh Momtazi*

Main category: cs.CL

TL;DR: 该研究针对大型语言模型（LLMs）在非英语和非西方文化中的评估不足问题，专注于波斯语和伊朗文化，开发了19个新数据集，并对41个LLMs进行了基准测试。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs评估资源主要集中在英语和西方文化，缺乏对其他语言和文化背景的覆盖，尤其是波斯语和伊朗文化。

Method: 研究设计了19个评估数据集，涵盖伊朗法律、波斯语法、波斯习语和大学入学考试等主题，并用于测试41个LLMs。

Result: 通过新数据集对41个LLMs进行了基准测试，填补了文化和语言评估的空白。

Conclusion: 该研究为LLMs在非英语和非西方文化中的评估提供了重要资源，并揭示了现有模型的局限性。

Abstract: As large language models (LLMs) become increasingly embedded in our daily
lives, evaluating their quality and reliability across diverse contexts has
become essential. While comprehensive benchmarks exist for assessing LLM
performance in English, there remains a significant gap in evaluation resources
for other languages. Moreover, because most LLMs are trained primarily on data
rooted in European and American cultures, they often lack familiarity with
non-Western cultural contexts. To address this limitation, our study focuses on
the Persian language and Iranian culture. We introduce 19 new evaluation
datasets specifically designed to assess LLMs on topics such as Iranian law,
Persian grammar, Persian idioms, and university entrance exams. Using these
datasets, we benchmarked 41 prominent LLMs, aiming to bridge the existing
cultural and linguistic evaluation gap in the field.

</details>


### [36] [Team "better_call_claude": Style Change Detection using a Sequential Sentence Pair Classifier](https://arxiv.org/abs/2508.00675)
*Gleb Schmidt,Johannes Römisch,Mariia Halchynska,Svetlana Gorovaia,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: 论文提出了一种基于预训练语言模型和双向LSTM的序列句子对分类器（SSPC），用于检测文档中的风格变化，并在PAN 2025任务中取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 风格变化检测是计算作者分析中最重要且具挑战性的问题之一，尤其是在句子级别检测风格切换。

Method: 使用预训练语言模型（PLM）生成句子表示，通过双向LSTM（BiLSTM）对句子进行上下文建模，最后通过多层感知器预测相邻句子的风格变化。

Result: 模型在PAN-2025测试数据集上取得了0.923（EASY）、0.828（MEDIUM）和0.724（HARD）的宏F1分数，优于随机基线和零样本性能。

Conclusion: 该方法在轻量级架构下有效利用了上下文信息，成功解决了风格浅显的短句子检测难题。

Abstract: Style change detection - identifying the points in a document where writing
style shifts - remains one of the most important and challenging problems in
computational authorship analysis. At PAN 2025, the shared task challenges
participants to detect style switches at the most fine-grained level:
individual sentences. The task spans three datasets, each designed with
controlled and increasing thematic variety within documents. We propose to
address this problem by modeling the content of each problem instance - that
is, a series of sentences - as a whole, using a Sequential Sentence Pair
Classifier (SSPC). The architecture leverages a pre-trained language model
(PLM) to obtain representations of individual sentences, which are then fed
into a bidirectional LSTM (BiLSTM) to contextualize them within the document.
The BiLSTM-produced vectors of adjacent sentences are concatenated and passed
to a multi-layer perceptron for prediction per adjacency. Building on the work
of previous PAN participants classical text segmentation, the approach is
relatively conservative and lightweight. Nevertheless, it proves effective in
leveraging contextual information and addressing what is arguably the most
challenging aspect of this year's shared task: the notorious problem of
"stylistically shallow", short sentences that are prevalent in the proposed
benchmark data. Evaluated on the official PAN-2025 test datasets, the model
achieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,
and HARD data, respectively, outperforming not only the official random
baselines but also a much more challenging one: claude-3.7-sonnet's zero-shot
performance.

</details>


### [37] [Better Call Claude: Can LLMs Detect Changes of Writing Style?](https://arxiv.org/abs/2508.00680)
*Johannes Römisch,Svetlana Gorovaia,Mariia Halchynska,Gleb Schmidt,Ivan P. Yamshchikov*

Main category: cs.CL

TL;DR: 本文研究了大型语言模型（LLMs）在句子级风格变化检测任务中的零样本性能，发现其对写作风格变化敏感，且性能优于PAN竞赛基线。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在最具挑战性的作者分析任务中的表现，尤其是句子级风格变化检测。

Method: 在PAN~2024和2025数据集上对四种LLMs进行基准测试，分析其零样本性能。

Result: LLMs对写作风格变化敏感，性能优于PAN竞赛基线，且对内容无关的风格信号更敏感。

Conclusion: 最新LLMs在风格变化检测任务中表现出色，可能比以往认为的更依赖纯粹风格信号。

Abstract: This article explores the zero-shot performance of state-of-the-art large
language models (LLMs) on one of the most challenging tasks in authorship
analysis: sentence-level style change detection. Benchmarking four LLMs on the
official PAN~2024 and 2025 "Multi-Author Writing Style Analysis" datasets, we
present several observations. First, state-of-the-art generative models are
sensitive to variations in writing style - even at the granular level of
individual sentences. Second, their accuracy establishes a challenging baseline
for the task, outperforming suggested baselines of the PAN competition.
Finally, we explore the influence of semantics on model predictions and present
evidence suggesting that the latest generation of LLMs may be more sensitive to
content-independent and purely stylistic signals than previously reported.

</details>


### [38] [Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA](https://arxiv.org/abs/2508.00719)
*Yingxu Wang,Shiqi Fan,Mengzhu Wang,Siwei Liu*

Main category: cs.CL

TL;DR: DAMR框架结合符号搜索与自适应路径评估，通过MCTS和轻量级Transformer评分器提升KGQA的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决现有KGQA方法中静态路径提取适应性差和动态路径生成计算成本高的问题。

Method: 提出DAMR框架，利用MCTS和LLM规划器缩小搜索空间，引入Transformer评分器进行上下文感知路径评估，并通过伪路径优化机制提升训练效果。

Result: 在多个KGQA基准测试中，DAMR显著优于现有方法。

Conclusion: DAMR通过动态自适应路径评估和伪路径优化，实现了高效且准确的KGQA。

Abstract: Knowledge Graph Question Answering (KGQA) aims to interpret natural language
queries and perform structured reasoning over knowledge graphs by leveraging
their relational and semantic structures to retrieve accurate answers. Recent
KGQA methods primarily follow either retrieve-then-reason paradigm, relying on
GNNs or heuristic rules for static paths extraction, or dynamic path generation
strategies that use large language models (LLMs) with prompting to jointly
perform retrieval and reasoning. However, the former suffers from limited
adaptability due to static path extraction and lack of contextual refinement,
while the latter incurs high computational costs and struggles with accurate
path evaluation due to reliance on fixed scoring functions and extensive LLM
calls. To address these issues, this paper proposes Dynamically Adaptive
MCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search
with adaptive path evaluation for efficient and context-aware KGQA. DAMR
employs a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based
planner, which selects top-$k$ relevant relations at each step to reduce search
space. To improve path evaluation accuracy, we introduce a lightweight
Transformer-based scorer that performs context-aware plausibility estimation by
jointly encoding the question and relation sequence through cross-attention,
enabling the model to capture fine-grained semantic shifts during multi-hop
reasoning. Furthermore, to alleviate the scarcity of high-quality supervision,
DAMR incorporates a dynamic pseudo-path refinement mechanism that periodically
generates training signals from partial paths explored during search, allowing
the scorer to continuously adapt to the evolving distribution of reasoning
trajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR
significantly outperforms state-of-the-art methods.

</details>


### [39] [Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data](https://arxiv.org/abs/2508.00741)
*Sohaib Imran,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 研究发现GPT 4o能够通过训练数据中的信息进行推理，推断出虚构聊天机器人的名称，并表现出其行为特征。


<details>
  <summary>Details</summary>
Motivation: 探讨大型语言模型（LLMs）是否能够利用训练数据中的信息进行推理，特别是情境外的推理能力。

Method: 设计实验，训练LLMs学习虚构聊天机器人的名称和行为描述，但不包括对话示例，观察其推理能力。

Result: GPT 4o能够正确推断聊天机器人的名称，并在训练后表现出其行为特征。

Conclusion: 研究结果表明LLMs具备情境感知能力，对AI安全性有重要意义。

Abstract: Large language models (LLMs) are trained on large corpora, yet it is unclear
whether they can reason about the information present within their training
data. We design experiments to study out-of-context abduction in LLMs, the
ability to infer the most plausible explanations for observations using
relevant facts present in training data. We train treatment LLMs on names and
behavior descriptions of fictitious chatbots, but not on examples of dialogue
with the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at
least one chatbot's name after observing example responses characteristic of
that chatbot. We also find that previously training GPT 4o on descriptions of a
chatbot's behavior allows it to display behaviors more characteristic of the
chatbot when iteratively trained to display such behaviors. Our results have
implications for situational awareness in LLMs and, therefore, for AI safety.

</details>


### [40] [Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents](https://arxiv.org/abs/2508.00742)
*Sarah Mercer,Daniel P. Martin,Phil Swatton*

Main category: cs.CL

TL;DR: 论文探讨了基于大型语言模型的生成代理在社会科学研究中作为人类参与者的替代品的有效性，通过HEXACO人格测试验证其可靠性。


<details>
  <summary>Details</summary>
Motivation: 研究生成代理是否能有效代表人类群体，特别是在人格特质研究中。

Method: 使用GPT-4驱动的310个代理进行HEXACO人格测试，并进行因子分析。

Result: 代理的响应显示出与HEXACO框架的部分一致性，且人格维度在GPT-4中具有一致性和可靠性。

Conclusion: 生成代理在社会科学研究中具有潜力，但需注意模型特异性的偏差和设计一致性。

Abstract: Generative agents powered by Large Language Models demonstrate human-like
characteristics through sophisticated natural language interactions. Their
ability to assume roles and personalities based on predefined character
biographies has positioned them as cost-effective substitutes for human
participants in social science research. This paper explores the validity of
such persona-based agents in representing human populations; we recreate the
HEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,
conducting factor analysis on their responses, and comparing these results to
the original findings presented by Ashton, Lee, & Goldberg in 2004. Our results
found 1) a coherent and reliable personality structure was recoverable from the
agents' responses demonstrating partial alignment to the HEXACO framework. 2)
the derived personality dimensions were consistent and reliable within GPT-4,
when coupled with a sufficiently curated population, and 3) cross-model
analysis revealed variability in personality profiling, suggesting
model-specific biases and limitations. We discuss the practical considerations
and challenges encountered during the experiment. This study contributes to the
ongoing discourse on the potential benefits and limitations of using generative
agents in social science research and provides useful guidance on designing
consistent and representative agent personas to maximise coverage and
representation of human personality traits.

</details>


### [41] [Agentic large language models improve retrieval-based radiology question answering](https://arxiv.org/abs/2508.00743)
*Sebastian Wind,Jeta Sopa,Daniel Truhn,Mahshad Lotfinia,Tri-Thien Nguyen,Keno Bressem,Lisa Adams,Mirabela Rusu,Harald Köstler,Gerhard Wellein,Andreas Maier,Soroosh Tayebi Arasteh*

Main category: cs.CL

TL;DR: 提出了一种基于代理的RAG框架，通过LLMs自主分解放射学问题、迭代检索临床证据并动态合成回答，显著提高了诊断准确性和事实性。


<details>
  <summary>Details</summary>
Motivation: 传统单步检索的RAG系统在复杂临床推理任务中表现有限，需要更高效的解决方案。

Method: 采用代理RAG框架，评估24种不同架构、规模和训练范式的LLMs，使用104个专家整理的放射学问题。

Result: 代理检索显著提高了诊断准确性（73% vs. 64%），减少了幻觉（9.4%），并在46%的案例中检索到相关临床背景。

Conclusion: 代理框架在放射学QA中提升了事实性和准确性，尤其是中等规模LLMs，未来需进一步验证其临床实用性。

Abstract: Clinical decision-making in radiology increasingly benefits from artificial
intelligence (AI), particularly through large language models (LLMs). However,
traditional retrieval-augmented generation (RAG) systems for radiology question
answering (QA) typically rely on single-step retrieval, limiting their ability
to handle complex clinical reasoning tasks. Here we propose an agentic RAG
framework enabling LLMs to autonomously decompose radiology questions,
iteratively retrieve targeted clinical evidence from Radiopaedia, and
dynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning
diverse architectures, parameter scales (0.5B to >670B), and training paradigms
(general-purpose, reasoning-optimized, clinically fine-tuned), using 104
expert-curated radiology questions from previously established RSNA-RadioQA and
ExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic
accuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional
online RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized
models (e.g., Mistral Large improved from 72% to 81%) and small-scale models
(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B
parameters) demonstrated minimal changes (<2% improvement). Additionally,
agentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically
relevant context in 46% of cases, substantially aiding factual grounding. Even
clinically fine-tuned models exhibited meaningful improvements (e.g.,
MedGemma-27B improved from 71% to 81%), indicating complementary roles of
retrieval and fine-tuning. These results highlight the potential of agentic
frameworks to enhance factuality and diagnostic accuracy in radiology QA,
particularly among mid-sized LLMs, warranting future studies to validate their
clinical utility.

</details>


### [42] [GLiDRE: Generalist Lightweight model for Document-level Relation Extraction](https://arxiv.org/abs/2508.00757)
*Robin Armingaud,Romaric Besançon*

Main category: cs.CL

TL;DR: GLiDRE是一种基于GLiNER思想的新型文档级关系抽取模型，在少样本场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 文档级关系抽取任务复杂，现有方法在零样本或少样本场景下表现不足，GLiNER的成功启发了GLiDRE的开发。

Method: GLiDRE基于GLiNER的关键思想构建，并在Re-DocRED数据集上进行了多数据设置的基准测试。

Result: GLiDRE在少样本场景下达到了最先进的性能。

Conclusion: GLiDRE在文档级关系抽取任务中表现出色，尤其在少样本场景下具有优势。

Abstract: Relation Extraction (RE) is a fundamental task in Natural Language
Processing, and its document-level variant poses significant challenges, due to
the need to model complex interactions between entities across sentences.
Current approaches, largely based on the ATLOP architecture, are commonly
evaluated on benchmarks like DocRED and Re-DocRED. However, their performance
in zero-shot or few-shot settings remains largely underexplored due to the
task's complexity. Recently, the GLiNER model has shown that a compact NER
model can outperform much larger Large Language Models. With a similar
motivation, we introduce GLiDRE, a new model for document-level relation
extraction that builds on the key ideas of GliNER. We benchmark GLiDRE against
state-of-the-art models across various data settings on the Re-DocRED dataset.
Our results demonstrate that GLiDRE achieves state-of-the-art performance in
few-shot scenarios. Our code is publicly available.

</details>


### [43] [MMBERT: Scaled Mixture-of-Experts Multimodal BERT for Robust Chinese Hate Speech Detection under Cloaking Perturbations](https://arxiv.org/abs/2508.00760)
*Qiyao Xue,Yuchen Dou,Ryan Shi,Xiang Lorraine Li,Wei Gao*

Main category: cs.CL

TL;DR: MMBERT是一种基于BERT的多模态框架，结合文本、语音和视觉模态，通过Mixture-of-Experts架构提升中文社交媒体仇恨言论检测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 中文社交媒体中仇恨言论检测面临独特挑战，尤其是逃避传统文本检测的伪装技术。现有研究多集中于英文数据集，中文多模态策略研究较少。

Method: 提出MMBERT框架，结合文本、语音和视觉模态，采用Mixture-of-Experts架构，并通过三阶段渐进训练解决不稳定性问题。

Result: 在多个中文仇恨言论数据集中，MMBERT显著优于微调的BERT模型、LLMs及上下文学习方法。

Conclusion: MMBERT为中文仇恨言论检测提供了一种高效的多模态解决方案，显著提升检测性能。

Abstract: Hate speech detection on Chinese social networks presents distinct
challenges, particularly due to the widespread use of cloaking techniques
designed to evade conventional text-based detection systems. Although large
language models (LLMs) have recently improved hate speech detection
capabilities, the majority of existing work has concentrated on English
datasets, with limited attention given to multimodal strategies in the Chinese
context. In this study, we propose MMBERT, a novel BERT-based multimodal
framework that integrates textual, speech, and visual modalities through a
Mixture-of-Experts (MoE) architecture. To address the instability associated
with directly integrating MoE into BERT-based models, we develop a progressive
three-stage training paradigm. MMBERT incorporates modality-specific experts, a
shared self-attention mechanism, and a router-based expert allocation strategy
to enhance robustness against adversarial perturbations. Empirical results in
several Chinese hate speech datasets show that MMBERT significantly surpasses
fine-tuned BERT-based encoder models, fine-tuned LLMs, and LLMs utilizing
in-context learning approaches.

</details>


### [44] [ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation](https://arxiv.org/abs/2508.00762)
*Atakan Site,Emre Hakan Erdemir,Gülşen Eryiğit*

Main category: cs.CL

TL;DR: 论文提出了一种基于大型语言模型的零样本解决方案，用于SemEval-2025任务8中的表格数据问答任务，通过生成Python代码实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 解决多样领域表格数据问答任务，探索大型语言模型在代码生成中的应用。

Method: 提出基于开源大型语言模型的Python代码生成框架，通过优化提示策略生成可执行的Pandas代码。

Result: 实验表明不同语言模型在代码生成中表现各异，Python代码生成在表格问答中优于其他方法。系统在开源模型类别中排名第八和第六。

Conclusion: Python代码生成是表格问答的有效方法，大型语言模型在零样本任务中表现优异。

Abstract: This paper presents our system for SemEval-2025 Task 8: DataBench,
Question-Answering over Tabular Data. The primary objective of this task is to
perform question answering on given tabular datasets from diverse domains under
two subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To
tackle both subtasks, we developed a zero-shot solution with a particular
emphasis on leveraging Large Language Model (LLM)-based code generation.
Specifically, we propose a Python code generation framework utilizing
state-of-the-art open-source LLMs to generate executable Pandas code via
optimized prompting strategies. Our experiments reveal that different LLMs
exhibit varying levels of effectiveness in Python code generation.
Additionally, results show that Python code generation achieves superior
performance in tabular question answering compared to alternative approaches.
Although our ranking among zero-shot systems is unknown at the time of this
paper's submission, our system achieved eighth place in Subtask I and sixth
place in Subtask~II among the 30 systems that outperformed the baseline in the
open-source models category.

</details>


### [45] [Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models](https://arxiv.org/abs/2508.00788)
*Xushuo Tang,Yi Ding,Zhengyi Yang,Yin Chen,Yongrui Gu,Wenke Yang,Mingchen Ju,Xin Cao,Yongfei Liu,Wenjie Zhang*

Main category: cs.CL

TL;DR: 论文介绍了MISGENDERED+，一个扩展更新的基准，用于评估大语言模型（LLMs）在代词使用上的表现，重点关注性别中立和新代词。结果显示模型在二元和性别中立代词上有进步，但在新代词和反向推理任务上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 公平和包容性在AI应用中至关重要，而代词使用（尤其是性别中立和新代词）是LLMs的一个关键挑战。现有研究如MISGENDERED基准已显局限性，需更新评估。

Method: 引入MISGENDERED+基准，评估五种代表性LLMs（GPT-4o、Claude 4、DeepSeek-V3、Qwen Turbo和Qwen2.5）在零样本、少样本和性别身份推理任务中的表现。

Result: 模型在二元和性别中立代词准确性上有显著提升，但新代词和反向推理任务表现不一致，显示身份敏感推理仍存在差距。

Conclusion: 研究揭示了LLMs在代词使用上的进步与不足，为未来包容性AI研究提供了方向。

Abstract: Large language models (LLMs) are increasingly deployed in sensitive contexts
where fairness and inclusivity are critical. Pronoun usage, especially
concerning gender-neutral and neopronouns, remains a key challenge for
responsible AI. Prior work, such as the MISGENDERED benchmark, revealed
significant limitations in earlier LLMs' handling of inclusive pronouns, but
was constrained to outdated models and limited evaluations. In this study, we
introduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'
pronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,
DeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender
identity inference. Our results show notable improvements compared with
previous studies, especially in binary and gender-neutral pronoun accuracy.
However, accuracy on neopronouns and reverse inference tasks remains
inconsistent, underscoring persistent gaps in identity-sensitive reasoning. We
discuss implications, model-specific observations, and avenues for future
inclusive AI research.

</details>


### [46] [Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models](https://arxiv.org/abs/2508.00819)
*Jinsong Li,Xiaoyi Dong,Yuhang Zang,Yuhang Cao,Jiaqi Wang,Dahua Lin*

Main category: cs.CL

TL;DR: DAEDAL是一种无需训练的去噪策略，通过动态自适应长度扩展解决了扩散大语言模型（DLLMs）的静态长度限制问题，提升了性能和计算效率。


<details>
  <summary>Details</summary>
Motivation: DLLMs在应用中受限于静态预定义生成长度，导致性能与计算效率的权衡问题。

Method: DAEDAL分两阶段：1）在去噪前通过序列完成度量扩展初始长度；2）在去噪过程中动态扩展不足的生成区域。

Result: DAEDAL在性能上与固定长度基线相当甚至更优，同时提高了计算效率。

Conclusion: DAEDAL解决了DLLMs的静态长度限制，为其与自回归模型的竞争铺平了道路。

Abstract: Diffusion Large Language Models (DLLMs) are emerging as a powerful
alternative to the dominant Autoregressive Large Language Models, offering
efficient parallel generation and capable global context modeling. However, the
practical application of DLLMs is hindered by a critical architectural
constraint: the need for a statically predefined generation length. This static
length allocation leads to a problematic trade-off: insufficient lengths
cripple performance on complex tasks, while excessive lengths incur significant
computational overhead and sometimes result in performance degradation. While
the inference framework is rigid, we observe that the model itself possesses
internal signals that correlate with the optimal response length for a given
task. To bridge this gap, we leverage these latent signals and introduce
DAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive
Length Expansion for Diffusion Large Language Models. DAEDAL operates in two
phases: 1) Before the denoising process, DAEDAL starts from a short initial
length and iteratively expands it to a coarse task-appropriate length, guided
by a sequence completion metric. 2) During the denoising process, DAEDAL
dynamically intervenes by pinpointing and expanding insufficient generation
regions through mask token insertion, ensuring the final output is fully
developed. Extensive experiments on DLLMs demonstrate that DAEDAL achieves
performance comparable, and in some cases superior, to meticulously tuned
fixed-length baselines, while simultaneously enhancing computational efficiency
by achieving a higher effective token ratio. By resolving the static length
constraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap
with their Autoregressive counterparts and paving the way for more efficient
and capable generation.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [47] [Rethinking Evidence Hierarchies in Medical Language Benchmarks: A Critical Evaluation of HealthBench](https://arxiv.org/abs/2508.00081)
*Fred Mutisya,Shikoh Gitau,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha*

Main category: cs.AI

TL;DR: HealthBench是一个用于评估AI医疗能力的基准，但其依赖专家意见而非临床证据，可能引入偏见。针对非洲等地区的独特挑战，作者提出基于临床实践指南（CPGs）的改进方案，以增强全球相关性和公平性。


<details>
  <summary>Details</summary>
Motivation: HealthBench存在依赖专家意见和自动化评分系统的偏见问题，尤其在低收入地区问题更突出。作者旨在通过改进基准设计，提升医疗AI的临床可信度和全球适用性。

Method: 提出基于版本控制的CPGs，结合系统评价和GRADE证据评级，设计证据稳健的强化学习框架，包括评分权重和上下文覆盖逻辑。

Result: 通过改进奖励函数和评分机制，提升医疗语言模型的临床可信度、伦理合规性和全球适用性。

Conclusion: 改进后的基准有望生成更可靠、公平且全球适用的医疗AI模型，同时保留透明度和医生参与。

Abstract: HealthBench, a benchmark designed to measure the capabilities of AI systems
for health better (Arora et al., 2025), has advanced medical language model
evaluation through physician-crafted dialogues and transparent rubrics.
However, its reliance on expert opinion, rather than high-tier clinical
evidence, risks codifying regional biases and individual clinician
idiosyncrasies, further compounded by potential biases in automated grading
systems. These limitations are particularly magnified in low- and middle-income
settings, where issues like sparse neglected tropical disease coverage and
region-specific guideline mismatches are prevalent.
  The unique challenges of the African context, including data scarcity,
inadequate infrastructure, and nascent regulatory frameworks, underscore the
urgent need for more globally relevant and equitable benchmarks. To address
these shortcomings, we propose anchoring reward functions in version-controlled
Clinical Practice Guidelines (CPGs) that incorporate systematic reviews and
GRADE evidence ratings.
  Our roadmap outlines "evidence-robust" reinforcement learning via
rubric-to-guideline linkage, evidence-weighted scoring, and contextual override
logic, complemented by a focus on ethical considerations and the integration of
delayed outcome feedback. By re-grounding rewards in rigorously vetted CPGs,
while preserving HealthBench's transparency and physician engagement, we aim to
foster medical language models that are not only linguistically polished but
also clinically trustworthy, ethically sound, and globally relevant.

</details>


### [48] [Hyperproperty-Constrained Secure Reinforcement Learning](https://arxiv.org/abs/2508.00106)
*Ernest Bonnah,Luan Viet Nguyen,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: 本文提出了一种基于HyperTWTL的安全强化学习方法（SecRL），解决了现有研究中安全感知强化学习的不足，并通过案例研究验证了其有效性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有研究中缺乏基于超属性（hyperproperties）的安全感知强化学习方法，尤其是在机器人应用中，HyperTWTL能有效表示安全、不透明性和并发性。

Method: 提出了一种结合动态Boltzmann softmax强化学习的方法，在满足HyperTWTL约束的条件下学习安全感知的最优策略。

Result: 通过案例研究表明，该方法在有效性和可扩展性上优于两种基线强化学习算法。

Conclusion: HyperTWTL约束的安全强化学习方法在机器人任务中表现出色，填补了现有研究的空白。

Abstract: Hyperproperties for Time Window Temporal Logic (HyperTWTL) is a
domain-specific formal specification language known for its effectiveness in
compactly representing security, opacity, and concurrency properties for
robotics applications. This paper focuses on HyperTWTL-constrained secure
reinforcement learning (SecRL). Although temporal logic-constrained safe
reinforcement learning (SRL) is an evolving research problem with several
existing literature, there is a significant research gap in exploring
security-aware reinforcement learning (RL) using hyperproperties. Given the
dynamics of an agent as a Markov Decision Process (MDP) and opacity/security
constraints formalized as HyperTWTL, we propose an approach for learning
security-aware optimal policies using dynamic Boltzmann softmax RL while
satisfying the HyperTWTL constraints. The effectiveness and scalability of our
proposed approach are demonstrated using a pick-up and delivery robotic mission
case study. We also compare our results with two other baseline RL algorithms,
showing that our proposed method outperforms them.

</details>


### [49] [No AI Without PI! Object-Centric Process Mining as the Enabler for Generative, Predictive, and Prescriptive Artificial Intelligence](https://arxiv.org/abs/2508.00116)
*Wil M. P. van der Aalst*

Main category: cs.AI

TL;DR: 论文探讨了AI在工业环境中的应用挑战，提出通过对象中心过程挖掘（OCPM）结合AI，形成过程智能（PI），以优化端到端操作流程。


<details>
  <summary>Details</summary>
Motivation: 组织在工业环境中成功应用AI面临挑战，尤其是端到端操作流程的诊断与改进。

Method: 采用对象中心过程挖掘（OCPM）作为连接数据和过程的桥梁，结合生成式、预测式和规范性AI。

Result: OCPM是AI在组织环境中落地的关键，过程智能（PI）能有效整合数据驱动技术，支持多种对象和事件类型。

Conclusion: AI需要PI的支持才能优化操作流程，OCPM与多种AI形式的结合为工业应用提供了新机会。

Abstract: The uptake of Artificial Intelligence (AI) impacts the way we work, interact,
do business, and conduct research. However, organizations struggle to apply AI
successfully in industrial settings where the focus is on end-to-end
operational processes. Here, we consider generative, predictive, and
prescriptive AI and elaborate on the challenges of diagnosing and improving
such processes. We show that AI needs to be grounded using Object-Centric
Process Mining (OCPM). Process-related data are structured and
organization-specific and, unlike text, processes are often highly dynamic.
OCPM is the missing link connecting data and processes and enables different
forms of AI. We use the term Process Intelligence (PI) to refer to the
amalgamation of process-centric data-driven techniques able to deal with a
variety of object and event types, enabling AI in an organizational context.
This paper explains why AI requires PI to improve operational processes and
highlights opportunities for successfully combining OCPM and generative,
predictive, and prescriptive AI.

</details>


### [50] [Algorithmic Detection of Rank Reversals, Transitivity Violations, and Decomposition Inconsistencies in Multi-Criteria Decision Analysis](https://arxiv.org/abs/2508.00129)
*Agustín Borda,Juan Bautista Cabral,Gonzalo Giarda,Diego Nicolás Gimenez Irusta,Paula Pacheco,Alvaro Roy Schachner*

Main category: cs.AI

TL;DR: 论文提出了三种检测多准则决策分析中排名反转问题的测试方法，并在Scikit-Criteria库中实现，讨论了实现中的挑战和设计考虑。


<details>
  <summary>Details</summary>
Motivation: 排名反转问题会严重影响多准则决策方法的结果，需要一种机制来衡量方法性能并比较不同方法的有效性。

Method: 提出三种测试方法检测排名反转，并在Scikit-Criteria库中实现，处理了通用场景中的实现挑战。

Result: 成功实现了三种测试方法，解决了通用场景中的设计问题。

Conclusion: 这些测试方法在多准则决策方法的评判中可能发挥重要作用。

Abstract: In Multi-Criteria Decision Analysis, Rank Reversals are a serious problem
that can greatly affect the results of a Multi-Criteria Decision Method against
a particular set of alternatives. It is therefore useful to have a mechanism
that allows one to measure the performance of a method on a set of
alternatives. This idea could be taken further to build a global ranking of the
effectiveness of different methods to solve a problem. In this paper, we
present three tests that detect the presence of Rank Reversals, along with
their implementation in the Scikit-Criteria library. We also address the
complications that arise when implementing these tests for general scenarios
and the design considerations we made to handle them. We close with a
discussion about how these additions could play a major role in the judgment of
multi-criteria decision methods for problem solving.

</details>


### [51] [MetaAgent: Toward Self-Evolving Agent via Tool Meta-Learning](https://arxiv.org/abs/2508.00271)
*Hongjin Qian,Zheng Liu*

Main category: cs.AI

TL;DR: MetaAgent是一种基于学习实践原则的智能代理，通过不断自我改进和工具学习提升任务解决能力。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种能够通过实践和自我反思持续提升能力的智能代理，解决知识发现中的挑战。

Method: MetaAgent从基础能力出发，通过生成帮助请求、自我反思和工具学习动态优化任务解决策略。

Result: 在GAIA、WebWalkerQA和BrowseCamp等基准测试中表现优异，超越基线并匹配端到端训练代理。

Conclusion: MetaAgent展示了自进化代理系统在通用知识发现中的潜力，无需调整模型参数或额外训练。

Abstract: In this work, we propose MetaAgent, an agentic paradigm inspired by the
principle of learning-by-doing, where expertise is developed through hands-on
practice and continual self-improvement. MetaAgent starts with a minimal
workflow, equipped only with basic reasoning and adaptive help-seeking
abilities. When a knowledge gap is encountered, MetaAgent generates natural
language help requests, which are routed to the most suitable external tool by
a dedicated tool router. As MetaAgent solves tasks, it continually conducts
self-reflection and answer verification, distilling actionable experience into
concise texts that are dynamically incorporated into future task contexts.
Besides, MetaAgent autonomously builds in-house tools and a persistent
knowledge base by organizing its tool-use history, further enhancing its
ability to retrieve and integrate relevant information We term this continual,
data-driven process as \textit{meta tool learning}, through which MetaAgent
incrementally refines its reasoning and tool-use strategies, without changing
model parameters or requiring further post-training. Evaluated on challenging
knowledge discovery benchmarks, including GAIA, WebWalkerQA, and BrowseCamp,
MetaAgent consistently outperforms workflow-based baselines and matches or
exceeds end-to-end trained agents, demonstrating the promise of self-evolving
agentic systems for robust, general-purpose knowledge discovery. We provide our
source codes in https://github.com/qhjqhj00/MetaAgent.

</details>


### [52] [SHACL Validation under Graph Updates (Extended Paper)](https://arxiv.org/abs/2508.00137)
*Shqiponja Ahmetaj,George Konstantinidis,Magdalena Ortiz,Paolo Pareti,Mantas Simkus*

Main category: cs.AI

TL;DR: 研究SHACL在RDF图更新时的静态验证问题，提出一种基于SHACL的更新语言，并通过回归技术将更新动作嵌入约束中，展示了其计算复杂性和原型实现。


<details>
  <summary>Details</summary>
Motivation: 探索RDF图在更新时如何保持SHACL规范的验证有效性，为动态图推理提供基础。

Method: 提出基于SHACL的更新语言，使用回归技术将更新动作嵌入SHACL约束，分析计算复杂性并实现原型。

Result: 静态验证问题可简化为SHACL约束的（不）可满足性问题，计算复杂性分析为关键片段提供了理论支持。

Conclusion: 该研究为动态RDF图的SHACL验证提供了理论框架和实用工具，展示了原型实现的可行性。

Abstract: SHACL (SHApe Constraint Language) is a W3C standardized constraint language
for RDF graphs. In this paper, we study SHACL validation in RDF graphs under
updates. We present a SHACL-based update language that can capture intuitive
and realistic modifications on RDF graphs and study the problem of static
validation under such updates. This problem asks to verify whether every graph
that validates a SHACL specification will still do so after applying a given
update sequence. More importantly, it provides a basis for further services for
reasoning about evolving RDF graphs. Using a regression technique that embeds
the update actions into SHACL constraints, we show that static validation under
updates can be reduced to (un)satisfiability of constraints in (a minor
extension of) SHACL. We analyze the computational complexity of the static
validation problem for SHACL and some key fragments. Finally, we present a
prototype implementation that performs static validation and other static
analysis tasks on SHACL constraints and demonstrate its behavior through
preliminary experiments.

</details>


### [53] [Co-Producing AI: Toward an Augmented, Participatory Lifecycle](https://arxiv.org/abs/2508.00138)
*Rashid Mushkani,Hugo Berard,Toumadher Ammar,Cassandre Chatonnier,Shin Koseki*

Main category: cs.AI

TL;DR: 论文提出了一种基于设计正义和参与式AI的AI生命周期重构方法，强调共同生产、多样性和多学科合作，以减轻AI对边缘化群体的不公平影响。


<details>
  <summary>Details</summary>
Motivation: AI算法可能对文化边缘化群体产生不成比例的影响，现有方法（如伦理指南和技术解决方案）未能根本解决问题。

Method: 提出一个增强的AI生命周期，包括五个相互关联的阶段（共同框架、共同设计、共同实施、共同部署、共同维护），基于多学科研讨会和分布式权威理念。

Result: 该方法结合了设计正义和参与式AI，为AI生产流程提供了新的架构，强调多样性和多学科协作。

Conclusion: 重构AI生命周期是减轻AI对边缘化群体危害的关键，未来研究需关注如何扩展参与式治理。

Abstract: Despite efforts to mitigate the inherent risks and biases of artificial
intelligence (AI) algorithms, these algorithms can disproportionately impact
culturally marginalized groups. A range of approaches has been proposed to
address or reduce these risks, including the development of ethical guidelines
and principles for responsible AI, as well as technical solutions that promote
algorithmic fairness. Drawing on design justice, expansive learning theory, and
recent empirical work on participatory AI, we argue that mitigating these harms
requires a fundamental re-architecture of the AI production pipeline. This
re-design should center co-production, diversity, equity, inclusion (DEI), and
multidisciplinary collaboration. We introduce an augmented AI lifecycle
consisting of five interconnected phases: co-framing, co-design,
co-implementation, co-deployment, and co-maintenance. The lifecycle is informed
by four multidisciplinary workshops and grounded in themes of distributed
authority and iterative knowledge exchange. Finally, we relate the proposed
lifecycle to several leading ethical frameworks and outline key research
questions that remain for scaling participatory governance.

</details>


### [54] [Beyond Agreement: Rethinking Ground Truth in Educational AI Annotation](https://arxiv.org/abs/2508.00143)
*Danielle R. Thomas,Conrad Borchers,Kenneth R. Koedinger*

Main category: cs.AI

TL;DR: 论文指出人类评估者的局限性，提出过度依赖IRR指标可能阻碍教育数据分类的进步，并推荐五种补充评估方法以提高数据质量和模型效果。


<details>
  <summary>Details</summary>
Motivation: 人类评估者存在偏见和不可靠性，传统IRR指标（如Cohen's kappa）在教育AI应用中可能不足以确保数据标注质量，需要更有效的方法。

Method: 提出五种补充评估方法，包括多标签标注方案、专家评估和闭环验证等，强调外部有效性和教育影响。

Result: 这些方法能比单独使用IRR产生更高质量的训练数据和模型，从而提升学生学习效果和可操作性。

Conclusion: 呼吁重新思考标注质量和"真实标准"，优先考虑有效性和教育影响，而非仅依赖共识。

Abstract: Humans can be notoriously imperfect evaluators. They are often biased,
unreliable, and unfit to define "ground truth." Yet, given the surging need to
produce large amounts of training data in educational applications using AI,
traditional inter-rater reliability (IRR) metrics like Cohen's kappa remain
central to validating labeled data. IRR remains a cornerstone of many machine
learning pipelines for educational data. Take, for example, the classification
of tutors' moves in dialogues or labeling open responses in machine-graded
assessments. This position paper argues that overreliance on human IRR as a
gatekeeper for annotation quality hampers progress in classifying data in ways
that are valid and predictive in relation to improving learning. To address
this issue, we highlight five examples of complementary evaluation methods,
such as multi-label annotation schemes, expert-based approaches, and
close-the-loop validity. We argue that these approaches are in a better
position to produce training data and subsequent models that produce improved
student learning and more actionable insights than IRR approaches alone. We
also emphasize the importance of external validity, for example, by
establishing a procedure of validating tutor moves and demonstrating that it
works across many categories of tutor actions (e.g., providing hints). We call
on the field to rethink annotation quality and ground truth--prioritizing
validity and educational impact over consensus alone.

</details>


### [55] [Model-Based Soft Maximization of Suitable Metrics of Long-Term Human Power](https://arxiv.org/abs/2508.00159)
*Jobst Heitzig,Ram Potham*

Main category: cs.AI

TL;DR: 论文探讨通过明确让AI赋能人类并管理人机权力平衡，以促进安全和福祉。提出了一种参数化、可分解的目标函数，并通过算法计算该指标。


<details>
  <summary>Details</summary>
Motivation: 研究如何在AI系统中平衡权力，以确保安全和人类福祉，避免权力失衡带来的风险。

Method: 采用部分公理化方法设计目标函数，考虑人类有限理性和社会规范，并通过逆向归纳或多智能体强化学习计算指标。

Result: 在多种典型情境中验证了该指标的效果，表明其可能比直接基于效用的目标更安全。

Conclusion: 软性最大化人类权力聚合指标可能是AI系统的有益目标，比直接效用目标更安全。

Abstract: Power is a key concept in AI safety: power-seeking as an instrumental goal,
sudden or gradual disempowerment of humans, power balance in human-AI
interaction and international AI governance. At the same time, power as the
ability to pursue diverse goals is essential for wellbeing.
  This paper explores the idea of promoting both safety and wellbeing by
forcing AI agents explicitly to empower humans and to manage the power balance
between humans and AI agents in a desirable way. Using a principled, partially
axiomatic approach, we design a parametrizable and decomposable objective
function that represents an inequality- and risk-averse long-term aggregate of
human power. It takes into account humans' bounded rationality and social
norms, and, crucially, considers a wide variety of possible human goals.
  We derive algorithms for computing that metric by backward induction or
approximating it via a form of multi-agent reinforcement learning from a given
world model. We exemplify the consequences of (softly) maximizing this metric
in a variety of paradigmatic situations and describe what instrumental
sub-goals it will likely imply. Our cautious assessment is that softly
maximizing suitable aggregate metrics of human power might constitute a
beneficial objective for agentic AI systems that is safer than direct
utility-based objectives.

</details>


### [56] [RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization](https://arxiv.org/abs/2508.00222)
*Yihong Dong,Xue Jiang,Yongding Tao,Huanyu Liu,Kechi Zhang,Lili Mou,Rongyu Cao,Yingwei Ma,Jue Chen,Binhua Li,Zhi Jin,Fei Huang,Yongbin Li,Ge Li*

Main category: cs.AI

TL;DR: RL-PLUS通过结合内部推理和外部数据，解决了RLVR方法在LLM中的能力边界问题，显著提升了推理能力。


<details>
  <summary>Details</summary>
Motivation: RLVR方法因固有的策略和稀疏奖励限制了LLM的能力边界，甚至导致能力边界崩溃。RL-PLUS旨在通过结合内部推理和外部数据突破这些限制。

Method: RL-PLUS采用多重重要性采样解决外部数据分布不匹配问题，并利用探索优势函数引导模型探索高价值推理路径。

Result: 在六个数学推理基准和六个分布外推理任务中，RL-PLUS表现优于现有RLVR方法，平均相对提升21.1%至69.2%。

Conclusion: RL-PLUS有效解决了能力边界崩溃问题，并在多个基准测试中展现了卓越的性能和通用性。

Abstract: Reinforcement Learning with Verifiable Reward (RLVR) has significantly
advanced the complex reasoning abilities of Large Language Models (LLMs).
However, it struggles to break through the inherent capability boundaries of
the base LLM, due to its inherently on-policy strategy with LLM's immense
action space and sparse reward. Further, RLVR can lead to the capability
boundary collapse, narrowing the LLM's problem-solving scope. To address this
problem, we propose RL-PLUS, a novel approach that synergizes internal
exploitation (i.e., Thinking) with external data (i.e., Learning) to achieve
stronger reasoning capabilities and surpass the boundaries of base models.
RL-PLUS integrates two core components: Multiple Importance Sampling to address
for distributional mismatch from external data, and an Exploration-Based
Advantage Function to guide the model towards high-value, unexplored reasoning
paths. We provide both theoretical analysis and extensive experiments to
demonstrate the superiority and generalizability of our approach. The results
show that RL-PLUS achieves state-of-the-art performance compared with existing
RLVR methods on six math reasoning benchmarks and exhibits superior performance
on six out-of-distribution reasoning tasks. It also achieves consistent and
significant gains across diverse model families, with average relative
improvements ranging from 21.1\% to 69.2\%. Moreover, Pass@k curves across
multiple benchmarks indicate that RL-PLUS effectively resolves the capability
boundary collapse problem.

</details>


### [57] [Mind the Gap: The Divergence Between Human and LLM-Generated Tasks](https://arxiv.org/abs/2508.00282)
*Yi-Long Lu,Jiajun Song,Chunhui Zhang,Wei Wang*

Main category: cs.AI

TL;DR: 论文比较了人类和LLM（GPT-4o）在任务生成中的差异，发现人类行为受心理驱动因素影响，而LLM未能模拟这些模式，任务生成更抽象且缺乏社交和物理性。


<details>
  <summary>Details</summary>
Motivation: 探究生成式代理（如LLM）是否能模拟人类基于内在动机的任务生成行为。

Method: 通过任务生成实验，比较人类和GPT-4o的行为，分析心理驱动因素（如个人价值观和认知风格）的影响。

Result: 人类任务生成受心理驱动因素影响，而LLM生成的任务更抽象、缺乏社交和物理性，尽管被认为更有趣和新颖。

Conclusion: LLM与人类认知存在核心差距，需在设计更人性化的代理时融入内在动机和物理基础。

Abstract: Humans constantly generate a diverse range of tasks guided by internal
motivations. While generative agents powered by large language models (LLMs)
aim to simulate this complex behavior, it remains uncertain whether they
operate on similar cognitive principles. To address this, we conducted a
task-generation experiment comparing human responses with those of an LLM agent
(GPT-4o). We find that human task generation is consistently influenced by
psychological drivers, including personal values (e.g., Openness to Change) and
cognitive style. Even when these psychological drivers are explicitly provided
to the LLM, it fails to reflect the corresponding behavioral patterns. They
produce tasks that are markedly less social, less physical, and thematically
biased toward abstraction. Interestingly, while the LLM's tasks were perceived
as more fun and novel, this highlights a disconnect between its linguistic
proficiency and its capacity to generate human-like, embodied goals.We conclude
that there is a core gap between the value-driven, embodied nature of human
cognition and the statistical patterns of LLMs, highlighting the necessity of
incorporating intrinsic motivation and physical grounding into the design of
more human-aligned agents.

</details>


### [58] [Oedipus and the Sphinx: Benchmarking and Improving Visual Language Models for Complex Graphic Reasoning](https://arxiv.org/abs/2508.00323)
*Jianyi Zhang,Xu Ji,Ziyin Zhou,Yuchen Zhou,Shubo Shi,Haoyu Wu,Zhen Li,Shizhao Liu*

Main category: cs.AI

TL;DR: ReasonBench是一个专注于结构化图形推理任务的评估基准，用于评估视觉语言模型（VLMs）在复杂图形推理中的表现，揭示了当前模型的局限性，并提出双重优化策略提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前VLMs在模拟人类图形推理能力方面存在明显不足，尤其在复杂图形推理和抽象问题解决上研究较少，现有研究仅关注简单图形。

Method: 提出ReasonBench基准，包含1,613个真实世界智力测试问题，涵盖位置、属性、数量和多元素任务维度，评估11种主流VLMs，并提出双重优化策略（DiaCoT和ReasonTune）。

Result: 实验揭示了当前模型的显著局限性，双重优化策略使VLM性能提升33.5%。

Conclusion: ReasonBench为复杂图形推理提供了全面评估框架，双重优化策略有效提升了VLMs的性能。

Abstract: Evaluating the performance of visual language models (VLMs) in graphic
reasoning tasks has become an important research topic. However, VLMs still
show obvious deficiencies in simulating human-level graphic reasoning
capabilities, especially in complex graphic reasoning and abstract problem
solving, which are less studied and existing studies only focus on simple
graphics. To evaluate the performance of VLMs in complex graphic reasoning, we
propose ReasonBench, the first evaluation benchmark focused on structured
graphic reasoning tasks, which includes 1,613 questions from real-world
intelligence tests. ReasonBench covers reasoning dimensions related to
location, attribute, quantity, and multi-element tasks, providing a
comprehensive evaluation of the performance of VLMs in spatial, relational, and
abstract reasoning capabilities. We benchmark 11 mainstream VLMs (including
closed-source and open-source models) and reveal significant limitations of
current models. Based on these findings, we propose a dual optimization
strategy: Diagrammatic Reasoning Chain (DiaCoT) enhances the interpretability
of reasoning by decomposing layers, and ReasonTune enhances the task
adaptability of model reasoning through training, all of which improves VLM
performance by 33.5\%. All experimental data and code are in the repository:
https://huggingface.co/datasets/cistine/ReasonBench.

</details>


### [59] [R1-ACT: Efficient Reasoning Model Safety Alignment by Activating Safety Knowledge](https://arxiv.org/abs/2508.00324)
*Yeonjun In,Wonjoong Kim,Sangwu Park,Chanyoung Park*

Main category: cs.AI

TL;DR: R1-Act是一种高效的后训练方法，通过结构化推理过程显式触发安全知识，显著提升大型推理模型的安全性，同时保持推理性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在执行复杂任务时表现出色，但容易响应有害指令，引发安全问题。研究发现模型已具备足够的安全知识，但在推理过程中未能激活。

Method: 提出R1-Act方法，通过结构化推理显式触发安全知识，仅需少量训练数据和计算资源。

Result: R1-Act在安全性和推理性能上均优于现有对齐方法，且具有鲁棒性和可扩展性。

Conclusion: R1-Act是一种简单高效的方法，能够显著提升模型安全性，适用于不同规模和架构的LRMs。

Abstract: Although large reasoning models (LRMs) have demonstrated impressive
capabilities on complex tasks, recent studies reveal that these models
frequently fulfill harmful user instructions, raising significant safety
concerns. In this paper, we investigate the underlying cause of LRM safety
risks and find that models already possess sufficient safety knowledge but fail
to activate it during reasoning. Based on this insight, we propose R1-Act, a
simple and efficient post-training method that explicitly triggers safety
knowledge through a structured reasoning process. R1-Act achieves strong safety
improvements while preserving reasoning performance, outperforming prior
alignment methods. Notably, it requires only 1,000 training examples and 90
minutes of training on a single RTX A6000 GPU. Extensive experiments across
multiple LRM backbones and sizes demonstrate the robustness, scalability, and
practical efficiency of our approach.

</details>


### [60] [CoRGI: Verified Chain-of-Thought Reasoning with Visual Grounding](https://arxiv.org/abs/2508.00378)
*Shixin Yi,Lin Shang*

Main category: cs.AI

TL;DR: CoRGI框架通过引入视觉验证机制，解决了视觉语言模型推理中的幻觉问题，提升了多模态推理的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有Chain-of-Thought提示方法在视觉语言模型中生成的解释缺乏视觉内容的支持，导致幻觉问题。

Method: 提出CoRGI框架，分三阶段：生成文本推理链、提取视觉证据、结合文本与视觉证据生成验证答案。

Result: 在VCR基准测试中，CoRGI提升了Qwen-2.5VL和LLaVA-1.6的性能，并通过人类评估验证了其解释的准确性和实用性。

Conclusion: 视觉验证对增强多模态推理的鲁棒性至关重要，CoRGI为现有模型提供了一种无需端到端重新训练的改进方案。

Abstract: Chain-of-Thought (CoT) prompting has shown promise in improving reasoning in
vision-language models (VLMs), but it often produces explanations that are
linguistically fluent yet lack grounding in visual content. We observe that
such hallucinations arise in part from the absence of an explicit verification
mechanism during multi-step reasoning. To address this, we propose
\textbf{CoRGI}(\textbf{C}hain \textbf{o}f \textbf{R}easoning with
\textbf{G}rounded \textbf{I}nsights), a modular framework that introduces
visual verification into the reasoning process. CoRGI follows a three-stage
pipeline: it first generates a textual reasoning chain, then extracts
supporting visual evidence for each reasoning step via a dedicated module
(VEVM), and finally synthesizes the textual rationale with visual evidence to
generate a grounded, verified answer. The framework can be integrated with
existing VLMs without end-to-end retraining. We evaluate CoRGI on the VCR
benchmark and find that it improves reasoning performance on two representative
open-source VLM backbones, Qwen-2.5VL and LLaVA-1.6. Ablation studies confirm
the contribution of each step in the verification module, and human evaluations
suggest that CoRGI leads to more factual and helpful explanations. We also
examine alternative designs for the visual verification step and discuss
potential limitations of post-hoc verification frameworks. These findings
highlight the importance of grounding intermediate reasoning steps in visual
evidence to enhance the robustness of multimodal reasoning.

</details>


### [61] [Theory of Mind Using Active Inference: A Framework for Multi-Agent Cooperation](https://arxiv.org/abs/2508.00401)
*Riddhi J. Pitliya,Ozan Catal,Toon Van de Maele,Corrado Pezzato,Tim Verbelen*

Main category: cs.AI

TL;DR: 提出了一种基于心智理论（ToM）的多智能体协作方法，通过主动推理实现，无需任务特定的共享生成模型或显式通信。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体协作中智能体如何理解并推理其他智能体的信念和目标的问题，提升协作效率。

Method: 在主动推理框架中引入ToM，智能体维护自身和他人的信念与目标表示，并扩展推理树规划算法进行递归推理。

Result: 在碰撞避免和觅食任务中，ToM智能体表现出更好的协作能力，能避免碰撞并减少冗余行为。

Conclusion: 该方法不仅推动了人工智能的实际应用，还为ToM的计算机制提供了新见解。

Abstract: We present a novel approach to multi-agent cooperation by implementing theory
of mind (ToM) within active inference. ToM - the ability to understand that
others can have differing knowledge and goals - enables agents to reason about
others' beliefs while planning their own actions. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication,
while being generalisable. In our framework, the ToM-equipped agent maintains
distinct representations of its own and others' beliefs and goals. We extend
the sophisticated inference tree-based planning algorithm to systematically
explore joint policy spaces through recursive reasoning. Our approach is
evaluated through collision avoidance and foraging task simulations. Results
demonstrate that ToM-equipped agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour. This work advances practical applications in artificial
intelligence while providing computational insights into ToM.

</details>


### [62] [Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training](https://arxiv.org/abs/2508.00414)
*Tianqing Fang,Zhisong Zhang,Xiaoyang Wang,Rui Wang,Can Qin,Yuxuan Wan,Jun-Yu Ma,Ce Zhang,Jiaqi Chen,Xiyun Li,Hongming Zhang,Haitao Mi,Dong Yu*

Main category: cs.AI

TL;DR: Cognitive Kernel-Pro 是一个完全开源且免费的 AI 代理框架，旨在推动高级 AI 代理的开发与评估，并在 GAIA 基准测试中取得了领先性能。


<details>
  <summary>Details</summary>
Motivation: 当前 AI 代理系统多为闭源或依赖付费 API，限制了研究的可访问性和可复现性。

Method: 通过构建高质量的训练数据（查询、轨迹和可验证答案），并探索代理的测试时反思和投票策略。

Result: 在 GAIA 上取得开源代理中的最佳性能，8B 参数模型超越 WebDancer 和 WebSailor。

Conclusion: Cognitive Kernel-Pro 为开源 AI 代理设立了新的性能标准，推动了 AI 代理的民主化发展。

Abstract: General AI Agents are increasingly recognized as foundational frameworks for
the next generation of artificial intelligence, enabling complex reasoning, web
interaction, coding, and autonomous research capabilities. However, current
agent systems are either closed-source or heavily reliant on a variety of paid
APIs and proprietary tools, limiting accessibility and reproducibility for the
research community. In this work, we present \textbf{Cognitive Kernel-Pro}, a
fully open-source and (to the maximum extent) free multi-module agent framework
designed to democratize the development and evaluation of advanced AI agents.
Within Cognitive Kernel-Pro, we systematically investigate the curation of
high-quality training data for Agent Foundation Models, focusing on the
construction of queries, trajectories, and verifiable answers across four key
domains: web, file, code, and general reasoning. Furthermore, we explore novel
strategies for agent test-time reflection and voting to enhance agent
robustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving
state-of-the-art results among open-source and free agents. Notably, our
8B-parameter open-source model surpasses previous leading systems such as
WebDancer and WebSailor, establishing a new performance standard for
accessible, high-capability AI agents. Code is available at
https://github.com/Tencent/CognitiveKernel-Pro

</details>


### [63] [Thinking Machines: Mathematical Reasoning in the Age of LLMs](https://arxiv.org/abs/2508.00459)
*Andrea Asperti,Alberto Naibo,Claudio Sacerdoti Coen*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在数学领域的应用，尤其是形式化数学证明中的挑战，并分析了其与代码生成的差异。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在形式化数学证明中的局限性，探索其推理机制和监督方式。

Method: 分析现有模型和基准，探讨形式化与非形式化数学训练的权衡、证明生成脆弱性的原因，以及LLMs是否真正跟踪逻辑状态。

Result: 发现形式化数学证明比代码生成更具挑战性，LLMs可能仅模仿而非真正表示逻辑状态。

Conclusion: 旨在明确当前技术的局限性，并提出可能的扩展方向。

Abstract: Large Language Models (LLMs) have shown remarkable abilities in structured
reasoning and symbolic tasks, with coding emerging as a particular area of
strength. This success has sparked growing interest in applying LLMs to
mathematics, both in informal problem-solving and formal theorem proving.
However, progress in formal mathematics has proven to be significantly more
difficult, despite surface-level similarities between programming and proof
construction. This discrepancy raises important questions about how LLMs
``reason'', how they are supervised, and whether they internally track a notion
of computational or deductive state. In this article, we address the
state-of-the-art of the discipline, focusing on recent models and benchmarks,
and explore three central issues at the intersection of machine learning and
mathematical cognition: (i) the trade-offs between formal and informal
mathematics as training domains; (ii) the deeper reasons why proof generation
remains more brittle than code synthesis; (iii) and the question of whether
LLMs represent, or merely mimic, a notion of evolving logical state. Our goal
is not to draw hard boundaries, but to identify where the current limits lie,
and how they might be extended.

</details>


### [64] [Pro2Guard: Proactive Runtime Enforcement of LLM Agent Safety via Probabilistic Model Checking](https://arxiv.org/abs/2508.00500)
*Haoyu Wang,Chris M. Poskitt,Jun Sun,Jiali Wei*

Main category: cs.AI

TL;DR: Pro2Guard是一个基于概率可达性分析的主动运行时安全框架，用于预测和防止LLM代理的不安全行为。


<details>
  <summary>Details</summary>
Motivation: 现有基于规则的安全系统（如AgentSpec）缺乏前瞻性，难以应对长期依赖和分布变化，导致安全风险。

Method: Pro2Guard将代理行为抽象为符号状态，并从执行轨迹中学习离散时间马尔可夫链（DTMC），通过估计到达不安全状态的概率来预测风险。

Result: 在家庭代理和自动驾驶场景中，Pro2Guard分别实现了93.6%和100%的安全预测，并能提前干预。

Conclusion: Pro2Guard通过主动预测和干预，显著提升了LLM代理的安全性，同时保持任务完成率。

Abstract: Large Language Model (LLM) agents exhibit powerful autonomous capabilities
across domains such as robotics, virtual assistants, and web automation.
However, their stochastic behavior introduces significant safety risks that are
difficult to anticipate. Existing rule-based enforcement systems, such as
AgentSpec, focus on developing reactive safety rules, which typically respond
only when unsafe behavior is imminent or has already occurred. These systems
lack foresight and struggle with long-horizon dependencies and distribution
shifts. To address these limitations, we propose Pro2Guard, a proactive runtime
enforcement framework grounded in probabilistic reachability analysis.
Pro2Guard abstracts agent behaviors into symbolic states and learns a
Discrete-Time Markov Chain (DTMC) from execution traces. At runtime, it
anticipates future risks by estimating the probability of reaching unsafe
states, triggering interventions before violations occur when the predicted
risk exceeds a user-defined threshold. By incorporating semantic validity
checks and leveraging PAC bounds, Pro2Guard ensures statistical reliability
while approximating the underlying ground-truth model. We evaluate Pro2Guard
extensively across two safety-critical domains: embodied household agents and
autonomous vehicles. In embodied agent tasks, Pro2Guard enforces safety early
on up to 93.6% of unsafe tasks using low thresholds, while configurable modes
(e.g., reflect) allow balancing safety with task success, maintaining up to
80.4% task completion. In autonomous driving scenarios, Pro2Guard achieves 100%
prediction of traffic law violations and collisions, anticipating risks up to
38.66 seconds ahead.

</details>


### [65] [MultiSHAP: A Shapley-Based Framework for Explaining Cross-Modal Interactions in Multimodal AI Models](https://arxiv.org/abs/2508.00576)
*Zhanliang Wang,Kai Wang*

Main category: cs.AI

TL;DR: MultiSHAP是一个模型无关的可解释性框架，通过Shapley Interaction Index量化多模态模型中的跨模态交互，适用于开源和闭源模型。


<details>
  <summary>Details</summary>
Motivation: 多模态AI模型在需要整合多种模态信息的任务中表现出色，但其“黑盒”特性在高风险应用中限制了部署，因为可解释性和可信度至关重要。

Method: 提出MultiSHAP框架，利用Shapley Interaction Index对视觉和文本细粒度元素之间的交互进行量化，提供实例级和数据集级解释。

Result: 实验证实MultiSHAP能准确捕捉跨模态推理机制，并通过实际案例验证其实用性。

Conclusion: MultiSHAP为复杂多模态AI模型提供了一种通用的可解释性解决方案，且可扩展到两种以上模态。

Abstract: Multimodal AI models have achieved impressive performance in tasks that
require integrating information from multiple modalities, such as vision and
language. However, their "black-box" nature poses a major barrier to deployment
in high-stakes applications where interpretability and trustworthiness are
essential. How to explain cross-modal interactions in multimodal AI models
remains a major challenge. While existing model explanation methods, such as
attention map and Grad-CAM, offer coarse insights into cross-modal
relationships, they cannot precisely quantify the synergistic effects between
modalities, and are limited to open-source models with accessible internal
weights. Here we introduce MultiSHAP, a model-agnostic interpretability
framework that leverages the Shapley Interaction Index to attribute multimodal
predictions to pairwise interactions between fine-grained visual and textual
elements (such as image patches and text tokens), while being applicable to
both open- and closed-source models. Our approach provides: (1) instance-level
explanations that reveal synergistic and suppressive cross-modal effects for
individual samples - "why the model makes a specific prediction on this input",
and (2) dataset-level explanation that uncovers generalizable interaction
patterns across samples - "how the model integrates information across
modalities". Experiments on public multimodal benchmarks confirm that MultiSHAP
faithfully captures cross-modal reasoning mechanisms, while real-world case
studies demonstrate its practical utility. Our framework is extensible beyond
two modalities, offering a general solution for interpreting complex multimodal
AI models.

</details>


### [66] [From EMR Data to Clinical Insight: An LLM-Driven Framework for Automated Pre-Consultation Questionnaire Generation](https://arxiv.org/abs/2508.00581)
*Ruiqing Ding,Qianfang Sun,Yongkang Leng,Hui Yin,Xiaojian Li*

Main category: cs.AI

TL;DR: 提出了一种多阶段LLM驱动框架，用于从复杂电子病历生成全面的预咨询问卷，解决了直接LLM方法在信息完整性、逻辑顺序和疾病级合成上的不足。


<details>
  <summary>Details</summary>
Motivation: 预咨询是医疗保健的关键环节，但直接从复杂电子病历生成问卷存在挑战，现有LLM方法难以满足需求。

Method: 三阶段框架：1.提取原子断言；2.构建个人因果网络并合成疾病知识；3.生成个性化与标准化问卷。

Result: 在真实电子病历数据集上验证，方法在信息覆盖、诊断相关性、可理解性和生成时间上表现优越。

Conclusion: 该框架通过显式临床知识构建，显著提升了预咨询问卷的质量和效率，具有实际应用潜力。

Abstract: Pre-consultation is a critical component of effective healthcare delivery.
However, generating comprehensive pre-consultation questionnaires from complex,
voluminous Electronic Medical Records (EMRs) is a challenging task. Direct
Large Language Model (LLM) approaches face difficulties in this task,
particularly regarding information completeness, logical order, and
disease-level synthesis. To address this issue, we propose a novel multi-stage
LLM-driven framework: Stage 1 extracts atomic assertions (key facts with
timing) from EMRs; Stage 2 constructs personal causal networks and synthesizes
disease knowledge by clustering representative networks from an EMR corpus;
Stage 3 generates tailored personal and standardized disease-specific
questionnaires based on these structured representations. This framework
overcomes limitations of direct methods by building explicit clinical
knowledge. Evaluated on a real-world EMR dataset and validated by clinical
experts, our method demonstrates superior performance in information coverage,
diagnostic relevance, understandability, and generation time, highlighting its
practical potential to enhance patient information collection.

</details>


### [67] [Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings](https://arxiv.org/abs/2508.00632)
*Alexia Jolicoeur-Martineau*

Main category: cs.AI

TL;DR: 论文提出AVR-Eval和AVR-Agent，分别用于评估和生成交互式多媒体内容，解决了现有LLMs在复杂内容生成中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前AI在生成交互式音频-视觉内容（如视频游戏）方面面临挑战，缺乏自动化评估指标且难以处理复杂内容。

Method: 提出AVR-Eval作为多媒体内容质量的相对评估指标，并开发AVR-Agent多代理系统生成和改进JavaScript代码。

Result: AVR-Agent生成的内容在实验中表现优于单次生成内容，但模型未能有效利用自定义资产和AVR反馈。

Conclusion: 人类与机器在内容创作方式上存在根本差异，当前模型尚无法充分利用高质量资产和反馈。

Abstract: While AI excels at generating text, audio, images, and videos, creating
interactive audio-visual content such as video games remains challenging.
Current LLMs can generate JavaScript games and animations, but lack automated
evaluation metrics and struggle with complex content that normally requires
teams of humans working for many months (multi-shot, multi-agents) using assets
made by artists. To tackle these issues, we built a new metric and a
multi-agent system.
  We propose AVR-Eval, a relative metric for multimedia content quality using
Audio-Visual Recordings (AVRs). An omni-modal model (processing text, video,
and audio) compares the AVRs of two contents, with a text model reviewing
evaluations to determine superiority. We show that AVR-Eval properly identifies
good from broken or mismatched content.
  We built AVR-Agent, a multi-agent system generating JavaScript code from a
bank of multimedia assets (audio, images, 3D models). The coding agent selects
relevant assets, generates multiple initial codes, uses AVR-Eval to identify
the best version, and iteratively improves it through omni-modal agent feedback
from the AVR.
  We run experiments on games and animations with AVR-Eval (win rate of content
A against B). We find that content generated by AVR-Agent has a significantly
higher win rate against content made through one-shot generation. However,
models struggle to leverage custom assets and AVR feedback effectively, showing
no higher win rate. This reveals a critical gap: while humans benefit from
high-quality assets and audio-visual feedback, current coding models do not
seem to utilize these resources as effectively, highlighting fundamental
differences between human and machine content creation approaches.

</details>


### [68] [Multi-Band Variable-Lag Granger Causality: A Unified Framework for Causal Time Series Inference across Frequencies](https://arxiv.org/abs/2508.00658)
*Chakattrai Sookkongwaree,Tattep Lakmuang,Chainarong Amornbunchornvej*

Main category: cs.AI

TL;DR: 论文提出了一种多频带变滞后格兰杰因果性（MB-VLGC）框架，解决了传统方法在复杂系统中固定滞后假设和频率依赖性因果延迟的局限性。


<details>
  <summary>Details</summary>
Motivation: 理解时间序列中的因果关系对多个领域至关重要，但现有方法（如变滞后格兰杰因果性）未能考虑频率依赖性因果延迟。

Method: 提出MB-VLGC框架，通过显式建模频率依赖性因果延迟，扩展了传统VLGC方法，并提供了理论证明和高效推理流程。

Result: 在合成和真实数据集上的实验表明，MB-VLGC显著优于现有方法。

Conclusion: MB-VLGC框架具有广泛适用性，适用于任何类型的时间序列数据。

Abstract: Understanding causal relationships in time series is fundamental to many
domains, including neuroscience, economics, and behavioral science. Granger
causality is one of the well-known techniques for inferring causality in time
series. Typically, Granger causality frameworks have a strong fix-lag
assumption between cause and effect, which is often unrealistic in complex
systems. While recent work on variable-lag Granger causality (VLGC) addresses
this limitation by allowing a cause to influence an effect with different time
lags at each time point, it fails to account for the fact that causal
interactions may vary not only in time delay but also across frequency bands.
For example, in brain signals, alpha-band activity may influence another region
with a shorter delay than slower delta-band oscillations. In this work, we
formalize Multi-Band Variable-Lag Granger Causality (MB-VLGC) and propose a
novel framework that generalizes traditional VLGC by explicitly modeling
frequency-dependent causal delays. We provide a formal definition of MB-VLGC,
demonstrate its theoretical soundness, and propose an efficient inference
pipeline. Extensive experiments across multiple domains demonstrate that our
framework significantly outperforms existing methods on both synthetic and
real-world datasets, confirming its broad applicability to any type of time
series data. Code and datasets are publicly available.

</details>


### [69] [Transparent Adaptive Learning via Data-Centric Multimodal Explainable AI](https://arxiv.org/abs/2508.00665)
*Maryam Mosleh,Marie Devlin,Ellis Solaiman*

Main category: cs.AI

TL;DR: 本文提出了一种混合框架，结合传统可解释AI技术与生成式AI模型和用户个性化，以生成多模态、个性化的解释，旨在提升教育中AI系统的透明度和用户体验。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的自适应学习系统缺乏透明度，且现有可解释AI技术忽视用户角色和理解能力。

Method: 提出混合框架，整合传统XAI技术、生成式AI模型和用户个性化，动态生成多模态解释。

Result: 框架重新定义可解释性为动态沟通过程，并探讨了教育中XAI的局限性和研究方向。

Conclusion: 目标是实现既能增强透明度又能支持以用户为中心的体验的可解释AI。

Abstract: Artificial intelligence-driven adaptive learning systems are reshaping
education through data-driven adaptation of learning experiences. Yet many of
these systems lack transparency, offering limited insight into how decisions
are made. Most explainable AI (XAI) techniques focus on technical outputs but
neglect user roles and comprehension. This paper proposes a hybrid framework
that integrates traditional XAI techniques with generative AI models and user
personalisation to generate multimodal, personalised explanations tailored to
user needs. We redefine explainability as a dynamic communication process
tailored to user roles and learning goals. We outline the framework's design,
key XAI limitations in education, and research directions on accuracy,
fairness, and personalisation. Our aim is to move towards explainable AI that
enhances transparency while supporting user-centred experiences.

</details>


### [70] [Context-Aware Visualization for Explainable AI Recommendations in Social Media: A Vision for User-Aligned Explanations](https://arxiv.org/abs/2508.00674)
*Banan Alkhateeb,Ellis Solaiman*

Main category: cs.AI

TL;DR: 论文提出了一种用户分段的上下文感知解释系统，通过可视化方法为不同用户提供个性化解释，以提高推荐系统的透明度和用户信任。


<details>
  <summary>Details</summary>
Motivation: 当前社交媒体的推荐系统缺乏针对用户具体需求的解释性，导致用户对推荐内容的理解不足，影响了推荐的价值和用户信任。

Method: 提出了一种视觉解释系统，根据用户需求和上下文提供不同形式的解释（如技术详细版和简化版），并首次在一个流程中同时调整解释风格（视觉 vs. 数字）和粒度（专家 vs. 普通用户）。

Result: 通过30名X用户的公开试点验证其对决策和信任的影响。

Conclusion: 该框架通过个性化解释提升了推荐系统的透明度和用户信任，为未来的解释性推荐系统提供了新方向。

Abstract: Social media platforms today strive to improve user experience through AI
recommendations, yet the value of such recommendations vanishes as users do not
understand the reasons behind them. This issue arises because explainability in
social media is general and lacks alignment with user-specific needs. In this
vision paper, we outline a user-segmented and context-aware explanation layer
by proposing a visual explanation system with diverse explanation methods. The
proposed system is framed by the variety of user needs and contexts, showing
explanations in different visualized forms, including a technically detailed
version for AI experts and a simplified one for lay users. Our framework is the
first to jointly adapt explanation style (visual vs. numeric) and granularity
(expert vs. lay) inside a single pipeline. A public pilot with 30 X users will
validate its impact on decision-making and trust.

</details>


### [71] [Unraveling Hidden Representations: A Multi-Modal Layer Analysis for Better Synthetic Content Forensics](https://arxiv.org/abs/2508.00784)
*Tom Or,Omri Azencot*

Main category: cs.AI

TL;DR: 论文提出利用大型预训练多模态模型检测生成内容，通过其潜在代码区分真假信息，实现跨模态的高效检测。


<details>
  <summary>Details</summary>
Motivation: 生成模型被恶意用于传播虚假信息，现有检测工具泛化能力差，需通用分类器。

Method: 使用大型预训练多模态模型，基于其潜在代码训练线性分类器。

Result: 在音频和图像检测中表现优异，计算高效且适用于少样本场景。

Conclusion: 该方法在跨模态检测中达到或超越基线性能，具有高效性和泛化能力。

Abstract: Generative models achieve remarkable results in multiple data domains,
including images and texts, among other examples. Unfortunately, malicious
users exploit synthetic media for spreading misinformation and disseminating
deepfakes. Consequently, the need for robust and stable fake detectors is
pressing, especially when new generative models appear everyday. While the
majority of existing work train classifiers that discriminate between real and
fake information, such tools typically generalize only within the same family
of generators and data modalities, yielding poor results on other generative
classes and data domains. Towards a universal classifier, we propose the use of
large pre-trained multi-modal models for the detection of generative content.
Effectively, we show that the latent code of these models naturally captures
information discriminating real from fake. Building on this observation, we
demonstrate that linear classifiers trained on these features can achieve
state-of-the-art results across various modalities, while remaining
computationally efficient, fast to train, and effective even in few-shot
settings. Our work primarily focuses on fake detection in audio and images,
achieving performance that surpasses or matches that of strong baseline
methods.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [72] [Audio Prototypical Network For Controllable Music Recommendation](https://arxiv.org/abs/2508.00194)
*Fırat Öncel,Emiliano Penaloza,Haolun Wu,Shubham Gupta,Mirco Ravanelli,Laurent Charlin,Cem Subakan*

Main category: cs.IR

TL;DR: 提出了一种基于音频原型网络的可控音乐推荐系统，既能保持推荐性能，又能提供可解释和可控的用户偏好表示。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统使用黑盒编码模型生成密集表示，缺乏可解释性，尤其在音乐推荐中，用户偏好高度个性化且多变。

Method: 采用音频原型网络，通过语义上有意义的音乐特征原型表达用户偏好。

Result: 模型在推荐性能上与基线模型相当，同时提供了可解释和可控的用户档案。

Conclusion: 该模型在音乐推荐中实现了性能与可解释性的平衡。

Abstract: Traditional recommendation systems represent user preferences in dense
representations obtained through black-box encoder models. While these models
often provide strong recommendation performance, they lack interpretability for
users, leaving users unable to understand or control the system's modeling of
their preferences. This limitation is especially challenging in music
recommendation, where user preferences are highly personal and often evolve
based on nuanced qualities like mood, genre, tempo, or instrumentation. In this
paper, we propose an audio prototypical network for controllable music
recommendation. This network expresses user preferences in terms of prototypes
representative of semantically meaningful features pertaining to musical
qualities. We show that the model obtains competitive recommendation
performance compared to popular baseline models while also providing
interpretable and controllable user profiles.

</details>


### [73] [When Relevance Meets Novelty: Dual-Stable Periodic Optimization for Exploratory Recommendation](https://arxiv.org/abs/2508.00450)
*Hongxiang Lin,Hao Guo,Zeshun Li,Erpeng Xue,Yongqian He,Xiangyu Hou,Zhaoyu Hu,Lei Wang,Sheng Chen*

Main category: cs.IR

TL;DR: 论文提出Co-Evolutionary Alignment (CoEA)方法，通过Dual-Stable Interest Exploration (DSIE)模块和Periodic Collaborative Optimization (PCO)机制，解决传统推荐系统的反馈循环问题和大语言模型(LLM)增强框架的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统容易陷入强反馈循环，限制用户探索机会；现有LLM增强框架存在兴趣建模偏差和静态优化缺陷。

Method: CoEA方法包括DSIE模块（联合建模长期群体身份和短期个体兴趣）和PCO机制（周期性协作优化，动态调整模型）。

Result: 在线和离线实验验证了CoEA在探索性推荐中的有效性。

Conclusion: CoEA方法通过动态闭环优化和双稳态兴趣探索，显著提升了推荐系统的探索能力和用户满意度。

Abstract: Traditional recommendation systems tend to trap users in strong feedback
loops by excessively pushing content aligned with their historical preferences,
thereby limiting exploration opportunities and causing content fatigue.
Although large language models (LLMs) demonstrate potential with their diverse
content generation capabilities, existing LLM-enhanced dual-model frameworks
face two major limitations: first, they overlook long-term preferences driven
by group identity, leading to biased interest modeling; second, they suffer
from static optimization flaws, as a one-time alignment process fails to
leverage incremental user data for closed-loop optimization. To address these
challenges, we propose the Co-Evolutionary Alignment (CoEA) method. For
interest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)
module, jointly modeling long-term group identity and short-term individual
interests through parallel processing of behavioral sequences. For static
optimization limitations, we design a Periodic Collaborative Optimization (PCO)
mechanism. This mechanism regularly conducts preference verification on
incremental data using the Relevance LLM, then guides the Novelty LLM to
perform fine-tuning based on the verification results, and subsequently feeds
back the output of the incrementally fine-tuned Novelty LLM to the Relevance
LLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.
Extensive online and offline experiments verify the effectiveness of the CoEA
model in exploratory recommendation.

</details>


### [74] [M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start Item Recommendation](https://arxiv.org/abs/2508.00452)
*Chuan He,Yongchao Liu,Qiang Li,Wenliang Zhong,Chuntao Hong,Xinwei Yao*

Main category: cs.IR

TL;DR: 提出了一种名为M^2VAE的生成模型，用于解决冷启动物品推荐中的多模态多视图问题，通过分离共享和特定模态特征，并结合用户偏好建模。


<details>
  <summary>Details</summary>
Motivation: 冷启动物品推荐中，新物品缺乏历史交互数据，现有方法未能充分利用多模态的多视图结构和特征区分。

Method: 使用多模态多视图变分自编码器（M^2VAE），生成类型特定潜在变量，通过PoE获得共同表示，并结合对比损失和MoE建模用户偏好。

Result: 在真实数据集上的实验验证了方法的有效性。

Conclusion: M^2VAE能有效解决冷启动推荐中的多模态多视图问题，提升推荐性能。

Abstract: Cold-start item recommendation is a significant challenge in recommendation
systems, particularly when new items are introduced without any historical
interaction data. While existing methods leverage multi-modal content to
alleviate the cold-start issue, they often neglect the inherent multi-view
structure of modalities, the distinction between shared and modality-specific
features. In this paper, we propose Multi-Modal Multi-View Variational
AutoEncoder (M^2VAE), a generative model that addresses the challenges of
modeling common and unique views in attribute and multi-modal features, as well
as user preferences over single-typed item features. Specifically, we generate
type-specific latent variables for item IDs, categorical attributes, and image
features, and use Product-of-Experts (PoE) to derive a common representation. A
disentangled contrastive loss decouples the common view from unique views while
preserving feature informativeness. To model user inclinations, we employ a
preference-guided Mixture-of-Experts (MoE) to adaptively fuse representations.
We further incorporate co-occurrence signals via contrastive learning,
eliminating the need for pretraining. Extensive experiments on real-world
datasets validate the effectiveness of our approach.

</details>


### [75] [Session-Based Recommendation with Validated and Enriched LLM Intents](https://arxiv.org/abs/2508.00570)
*Gyuseok Lee,Yaokun Liu,Yifan Liu,Susik Yoon,Dong Wang,SeongKu Kang*

Main category: cs.IR

TL;DR: VELI4SBR是一个两阶段框架，利用验证和丰富的LLM生成意图来增强基于会话的推荐（SBR），解决了意图质量验证、多意图融合和LLM失败补偿的挑战。


<details>
  <summary>Details</summary>
Motivation: SBR因会话的短和匿名性导致数据稀疏，现有方法利用LLM生成意图作为辅助信号，但面临意图质量验证、多意图融合和LLM失败补偿的挑战。

Method: VELI4SBR分为两阶段：1）通过预测-校正循环生成高质量意图；2）通过轻量级多意图预测和融合机制增强SBR模型，并引入训练策略补偿LLM失败。

Result: 实验表明VELI4SBR优于现有基线，同时提高了可解释性。

Conclusion: VELI4SBR通过验证和丰富LLM生成的意图，有效提升了SBR的性能和可解释性。

Abstract: Session-based recommendation (SBR) aims to predict the next item for an
anonymous user in a timely manner. However, SBR suffers from data sparsity due
to the short and anonymous nature of sessions. Recently, an emerging line of
work has explored inferring the underlying user intents of a session using
large language models (LLMs), with the generated intents serving as auxiliary
training signals to enhance SBR models. Despite its promise, this approach
faces three key challenges: validating intent quality, incorporating
session-level multi-intents, and complementing inevitable LLM failure cases. In
this paper, we propose VELI4SBR, a two-stage framework that leverages Validated
and Enriched LLM-generated Intents for SBR. In the first stage, we generate
high-quality intents using a predict-and-correct loop that validates the
informativeness of LLM-generated intents with a global intent pool to constrain
the LLM's output space and reduce hallucination. In the second stage, we
enhance the SBR model using the generated intents through a lightweight
multi-intent prediction and fusion mechanism. Furthermore, we introduce a
training strategy that compensates for LLM failures by inferring intents from
inter-session behavioral similarities. Extensive experiments show that VELI4SBR
outperforms state-of-the-art baselines while improving explainability.

</details>


### [76] [Experimental Evaluation of Dynamic Topic Modeling Algorithms](https://arxiv.org/abs/2508.00710)
*Ngozichukwuka Onah,Nadine Steinmetz,Hani Al-Sayeh,Kai-Uwe Sattler*

Main category: cs.IR

TL;DR: 比较自驱动主题模型并提出评估指标以记录主题随时间变化。


<details>
  <summary>Details</summary>
Motivation: 社交媒体文本量大且分析有价值，但缺乏对自驱动主题模型的定量比较。

Method: 比较现有自驱动主题模型并提出新评估指标。

Result: 提出了一种记录主题随时间变化的评估方法。

Conclusion: 研究填补了自驱动主题模型定量比较的空白，并提供了新评估工具。

Abstract: The amount of text generated daily on social media is gigantic and analyzing
this text is useful for many purposes. To understand what lies beneath a huge
amount of text, we need dependable and effective computing techniques from
self-powered topic models. Nevertheless, there are currently relatively few
thorough quantitative comparisons between these models. In this study, we
compare these models and propose an assessment metric that documents how the
topics change in time.

</details>


### [77] [Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking](https://arxiv.org/abs/2508.00751)
*Qing Zhang,Alex Deng,Michelle Du,Huiji Gao,Liwei He,Sanjeev Katariya*

Main category: cs.IR

TL;DR: 论文提出了一种结合交错和反事实评估的方法，以快速筛选A/B测试候选方案，显著提升实验敏感性和效率。


<details>
  <summary>Details</summary>
Motivation: 在线平台在评估排名算法时面临统计功效不足和传统A/B测试耗时长的问题，需要更高效的评估方法。

Method: 开发了交错和反事实评估方法，用于快速在线评估并筛选A/B测试候选方案。

Result: 实验敏感性提升高达100倍，同时优化了实验流程。

Conclusion: 该方法为在线平台提供了一种高效评估工具，适用于类似需求的机构。

Abstract: Evaluation plays a crucial role in the development of ranking algorithms on
search and recommender systems. It enables online platforms to create
user-friendly features that drive commercial success in a steady and effective
manner. The online environment is particularly conducive to applying causal
inference techniques, such as randomized controlled experiments (known as A/B
test), which are often more challenging to implement in fields like medicine
and public policy. However, businesses face unique challenges when it comes to
effective A/B test. Specifically, achieving sufficient statistical power for
conversion-based metrics can be time-consuming, especially for significant
purchases like booking accommodations. While offline evaluations are quicker
and more cost-effective, they often lack accuracy and are inadequate for
selecting candidates for A/B test. To address these challenges, we developed
interleaving and counterfactual evaluation methods to facilitate rapid online
assessments for identifying the most promising candidates for A/B tests. Our
approach not only increased the sensitivity of experiments by a factor of up to
100 (depending on the approach and metrics) compared to traditional A/B testing
but also streamlined the experimental process. The practical insights gained
from usage in production can also benefit organizations with similar interests.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [78] [Predicting Large-scale Urban Network Dynamics with Energy-informed Graph Neural Diffusion](https://arxiv.org/abs/2508.00037)
*Tong Nie,Jian Sun,Wei Ma*

Main category: cs.LG

TL;DR: 论文提出了一种基于物理定律启发的可扩展时空Transformer（ScaleSTF），用于预测大规模城市网络中的动态，解决了现有模型在效能与效率之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 城市网络系统涉及复杂过程，现有数据驱动模型（如图神经网络）在计算需求上面临效能与效率的权衡，限制了其在大规模网络中的应用。

Method: 通过结合物理定律设计模型，提出了一种基于Transformer结构的可解释神经扩散方案，其注意力层由低维嵌入诱导，具有线性复杂度。

Result: ScaleSTF在交通流量、太阳能发电和智能电表等大规模城市系统中验证了其先进性能和卓越的可扩展性。

Conclusion: 该研究为大规模城市网络动态预测提供了新视角，展示了模型的高效性和可扩展性。

Abstract: Networked urban systems facilitate the flow of people, resources, and
services, and are essential for economic and social interactions. These systems
often involve complex processes with unknown governing rules, observed by
sensor-based time series. To aid decision-making in industrial and engineering
contexts, data-driven predictive models are used to forecast spatiotemporal
dynamics of urban systems. Current models such as graph neural networks have
shown promise but face a trade-off between efficacy and efficiency due to
computational demands. Hence, their applications in large-scale networks still
require further efforts. This paper addresses this trade-off challenge by
drawing inspiration from physical laws to inform essential model designs that
align with fundamental principles and avoid architectural redundancy. By
understanding both micro- and macro-processes, we present a principled
interpretable neural diffusion scheme based on Transformer-like structures
whose attention layers are induced by low-dimensional embeddings. The proposed
scalable spatiotemporal Transformer (ScaleSTF), with linear complexity, is
validated on large-scale urban systems including traffic flow, solar power, and
smart meters, showing state-of-the-art performance and remarkable scalability.
Our results constitute a fresh perspective on the dynamics prediction in
large-scale urban networks.

</details>


### [79] [Hybrid LSTM-Transformer Models for Profiling Highway-Railway Grade Crossings](https://arxiv.org/abs/2508.00039)
*Kaustav Chatterjee,Joshua Q. Li,Fatemeh Ansari,Masud Rana Munna,Kundan Parajulee,Jared Schwennesen*

Main category: cs.LG

TL;DR: 研究提出了一种结合LSTM和Transformer的混合深度学习框架，用于高效测量高速公路铁路平交道口（HRGC）的剖面，以解决传统方法成本高、耗时长的问题。


<details>
  <summary>Details</summary>
Motivation: HRGC的高剖面可能导致车辆悬挂问题，传统测量方法成本高且效率低，亟需一种更高效、经济的解决方案。

Method: 开发了三种混合深度学习模型（Transformer-LSTM、LSTM-Transformer序列和并行模型），利用IMU和GPS传感器采集数据，并与工业标准步行剖面仪数据对比。

Result: 模型2和3表现最佳，能够生成2D/3D HRGC剖面，显著提升了测量效率和准确性。

Conclusion: 该深度学习框架为HRGC剖面测量提供了一种快速、准确的解决方案，有助于提升公路和铁路的安全性。

Abstract: Hump crossings, or high-profile Highway Railway Grade Crossings (HRGCs), pose
safety risks to highway vehicles due to potential hang-ups. These crossings
typically result from post-construction railway track maintenance activities or
non-compliance with design guidelines for HRGC vertical alignments.
Conventional methods for measuring HRGC profiles are costly, time-consuming,
traffic-disruptive, and present safety challenges. To address these issues,
this research employed advanced, cost-effective techniques and innovative
modeling approaches for HRGC profile measurement. A novel hybrid deep learning
framework combining Long Short-Term Memory (LSTM) and Transformer architectures
was developed by utilizing instrumentation and ground truth data.
Instrumentation data were gathered using a highway testing vehicle equipped
with Inertial Measurement Unit (IMU) and Global Positioning System (GPS)
sensors, while ground truth data were obtained via an industrial-standard
walking profiler. Field data was collected at the Red Rock Railroad Corridor in
Oklahoma. Three advanced deep learning models Transformer-LSTM sequential
(model 1), LSTM-Transformer sequential (model 2), and LSTM-Transformer parallel
(model 3) were evaluated to identify the most efficient architecture. Models 2
and 3 outperformed the others and were deployed to generate 2D/3D HRGC
profiles. The deep learning models demonstrated significant potential to
enhance highway and railroad safety by enabling rapid and accurate assessment
of HRGC hang-up susceptibility.

</details>


### [80] [Regime-Aware Conditional Neural Processes with Multi-Criteria Decision Support for Operational Electricity Price Forecasting](https://arxiv.org/abs/2508.00040)
*Abhinav Das,Stephan Schlüter*

Main category: cs.LG

TL;DR: 该论文提出了一种结合贝叶斯机制检测和条件神经过程的方法（R-NP），用于预测德国市场的24小时电价，并通过多标准评估（TOPSIS）验证其综合性能优于传统模型。


<details>
  <summary>Details</summary>
Motivation: 电力市场价格的动态性和复杂性需要更精确的预测方法，以支持电池存储优化等实际应用。

Method: 使用DS-HDP-HMM进行机制检测，每个机制由独立的CNP建模，最终预测为机制加权的CNP输出。

Result: R-NP在TOPSIS评估中表现最均衡，优于DNN和LEAR模型。

Conclusion: R-NP是一种综合性能优越的电价预测方法，适用于多种实际应用场景。

Abstract: This work integrates Bayesian regime detection with conditional neural
processes for 24-hour electricity price prediction in the German market. Our
methodology integrates regime detection using a disentangled sticky
hierarchical Dirichlet process hidden Markov model (DS-HDP-HMM) applied to
daily electricity prices. Each identified regime is subsequently modeled by an
independent conditional neural process (CNP), trained to learn localized
mappings from input contexts to 24-dimensional hourly price trajectories, with
final predictions computed as regime-weighted mixtures of these CNP outputs. We
rigorously evaluate R-NP against deep neural networks (DNN) and Lasso estimated
auto-regressive (LEAR) models by integrating their forecasts into diverse
battery storage optimization frameworks, including price arbitrage, risk
management, grid services, and cost minimization. This operational utility
assessment revealed complex performance trade-offs: LEAR often yielded superior
absolute profits or lower costs, while DNN showed exceptional optimality in
specific cost-minimization contexts. Recognizing that raw prediction accuracy
doesn't always translate to optimal operational outcomes, we employed TOPSIS as
a comprehensive multi-criteria evaluation layer. Our TOPSIS analysis identified
LEAR as the top-ranked model for 2021, but crucially, our proposed R-NP model
emerged as the most balanced and preferred solution for 2021, 2022 and 2023.

</details>


### [81] [Learning Like Humans: Resource-Efficient Federated Fine-Tuning through Cognitive Developmental Stages](https://arxiv.org/abs/2508.00041)
*Yebo Wu,Jingguang Li,Zhijiang Guo,Li Li*

Main category: cs.LG

TL;DR: DevFT是一种资源高效的联邦微调方法，通过分阶段构建LLM，显著提升性能并减少资源消耗。


<details>
  <summary>Details</summary>
Motivation: 解决联邦微调在边缘设备上资源消耗大的问题，同时保持数据隐私。

Method: 分阶段微调，逐步增加子模型参数容量，通过知识传递优化初始化参数，并采用层分组和融合技术。

Result: 在多个基准测试中表现优异，收敛速度提升4.59倍，通信开销减少10.67倍，性能平均提升9.07%。

Conclusion: DevFT是一种高效且兼容现有方法的联邦微调方案。

Abstract: Federated fine-tuning enables Large Language Models (LLMs) to adapt to
downstream tasks while preserving data privacy, but its resource-intensive
nature limits deployment on edge devices. In this paper, we introduce
Developmental Federated Tuning (DevFT), a resource-efficient approach inspired
by cognitive development that progressively builds a powerful LLM from a
compact foundation. DevFT decomposes the fine-tuning process into developmental
stages, each optimizing submodels with increasing parameter capacity. Knowledge
from earlier stages transfers to subsequent submodels, providing optimized
initialization parameters that prevent convergence to local minima and
accelerate training. This paradigm mirrors human learning, gradually
constructing comprehensive knowledge structure while refining existing skills.
To efficiently build stage-specific submodels, DevFT introduces
deconfliction-guided layer grouping and differential-based layer fusion to
distill essential information and construct representative layers. Evaluations
across multiple benchmarks demonstrate that DevFT significantly outperforms
state-of-the-art methods, achieving up to 4.59$\times$ faster convergence,
10.67$\times$ reduction in communication overhead, and 9.07% average
performance improvement, while maintaining compatibility with existing
approaches.

</details>


### [82] [Improved Robustness and Functional Localization in Topographic CNNs Through Weight Similarity](https://arxiv.org/abs/2508.00043)
*Nhut Truong,Uri Hasson*

Main category: cs.LG

TL;DR: 比较了两种空间约束（权重相似性和激活相似性）对地形卷积神经网络的影响，发现权重相似性在鲁棒性、输入敏感性和功能定位方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 系统地研究不同地形约束对神经网络学习表示的影响，填补现有研究的空白。

Method: 比较了权重相似性（WS）和激活相似性（AS）两种空间约束，评估了分类准确性、鲁棒性和表示的空间组织。

Result: WS在噪声鲁棒性、输入敏感性和功能定位方面优于AS和标准CNN，并影响了网络表示几何。

Conclusion: 权重相似性约束在端到端训练中产生更鲁棒的表示，对生物物理启发模型的特征学习和功能组织有重要影响。

Abstract: Topographic neural networks are computational models that can simulate the
spatial and functional organization of the brain. Topographic constraints in
neural networks can be implemented in multiple ways, with potentially different
impacts on the representations learned by the network. The impact of such
different implementations has not been systematically examined. To this end,
here we compare topographic convolutional neural networks trained with two
spatial constraints: Weight Similarity (WS), which pushes neighboring units to
develop similar incoming weights, and Activation Similarity (AS), which
enforces similarity in unit activations. We evaluate the resulting models on
classification accuracy, robustness to weight perturbations and input
degradation, and the spatial organization of learned representations. Compared
to both AS and standard CNNs, WS provided three main advantages: i) improved
robustness to noise, also showing higher accuracy under weight corruption; ii)
greater input sensitivity, reflected in higher activation variance; and iii)
stronger functional localization, with units showing similar activations
positioned at closer distances. In addition, WS produced differences in
orientation tuning, symmetry sensitivity, and eccentricity profiles of units,
indicating an influence of this spatial constraint on the representational
geometry of the network. Our findings suggest that during end-to-end training,
WS constraints produce more robust representations than AS or non-topographic
CNNs. These findings also suggest that weight-based spatial constraints can
shape feature learning and functional organization in biophysical inspired
models.

</details>


### [83] [Benchmarking Partial Observability in Reinforcement Learning with a Suite of Memory-Improvable Domains](https://arxiv.org/abs/2508.00046)
*Ruo Yu Tao,Kaicheng Guo,Cameron Allen,George Konidaris*

Main category: cs.LG

TL;DR: 论文提出了一个用于评估强化学习算法在部分可观测性下性能的基准框架POBAX，强调基准需覆盖多种部分可观测形式并体现记忆改进性。


<details>
  <summary>Details</summary>
Motivation: 现有基准仅评估简单的部分可观测形式（如特征掩码和高斯噪声），无法反映真实场景（如视觉遮挡或未知对手意图），需要更全面的基准。

Method: 提出POBAX开源库，包含多种部分可观测环境（如定位与建图、视觉控制、游戏等），并提供超参数和算法实现。

Result: 所选任务均具有记忆改进性且需学习复杂记忆功能，为部分可观测性研究提供了明确信号。

Conclusion: POBAX为部分可观测性研究提供了标准化、可扩展的基准框架，支持快速评估和GPU加速实验。

Abstract: Mitigating partial observability is a necessary but challenging task for
general reinforcement learning algorithms. To improve an algorithm's ability to
mitigate partial observability, researchers need comprehensive benchmarks to
gauge progress. Most algorithms tackling partial observability are only
evaluated on benchmarks with simple forms of state aliasing, such as feature
masking and Gaussian noise. Such benchmarks do not represent the many forms of
partial observability seen in real domains, like visual occlusion or unknown
opponent intent. We argue that a partially observable benchmark should have two
key properties. The first is coverage in its forms of partial observability, to
ensure an algorithm's generalizability. The second is a large gap between the
performance of a agents with more or less state information, all other factors
roughly equal. This gap implies that an environment is memory improvable: where
performance gains in a domain are from an algorithm's ability to cope with
partial observability as opposed to other factors. We introduce best-practice
guidelines for empirically benchmarking reinforcement learning under partial
observability, as well as the open-source library POBAX: Partially Observable
Benchmarks in JAX. We characterize the types of partial observability present
in various environments and select representative environments for our
benchmark. These environments include localization and mapping, visual control,
games, and more. Additionally, we show that these tasks are all memory
improvable and require hard-to-learn memory functions, providing a concrete
signal for partial observability research. This framework includes recommended
hyperparameters as well as algorithm implementations for fast, out-of-the-box
evaluation, as well as highly performant environments implemented in JAX for
GPU-scalable experimentation.

</details>


### [84] [TriP-LLM: A Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly Detection](https://arxiv.org/abs/2508.00047)
*Yuan-Cheng Yu,Yen-Chieh Ouyang,Chun-An Lin*

Main category: cs.LG

TL;DR: 论文提出了一种名为TriP-LLM的无监督时间序列异常检测框架，利用预训练的大型语言模型（LLM）处理高维异构数据，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 随着物联网和智能制造的普及，时间序列数据的规模和维度急剧增加，传统统计方法难以应对其高异质性和复杂性。

Method: TriP-LLM通过三分支设计（Patching、Selection、Global）整合局部和全局时间特征，将时间序列编码为补丁令牌，由冻结的预训练LLM处理，并通过轻量级解码器重构输入以计算异常分数。

Result: 实验表明，TriP-LLM在多个公共基准数据集上优于现有方法，且内存消耗显著低于基于通道独立（CI）的LLM方法。

Conclusion: TriP-LLM展示了LLM在时间序列异常检测中的强大能力，适用于GPU内存受限环境，代码和模型已开源。

Abstract: Time-series anomaly detection plays a central role across a wide range of
application domains. With the increasing proliferation of the Internet of
Things (IoT) and smart manufacturing, time-series data has dramatically
increased in both scale and dimensionality. This growth has exposed the
limitations of traditional statistical methods in handling the high
heterogeneity and complexity of such data. Inspired by the recent success of
large language models (LLMs) in multimodal tasks across language and vision
domains, we propose a novel unsupervised anomaly detection framework: A
Tri-Branch Patch-wise Large Language Model Framework for Time-Series Anomaly
Detection (TriP-LLM). TriP-LLM integrates local and global temporal features
through a tri-branch design-Patching, Selection, and Global-to encode the input
time series into patch-wise tokens, which are then processed by a frozen,
pretrained LLM. A lightweight patch-wise decoder reconstructs the input, from
which anomaly scores are derived. We evaluate TriP-LLM on several public
benchmark datasets using PATE, a recently proposed threshold-free evaluation
metric, and conduct all comparisons within a unified open-source framework to
ensure fairness. Experimental results show that TriP-LLM consistently
outperforms recent state-of-the-art methods across all datasets, demonstrating
strong detection capabilities. Furthermore, through extensive ablation studies,
we verify the substantial contribution of the LLM to the overall architecture.
Compared to LLM-based approaches using Channel Independence (CI) patch
processing, TriP-LLM achieves significantly lower memory consumption, making it
more suitable for GPU memory-constrained environments. All code and model
checkpoints are publicly available on https://github.com/YYZStart/TriP-LLM.git

</details>


### [85] [Evaluating COVID 19 Feature Contributions to Bitcoin Return Forecasting: Methodology Based on LightGBM and Genetic Optimization](https://arxiv.org/abs/2508.00078)
*Imen Mahmoud,Andrei Velichko*

Main category: cs.LG

TL;DR: 该研究提出了一种结合LightGBM回归模型和遗传算法优化的新方法，用于评估COVID-19相关指标对比特币回报预测的贡献。结果表明，疫情指标显著提升了预测准确性，尤其是疫苗接种数据。


<details>
  <summary>Details</summary>
Motivation: 研究旨在确定是否包含疫情相关健康数据能显著提升比特币回报预测的准确性，而非仅预测回报。

Method: 构建了包含比特币回报和COVID-19指标的每日数据集，使用遗传算法优化模型，并通过统计测试比较性能。

Result: COVID-19指标显著提升了模型性能（R2提高40%，RMSE降低2%），疫苗接种数据是主要预测因子。

Conclusion: 该方法通过纳入公共卫生信号扩展了金融分析工具，为投资者和政策制定者提供了应对市场不确定性的新指标。

Abstract: This study proposes a novel methodological framework integrating a LightGBM
regression model and genetic algorithm (GA) optimization to systematically
evaluate the contribution of COVID-19-related indicators to Bitcoin return
prediction. The primary objective was not merely to forecast Bitcoin returns
but rather to determine whether including pandemic-related health data
significantly enhances prediction accuracy. A comprehensive dataset comprising
daily Bitcoin returns and COVID-19 metrics (vaccination rates,
hospitalizations, testing statistics) was constructed. Predictive models,
trained with and without COVID-19 features, were optimized using GA over 31
independent runs, allowing robust statistical assessment. Performance metrics
(R2, RMSE, MAE) were statistically compared through distribution overlaps and
Mann-Whitney U tests. Permutation Feature Importance (PFI) analysis quantified
individual feature contributions. Results indicate that COVID-19 indicators
significantly improved model performance, particularly in capturing extreme
market fluctuations (R2 increased by 40%, RMSE decreased by 2%, both highly
significant statistically). Among COVID-19 features, vaccination metrics,
especially the 75th percentile of fully vaccinated individuals, emerged as
dominant predictors. The proposed methodology extends existing financial
analytics tools by incorporating public health signals, providing investors and
policymakers with refined indicators to navigate market uncertainty during
systemic crises.

</details>


### [86] [Stress-Aware Resilient Neural Training](https://arxiv.org/abs/2508.00098)
*Ashkan Shakarami,Yousef Yeganeh,Azade Farshad,Lorenzo Nicole,Stefano Ghidoni,Nassir Navab*

Main category: cs.LG

TL;DR: 论文提出了一种基于材料科学中结构疲劳概念的弹性学习范式，通过动态调整优化行为提升神经网络的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 受材料科学中临时（弹性）和永久（塑性）变形概念的启发，旨在解决神经网络在训练过程中因优化困难而陷入尖锐最小值的问题。

Method: 提出了塑性变形优化器（Plastic Deformation Optimizer），通过内部应力信号检测训练停滞，并自适应地向模型参数注入噪声，帮助模型逃离尖锐最小值。

Result: 在六种架构、四种优化器和七个视觉基准测试中验证了方法的有效性，显著提升了模型的鲁棒性和泛化能力，且计算开销极小。

Conclusion: Stress-Aware Learning为神经网络提供了一种动态适应优化行为的有效方法，尤其在不确定动态环境中表现优异。

Abstract: This paper introduces Stress-Aware Learning, a resilient neural training
paradigm in which deep neural networks dynamically adjust their optimization
behavior - whether under stable training regimes or in settings with uncertain
dynamics - based on the concept of Temporary (Elastic) and Permanent (Plastic)
Deformation, inspired by structural fatigue in materials science. To
instantiate this concept, we propose Plastic Deformation Optimizer, a
stress-aware mechanism that injects adaptive noise into model parameters
whenever an internal stress signal - reflecting stagnation in training loss and
accuracy - indicates persistent optimization difficulty. This enables the model
to escape sharp minima and converge toward flatter, more generalizable regions
of the loss landscape. Experiments across six architectures, four optimizers,
and seven vision benchmarks demonstrate improved robustness and generalization
with minimal computational overhead. The code and 3D visuals will be available
on GitHub: https://github.com/Stress-Aware-Learning/SAL.

</details>


### [87] [StackLiverNet: A Novel Stacked Ensemble Model for Accurate and Interpretable Liver Disease Detection](https://arxiv.org/abs/2508.00117)
*Md. Ehsanul Haque,S. M. Jahidul Islam,Shakil Mia,Rumana Sharmin,Ashikuzzaman,Md Samir Morshed,Md. Tahmidul Huque*

Main category: cs.LG

TL;DR: 提出了一种名为StackLiverNet的可解释堆叠集成模型，用于肝病检测，解决了现有模型的高误分类、低可解释性和高计算成本等问题。


<details>
  <summary>Details</summary>
Motivation: 肝病诊断需要精确且及时的检测方法，现有机器学习模型存在误分类率高、可解释性差等问题，因此开发了StackLiverNet。

Method: 采用高级数据预处理和特征选择技术，随机欠采样处理类别不平衡，通过LightGBM元模型集成多个超参数优化的基分类器。

Result: 测试准确率达99.89%，Cohen Kappa为0.9974，AUC为0.9993，仅5次误分类，训练和推理速度快。

Conclusion: StackLiverNet在肝病检测中表现出色，兼具高性能和可解释性，适合临床实践。

Abstract: Liver diseases are a serious health concern in the world, which requires
precise and timely diagnosis to enhance the survival chances of patients. The
current literature implemented numerous machine learning and deep learning
models to classify liver diseases, but most of them had some issues like high
misclassification error, poor interpretability, prohibitive computational
expense, and lack of good preprocessing strategies. In order to address these
drawbacks, we introduced StackLiverNet in this study; an interpretable stacked
ensemble model tailored to the liver disease detection task. The framework uses
advanced data preprocessing and feature selection technique to increase model
robustness and predictive ability. Random undersampling is performed to deal
with class imbalance and make the training balanced. StackLiverNet is an
ensemble of several hyperparameter-optimized base classifiers, whose
complementary advantages are used through a LightGBM meta-model. The provided
model demonstrates excellent performance, with the testing accuracy of 99.89%,
Cohen Kappa of 0.9974, and AUC of 0.9993, having only 5 misclassifications, and
efficient training and inference speeds that are amenable to clinical practice
(training time 4.2783 seconds, inference time 0.1106 seconds). Besides, Local
Interpretable Model-Agnostic Explanations (LIME) are applied to generate
transparent explanations of individual predictions, revealing high
concentrations of Alkaline Phosphatase and moderate SGOT as important
observations of liver disease. Also, SHAP was used to rank features by their
global contribution to predictions, while the Morris method confirmed the most
influential features through sensitivity analysis.

</details>


### [88] [Structured Transformations for Stable and Interpretable Neural Computation](https://arxiv.org/abs/2508.00127)
*Saleh Nikooroo,Thomas Engel*

Main category: cs.LG

TL;DR: 论文提出了一种新的神经网络层变换方法，通过分解为结构化线性算子和残差校正组件，提升训练稳定性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当代神经网络缺乏结构保障，导致学习不稳定和行为难以解释。

Method: 将层变换分解为结构化线性算子和残差校正组件，支持稳定的信号传播和训练动态。

Result: 实验表明，该方法改善了梯度条件、降低了对扰动的敏感性，并增强了层间鲁棒性。

Conclusion: 该方法为构建更稳定、透明的神经网络架构提供了基础，同时保持了表达能力。

Abstract: Despite their impressive performance, contemporary neural networks often lack
structural safeguards that promote stable learning and interpretable behavior.
In this work, we introduce a reformulation of layer-level transformations that
departs from the standard unconstrained affine paradigm. Each transformation is
decomposed into a structured linear operator and a residual corrective
component, enabling more disciplined signal propagation and improved training
dynamics. Our formulation encourages internal consistency and supports stable
information flow across depth, while remaining fully compatible with standard
learning objectives and backpropagation. Through a series of synthetic and
real-world experiments, we demonstrate that models constructed with these
structured transformations exhibit improved gradient conditioning, reduced
sensitivity to perturbations, and layer-wise robustness. We further show that
these benefits persist across architectural scales and training regimes. This
study serves as a foundation for a more principled class of neural
architectures that prioritize stability and transparency-offering new tools for
reasoning about learning behavior without sacrificing expressive power.

</details>


### [89] [ECG Latent Feature Extraction with Autoencoders for Downstream Prediction Tasks](https://arxiv.org/abs/2508.00131)
*Christopher Harvey,Sumaiya Shomaji,Zijun Yao,Amit Noheria*

Main category: cs.LG

TL;DR: 论文探讨了使用PCA和自动编码器简化ECG数据的方法，并提出了三种新型VAE变体，显著提高了信号重建和下游预测任务的性能。


<details>
  <summary>Details</summary>
Motivation: ECG信号的高复杂性和个体间变异性使其在深度学习模型中应用困难，尤其是在小规模数据集上。

Method: 研究比较了PCA和自动编码器，并提出了三种新型VAE变体（SAE、A beta-VAE、C beta-VAE），结合LGBM进行预测任务。

Result: A beta-VAE在信号重建上表现最佳（MAE为15.7±3.2μV），SAE编码结合传统特征提高了LVEF预测（AUROC为0.901）。

Conclusion: 新型VAE编码不仅简化了ECG数据，还为小规模数据集的深度学习应用提供了实用解决方案。

Abstract: The electrocardiogram (ECG) is an inexpensive and widely available tool for
cardiac assessment. Despite its standardized format and small file size, the
high complexity and inter-individual variability of ECG signals (typically a
60,000-size vector with 12 leads at 500 Hz) make it challenging to use in deep
learning models, especially when only small training datasets are available.
This study addresses these challenges by exploring feature generation methods
from representative beat ECGs, focusing on Principal Component Analysis (PCA)
and Autoencoders to reduce data complexity. We introduce three novel
Variational Autoencoder (VAE) variants-Stochastic Autoencoder (SAE), Annealed
beta-VAE (A beta-VAE), and Cyclical beta VAE (C beta-VAE)-and compare their
effectiveness in maintaining signal fidelity and enhancing downstream
prediction tasks using a Light Gradient Boost Machine (LGBM). The A beta-VAE
achieved superior signal reconstruction, reducing the mean absolute error (MAE)
to 15.7+/-3.2 muV, which is at the level of signal noise. Moreover, the SAE
encodings, when combined with traditional ECG summary features, improved the
prediction of reduced Left Ventricular Ejection Fraction (LVEF), achieving an
holdout test set area under the receiver operating characteristic curve (AUROC)
of 0.901 with a LGBM classifier. This performance nearly matches the 0.909
AUROC of state-of-the-art CNN model but requires significantly less
computational resources. Further, the ECG feature extraction-LGBM pipeline
avoids overfitting and retains predictive performance when trained with less
data. Our findings demonstrate that these VAE encodings are not only effective
in simplifying ECG data but also provide a practical solution for applying deep
learning in contexts with limited-scale labeled training data.

</details>


### [90] [INSPIRE-GNN: Intelligent Sensor Placement to Improve Sparse Bicycling Network Prediction via Reinforcement Learning Boosted Graph Neural Networks](https://arxiv.org/abs/2508.00141)
*Mohit Gupta,Debjit Bhowmick,Rhys Newbury,Meead Saberi,Shirui Pan,Ben Beck*

Main category: cs.LG

TL;DR: INSPIRE-GNN是一种结合强化学习和图神经网络的框架，用于优化自行车流量传感器的布局并提高数据稀疏环境下的流量估计精度。


<details>
  <summary>Details</summary>
Motivation: 许多城市因自行车流量传感器覆盖不足导致数据稀疏，影响了可持续城市交通规划的准确性。

Method: INSPIRE-GNN整合了图卷积网络（GCN）、图注意力网络（GAT）和基于深度Q网络（DQN）的强化学习代理，通过数据驱动策略选择传感器位置。

Result: 在墨尔本的自行车网络中，INSPIRE-GNN显著提升了流量估计性能，优于传统启发式方法和标准机器学习模型。

Conclusion: 该框架为交通规划者提供了优化传感器布局和提高数据可靠性的实用工具。

Abstract: Accurate link-level bicycling volume estimation is essential for sustainable
urban transportation planning. However, many cities face significant challenges
of high data sparsity due to limited bicycling count sensor coverage. To
address this issue, we propose INSPIRE-GNN, a novel Reinforcement Learning
(RL)-boosted hybrid Graph Neural Network (GNN) framework designed to optimize
sensor placement and improve link-level bicycling volume estimation in
data-sparse environments. INSPIRE-GNN integrates Graph Convolutional Networks
(GCN) and Graph Attention Networks (GAT) with a Deep Q-Network (DQN)-based RL
agent, enabling a data-driven strategic selection of sensor locations to
maximize estimation performance. Applied to Melbourne's bicycling network,
comprising 15,933 road segments with sensor coverage on only 141 road segments
(99% sparsity) - INSPIRE-GNN demonstrates significant improvements in volume
estimation by strategically selecting additional sensor locations in
deployments of 50, 100, 200 and 500 sensors. Our framework outperforms
traditional heuristic methods for sensor placement such as betweenness
centrality, closeness centrality, observed bicycling activity and random
placement, across key metrics such as Mean Squared Error (MSE), Root Mean
Squared Error (RMSE) and Mean Absolute Error (MAE). Furthermore, our
experiments benchmark INSPIRE-GNN against standard machine learning and deep
learning models in the bicycle volume estimation performance, underscoring its
effectiveness. Our proposed framework provides transport planners actionable
insights to effectively expand sensor networks, optimize sensor placement and
maximize volume estimation accuracy and reliability of bicycling data for
informed transportation planning decisions.

</details>


### [91] [Watch the Weights: Unsupervised monitoring and control of fine-tuned LLMs](https://arxiv.org/abs/2508.00161)
*Ziqian Zhong,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 提出了一种基于权重而非激活的新方法，用于理解和监控微调后的LLM，无需类似训练数据即可检测新行为或潜在威胁。


<details>
  <summary>Details</summary>
Motivation: 现有基于激活的解释方法需要分布相似的数据，限制了检测新威胁（如后门）的能力。

Method: 通过分析微调模型与基础模型权重差的顶部奇异向量，监控激活的余弦相似度来检测新行为。

Result: 成功阻止100%的后门攻击（假阳性率<1.2%），检测遗忘主题准确率达95.42%，并能恢复“遗忘”信息。

Conclusion: 该方法在监控和审计微调LLM方面具有高效性和潜力，适用于实际部署前的模型分析。

Abstract: The releases of powerful open-weight large language models (LLMs) are often
not accompanied by access to their full training data. Existing
interpretability methods, particularly those based on activations, often
require or assume distributionally similar data. This is a significant
limitation when detecting and defending against novel potential threats like
backdoors, which are by definition out-of-distribution.
  In this work, we introduce a new method for understanding, monitoring and
controlling fine-tuned LLMs that interprets weights, rather than activations,
thereby side stepping the need for data that is distributionally similar to the
unknown training data. We demonstrate that the top singular vectors of the
weight difference between a fine-tuned model and its base model correspond to
newly acquired behaviors. By monitoring the cosine similarity of activations
along these directions, we can detect salient behaviors introduced during
fine-tuning with high precision.
  For backdoored models that bypasses safety mechanisms when a secret trigger
is present, our method stops up to 100% of attacks with a false positive rate
below 1.2%. For models that have undergone unlearning, we detect inference on
erased topics with accuracy up to 95.42% and can even steer the model to
recover "unlearned" information. Besides monitoring, our method also shows
potential for pre-deployment model auditing: by analyzing commercial
instruction-tuned models (OLMo, Llama, Qwen), we are able to uncover
model-specific fine-tuning focus including marketing strategies and Midjourney
prompt generation.
  Our implementation can be found at https://github.com/fjzzq2002/WeightWatch.

</details>


### [92] [DiSC-Med: Diffusion-based Semantic Communications for Robust Medical Image Transmission](https://arxiv.org/abs/2508.00172)
*Fupei Guo,Hao Zheng,Xiang Zhang,Li Chen,Yue Wang,Songyang Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于扩散的语义通信框架DiSC-Med，用于高效且鲁棒的医疗图像传输。


<details>
  <summary>Details</summary>
Motivation: 人工智能和无线通信技术的发展推动了远程医疗的需求，但医疗数据在有限带宽和噪声信道中的高效传输仍是一个关键挑战。

Method: 开发了医疗增强的压缩和去噪模块，通过语义通信框架DiSC-Med实现高效带宽利用和鲁棒性。

Result: 在真实医疗数据集上的实验表明，DiSC-Med能捕捉关键语义信息，并在噪声信道中实现高效重建。

Conclusion: DiSC-Med框架为远程医疗提供了高效且鲁棒的解决方案，具有实际应用潜力。

Abstract: The rapid development of artificial intelligence has driven smart health with
next-generation wireless communication technologies, stimulating exciting
applications in remote diagnosis and intervention. To enable a timely and
effective response for remote healthcare, efficient transmission of medical
data through noisy channels with limited bandwidth emerges as a critical
challenge. In this work, we propose a novel diffusion-based semantic
communication framework, namely DiSC-Med, for the medical image transmission,
where medical-enhanced compression and denoising blocks are developed for
bandwidth efficiency and robustness, respectively. Unlike conventional
pixel-wise communication framework, our proposed DiSC-Med is able to capture
the key semantic information and achieve superior reconstruction performance
with ultra-high bandwidth efficiency against noisy channels. Extensive
experiments on real-world medical datasets validate the effectiveness of our
framework, demonstrating its potential for robust and efficient telehealth
applications.

</details>


### [93] [RL as Regressor: A Reinforcement Learning Approach for Function Approximation](https://arxiv.org/abs/2508.00174)
*Yongchao Huang*

Main category: cs.LG

TL;DR: 论文提出将回归问题转化为强化学习问题，通过自定义奖励信号和RL算法实现更灵活的目标定义和学习过程。


<details>
  <summary>Details</summary>
Motivation: 传统回归方法受限于预定义的可微损失函数，难以处理非对称成本或复杂目标。

Method: 将模型预测视为动作，定义基于预测误差的奖励信号，并利用Actor-Critic算法及改进技术（如优先经验回放、网络容量增加和位置编码）进行函数逼近。

Result: RL框架成功解决了回归问题，并在目标定义和学习过程中展现出更高的灵活性。

Conclusion: 强化学习为回归问题提供了新的解决范式，尤其在处理复杂目标时更具优势。

Abstract: Standard regression techniques, while powerful, are often constrained by
predefined, differentiable loss functions such as mean squared error. These
functions may not fully capture the desired behavior of a system, especially
when dealing with asymmetric costs or complex, non-differentiable objectives.
In this paper, we explore an alternative paradigm: framing regression as a
Reinforcement Learning (RL) problem. We demonstrate this by treating a model's
prediction as an action and defining a custom reward signal based on the
prediction error, and we can leverage powerful RL algorithms to perform
function approximation. Through a progressive case study of learning a noisy
sine wave, we illustrate the development of an Actor-Critic agent, iteratively
enhancing it with Prioritized Experience Replay, increased network capacity,
and positional encoding to enable a capable RL agent for this regression task.
Our results show that the RL framework not only successfully solves the
regression problem but also offers enhanced flexibility in defining objectives
and guiding the learning process.

</details>


### [94] [EMA Without the Lag: Bias-Corrected Iterate Averaging Schemes](https://arxiv.org/abs/2508.00180)
*Adam Block,Cyril Zhang*

Main category: cs.LG

TL;DR: 论文提出了一种改进的指数移动平均方法（BEMA），用于减少语言模型微调中的训练不稳定性，同时避免传统EMA引入的偏差。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型微调中因小批量训练导致的训练不稳定问题，同时避免传统EMA方法因旧迭代权重引入的优化滞后。

Method: 提出BEMA方法，通过理论模型证明其优于传统EMA和普通训练，并通过实验验证其在语言模型上的效果。

Result: BEMA在多种标准语言模型基准测试中显著提升了收敛速度和最终性能。

Conclusion: BEMA是一种实用且理论支持的方法，能更稳定高效地进行语言模型微调。

Abstract: Stochasticity in language model fine-tuning, often caused by the small batch
sizes typically used in this regime, can destabilize training by introducing
large oscillations in generation quality. A popular approach to mitigating this
instability is to take an Exponential moving average (EMA) of weights
throughout training. While EMA reduces stochasticity, thereby smoothing
training, the introduction of bias from old iterates often creates a lag in
optimization relative to vanilla training. In this work, we propose the
Bias-Corrected Exponential Moving Average (BEMA), a simple and practical
augmentation of EMA that retains variance-reduction benefits while eliminating
bias. BEMA is motivated by a simple theoretical model wherein we demonstrate
provable acceleration of BEMA over both a standard EMA and vanilla training.
Through an extensive suite of experiments on Language Models, we show that BEMA
leads to significantly improved convergence rates and final performance over
both EMA and vanilla training in a variety of standard LM benchmarks, making
BEMA a practical and theoretically motivated intervention for more stable and
efficient fine-tuning.

</details>


### [95] [RecoMind: A Reinforcement Learning Framework for Optimizing In-Session User Satisfaction in Recommendation Systems](https://arxiv.org/abs/2508.00201)
*Mehdi Ben Ayed,Fei Feng,Jay Adams,Vishwakarma Singh,Kritarth Anand,Jiajing Xu*

Main category: cs.LG

TL;DR: RecoMind是一个基于模拟器的强化学习框架，用于优化大规模推荐系统中的会话目标，显著提升用户满意度。


<details>
  <summary>Details</summary>
Motivation: 现有推荐系统主要依赖监督学习，难以优化长期目标（如会话参与度），而强化学习在大规模应用时面临动作空间大和工程复杂性的挑战。

Method: RecoMind利用现有推荐模型构建模拟环境，并通过自定义探索策略高效探索大规模动作空间，简化了强化学习策略的训练和部署。

Result: 离线模拟和在线A/B测试显示，RecoMind训练的强化学习策略显著优于传统监督学习方法，会话用户满意度提升明显。

Conclusion: RecoMind为大规模推荐系统提供了一种系统化、可扩展的强化学习嵌入方法，优化会话用户满意度效果显著。

Abstract: Existing web-scale recommendation systems commonly use supervised learning
methods that prioritize immediate user feedback. Although reinforcement
learning (RL) offers a solution to optimize longer-term goals, such as
in-session engagement, applying it at web scale is challenging due to the
extremely large action space and engineering complexity. In this paper, we
introduce RecoMind, a simulator-based RL framework designed for the effective
optimization of session-based goals at web-scale. RecoMind leverages existing
recommendation models to establish a simulation environment and to bootstrap
the RL policy to optimize immediate user interactions from the outset. This
method integrates well with existing industry pipelines, simplifying the
training and deployment of RL policies. Additionally, RecoMind introduces a
custom exploration strategy to efficiently explore web-scale action spaces with
hundreds of millions of items. We evaluated RecoMind through extensive offline
simulations and online A/B testing on a video streaming platform. Both methods
showed that the RL policy trained using RecoMind significantly outperforms
traditional supervised learning recommendation approaches in in-session user
satisfaction. In online A/B tests, the RL policy increased videos watched for
more than 10 seconds by 15.81\% and improved session depth by 4.71\% for
sessions with at least 10 interactions. As a result, RecoMind presents a
systematic and scalable approach for embedding RL into web-scale recommendation
systems, showing great promise for optimizing session-based user satisfaction.

</details>


### [96] [Robust Classification under Noisy Labels: A Geometry-Aware Reliability Framework for Foundation Models](https://arxiv.org/abs/2508.00202)
*Ecem Bozkurt,Antonio Ortega*

Main category: cs.LG

TL;DR: 论文提出了一种两阶段框架，利用几何信息在标签噪声下实现鲁棒分类，无需重新训练模型。


<details>
  <summary>Details</summary>
Motivation: 在噪声标签数据下微调基础模型时，现有方法依赖局部几何信息，但性能仍有提升空间。

Method: 采用两阶段方法：可靠性估计和可靠性加权推断，引入非负核（NNK）邻域构建。

Result: 在CIFAR-10和DermaMNIST上，方法在多种噪声条件下表现优于标准K-NN和自适应邻域基线。

Conclusion: 通过几何信息和可靠性加权推断，显著提升了噪声标签数据下的分类鲁棒性。

Abstract: Foundation models (FMs) pretrained on large datasets have become fundamental
for various downstream machine learning tasks, in particular in scenarios where
obtaining perfectly labeled data is prohibitively expensive. In this paper, we
assume an FM has to be fine-tuned with noisy data and present a two-stage
framework to ensure robust classification in the presence of label noise
without model retraining. Recent work has shown that simple k-nearest neighbor
(kNN) approaches using an embedding derived from an FM can achieve good
performance even in the presence of severe label noise. Our work is motivated
by the fact that these methods make use of local geometry. In this paper,
following a similar two-stage procedure, reliability estimation followed by
reliability-weighted inference, we show that improved performance can be
achieved by introducing geometry information. For a given instance, our
proposed inference uses a local neighborhood of training data, obtained using
the non-negative kernel (NNK) neighborhood construction. We propose several
methods for reliability estimation that can rely less on distance and local
neighborhood as the label noise increases. Our evaluation on CIFAR-10 and
DermaMNIST shows that our methods improve robustness across various noise
conditions, surpassing standard K-NN approaches and recent
adaptive-neighborhood baselines.

</details>


### [97] [Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product](https://arxiv.org/abs/2508.00230)
*Paul Albert,Frederic Z. Zhang,Hemanth Saratchandran,Anton van den Hengel,Ehsan Abbasnejad*

Main category: cs.LG

TL;DR: KRAdapter是一种新的PEFT算法，通过Khatri-Rao乘积生成权重更新，解决了LoRA在近似高有效秩矩阵时的局限性，并在大规模模型上表现出色。


<details>
  <summary>Details</summary>
Motivation: LoRA在多模态和大语言模型中的表现受限，尤其是在处理高有效秩矩阵时。

Method: 提出KRAdapter算法，利用Khatri-Rao乘积生成权重更新，以解决高有效秩矩阵的近似问题。

Result: KRAdapter在1B参数的视觉语言模型和8B参数的大语言模型上表现优于LoRA，尤其在未见常识推理任务中。

Conclusion: KRAdapter保持了LoRA的内存和计算效率，是调整十亿级参数模型的实用且鲁棒的替代方案。

Abstract: Parameter-efficient fine-tuning (PEFT) has become a standard approach for
adapting large pre-trained models. Amongst PEFT methods, low-rank adaptation
(LoRA) has achieved notable success. However, recent studies have highlighted
its limitations compared against full-rank alternatives, particularly when
applied to multimodal and large language models. In this work, we present a
quantitative comparison amongst full-rank and low-rank PEFT methods using a
synthetic matrix approximation benchmark with controlled spectral properties.
Our results confirm that LoRA struggles to approximate matrices with relatively
flat spectrums or high frequency components -- signs of high effective ranks.
To this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the
Khatri-Rao product to produce weight updates, which, by construction, tends to
produce matrix product with a high effective rank. We demonstrate performance
gains with KRAdapter on vision-language models up to 1B parameters and on large
language models up to 8B parameters, particularly on unseen common-sense
reasoning tasks. In addition, KRAdapter maintains the memory and compute
efficiency of LoRA, making it a practical and robust alternative to fine-tune
billion-scale parameter models.

</details>


### [98] [Calibrated Language Models and How to Find Them with Label Smoothing](https://arxiv.org/abs/2508.00264)
*Jerry Huang,Peng Lu,Qiuhao Zeng*

Main category: cs.LG

TL;DR: 研究探讨了指令微调对大语言模型（LLMs）置信度校准的影响，发现校准退化现象，并提出标签平滑作为解决方案，同时解决了内存占用问题。


<details>
  <summary>Details</summary>
Motivation: 尽管指令微调提升了LLMs的交互能力，但其对置信度校准的影响尚未充分研究，需要探索解决方案。

Method: 分析了多种开源LLMs的校准退化现象，提出并验证标签平滑的有效性，针对大词汇量LLMs设计了定制化内核以减少内存占用。

Result: 标签平滑能有效维持校准，但在大词汇量LLMs中效果受限；定制化内核显著降低了内存消耗。

Conclusion: 标签平滑是解决指令微调后校准退化的实用方法，但需针对大词汇量模型进一步优化；定制化内核为高效计算提供了可行方案。

Abstract: Recent advances in natural language processing (NLP) have opened up greater
opportunities to enable fine-tuned large language models (LLMs) to behave as
more powerful interactive agents through improved instruction-following
ability. However, understanding how this impacts confidence calibration for
reliable model output has not been researched in full. In this work, we examine
various open-sourced LLMs, identifying significant calibration degradation
after instruction tuning in each. Seeking a practical solution, we look towards
label smoothing, which has been shown as an effective method to regularize for
overconfident predictions but has yet to be widely adopted in the supervised
fine-tuning (SFT) of LLMs. We first provide insight as to why label smoothing
is sufficient to maintain calibration throughout the SFT process. However,
settings remain where the effectiveness of smoothing is severely diminished, in
particular the case of large vocabulary LLMs (LV-LLMs). We posit the cause to
stem from the ability to become over-confident, which has a direct relationship
with the hidden size and vocabulary size, and justify this theoretically and
experimentally. Finally, we address an outstanding issue regarding the memory
footprint of the cross-entropy loss computation in the label smoothed loss
setting, designing a customized kernel to dramatically reduce memory
consumption without sacrificing speed or performance in comparison to existing
solutions for non-smoothed losses.

</details>


### [99] [Learning to Optimize Feedback for One Million Students: Insights from Multi-Armed and Contextual Bandits in Large-Scale Online Tutoring](https://arxiv.org/abs/2508.00270)
*Robin Schmucker,Nimish Pachapurkar,Shanmuga Bala,Miral Shah,Tom Mitchell*

Main category: cs.LG

TL;DR: 论文介绍了一个在线辅导系统，通过多臂老虎机框架和离线策略评估优化学生反馈，提升学习效果。


<details>
  <summary>Details</summary>
Motivation: 目标是优化学生答题后的反馈，以提高学习效果，通过数据驱动的方法为不同问题选择合适的辅助策略。

Method: 使用多臂老虎机（MAB）框架和离线策略评估，分析43,000个辅助动作，并设计算法为每个问题选择最佳策略。

Result: 在166,000次练习会话中验证，MAB策略显著提升学生表现，但上下文老虎机（CB）策略的个性化效果有限。

Conclusion: 数据驱动的反馈优化系统已大规模部署，未来可进一步探索个性化策略的潜力。

Abstract: We present an online tutoring system that learns to provide effective
feedback to students after they answer questions incorrectly. Using data from
one million students, the system learns which assistance action (e.g., one of
multiple hints) to provide for each question to optimize student learning.
Employing the multi-armed bandit (MAB) framework and offline policy evaluation,
we assess 43,000 assistance actions, and identify trade-offs between assistance
policies optimized for different student outcomes (e.g., response correctness,
session completion). We design an algorithm that for each question decides on a
suitable policy training objective to enhance students' immediate second
attempt success and overall practice session performance. We evaluate the
resulting MAB policies in 166,000 practice sessions, verifying significant
improvements in student outcomes. While MAB policies optimize feedback for the
overall student population, we further investigate whether contextual bandit
(CB) policies can enhance outcomes by personalizing feedback based on
individual student features (e.g., ability estimates, response times). Using
causal inference, we examine (i) how effects of assistance actions vary across
students and (ii) whether CB policies, which leverage such effect
heterogeneity, outperform MAB policies. While our analysis reveals that some
actions for some questions exhibit effect heterogeneity, effect sizes may often
be too small for CB policies to provide significant improvements beyond what
well-optimized MAB policies that deliver the same action to all students
already achieve. We discuss insights gained from deploying data-driven systems
at scale and implications for future refinements. Today, the teaching policies
optimized by our system support thousands of students daily.

</details>


### [100] [Toward using explainable data-driven surrogate models for treating performance-based seismic design as an inverse engineering problem](https://arxiv.org/abs/2508.00286)
*Mohsen Zaker Esteghamati*

Main category: cs.LG

TL;DR: 本文提出了一种基于可解释机器学习模型的性能驱动抗震设计方法，通过逆向工程直接映射设计变量与性能指标，优化计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统性能驱动抗震设计存在计算效率低的问题，本研究旨在通过机器学习模型直接映射设计变量与性能目标，提升效率。

Method: 采用可解释机器学习模型映射设计变量与性能指标，并将其作为评估函数集成到遗传优化算法中，解决逆向工程问题。

Result: 在洛杉矶和查尔斯顿的钢结构和混凝土框架中应用该方法，结果显示代理模型精度高（R2>90%），优化算法能识别符合工程原理的最优构件属性。

Conclusion: 该方法通过机器学习与优化算法结合，高效实现了性能驱动的抗震设计，适用于多种建筑类型和地震环境。

Abstract: This study presents a methodology to treat performance-based seismic design
as an inverse engineering problem, where design parameters are directly derived
to achieve specific performance objectives. By implementing explainable machine
learning models, this methodology directly maps design variables and
performance metrics, tackling computational inefficiencies of performance-based
design. The resultant machine learning model is integrated as an evaluation
function into a genetic optimization algorithm to solve the inverse problem.
The developed methodology is then applied to two different inventories of steel
and concrete moment frames in Los Angeles and Charleston to obtain sectional
properties of frame members that minimize expected annualized seismic loss in
terms of repair costs. The results show high accuracy of the surrogate models
(e.g., R2> 90%) across a diverse set of building types, geometries, seismic
design, and site hazard, where the optimization algorithm could identify the
optimum values of members' properties for a fixed set of geometric variables,
consistent with engineering principles.

</details>


### [101] [Invariant Graph Transformer for Out-of-Distribution Generalization](https://arxiv.org/abs/2508.00304)
*Tianyin Liao,Ziwei Zhang,Yufei Sun,Chunyu Hu,Jianxin Li*

Main category: cs.LG

TL;DR: GOODFormer是一种基于图不变学习的Transformer模型，旨在解决图数据分布偏移下的泛化问题，通过分离不变与可变子图、动态编码和不变学习模块实现。


<details>
  <summary>Details</summary>
Motivation: 现有图Transformer在分布偏移下泛化能力不足，图不变学习可能提供解决方案，但如何设计相关机制仍具挑战性。

Method: 提出GOODFormer，包含熵引导不变子图解耦器、动态子图编码器和不变学习模块，联合优化以捕获不变关系。

Result: 在基准数据集上表现优于现有方法，证明了其在分布偏移下的优越性。

Conclusion: GOODFormer通过不变学习有效提升图Transformer的泛化能力，为分布偏移问题提供了新思路。

Abstract: Graph Transformers (GTs) have demonstrated great effectiveness across various
graph analytical tasks. However, the existing GTs focus on training and testing
graph data originated from the same distribution, but fail to generalize under
distribution shifts. Graph invariant learning, aiming to capture generalizable
graph structural patterns with labels under distribution shifts, is potentially
a promising solution, but how to design attention mechanisms and positional and
structural encodings (PSEs) based on graph invariant learning principles
remains challenging. To solve these challenges, we introduce Graph
Out-Of-Distribution generalized Transformer (GOODFormer), aiming to learn
generalized graph representations by capturing invariant relationships between
predictive graph structures and labels through jointly optimizing three
modules. Specifically, we first develop a GT-based entropy-guided invariant
subgraph disentangler to separate invariant and variant subgraphs while
preserving the sharpness of the attention function. Next, we design an evolving
subgraph positional and structural encoder to effectively and efficiently
capture the encoding information of dynamically changing subgraphs during
training. Finally, we propose an invariant learning module utilizing subgraph
node representations and encodings to derive generalizable graph
representations that can to unseen graphs. We also provide theoretical
justifications for our method. Extensive experiments on benchmark datasets
demonstrate the superiority of our method over state-of-the-art baselines under
distribution shifts.

</details>


### [102] [PnP-DA: Towards Principled Plug-and-Play Integration of Variational Data Assimilation and Generative Models](https://arxiv.org/abs/2508.00325)
*Yongquan Qu,Matthieu Blanke,Sara Shamekh,Pierre Gentine*

Main category: cs.LG

TL;DR: PnP-DA是一种新的数据同化方法，通过结合轻量级梯度更新和预训练生成先验，减少地球系统建模中的误差积累。


<details>
  <summary>Details</summary>
Motivation: 地球系统建模中的误差积累和传统变分方法对高斯误差统计的假设限制了预测准确性。

Method: PnP-DA交替使用梯度分析更新和预训练生成先验，避免复杂神经网络的反向传播。

Result: 在标准混沌测试中，PnP-DA显著降低了预测误差，优于传统变分方法。

Conclusion: PnP-DA通过放松统计假设和利用历史数据，提供了一种高效的数据同化解决方案。

Abstract: Earth system modeling presents a fundamental challenge in scientific
computing: capturing complex, multiscale nonlinear dynamics in computationally
efficient models while minimizing forecast errors caused by necessary
simplifications. Even the most powerful AI- or physics-based forecast system
suffer from gradual error accumulation. Data assimilation (DA) aims to mitigate
these errors by optimally blending (noisy) observations with prior model
forecasts, but conventional variational methods often assume Gaussian error
statistics that fail to capture the true, non-Gaussian behavior of chaotic
dynamical systems. We propose PnP-DA, a Plug-and-Play algorithm that alternates
(1) a lightweight, gradient-based analysis update (using a Mahalanobis-distance
misfit on new observations) with (2) a single forward pass through a pretrained
generative prior conditioned on the background forecast via a conditional
Wasserstein coupling. This strategy relaxes restrictive statistical assumptions
and leverages rich historical data without requiring an explicit regularization
functional, and it also avoids the need to backpropagate gradients through the
complex neural network that encodes the prior during assimilation cycles.
Experiments on standard chaotic testbeds demonstrate that this strategy
consistently reduces forecast errors across a range of observation sparsities
and noise levels, outperforming classical variational methods.

</details>


### [103] [Embryology of a Language Model](https://arxiv.org/abs/2508.00331)
*George Wang,Garrett Baker,Andrew Gordon,Daniel Murfet*

Main category: cs.LG

TL;DR: 论文提出了一种基于UMAP和敏感性分析的胚胎学方法，用于可视化语言模型在训练过程中的结构发展，揭示了新的网络机制。


<details>
  <summary>Details</summary>
Motivation: 理解语言模型内部计算结构的形成是深度学习科学的核心问题，而敏感性分析作为一种工具尚未充分发挥其潜力。

Method: 应用UMAP对敏感性矩阵进行可视化，展示模型在训练过程中的结构发展。

Result: 可视化结果揭示了明确的“身体计划”，包括已知特征（如感应电路）和新发现的结构（如用于计数空格标记的“间距鳍”）。

Conclusion: 敏感性分析不仅能验证模型，还能揭示新机制，为研究复杂神经网络的发育原理提供了强大的工具。

Abstract: Understanding how language models develop their internal computational
structure is a central problem in the science of deep learning. While
susceptibilities, drawn from statistical physics, offer a promising analytical
tool, their full potential for visualizing network organization remains
untapped. In this work, we introduce an embryological approach, applying UMAP
to the susceptibility matrix to visualize the model's structural development
over training. Our visualizations reveal the emergence of a clear ``body
plan,'' charting the formation of known features like the induction circuit and
discovering previously unknown structures, such as a ``spacing fin'' dedicated
to counting space tokens. This work demonstrates that susceptibility analysis
can move beyond validation to uncover novel mechanisms, providing a powerful,
holistic lens for studying the developmental principles of complex neural
networks.

</details>


### [104] [BOOD: Boundary-based Out-Of-Distribution Data Generation](https://arxiv.org/abs/2508.00350)
*Qilin Liao,Shuo Yang,Bo Zhao,Ping Luo,Hengshuang Zhao*

Main category: cs.LG

TL;DR: BOOD框架利用扩散模型生成高质量的OOD特征和图像，显著提升OOD检测性能。


<details>
  <summary>Details</summary>
Motivation: 提取ID边界外的有效特征以增强OOD检测性能。

Method: 学习文本条件潜在特征空间，扰动ID边界特征生成OOD特征，并用扩散模型解码为图像。

Result: 在CIFAR-100上，FPR95降低29.64%，AUROC提升7.27%。

Conclusion: BOOD提供了一种高效的OOD特征生成方法，显著优于现有技术。

Abstract: Harnessing the power of diffusion models to synthesize auxiliary training
data based on latent space features has proven effective in enhancing
out-of-distribution (OOD) detection performance. However, extracting effective
features outside the in-distribution (ID) boundary in latent space remains
challenging due to the difficulty of identifying decision boundaries between
classes. This paper proposes a novel framework called Boundary-based
Out-Of-Distribution data generation (BOOD), which synthesizes high-quality OOD
features and generates human-compatible outlier images using diffusion models.
BOOD first learns a text-conditioned latent feature space from the ID dataset,
selects ID features closest to the decision boundary, and perturbs them to
cross the decision boundary to form OOD features. These synthetic OOD features
are then decoded into images in pixel space by a diffusion model. Compared to
previous works, BOOD provides a more training efficient strategy for
synthesizing informative OOD features, facilitating clearer distinctions
between ID and OOD data. Extensive experimental results on common benchmarks
demonstrate that BOOD surpasses the state-of-the-art method significantly,
achieving a 29.64% decrease in average FPR95 (40.31% vs. 10.67%) and a 7.27%
improvement in average AUROC (90.15% vs. 97.42%) on the CIFAR-100 dataset.

</details>


### [105] [Sheaf Graph Neural Networks via PAC-Bayes Spectral Optimization](https://arxiv.org/abs/2508.00357)
*Yoonhyuk Choi,Jiho Choi,Chong-Kwon Kim*

Main category: cs.LG

TL;DR: SGPC提出了一种结合蜂窝层束消息传递的新架构，通过优化传输提升、方差减少扩散和PAC-Bayes谱正则化，解决了GNN中的过平滑问题，并在异质图分类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: GNN在异质图上的过平滑问题导致节点特征崩溃，现有层束神经网络方法依赖静态或高参数化结构，缺乏泛化性和稳定性保证。

Method: SGPC结合蜂窝层束消息传递、优化传输提升、方差减少扩散和PAC-Bayes谱正则化，通过端到端训练实现线性复杂度。

Result: 在九个同质和异质基准测试中，SGPC优于现有谱和层束GNN方法，并提供未见节点的置信区间。

Conclusion: SGPC通过理论性能界限和实验验证，为GNN在异质图上的过平滑问题提供了高效且稳定的解决方案。

Abstract: Over-smoothing in Graph Neural Networks (GNNs) causes collapse in distinct
node features, particularly on heterophilic graphs where adjacent nodes often
have dissimilar labels. Although sheaf neural networks partially mitigate this
problem, they typically rely on static or heavily parameterized sheaf
structures that hinder generalization and scalability. Existing sheaf-based
models either predefine restriction maps or introduce excessive complexity, yet
fail to provide rigorous stability guarantees. In this paper, we introduce a
novel scheme called SGPC (Sheaf GNNs with PAC-Bayes Calibration), a unified
architecture that combines cellular-sheaf message passing with several
mechanisms, including optimal transport-based lifting, variance-reduced
diffusion, and PAC-Bayes spectral regularization for robust semi-supervised
node classification. We establish performance bounds theoretically and
demonstrate that the resulting bound-aware objective can be achieved via
end-to-end training in linear computational complexity. Experiments on nine
homophilic and heterophilic benchmarks show that SGPC outperforms
state-of-the-art spectral and sheaf-based GNNs while providing certified
confidence intervals on unseen nodes.

</details>


### [106] [OID-PPO: Optimal Interior Design using Proximal Policy Optimization by Transforming Design Guidelines into Reward Functions](https://arxiv.org/abs/2508.00364)
*Chanyoung Yoon,Sangbong Yoo,Soobin Yim,Chansoo Kim,Yun Jang*

Main category: cs.LG

TL;DR: OID-PPO是一种基于强化学习的室内设计优化框架，通过结合专家定义的功能和视觉准则，显著提升了布局质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 住宅室内设计对居住者满意度影响重大，但现有方法因计算成本高、数据稀缺或设计原则不足而受限。

Method: 提出OID-PPO框架，利用近端策略优化（PPO）和连续家具放置策略，整合专家准则到奖励函数中。

Result: 实验表明，OID-PPO在布局质量和计算效率上优于现有方法，并通过消融研究验证了设计准则的贡献。

Conclusion: OID-PPO为室内设计提供了一种高效且灵活的解决方案，同时强调了设计准则的重要性。

Abstract: Designing residential interiors strongly impacts occupant satisfaction but
remains challenging due to unstructured spatial layouts, high computational
demands, and reliance on expert knowledge. Existing methods based on
optimization or deep learning are either computationally expensive or
constrained by data scarcity. Reinforcement learning (RL) approaches often
limit furniture placement to discrete positions and fail to incorporate design
principles adequately. We propose OID-PPO, a novel RL framework for Optimal
Interior Design using Proximal Policy Optimization, which integrates
expert-defined functional and visual guidelines into a structured reward
function. OID-PPO utilizes a diagonal Gaussian policy for continuous and
flexible furniture placement, effectively exploring latent environmental
dynamics under partial observability. Experiments conducted across diverse room
shapes and furniture configurations demonstrate that OID-PPO significantly
outperforms state-of-the-art methods in terms of layout quality and
computational efficiency. Ablation studies further demonstrate the impact of
structured guideline integration and reveal the distinct contributions of
individual design constraints.

</details>


### [107] [Dual Adaptivity: Universal Algorithms for Minimizing the Adaptive Regret of Convex Functions](https://arxiv.org/abs/2508.00392)
*Lijun Zhang,Wenhao Yang,Guanghui Wang,Wei Jiang,Zhi-Hua Zhou*

Main category: cs.LG

TL;DR: 本文提出了一种具有双重适应性的通用算法框架，用于在线学习中最小化自适应遗憾，适用于多种凸函数类型和动态环境。


<details>
  <summary>Details</summary>
Motivation: 现有算法缺乏通用性，只能处理单一类型凸函数且需要先验参数知识，限制了实际应用。

Method: 提出基于元专家框架的双重自适应算法，动态创建多个专家并通过元算法聚合，结合睡眠专家技术捕捉环境变化。

Result: 理论分析表明，算法能同时最小化多种凸函数的自适应遗憾，并允许函数类型在轮次间切换。

Conclusion: 该框架可扩展到在线复合优化，为复合函数的自适应遗憾最小化提供通用解决方案。

Abstract: To deal with changing environments, a new performance measure -- adaptive
regret, defined as the maximum static regret over any interval, was proposed in
online learning. Under the setting of online convex optimization, several
algorithms have been successfully developed to minimize the adaptive regret.
However, existing algorithms lack universality in the sense that they can only
handle one type of convex functions and need apriori knowledge of parameters,
which hinders their application in real-world scenarios. To address this
limitation, this paper investigates universal algorithms with dual adaptivity,
which automatically adapt to the property of functions (convex, exponentially
concave, or strongly convex), as well as the nature of environments (stationary
or changing). Specifically, we propose a meta-expert framework for dual
adaptive algorithms, where multiple experts are created dynamically and
aggregated by a meta-algorithm. The meta-algorithm is required to yield a
second-order bound, which can accommodate unknown function types. We further
incorporate the technique of sleeping experts to capture the changing
environments. For the construction of experts, we introduce two strategies
(increasing the number of experts or enhancing the capabilities of experts) to
achieve universality. Theoretical analysis shows that our algorithms are able
to minimize the adaptive regret for multiple types of convex functions
simultaneously, and also allow the type of functions to switch between rounds.
Moreover, we extend our meta-expert framework to online composite optimization,
and develop a universal algorithm for minimizing the adaptive regret of
composite functions.

</details>


### [108] [ExeKGLib: A Platform for Machine Learning Analytics based on Knowledge Graphs](https://arxiv.org/abs/2508.00394)
*Antonis Klironomos,Baifan Zhou,Zhipeng Tan,Zhuoxun Zheng,Mohamed H. Gad-Elrab,Heiko Paulheim,Evgeny Kharlamov*

Main category: cs.LG

TL;DR: ExeKGLib是一个Python库，带有图形界面，帮助非机器学习专家构建ML管道，利用知识图谱简化ML知识。


<details>
  <summary>Details</summary>
Motivation: 解决领域专家缺乏ML专业知识但需要ML分析工具的问题。

Method: 通过知识图谱编码ML知识，提供图形界面简化管道构建。

Result: ExeKGLib提高了ML管道的透明度和可重用性，并确保其可执行性。

Conclusion: ExeKGLib为非ML专家提供了实用的ML工具，展示了其可用性和实用性。

Abstract: Nowadays machine learning (ML) practitioners have access to numerous ML
libraries available online. Such libraries can be used to create ML pipelines
that consist of a series of steps where each step may invoke up to several ML
libraries that are used for various data-driven analytical tasks. Development
of high-quality ML pipelines is non-trivial; it requires training, ML
expertise, and careful development of each step. At the same time, domain
experts in science and engineering may not possess such ML expertise and
training while they are in pressing need of ML-based analytics. In this paper,
we present our ExeKGLib, a Python library enhanced with a graphical interface
layer that allows users with minimal ML knowledge to build ML pipelines. This
is achieved by relying on knowledge graphs that encode ML knowledge in simple
terms accessible to non-ML experts. ExeKGLib also allows improving the
transparency and reusability of the built ML workflows and ensures that they
are executable. We show the usability and usefulness of ExeKGLib by presenting
real use cases.

</details>


### [109] [Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement](https://arxiv.org/abs/2508.00410)
*Zizhuo Zhang,Jianing Zhu,Xinmu Ge,Zihua Zhao,Zhanke Zhou,Xuan Li,Xiao Feng,Jiangchao Yao,Bo Han*

Main category: cs.LG

TL;DR: Co-Reward是一种基于对比一致性的自监督强化学习框架，通过类比问题的语义一致性构建奖励，显著提升了语言模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有自奖励方法在复杂任务中因依赖人工标注而难以扩展的问题，并避免模型崩溃。

Method: 通过构造语义相似的问题对，利用投票生成代理标签，并通过交叉引用构建奖励以增强推理一致性。

Result: 在多个推理基准测试中表现优于其他自奖励方法，甚至超越人工标注奖励，最高提升6.8%。

Conclusion: Co-Reward通过自监督奖励机制有效提升了语言模型的推理能力，且无需依赖人工标注。

Abstract: Although reinforcement learning with verifiable rewards (RLVR) shows promise
in improving the reasoning ability of large language models (LLMs), the scaling
up dilemma remains due to the reliance on human annotated labels especially for
complex tasks. Recent alternatives that explore various self-reward signals
exhibit the eliciting potential of LLM reasoning, but suffer from the
non-negligible collapse issue. Inspired by the success of self-supervised
learning, we propose \textit{Co-Reward}, a novel RL framework that leverages
contrastive agreement across semantically analogical questions as a reward
basis. Specifically, we construct a similar question for each training sample
(without labels) and synthesize their individual surrogate labels through a
simple rollout voting, and then the reward is constructed by cross-referring
the labels of each question pair to enforce the internal reasoning consistency
across analogical inputs. Intuitively, such a self-supervised reward-shaping
mechanism increases the difficulty of learning collapse into a trivial
solution, and promotes stable reasoning elicitation and improvement through
expanding the input sample variants. Empirically, Co-Reward achieves superior
performance compared to other self-reward baselines on multiple reasoning
benchmarks and LLM series, and reaches or even surpasses ground-truth (GT)
labeled reward, with improvements of up to $+6.8\%$ on MATH500 over GT reward
on Llama-3.2-3B-Instruct. Our code is publicly available at
https://github.com/tmlr-group/Co-Reward.

</details>


### [110] [Transforming Credit Risk Analysis: A Time-Series-Driven ResE-BiLSTM Framework for Post-Loan Default Detection](https://arxiv.org/abs/2508.00415)
*Yue Yang,Yuxiang Lin,Ying Zhang,Zihan Su,Chang Chuan Goh,Tangtangfang Fang,Anthony Graham Bellotti,Boon Giin Lee*

Main category: cs.LG

TL;DR: 本文提出了一种名为ResE-BiLSTM的模型，用于预测贷款违约，通过滑动窗口技术和多种基线模型对比，证明了其优越性能。


<details>
  <summary>Details</summary>
Motivation: 贷款违约预测对信用风险管理至关重要，现有方法需要进一步提升性能。

Method: 采用ResE-BiLSTM模型和滑动窗口技术，与LSTM、BiLSTM、GRU、CNN和RNN等基线模型对比，并通过消融实验和SHAP分析评估模型。

Result: 实验结果显示ResE-BiLSTM在多个指标上优于基线模型，具有实际应用价值。

Conclusion: ResE-BiLSTM在贷款违约预测中表现出色，适用于实际场景。

Abstract: Prediction of post-loan default is an important task in credit risk
management, and can be addressed by detection of financial anomalies using
machine learning. This study introduces a ResE-BiLSTM model, using a sliding
window technique, and is evaluated on 44 independent cohorts from the extensive
Freddie Mac US mortgage dataset, to improve prediction performance. The
ResE-BiLSTM is compared with five baseline models: Long Short-Term Memory
(LSTM), BiLSTM, Gated Recurrent Units (GRU), Convolutional Neural Networks
(CNN), and Recurrent Neural Networks (RNN), across multiple metrics, including
Accuracy, Precision, Recall, F1, and AUC. An ablation study was conducted to
evaluate the contribution of individual components in the ResE-BiLSTM
architecture. Additionally, SHAP analysis was employed to interpret the
underlying features the model relied upon for its predictions. Experimental
results demonstrate that ResE-BiLSTM achieves superior predictive performance
compared to baseline models, underscoring its practical value and applicability
in real-world scenarios.

</details>


### [111] [A Conditional GAN for Tabular Data Generation with Probabilistic Sampling of Latent Subspaces](https://arxiv.org/abs/2508.00472)
*Leonidas Akritidis,Panayiotis Bozanis*

Main category: cs.LG

TL;DR: 论文提出ctdGAN，一种用于缓解表格数据类别不平衡的条件GAN，通过空间分区和概率采样策略生成高质量样本。


<details>
  <summary>Details</summary>
Motivation: 表格数据中的类别不平衡问题严重影响机器学习性能，现有GAN方法未考虑输入样本的向量子空间且类别标签处理不足。

Method: ctdGAN通过空间分区分配聚类标签，结合概率采样策略和新损失函数生成样本，并引入聚类缩放技术。

Result: 在14个不平衡数据集上的实验表明，ctdGAN能生成高保真样本并显著提升分类准确率。

Conclusion: ctdGAN通过改进样本生成策略和损失函数，有效解决了表格数据类别不平衡问题。

Abstract: The tabular form constitutes the standard way of representing data in
relational database systems and spreadsheets. But, similarly to other forms,
tabular data suffers from class imbalance, a problem that causes serious
performance degradation in a wide variety of machine learning tasks. One of the
most effective solutions dictates the usage of Generative Adversarial Networks
(GANs) in order to synthesize artificial data instances for the
under-represented classes. Despite their good performance, none of the proposed
GAN models takes into account the vector subspaces of the input samples in the
real data space, leading to data generation in arbitrary locations. Moreover,
the class labels are treated in the same manner as the other categorical
variables during training, so conditional sampling by class is rendered less
effective. To overcome these problems, this study presents ctdGAN, a
conditional GAN for alleviating class imbalance in tabular datasets. Initially,
ctdGAN executes a space partitioning step to assign cluster labels to the input
samples. Subsequently, it utilizes these labels to synthesize samples via a
novel probabilistic sampling strategy and a new loss function that penalizes
both cluster and class mis-predictions. In this way, ctdGAN is trained to
generate samples in subspaces that resemble those of the original data
distribution. We also introduce several other improvements, including a simple,
yet effective cluster-wise scaling technique that captures multiple feature
modes without affecting data dimensionality. The exhaustive evaluation of
ctdGAN with 14 imbalanced datasets demonstrated its superiority in generating
high fidelity samples and improving classification accuracy.

</details>


### [112] [Court of LLMs: Evidence-Augmented Generation via Multi-LLM Collaboration for Text-Attributed Graph Anomaly Detection](https://arxiv.org/abs/2508.00507)
*Yiming Xu,Jiarun Chen,Zhen Peng,Zihan Chen,Qika Lin,Lan Ma,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 论文提出了一种结合大型语言模型（LLMs）和图神经网络（GNNs）的新框架CoLL，用于文本属性图（TAGs）中的异常检测，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有图异常检测方法忽视了文本模态的互补价值，且文本特征编码方式浅层，可能遗漏异常相关的语义信息。

Method: CoLL框架通过多LLM协作增强证据生成，并结合GNN的门控机制自适应融合文本特征与拓扑信息。

Result: 实验表明CoLL平均提升了13.37%的AP值。

Conclusion: 该研究为LLMs在图异常检测中的应用开辟了新途径。

Abstract: The natural combination of intricate topological structures and rich textual
information in text-attributed graphs (TAGs) opens up a novel perspective for
graph anomaly detection (GAD). However, existing GAD methods primarily focus on
designing complex optimization objectives within the graph domain, overlooking
the complementary value of the textual modality, whose features are often
encoded by shallow embedding techniques, such as bag-of-words or skip-gram, so
that semantic context related to anomalies may be missed. To unleash the
enormous potential of textual modality, large language models (LLMs) have
emerged as promising alternatives due to their strong semantic understanding
and reasoning capabilities. Nevertheless, their application to TAG anomaly
detection remains nascent, and they struggle to encode high-order structural
information inherent in graphs due to input length constraints. For
high-quality anomaly detection in TAGs, we propose CoLL, a novel framework that
combines LLMs and graph neural networks (GNNs) to leverage their complementary
strengths. CoLL employs multi-LLM collaboration for evidence-augmented
generation to capture anomaly-relevant contexts while delivering human-readable
rationales for detected anomalies. Moreover, CoLL integrates a GNN equipped
with a gating mechanism to adaptively fuse textual features with evidence while
preserving high-order topological information. Extensive experiments
demonstrate the superiority of CoLL, achieving an average improvement of 13.37%
in AP. This study opens a new avenue for incorporating LLMs in advancing GAD.

</details>


### [113] [Text-Attributed Graph Anomaly Detection via Multi-Scale Cross- and Uni-Modal Contrastive Learning](https://arxiv.org/abs/2508.00513)
*Yiming Xu,Xu Hua,Zhen Peng,Bin Shi,Jiarun Chen,Xingbo Fu,Song Wang,Bo Dong*

Main category: cs.LG

TL;DR: 论文提出了一种名为CMUCL的端到端文本属性图异常检测方法，通过跨模态和多尺度一致性联合训练文本和图编码器，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法将文本编码与异常检测目标分离，导致提取的文本特征可能不聚焦于异常检测相关信息，限制了检测能力。

Method: 提出CMUCL方法，同时建模文本和图结构数据，利用跨模态和单模态多尺度一致性联合训练编码器，并通过不一致性挖掘设计异常评分器。

Result: CMUCL在文本属性图异常检测中表现优异，平均准确率（AP）比次优方法提升了11.13%。

Conclusion: CMUCL为文本属性图异常检测提供了高效解决方案，并发布了8个基准数据集以推动未来研究。

Abstract: The widespread application of graph data in various high-risk scenarios has
increased attention to graph anomaly detection (GAD). Faced with real-world
graphs that often carry node descriptions in the form of raw text sequences,
termed text-attributed graphs (TAGs), existing graph anomaly detection
pipelines typically involve shallow embedding techniques to encode such textual
information into features, and then rely on complex self-supervised tasks
within the graph domain to detect anomalies. However, this text encoding
process is separated from the anomaly detection training objective in the graph
domain, making it difficult to ensure that the extracted textual features focus
on GAD-relevant information, seriously constraining the detection capability.
How to seamlessly integrate raw text and graph topology to unleash the vast
potential of cross-modal data in TAGs for anomaly detection poses a challenging
issue. This paper presents a novel end-to-end paradigm for text-attributed
graph anomaly detection, named CMUCL. We simultaneously model data from both
text and graph structures, and jointly train text and graph encoders by
leveraging cross-modal and uni-modal multi-scale consistency to uncover
potential anomaly-related information. Accordingly, we design an anomaly score
estimator based on inconsistency mining to derive node-specific anomaly scores.
Considering the lack of benchmark datasets tailored for anomaly detection on
TAGs, we release 8 datasets to facilitate future research. Extensive
evaluations show that CMUCL significantly advances in text-attributed graph
anomaly detection, delivering an 11.13% increase in average accuracy (AP) over
the suboptimal.

</details>


### [114] [Online Nonsubmodular Optimization with Delayed Feedback in the Bandit Setting](https://arxiv.org/abs/2508.00523)
*Sifan Yang,Yuanyu Wan,Lijun Zhang*

Main category: cs.LG

TL;DR: 论文研究了在线非子模优化问题，针对延迟反馈和强盗设置提出了两种算法，分别改进了现有遗憾界。


<details>
  <summary>Details</summary>
Motivation: 现有方法对延迟敏感且未解耦延迟与强盗反馈的影响，因此需要更优的算法。

Method: 提出了DBGD-NF算法和基于块更新的扩展算法，分别优化了平均延迟和延迟与反馈的耦合问题。

Result: DBGD-NF的遗憾界为O(n¯d^{1/3}T^{2/3})，扩展算法为O(n(T^{2/3} + √(dT))，优于现有方法。

Conclusion: 新算法在理论和实验中均表现出优越性，特别是在延迟较小的情况下。

Abstract: We investigate the online nonsubmodular optimization with delayed feedback in
the bandit setting, where the loss function is $\alpha$-weakly DR-submodular
and $\beta$-weakly DR-supermodular. Previous work has established an
$(\alpha,\beta)$-regret bound of $\mathcal{O}(nd^{1/3}T^{2/3})$, where $n$ is
the dimensionality and $d$ is the maximum delay. However, its regret bound
relies on the maximum delay and is thus sensitive to irregular delays.
Additionally, it couples the effects of delays and bandit feedback as its bound
is the product of the delay term and the $\mathcal{O}(nT^{2/3})$ regret bound
in the bandit setting without delayed feedback. In this paper, we develop two
algorithms to address these limitations, respectively. Firstly, we propose a
novel method, namely DBGD-NF, which employs the one-point gradient estimator
and utilizes all the available estimated gradients in each round to update the
decision. It achieves a better $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ regret
bound, which is relevant to the average delay $\bar{d} =
\frac{1}{T}\sum_{t=1}^T d_t\leq d$. Secondly, we extend DBGD-NF by employing a
blocking update mechanism to decouple the joint effect of the delays and bandit
feedback, which enjoys an $\mathcal{O}(n(T^{2/3} + \sqrt{dT}))$ regret bound.
When $d = \mathcal{O}(T^{1/3})$, our regret bound matches the
$\mathcal{O}(nT^{2/3})$ bound in the bandit setting without delayed feedback.
Compared to our first $\mathcal{O}(n\bar{d}^{1/3}T^{2/3})$ bound, it is more
advantageous when the maximum delay $d = o(\bar{d}^{2/3}T^{1/3})$. Finally, we
conduct experiments on structured sparse learning to demonstrate the
superiority of our methods.

</details>


### [115] [Phase-Locked SNR Band Selection for Weak Mineral Signal Detection in Hyperspectral Imagery](https://arxiv.org/abs/2508.00539)
*Judy X Yang*

Main category: cs.LG

TL;DR: 提出了一种两阶段框架，用于增强Cuprite矿区的矿物检测，通过信噪比筛选和光谱平滑优化数据，再结合聚类和分解方法提高检测精度。


<details>
  <summary>Details</summary>
Motivation: 解决高光谱成像中弱矿物信号被噪声和冗余波段掩盖的问题。

Method: 第一阶段通过信噪比筛选和Savitzky-Golay滤波优化数据；第二阶段使用KMeans聚类和NNLS分解进行矿物检测。

Result: 实验表明该框架提高了分解精度，增强了弱矿物区域的检测能力。

Conclusion: 两阶段策略为地质高光谱应用提供了一种实用且可重复的解决方案。

Abstract: Hyperspectral imaging offers detailed spectral information for mineral
mapping; however, weak mineral signatures are often masked by noisy and
redundant bands, limiting detection performance. To address this, we propose a
two-stage integrated framework for enhanced mineral detection in the Cuprite
mining district. In the first stage, we compute the signal-to-noise ratio (SNR)
for each spectral band and apply a phase-locked thresholding technique to
discard low-SNR bands, effectively removing redundancy and suppressing
background noise. Savitzky-Golay filtering is then employed for spectral
smoothing, serving a dual role first to stabilize trends during band selection,
and second to preserve fine-grained spectral features during preprocessing. In
the second stage, the refined HSI data is reintroduced into the model, where
KMeans clustering is used to extract 12 endmember spectra (W1 custom), followed
by non negative least squares (NNLS) for abundance unmixing. The resulting
endmembers are quantitatively compared with laboratory spectra (W1 raw) using
cosine similarity and RMSE metrics. Experimental results confirm that our
proposed pipeline improves unmixing accuracy and enhances the detection of weak
mineral zones. This two-pass strategy demonstrates a practical and reproducible
solution for spectral dimensionality reduction and unmixing in geological HSI
applications.

</details>


### [116] [Foundations of Interpretable Models](https://arxiv.org/abs/2508.00545)
*Pietro Barbiero,Mateo Espinosa Zarlenga,Alberto Termine,Mateja Jamnik,Giuseppe Marra*

Main category: cs.LG

TL;DR: 论文提出了一种新的可解释性定义，解决了现有定义不具操作性的问题，并设计了一个可解释模型的通用蓝图和开源库。


<details>
  <summary>Details</summary>
Motivation: 现有可解释性定义缺乏操作性，无法指导用户设计通用、可靠且鲁棒的可解释模型。

Method: 提出了一种通用且简单的可解释性定义，并基于此设计了一个可解释模型的通用蓝图和开源库。

Result: 新定义具有操作性，揭示了设计可解释模型所需的基础属性、假设、原则、数据结构和架构特征。

Conclusion: 论文为可解释性研究提供了更清晰的方向和工具，推动了可解释AI的发展。

Abstract: We argue that existing definitions of interpretability are not actionable in
that they fail to inform users about general, sound, and robust interpretable
model design. This makes current interpretability research fundamentally
ill-posed. To address this issue, we propose a definition of interpretability
that is general, simple, and subsumes existing informal notions within the
interpretable AI community. We show that our definition is actionable, as it
directly reveals the foundational properties, underlying assumptions,
principles, data structures, and architectural features necessary for designing
interpretable models. Building on this, we propose a general blueprint for
designing interpretable models and introduce the first open-sourced library
with native support for interpretable data structures and processes.

</details>


### [117] [Learning Potential Energy Surfaces of Hydrogen Atom Transfer Reactions in Peptides](https://arxiv.org/abs/2508.00578)
*Marlen Neubert,Patrick Reiser,Frauke Gräter,Pascal Friederich*

Main category: cs.LG

TL;DR: 论文研究了氢原子转移（HAT）反应的机理，通过机器学习势能面（PES）模拟，比较了三种图神经网络架构，发现MACE表现最佳，并展示了其在胶原蛋白模拟中的应用潜力。


<details>
  <summary>Details</summary>
Motivation: HAT反应在生物过程中至关重要，但其机理尚未完全理解。传统模拟方法无法满足量子化学精度需求，机器学习提供了一种替代方案。

Method: 系统生成HAT构型数据集，使用半经验方法和DFT，并比较三种图神经网络架构（SchNet、Allegro、MACE）的性能。

Result: MACE在能量、力和反应势垒预测中表现最优，平均绝对误差为1.13 kcal/mol，适用于大规模胶原蛋白模拟。

Conclusion: 该方法可推广至其他生物分子系统，实现复杂环境中化学反应的量子精确模拟。

Abstract: Hydrogen atom transfer (HAT) reactions are essential in many biological
processes, such as radical migration in damaged proteins, but their mechanistic
pathways remain incompletely understood. Simulating HAT is challenging due to
the need for quantum chemical accuracy at biologically relevant scales; thus,
neither classical force fields nor DFT-based molecular dynamics are applicable.
Machine-learned potentials offer an alternative, able to learn potential energy
surfaces (PESs) with near-quantum accuracy. However, training these models to
generalize across diverse HAT configurations, especially at radical positions
in proteins, requires tailored data generation and careful model selection.
Here, we systematically generate HAT configurations in peptides to build large
datasets using semiempirical methods and DFT. We benchmark three graph neural
network architectures (SchNet, Allegro, and MACE) on their ability to learn HAT
PESs and indirectly predict reaction barriers from energy predictions. MACE
consistently outperforms the others in energy, force, and barrier prediction,
achieving a mean absolute error of 1.13 kcal/mol on out-of-distribution DFT
barrier predictions. This accuracy enables integration of ML potentials into
large-scale collagen simulations to compute reaction rates from predicted
barriers, advancing mechanistic understanding of HAT and radical migration in
peptides. We analyze scaling laws, model transferability, and cost-performance
trade-offs, and outline strategies for improvement by combining ML potentials
with transition state search algorithms and active learning. Our approach is
generalizable to other biomolecular systems, enabling quantum-accurate
simulations of chemical reactivity in complex environments.

</details>


### [118] [The Role of Active Learning in Modern Machine Learning](https://arxiv.org/abs/2508.00586)
*Thorben Werner,Lars Schmidt-Thieme,Vijaya Krishna Yalavarthi*

Main category: cs.LG

TL;DR: 研究发现，主动学习（AL）在低数据场景下效率最低，仅提升1-4%，而数据增强（DA）和半监督学习（SSL）可提升高达60%。但AL与DA和SSL结合后仍能提供额外提升。


<details>
  <summary>Details</summary>
Motivation: 探讨主动学习（AL）在实际应用中较少使用的原因，并研究在低数据场景下不同方法（DA、SSL、AL）的效果。

Method: 比较数据增强（DA）、半监督学习（SSL）和主动学习（AL）在低数据场景下的性能提升效果。

Result: AL单独效果较差（提升1-4%），但结合DA和SSL后仍能提供额外性能提升。

Conclusion: AL应作为DA和SSL后的补充方法，用于进一步提升性能。

Abstract: Even though Active Learning (AL) is widely studied, it is rarely applied in
contexts outside its own scientific literature. We posit that the reason for
this is AL's high computational cost coupled with the comparatively small lifts
it is typically able to generate in scenarios with few labeled points. In this
work we study the impact of different methods to combat this low data scenario,
namely data augmentation (DA), semi-supervised learning (SSL) and AL. We find
that AL is by far the least efficient method of solving the low data problem,
generating a lift of only 1-4\% over random sampling, while DA and SSL methods
can generate up to 60\% lift in combination with random sampling. However, when
AL is combined with strong DA and SSL techniques, it surprisingly is still able
to provide improvements. Based on these results, we frame AL not as a method to
combat missing labels, but as the final building block to squeeze the last bits
of performance out of data after appropriate DA and SSL methods as been
applied.

</details>


### [119] [Similarity-Based Self-Construct Graph Model for Predicting Patient Criticalness Using Graph Neural Networks and EHR Data](https://arxiv.org/abs/2508.00615)
*Mukesh Kumar Sahu,Pinki Roy*

Main category: cs.LG

TL;DR: 提出了一种基于相似性的自构建图模型（SBSCGM）和混合图神经网络（HybridGraphMedGNN），用于预测ICU患者的死亡风险和连续关键性评分，性能优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 传统模型难以利用电子健康记录（EHR）中的关系结构，且孤立处理患者，无法动态捕捉患者间的相似性。

Method: SBSCGM通过混合相似性度量动态构建患者相似图，HybridGraphMedGNN结合GCN、GraphSAGE和GAT层学习患者表示。

Result: 在MIMIC-III数据集上，模型AUC-ROC达0.94，优于基线模型，并提供可解释性。

Conclusion: 该框架为ICU风险预测提供了可扩展且可解释的解决方案，有望支持临床决策。

Abstract: Accurately predicting the criticalness of ICU patients (such as in-ICU
mortality risk) is vital for early intervention in critical care. However,
conventional models often treat each patient in isolation and struggle to
exploit the relational structure in Electronic Health Records (EHR). We propose
a Similarity-Based Self-Construct Graph Model (SBSCGM) that dynamically builds
a patient similarity graph from multi-modal EHR data, and a HybridGraphMedGNN
architecture that operates on this graph to predict patient mortality and a
continuous criticalness score. SBSCGM uses a hybrid similarity measure
(combining feature-based and structural similarities) to connect patients with
analogous clinical profiles in real-time. The HybridGraphMedGNN integrates
Graph Convolutional Network (GCN), GraphSAGE, and Graph Attention Network (GAT)
layers to learn robust patient representations, leveraging both local and
global graph patterns. In experiments on 6,000 ICU stays from the MIMIC-III
dataset, our model achieves state-of-the-art performance (AUC-ROC $0.94$)
outperforming baseline classifiers and single-type GNN models. We also
demonstrate improved precision/recall and show that the attention mechanism
provides interpretable insights into model predictions. Our framework offers a
scalable and interpretable solution for critical care risk prediction, with
potential to support clinicians in real-world ICU deployment.

</details>


### [120] [IAMAP: Unlocking Deep Learning in QGIS for non-coders and limited computing resources](https://arxiv.org/abs/2508.00627)
*Paul Tresson,Pierre Le Coz,Hadrien Tulet,Anthony Malkassian,Maxime Réjou Méchain*

Main category: cs.LG

TL;DR: IAMAP是一个用户友好的QGIS插件，通过自监督学习简化了遥感图像分析，解决了深度学习对大数据、计算资源和编码技能的需求。


<details>
  <summary>Details</summary>
Motivation: 深度学习在遥感领域的应用受限于大数据需求、计算资源和编码技能，IAMAP旨在解决这些问题，使非专家也能使用高质量特征。

Method: IAMAP基于自监督学习策略，提供多种深度学习架构、降维算法、聚类和相似性映射功能，无需GPU或大量数据。

Result: IAMAP成功实现了非专家也能高效利用深度学习特征的目标，推动了计算高效和节能的深度学习方法的普及。

Conclusion: IAMAP通过简化流程和降低门槛，促进了深度学习在遥感领域的广泛应用。

Abstract: Remote sensing has entered a new era with the rapid development of artificial
intelligence approaches. However, the implementation of deep learning has
largely remained restricted to specialists and has been impractical because it
often requires (i) large reference datasets for model training and validation;
(ii) substantial computing resources; and (iii) strong coding skills. Here, we
introduce IAMAP, a user-friendly QGIS plugin that addresses these three
challenges in an easy yet flexible way. IAMAP builds on recent advancements in
self-supervised learning strategies, which now provide robust feature
extractors, often referred to as foundation models. These generalist models can
often be reliably used in few-shot or zero-shot scenarios (i.e., with little to
no fine-tuning). IAMAP's interface allows users to streamline several key steps
in remote sensing image analysis: (i) extracting image features using a wide
range of deep learning architectures; (ii) reducing dimensionality with
built-in algorithms; (iii) performing clustering on features or their reduced
representations; (iv) generating feature similarity maps; and (v) calibrating
and validating supervised machine learning models for prediction. By enabling
non-AI specialists to leverage the high-quality features provided by recent
deep learning approaches without requiring GPU capacity or extensive reference
datasets, IAMAP contributes to the democratization of computationally efficient
and energy-conscious deep learning methods.

</details>


### [121] [Separated-Variable Spectral Neural Networks: A Physics-Informed Learning Approach for High-Frequency PDEs](https://arxiv.org/abs/2508.00628)
*Xiong Xiong,Zhuo Zhang,Rongchun Hu,Chen Gao,Zichen Deng*

Main category: cs.LG

TL;DR: SV-SNN是一种新型神经网络框架，通过分离变量和自适应谱方法解决高频振荡PDE求解中的谱偏差问题，显著提升精度并减少参数和训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统PINN在高频振荡PDE求解中存在谱偏差问题，限制了其捕捉高频解的能力。

Method: SV-SNN结合变量分离和自适应谱方法，包括分解多元函数为单变量乘积、自适应傅里叶谱特征和基于SVD的理论框架。

Result: 在多个基准问题上，SV-SNN精度提升1-3个数量级，参数减少90%，训练时间减少60%。

Conclusion: SV-SNN有效解决了神经PDE求解中的谱偏差问题，具有显著优势。

Abstract: Solving high-frequency oscillatory partial differential equations (PDEs) is a
critical challenge in scientific computing, with applications in fluid
mechanics, quantum mechanics, and electromagnetic wave propagation. Traditional
physics-informed neural networks (PINNs) suffer from spectral bias, limiting
their ability to capture high-frequency solution components. We introduce
Separated-Variable Spectral Neural Networks (SV-SNN), a novel framework that
addresses these limitations by integrating separation of variables with
adaptive spectral methods. Our approach features three key innovations: (1)
decomposition of multivariate functions into univariate function products,
enabling independent spatial and temporal networks; (2) adaptive Fourier
spectral features with learnable frequency parameters for high-frequency
capture; and (3) theoretical framework based on singular value decomposition to
quantify spectral bias. Comprehensive evaluation on benchmark problems
including Heat equation, Helmholtz equation, Poisson equations and
Navier-Stokes equations demonstrates that SV-SNN achieves 1-3 orders of
magnitude improvement in accuracy while reducing parameter count by over 90\%
and training time by 60\%. These results establish SV-SNN as an effective
solution to the spectral bias problem in neural PDE solving. The implementation
will be made publicly available upon acceptance at
https://github.com/xgxgnpu/SV-SNN.

</details>


### [122] [KFS: KAN based adaptive Frequency Selection learning architecture for long term time series forecasting](https://arxiv.org/abs/2508.00635)
*Changning Wu,Gao Wu,Rongyao Cai,Yong Liu,Kexin Zhang*

Main category: cs.LG

TL;DR: 提出了一种基于KAN的自适应频率选择学习架构（KFS），通过能量分布主导频率选择和时序对齐，解决了多尺度时间序列预测中的噪声干扰和异构信息分布问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列在不同尺度上存在噪声干扰和异构信息分布，导致多尺度表示效果不佳。

Method: 结合Kolmogorov-Arnold Networks（KAN）和Parseval定理，提出了KFS架构，包括FreK模块（能量分布主导频率选择）、时间戳嵌入对齐和特征混合模块。

Result: 在多个真实时间序列数据集上的实验表明，KFS达到了最先进的性能。

Conclusion: KFS是一种简单而有效的架构，能够显著提升多尺度时间序列预测的性能。

Abstract: Multi-scale decomposition architectures have emerged as predominant
methodologies in time series forecasting. However, real-world time series
exhibit noise interference across different scales, while heterogeneous
information distribution among frequency components at varying scales leads to
suboptimal multi-scale representation. Inspired by Kolmogorov-Arnold Networks
(KAN) and Parseval's theorem, we propose a KAN based adaptive Frequency
Selection learning architecture (KFS) to address these challenges. This
framework tackles prediction challenges stemming from cross-scale noise
interference and complex pattern modeling through its FreK module, which
performs energy-distribution-based dominant frequency selection in the spectral
domain. Simultaneously, KAN enables sophisticated pattern representation while
timestamp embedding alignment synchronizes temporal representations across
scales. The feature mixing module then fuses scale-specific patterns with
aligned temporal features. Extensive experiments across multiple real-world
time series datasets demonstrate that KT achieves state-of-the-art performance
as a simple yet effective architecture.

</details>


### [123] [Reinforcement Learning for Decision-Level Interception Prioritization in Drone Swarm Defense](https://arxiv.org/abs/2508.00641)
*Alessandro Palmas*

Main category: cs.LG

TL;DR: 论文探讨了如何利用强化学习优化防御系统对低成本自杀式无人机群的拦截决策，通过高保真模拟环境验证其效果优于传统规则基线。


<details>
  <summary>Details</summary>
Motivation: 低成本自杀式无人机群对现代防御系统构成严峻挑战，需要快速、战略性的决策来优化拦截优先级。

Method: 提出了一种基于强化学习的决策代理，在高保真模拟环境中学习协调多效应器，根据状态特征（如位置、类别和效应器状态）选择拦截目标。

Result: 强化学习策略在数百次模拟攻击中表现优于手工规则基线，平均损害更低，防御效率更高。

Conclusion: 研究表明强化学习可作为防御架构的战略层，提升韧性而不取代现有控制系统，代码和模拟资源已公开。

Abstract: The growing threat of low-cost kamikaze drone swarms poses a critical
challenge to modern defense systems demanding rapid and strategic
decision-making to prioritize interceptions across multiple effectors and
high-value target zones. In this work, we present a case study demonstrating
the practical advantages of reinforcement learning in addressing this
challenge. We introduce a high-fidelity simulation environment that captures
realistic operational constraints, within which a decision-level reinforcement
learning agent learns to coordinate multiple effectors for optimal interception
prioritization. Operating in a discrete action space, the agent selects which
drone to engage per effector based on observed state features such as
positions, classes, and effector status. We evaluate the learned policy against
a handcrafted rule-based baseline across hundreds of simulated attack
scenarios. The reinforcement learning based policy consistently achieves lower
average damage and higher defensive efficiency in protecting critical zones.
This case study highlights the potential of reinforcement learning as a
strategic layer within defense architectures, enhancing resilience without
displacing existing control systems. All code and simulation assets are
publicly released for full reproducibility, and a video demonstration
illustrates the policy's qualitative behavior.

</details>


### [124] [Light-Weight Diffusion Multiplier and Uncertainty Quantification for Fourier Neural Operators](https://arxiv.org/abs/2508.00643)
*Albert Matveev,Sanmitra Ghosh,Aamal Hussain,James-Michael Leahy,Michalis Michaelides*

Main category: cs.LG

TL;DR: DINOZAUR是一种基于扩散的神经算子参数化方法，解决了FNOs的过参数化和不确定性量化问题，通过减少参数数量和内存占用，同时保持预测性能。


<details>
  <summary>Details</summary>
Motivation: FNOs存在过参数化和缺乏原生不确定性量化的问题，限制了其在科学和工程应用中的可靠性。

Method: DINOZAUR采用扩散乘子替代FNOs中的密集张量乘子，通过贝叶斯方法定义时间参数的先验，实现空间相关输出和校准的不确定性估计。

Result: DINOZAUR在多个PDE基准测试中表现优异，同时提供高效的不确定性量化。

Conclusion: DINOZAUR是一种高效且可靠的神经算子参数化方法，适用于需要不确定性量化的科学和工程应用。

Abstract: Operator learning is a powerful paradigm for solving partial differential
equations, with Fourier Neural Operators serving as a widely adopted
foundation. However, FNOs face significant scalability challenges due to
overparameterization and offer no native uncertainty quantification -- a key
requirement for reliable scientific and engineering applications. Instead,
neural operators rely on post hoc UQ methods that ignore geometric inductive
biases. In this work, we introduce DINOZAUR: a diffusion-based neural operator
parametrization with uncertainty quantification. Inspired by the structure of
the heat kernel, DINOZAUR replaces the dense tensor multiplier in FNOs with a
dimensionality-independent diffusion multiplier that has a single learnable
time parameter per channel, drastically reducing parameter count and memory
footprint without compromising predictive performance. By defining priors over
those time parameters, we cast DINOZAUR as a Bayesian neural operator to yield
spatially correlated outputs and calibrated uncertainty estimates. Our method
achieves competitive or superior performance across several PDE benchmarks
while providing efficient uncertainty quantification.

</details>


### [125] [TrajSurv: Learning Continuous Latent Trajectories from Electronic Health Records for Trustworthy Survival Prediction](https://arxiv.org/abs/2508.00657)
*Sihang Zeng,Lucas Jing Liu,Jun Wen,Meliha Yetisgen,Ruth Etzioni,Gang Luo*

Main category: cs.LG

TL;DR: TrajSurv利用神经控制微分方程从纵向电子健康记录中学习连续潜在轨迹，用于可信赖的生存预测，并通过对比学习和两阶段解释过程提高透明度和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决纵向电子健康记录中不规则采样数据的连续临床进展建模问题，并透明地将进展与生存结果关联。

Method: 使用神经控制微分方程（NCDE）提取连续时间潜在状态，通过时间感知对比学习对齐潜在状态空间，并采用两阶段解释过程。

Result: 在MIMIC-III和eICU数据集上，TrajSurv表现出竞争性准确性和优于现有深度学习方法的透明度。

Conclusion: TrajSurv为临床决策提供了准确且透明的生存预测工具。

Abstract: Trustworthy survival prediction is essential for clinical decision making.
Longitudinal electronic health records (EHRs) provide a uniquely powerful
opportunity for the prediction. However, it is challenging to accurately model
the continuous clinical progression of patients underlying the irregularly
sampled clinical features and to transparently link the progression to survival
outcomes. To address these challenges, we develop TrajSurv, a model that learns
continuous latent trajectories from longitudinal EHR data for trustworthy
survival prediction. TrajSurv employs a neural controlled differential equation
(NCDE) to extract continuous-time latent states from the irregularly sampled
data, forming continuous latent trajectories. To ensure the latent trajectories
reflect the clinical progression, TrajSurv aligns the latent state space with
patient state space through a time-aware contrastive learning approach. To
transparently link clinical progression to the survival outcome, TrajSurv uses
latent trajectories in a two-step divide-and-conquer interpretation process.
First, it explains how the changes in clinical features translate into the
latent trajectory's evolution using a learned vector field. Second, it clusters
these latent trajectories to identify key clinical progression patterns
associated with different survival outcomes. Evaluations on two real-world
medical datasets, MIMIC-III and eICU, show TrajSurv's competitive accuracy and
superior transparency over existing deep learning methods.

</details>


### [126] [DP-DGAD: A Generalist Dynamic Graph Anomaly Detector with Dynamic Prototypes](https://arxiv.org/abs/2508.00664)
*Jialun Zheng,Jie Liu,Jiannong Cao,Xiao Wang,Hanchen Yang,Yankai Chen,Philip S. Yu*

Main category: cs.LG

TL;DR: 提出了一种动态原型（DP）的动态图异常检测模型（DP-DGAD），用于捕捉跨领域的动态异常模式，并在多个真实数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 动态图异常检测（DGAD）在多领域（如金融、交通、社交网络）中至关重要，但现有通用模型难以捕捉动态图中的异常模式，且新领域缺乏标注数据。

Method: DP-DGAD通过动态原型提取和存储领域特定与领域无关的异常模式，结合选择性更新和伪标记技术，实现自监督适应。

Result: 在十个真实数据集上的实验表明，DP-DGAD达到了最先进的性能。

Conclusion: DP-DGAD通过动态原型和自监督适应，有效解决了跨领域动态图异常检测的挑战。

Abstract: Dynamic graph anomaly detection (DGAD) is essential for identifying anomalies
in evolving graphs across domains such as finance, traffic, and social
networks. Recently, generalist graph anomaly detection (GAD) models have shown
promising results. They are pretrained on multiple source datasets and
generalize across domains. While effective on static graphs, they struggle to
capture evolving anomalies in dynamic graphs. Moreover, the continuous
emergence of new domains and the lack of labeled data further challenge
generalist DGAD. Effective cross-domain DGAD requires both domain-specific and
domain-agnostic anomalous patterns. Importantly, these patterns evolve
temporally within and across domains. Building on these insights, we propose a
DGAD model with Dynamic Prototypes (DP) to capture evolving domain-specific and
domain-agnostic patterns. Firstly, DP-DGAD extracts dynamic prototypes, i.e.,
evolving representations of normal and anomalous patterns, from temporal
ego-graphs and stores them in a memory buffer. The buffer is selectively
updated to retain general, domain-agnostic patterns while incorporating new
domain-specific ones. Then, an anomaly scorer compares incoming data with
dynamic prototypes to flag both general and domain-specific anomalies. Finally,
DP-DGAD employs confidence-based pseudo-labeling for effective self-supervised
adaptation in target domains. Extensive experiments demonstrate
state-of-the-art performance across ten real-world datasets from different
domains.

</details>


### [127] [Wind Power Scenario Generation based on the Generalized Dynamic Factor Model and Generative Adversarial Network](https://arxiv.org/abs/2508.00692)
*Young-ho Cho,Hao Zhu,Duehee Lee,Ross Baldick*

Main category: cs.LG

TL;DR: 论文提出了一种结合广义动态因子模型（GDFM）和生成对抗网络（GAN）的方法，用于合成分布式风电场的长期风电功率场景，以更好地捕捉时空相关性和波形特征。


<details>
  <summary>Details</summary>
Motivation: 资源充足性研究需要生成具有时空相关性和统计特性的风电功率场景，但现有方法（如GDFM和GAN单独使用）无法同时满足波形模仿和时空相关性需求。

Method: 结合GDFM和GAN的优势，利用GAN提取动态因子并作为GDFM的滤波器，以同时捕捉时空和频率相关性。

Result: 在澳大利亚风电数据的测试中，该方法在合成风电功率场景时表现优于其他替代方案，更接近实际风电的统计特性。

Conclusion: 该方法通过结合GDFM和GAN，显著提升了风电功率场景合成的性能，为资源充足性研究提供了更可靠的输入。

Abstract: For conducting resource adequacy studies, we synthesize multiple long-term
wind power scenarios of distributed wind farms simultaneously by using the
spatio-temporal features: spatial and temporal correlation, waveforms, marginal
and ramp rates distributions of waveform, power spectral densities, and
statistical characteristics. Generating the spatial correlation in scenarios
requires the design of common factors for neighboring wind farms and
antithetical factors for distant wind farms. The generalized dynamic factor
model (GDFM) can extract the common factors through cross spectral density
analysis, but it cannot closely imitate waveforms. The GAN can synthesize
plausible samples representing the temporal correlation by verifying samples
through a fake sample discriminator. To combine the advantages of GDFM and GAN,
we use the GAN to provide a filter that extracts dynamic factors with temporal
information from the observation data, and we then apply this filter in the
GDFM to represent both spatial and frequency correlations of plausible
waveforms. Numerical tests on the combination of GDFM and GAN have demonstrated
performance improvements over competing alternatives in synthesizing wind power
scenarios from Australia, better realizing plausible statistical
characteristics of actual wind power compared to alternatives such as the GDFM
with a filter synthesized from distributions of actual dynamic filters and the
GAN with direct synthesis without dynamic factors.

</details>


### [128] [Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach](https://arxiv.org/abs/2508.00695)
*Sergio Rubio-Martín,María Teresa García-Ordás,Antonio Serrano-García,Clara Margarita Franch-Pato,Arturo Crespo-Álvaro,José Alberto Benítez-Andrades*

Main category: cs.LG

TL;DR: 比较多种AI模型（传统机器学习和深度学习）对临床笔记分类的效果，发现超参数调优显著提升性能，过采样技术影响有限。


<details>
  <summary>Details</summary>
Motivation: 研究AI模型在心理健康诊断中的分类效果，为AI辅助诊断工具提供参考。

Method: 使用多种机器学习和深度学习模型，结合不同过采样技术和超参数调优。

Result: 决策树和XGBoost在机器学习中表现最佳（96%准确率），BERT模型在深度学习中同样达到96%。超参数调优显著提升性能。

Conclusion: 超参数调优对模型性能至关重要，过采样技术效果有限，研究为心理健康AI诊断提供了实用见解。

Abstract: The classification of clinical notes into specific diagnostic categories is
critical in healthcare, especially for mental health conditions like Anxiety
and Adjustment Disorder. In this study, we compare the performance of various
Artificial Intelligence models, including both traditional Machine Learning
approaches (Random Forest, Support Vector Machine, K-nearest neighbors,
Decision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT
and SciBERT), to classify clinical notes into these two diagnoses.
Additionally, we implemented three oversampling strategies: No Oversampling,
Random Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to
assess their impact on model performance. Hyperparameter tuning was also
applied to optimize model accuracy. Our results indicate that oversampling
techniques had minimal impact on model performance overall. The only exception
was SMOTE, which showed a positive effect specifically with BERT-based models.
However, hyperparameter optimization significantly improved accuracy across the
models, enhancing their ability to generalize and perform on the dataset. The
Decision Tree and eXtreme Gradient Boost models achieved the highest accuracy
among machine learning approaches, both reaching 96%, while the DistilBERT and
SciBERT models also attained 96% accuracy in the deep learning category. These
findings underscore the importance of hyperparameter tuning in maximizing model
performance. This study contributes to the ongoing research on AI-assisted
diagnostic tools in mental health by providing insights into the efficacy of
different model architectures and data balancing methods.

</details>


### [129] [Learning Network Dismantling without Handcrafted Inputs](https://arxiv.org/abs/2508.00706)
*Haozhe Tian,Pietro Ferraro,Robert Shorten,Mahdi Jalili,Homayoun Hamedmoghadam*

Main category: cs.LG

TL;DR: 论文提出了一种无需手工特征的消息传递图神经网络框架MIND，用于解决网络拆除问题，并在大规模真实网络上表现出色。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖手工特征，增加了计算成本并引入偏差，因此需要一种更高效且通用的方法。

Method: 引入注意力机制和消息迭代配置文件，并生成结构多样的小型合成网络训练集。

Result: MIND模型在大型真实网络上表现优于现有方法，具有高效性和通用性。

Conclusion: MIND框架不仅适用于网络拆除问题，还可推广到其他复杂网络问题。

Abstract: The application of message-passing Graph Neural Networks has been a
breakthrough for important network science problems. However, the competitive
performance often relies on using handcrafted structural features as inputs,
which increases computational cost and introduces bias into the otherwise
purely data-driven network representations. Here, we eliminate the need for
handcrafted features by introducing an attention mechanism and utilizing
message-iteration profiles, in addition to an effective algorithmic approach to
generate a structurally diverse training set of small synthetic networks.
Thereby, we build an expressive message-passing framework and use it to
efficiently solve the NP-hard problem of Network Dismantling, virtually
equivalent to vital node identification, with significant real-world
applications. Trained solely on diversified synthetic networks, our proposed
model -- MIND: Message Iteration Network Dismantler -- generalizes to large,
unseen real networks with millions of nodes, outperforming state-of-the-art
network dismantling methods. Increased efficiency and generalizability of the
proposed model can be leveraged beyond dismantling in a range of complex
network problems.

</details>


### [130] [Efficient Solution and Learning of Robust Factored MDPs](https://arxiv.org/abs/2508.00707)
*Yannik Schnitzer,Alessandro Abate,David Parker*

Main category: cs.LG

TL;DR: 提出基于分解状态空间表示的新方法，用于解决和学习鲁棒马尔可夫决策过程（r-MDPs），显著提高样本效率。


<details>
  <summary>Details</summary>
Motivation: r-MDPs通过显式建模转移动态的认知不确定性扩展了MDPs，但学习r-MDPs需要大量样本交互。

Method: 利用系统组件间模型不确定性的独立性，将非凸优化问题转化为可处理的线性规划，并直接学习分解模型表示。

Result: 实验表明，利用分解结构可显著提高样本效率，生成比现有方法更有效的鲁棒策略和更严格的性能保证。

Conclusion: 分解状态空间表示是解决和学习r-MDPs的有效方法，具有实际应用潜力。

Abstract: Robust Markov decision processes (r-MDPs) extend MDPs by explicitly modelling
epistemic uncertainty about transition dynamics. Learning r-MDPs from
interactions with an unknown environment enables the synthesis of robust
policies with provable (PAC) guarantees on performance, but this can require a
large number of sample interactions. We propose novel methods for solving and
learning r-MDPs based on factored state-space representations that leverage the
independence between model uncertainty across system components. Although
policy synthesis for factored r-MDPs leads to hard, non-convex optimisation
problems, we show how to reformulate these into tractable linear programs.
Building on these, we also propose methods to learn factored model
representations directly. Our experimental results show that exploiting
factored structure can yield dimensional gains in sample efficiency, producing
more effective robust policies with tighter performance guarantees than
state-of-the-art methods.

</details>


### [131] [JSON-Bag: A generic game trajectory representation](https://arxiv.org/abs/2508.00712)
*Dien Nguyen,Diego Perez-Liebana,Simon Lucas*

Main category: cs.LG

TL;DR: JSON-Bag模型通过JSON描述的游戏轨迹进行通用表示，使用Jensen-Shannon距离作为度量标准，在多个游戏分类任务中表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 提出一种通用的游戏轨迹表示方法，以解决传统手工特征提取的局限性。

Method: 使用JSON-Bag模型对游戏轨迹进行标记化表示，并采用Jensen-Shannon距离和原型最近邻搜索（P-NNS）进行评估。

Result: 在多数任务中优于基线方法，且样本效率高；自动特征提取显著提升低效任务的准确性。

Conclusion: JSON-Bag模型是一种高效且通用的游戏轨迹表示方法，能够有效捕捉策略差异。

Abstract: We introduce JSON Bag-of-Tokens model (JSON-Bag) as a method to generically
represent game trajectories by tokenizing their JSON descriptions and apply
Jensen-Shannon distance (JSD) as distance metric for them. Using a
prototype-based nearest-neighbor search (P-NNS), we evaluate the validity of
JSON-Bag with JSD on six tabletop games -- \textit{7 Wonders},
\textit{Dominion}, \textit{Sea Salt and Paper}, \textit{Can't Stop},
\textit{Connect4}, \textit{Dots and boxes} -- each over three game trajectory
classification tasks: classifying the playing agents, game parameters, or game
seeds that were used to generate the trajectories.
  Our approach outperforms a baseline using hand-crafted features in the
majority of tasks. Evaluating on N-shot classification suggests using JSON-Bag
prototype to represent game trajectory classes is also sample efficient.
Additionally, we demonstrate JSON-Bag ability for automatic feature extraction
by treating tokens as individual features to be used in Random Forest to solve
the tasks above, which significantly improves accuracy on underperforming
tasks. Finally, we show that, across all six games, the JSD between JSON-Bag
prototypes of agent classes highly correlates with the distances between
agents' policies.

</details>


### [132] [Nested Graph Pseudo-Label Refinement for Noisy Label Domain Adaptation Learning](https://arxiv.org/abs/2508.00716)
*Yingxu Wang,Mengzhu Wang,Zhichao Huang,Suyu Liu*

Main category: cs.LG

TL;DR: NeGPR提出了一种针对带噪声标签的图域自适应框架，通过双分支预训练和嵌套伪标签细化机制提升跨域学习鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现实场景中源标签常含噪声，现有图域自适应方法依赖干净标签假设，性能受限。

Method: NeGPR采用双分支（语义与拓扑）预训练，通过邻域一致性减少噪声影响，并嵌套细化伪标签以跨域学习。

Result: 实验显示NeGPR在严重噪声下优于现有方法，准确率提升达12.7%。

Conclusion: NeGPR通过噪声感知正则化有效提升带噪声标签的图域自适应性能。

Abstract: Graph Domain Adaptation (GDA) facilitates knowledge transfer from labeled
source graphs to unlabeled target graphs by learning domain-invariant
representations, which is essential in applications such as molecular property
prediction and social network analysis. However, most existing GDA methods rely
on the assumption of clean source labels, which rarely holds in real-world
scenarios where annotation noise is pervasive. This label noise severely
impairs feature alignment and degrades adaptation performance under domain
shifts. To address this challenge, we propose Nested Graph Pseudo-Label
Refinement (NeGPR), a novel framework tailored for graph-level domain
adaptation with noisy labels. NeGPR first pretrains dual branches, i.e.,
semantic and topology branches, by enforcing neighborhood consistency in the
feature space, thereby reducing the influence of noisy supervision. To bridge
domain gaps, NeGPR employs a nested refinement mechanism in which one branch
selects high-confidence target samples to guide the adaptation of the other,
enabling progressive cross-domain learning. Furthermore, since pseudo-labels
may still contain noise and the pre-trained branches are already overfitted to
the noisy labels in the source domain, NeGPR incorporates a noise-aware
regularization strategy. This regularization is theoretically proven to
mitigate the adverse effects of pseudo-label noise, even under the presence of
source overfitting, thus enhancing the robustness of the adaptation process.
Extensive experiments on benchmark datasets demonstrate that NeGPR consistently
outperforms state-of-the-art methods under severe label noise, achieving gains
of up to 12.7% in accuracy.

</details>


### [133] [Democratizing Tabular Data Access with an Open$\unicode{x2013}$Source Synthetic$\unicode{x2013}$Data SDK](https://arxiv.org/abs/2508.00718)
*Ivona Krchova,Mariana Vargas Vieyra,Mario Scriminaci,Andrey Sidorenko*

Main category: cs.LG

TL;DR: MOSTLY AI SDK是一个开源工具包，用于生成高质量的合成表格数据，解决数据访问限制问题。


<details>
  <summary>Details</summary>
Motivation: 由于隐私、专有利益和伦理问题，高质量数据的获取受限，合成数据成为解决方案。

Method: 基于TabularARGN自回归框架，集成差分隐私、公平性生成和自动化质量保证，支持多种数据类型和复杂数据集。

Result: SDK在速度和可用性上表现优异，已作为云服务和本地软件快速部署。

Conclusion: MOSTLY AI SDK有效解决数据瓶颈，推动数据民主化。

Abstract: Machine learning development critically depends on access to high-quality
data. However, increasing restrictions due to privacy, proprietary interests,
and ethical concerns have created significant barriers to data accessibility.
Synthetic data offers a viable solution by enabling safe, broad data usage
without compromising sensitive information. This paper presents the MOSTLY AI
Synthetic Data Software Development Kit (SDK), an open-source toolkit designed
specifically for synthesizing high-quality tabular data. The SDK integrates
robust features such as differential privacy guarantees, fairness-aware data
generation, and automated quality assurance into a flexible and accessible
Python interface. Leveraging the TabularARGN autoregressive framework, the SDK
supports diverse data types and complex multi-table and sequential datasets,
delivering competitive performance with notable improvements in speed and
usability. Currently deployed both as a cloud service and locally installable
software, the SDK has seen rapid adoption, highlighting its practicality in
addressing real-world data bottlenecks and promoting widespread data
democratization.

</details>


### [134] [Adaptive Machine Learning-Driven Multi-Fidelity Stratified Sampling for Failure Analysis of Nonlinear Stochastic Systems](https://arxiv.org/abs/2508.00734)
*Liuyun Xu,Seymour M. J. Spence*

Main category: cs.LG

TL;DR: 提出一种多保真度分层采样方法，结合自适应机器学习元模型，高效估计小失效概率，显著节省计算资源。


<details>
  <summary>Details</summary>
Motivation: 现有方差缩减技术在稀有事件分析中仍需大量模型评估，计算成本高，尤其在复杂非线性有限元模型中。

Method: 采用分层采样生成高保真数据集训练深度学习元模型，作为低成本低保真模型，结合多保真蒙特卡洛框架估计失效概率。

Result: 应用于高层钢结构建筑，能准确估计非线性响应的超越概率曲线，计算效率显著优于单保真方法。

Conclusion: 该方法在计算效率和准确性上均优于传统方法，适用于复杂系统的稀有事件分析。

Abstract: Existing variance reduction techniques used in stochastic simulations for
rare event analysis still require a substantial number of model evaluations to
estimate small failure probabilities. In the context of complex, nonlinear
finite element modeling environments, this can become computationally
challenging-particularly for systems subjected to stochastic excitation. To
address this challenge, a multi-fidelity stratified sampling scheme with
adaptive machine learning metamodels is introduced for efficiently propagating
uncertainties and estimating small failure probabilities. In this approach, a
high-fidelity dataset generated through stratified sampling is used to train a
deep learning-based metamodel, which then serves as a cost-effective and highly
correlated low-fidelity model. An adaptive training scheme is proposed to
balance the trade-off between approximation quality and computational demand
associated with the development of the low-fidelity model. By integrating the
low-fidelity outputs with additional high-fidelity results, an unbiased
estimate of the strata-wise failure probabilities is obtained using a
multi-fidelity Monte Carlo framework. The overall probability of failure is
then computed using the total probability theorem. Application to a full-scale
high-rise steel building subjected to stochastic wind excitation demonstrates
that the proposed scheme can accurately estimate exceedance probability curves
for nonlinear responses of interest, while achieving significant computational
savings compared to single-fidelity variance reduction approaches.

</details>


### [135] [A Simple and Effective Method for Uncertainty Quantification and OOD Detection](https://arxiv.org/abs/2508.00754)
*Yaxin Ma,Benjamin Colburn,Jose C. Principe*

Main category: cs.LG

TL;DR: 提出一种基于特征空间密度的单确定性模型方法，用于量化分布偏移和OOD检测，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 贝叶斯神经网络和深度集成方法计算和存储成本高，需更高效的不确定性量化方法。

Method: 利用核密度估计的信息势场近似训练集特征空间密度，通过比较测试样本特征空间表示检测分布偏移。

Result: 在2D合成数据集和OOD检测任务中表现优于基线模型。

Conclusion: 单确定性模型方法在不确定性量化和OOD检测中高效且有效。

Abstract: Bayesian neural networks and deep ensemble methods have been proposed for
uncertainty quantification; however, they are computationally intensive and
require large storage. By utilizing a single deterministic model, we can solve
the above issue. We propose an effective method based on feature space density
to quantify uncertainty for distributional shifts and out-of-distribution (OOD)
detection. Specifically, we leverage the information potential field derived
from kernel density estimation to approximate the feature space density of the
training set. By comparing this density with the feature space representation
of test samples, we can effectively determine whether a distributional shift
has occurred. Experiments were conducted on a 2D synthetic dataset (Two Moons
and Three Spirals) as well as an OOD detection task (CIFAR-10 vs. SVHN). The
results demonstrate that our method outperforms baseline models.

</details>


### [136] [Diffusion-Scheduled Denoising Autoencoders for Anomaly Detection in Tabular Data](https://arxiv.org/abs/2508.00758)
*Timur Sattarov,Marco Schreyer,Damian Borth*

Main category: cs.LG

TL;DR: 提出了一种结合扩散模型噪声调度和对比学习的去噪自编码器（DDAE），用于提升表格数据异常检测性能，在ADBench的57个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 表格数据异常检测因复杂特征交互和异常样本稀缺而具有挑战性，现有方法（如去噪自编码器和扩散模型）存在噪声适应性不足或缺乏显式重建映射的问题。

Method: 提出DDAE框架，整合扩散模型的噪声调度和对比学习到编码过程，优化异常检测。

Result: 在ADBench的57个数据集上，DDAE在半监督和无监督设置中均表现优异，PR-AUC和ROC-AUC分别提升65%（9%）和16%（6%）。

Conclusion: 研究表明，噪声策略对表格异常检测至关重要，高噪声水平适合无监督训练，低噪声线性调度适合半监督设置。

Abstract: Anomaly detection in tabular data remains challenging due to complex feature
interactions and the scarcity of anomalous examples. Denoising autoencoders
rely on fixed-magnitude noise, limiting adaptability to diverse data
distributions. Diffusion models introduce scheduled noise and iterative
denoising, but lack explicit reconstruction mappings. We propose the
Diffusion-Scheduled Denoising Autoencoder (DDAE), a framework that integrates
diffusion-based noise scheduling and contrastive learning into the encoding
process to improve anomaly detection. We evaluated DDAE on 57 datasets from
ADBench. Our method outperforms in semi-supervised settings and achieves
competitive results in unsupervised settings, improving PR-AUC by up to 65%
(9%) and ROC-AUC by 16% (6%) over state-of-the-art autoencoder (diffusion)
model baselines. We observed that higher noise levels benefit unsupervised
training, while lower noise with linear scheduling is optimal in
semi-supervised settings. These findings underscore the importance of
principled noise strategies in tabular anomaly detection.

</details>


### [137] [Evaluating Angle and Amplitude Encoding Strategies for Variational Quantum Machine Learning: their impact on model's accuracy](https://arxiv.org/abs/2508.00768)
*Antonio Tudisco,Andrea Marchesin,Maurizio Zamboni,Mariagrazia Graziano,Giovanna Turvani*

Main category: cs.LG

TL;DR: 该论文研究了量子机器学习中变分量子电路（VQC）的性能，比较了振幅和角度编码模型，并分析了旋转门类型对分类性能的影响。


<details>
  <summary>Details</summary>
Motivation: 量子计算和机器学习的结合引发了量子机器学习（QML）的关注，尤其是变分量子电路（VQC）的性能优化。

Method: 通过振幅和角度编码模型，结合不同旋转门，在Wine和Diabetes数据集上训练并评估性能。

Result: 在相同拓扑结构下，最佳和最差模型的准确率差异为10%至30%，最高达41%。旋转门选择对性能有显著影响。

Conclusion: 编码方式（嵌入）是VQC模型的重要超参数，旋转门的选择对分类性能至关重要。

Abstract: Recent advancements in Quantum Computing and Machine Learning have increased
attention to Quantum Machine Learning (QML), which aims to develop machine
learning models by exploiting the quantum computing paradigm. One of the widely
used models in this area is the Variational Quantum Circuit (VQC), a hybrid
model where the quantum circuit handles data inference while classical
optimization adjusts the parameters of the circuit. The quantum circuit
consists of an encoding layer, which loads data into the circuit, and a
template circuit, known as the ansatz, responsible for processing the data.
This work involves performing an analysis by considering both Amplitude- and
Angle-encoding models, and examining how the type of rotational gate applied
affects the classification performance of the model. This comparison is carried
out by training the different models on two datasets, Wine and Diabetes, and
evaluating their performance. The study demonstrates that, under identical
model topologies, the difference in accuracy between the best and worst models
ranges from 10% to 30%, with differences reaching up to 41%. Moreover, the
results highlight how the choice of rotational gates used in encoding can
significantly impact the model's classification performance. The findings
confirm that the embedding represents a hyperparameter for VQC models.

</details>


### [138] [Explainable AI and Machine Learning for Exam-based Student Evaluation: Causal and Predictive Analysis of Socio-academic and Economic Factors](https://arxiv.org/abs/2508.00785)
*Bushra Akter,Md Biplob Hosen,Sabbir Ahmed,Mehrin Anannya,Md. Farhad Hossain*

Main category: cs.LG

TL;DR: 该研究通过文献综述和在线调查，分析了影响学生CGPA的多变量因素，并利用因果分析和机器学习模型（如Ridge回归和随机森林）预测和分类学术表现。最终开发了一个基于网络的个性化建议工具。


<details>
  <summary>Details</summary>
Motivation: 研究旨在识别和优化影响学生学术表现的多变量因素，以帮助学生提升CGPA。

Method: 通过文献综述构建假设因果图，进行在线调查（1050名学生），数据预处理后，采用因果分析、回归和分类模型（如Ridge回归和随机森林），并结合可解释AI技术（SHAP、LIME）。

Result: Ridge回归预测CGPA的MAE为0.12，MSE为0.023；随机森林分类的F1-score接近完美，准确率达98.68%。关键因素包括学习时间、奖学金、父母教育背景和先前学术表现。

Conclusion: 研究成功识别了影响CGPA的关键因素，并开发了基于网络的个性化工具，帮助学生优化学术表现。

Abstract: Academic performance depends on a multivariable nexus of socio-academic and
financial factors. This study investigates these influences to develop
effective strategies for optimizing students' CGPA. To achieve this, we
reviewed various literature to identify key influencing factors and constructed
an initial hypothetical causal graph based on the findings. Additionally, an
online survey was conducted, where 1,050 students participated, providing
comprehensive data for analysis. Rigorous data preprocessing techniques,
including cleaning and visualization, ensured data quality before analysis.
Causal analysis validated the relationships among variables, offering deeper
insights into their direct and indirect effects on CGPA. Regression models were
implemented for CGPA prediction, while classification models categorized
students based on performance levels. Ridge Regression demonstrated strong
predictive accuracy, achieving a Mean Absolute Error of 0.12 and a Mean Squared
Error of 0.023. Random Forest outperformed in classification, attaining an
F1-score near perfection and an accuracy of 98.68%. Explainable AI techniques
such as SHAP, LIME, and Interpret enhanced model interpretability, highlighting
critical factors such as study hours, scholarships, parental education, and
prior academic performance. The study culminated in the development of a
web-based application that provides students with personalized insights,
allowing them to predict academic performance, identify areas for improvement,
and make informed decisions to enhance their outcomes.

</details>


### [139] [Adacc: Adaptive Compression and Activation Checkpointing for LLM Memory Management](https://arxiv.org/abs/2508.00806)
*Ping Chen,Zhuohong Deng,Ping Li,Shuibing He,Hongzi Zhu,Yi Zheng,Zhefeng Wang,Baoxing Huai,Minyi Guo*

Main category: cs.LG

TL;DR: Adacc是一个结合自适应压缩和激活检查点的内存管理框架，旨在减少GPU内存占用并加速大型语言模型训练。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练中的重计算会引入高达30%的开销，Adacc旨在通过优化内存管理来减少这一开销。

Method: Adacc包含三个模块：层特定压缩算法、基于MILP的最优调度策略和自适应策略演化机制。

Result: 实验表明，Adacc能将LLM训练速度提升1.01x至1.37x，同时保持与基线相当的模型精度。

Conclusion: Adacc通过自适应压缩和调度策略，有效减少了内存占用并提升了训练效率。

Abstract: Training large language models often employs recomputation to alleviate
memory pressure, which can introduce up to 30% overhead in real-world
scenarios. In this paper, we propose Adacc, a novel memory management framework
that combines adaptive compression and activation checkpointing to reduce the
GPU memory footprint. It comprises three modules: (1) We design layer-specific
compression algorithms that account for outliers in LLM tensors, instead of
directly quantizing floats from FP16 to INT4, to ensure model accuracy. (2) We
propose an optimal scheduling policy that employs MILP to determine the best
memory optimization for each tensor. (3) To accommodate changes in training
tensors, we introduce an adaptive policy evolution mechanism that adjusts the
policy during training to enhance throughput. Experimental results show that
Adacc can accelerate the LLM training by 1.01x to 1.37x compared to
state-of-the-art frameworks, while maintaining comparable model accuracy to the
Baseline.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [140] [Melody-Lyrics Matching with Contrastive Alignment Loss](https://arxiv.org/abs/2508.00123)
*Changhong Wang,Michel Olvera,Gaël Richard*

Main category: eess.AS

TL;DR: 论文提出了一种名为旋律-歌词匹配（MLM）的新任务，通过自监督表示学习框架和对比对齐损失，利用现有歌曲中的旋律和歌词关系，无需对齐标注。


<details>
  <summary>Details</summary>
Motivation: 探索音乐与歌词之间超越语义的联系，如节奏与押韵、音符时长与音节重音等，填补音乐信息检索领域的空白。

Method: 提出自监督表示学习框架，使用对比对齐损失，并引入音节级歌词表示（sylphone）。

Result: 方法能够匹配旋律与连贯且可唱的歌词，并通过实证结果和直观示例验证。

Conclusion: MLM任务和提出的框架为音乐与歌词关系的研究提供了新方向，代码和示例已开源。

Abstract: The connection between music and lyrics is far beyond semantic bonds.
Conceptual pairs in the two modalities such as rhythm and rhyme, note duration
and syllabic stress, and structure correspondence, raise a compelling yet
seldom-explored direction in the field of music information retrieval. In this
paper, we present melody-lyrics matching (MLM), a new task which retrieves
potential lyrics for a given symbolic melody from text sources. Rather than
generating lyrics from scratch, MLM essentially exploits the relationships
between melody and lyrics. We propose a self-supervised representation learning
framework with contrastive alignment loss for melody and lyrics. This has the
potential to leverage the abundance of existing songs with paired melody and
lyrics. No alignment annotations are required. Additionally, we introduce
sylphone, a novel representation for lyrics at syllable-level activated by
phoneme identity and vowel stress. We demonstrate that our method can match
melody with coherent and singable lyrics with empirical results and intuitive
examples. We open source code and provide matching examples on the companion
webpage: https://github.com/changhongw/mlm.

</details>


### [141] [Beamformed 360° Sound Maps: U-Net-Driven Acoustic Source Segmentation and Localization](https://arxiv.org/abs/2508.00307)
*Belman Jahir Rodriguez,Sergio F. Chevtchenko,Marcelo Herrera Martinez,Yeshwant Bethy,Saeed Afshar*

Main category: eess.AS

TL;DR: 提出了一种基于U-net的360度声源定位方法，通过球形语义分割任务实现，优于传统的离散方向估计。


<details>
  <summary>Details</summary>
Motivation: 传统声源定位方法依赖离散方向估计，无法有效处理空间分布声源。本文旨在通过语义分割实现更精确的声源区域识别。

Method: 使用延迟求和波束形成生成音频地图，通过改进的U-net进行频率域分割，采用Tversky损失解决类别不平衡问题。

Result: 实验表明，该方法在不同环境中具有泛化能力，提供更高的角度精度。

Conclusion: 该方法为密集空间音频理解提供了新范式，优于传统声源定位技术。

Abstract: We introduce a U-net model for 360{\deg} acoustic source localization
formulated as a spherical semantic segmentation task. Rather than regressing
discrete direction-of-arrival (DoA) angles, our model segments beamformed audio
maps (azimuth and elevation) into regions of active sound presence. Using
delay-and-sum (DAS) beamforming on a custom 24-microphone array, we generate
signals aligned with drone GPS telemetry to create binary supervision masks. A
modified U-Net, trained on frequency-domain representations of these maps,
learns to identify spatially distributed source regions while addressing class
imbalance via the Tversky loss. Because the network operates on beamformed
energy maps, the approach is inherently array-independent and can adapt to
different microphone configurations without retraining from scratch. The
segmentation outputs are post-processed by computing centroids over activated
regions, enabling robust DoA estimates. Our dataset includes real-world
open-field recordings of a DJI Air 3 drone, synchronized with 360{\deg} video
and flight logs across multiple dates and locations. Experimental results show
that U-net generalizes across environments, providing improved angular
precision, offering a new paradigm for dense spatial audio understanding beyond
traditional Sound Source Localization (SSL).

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [142] [XRoboToolkit: A Cross-Platform Framework for Robot Teleoperation](https://arxiv.org/abs/2508.00097)
*Zhigen Zhao,Liuchuan Yu,Ke Jing,Ning Yang*

Main category: cs.RO

TL;DR: XRoboToolkit是一个基于OpenXR标准的跨平台扩展现实机器人遥操作框架，解决了现有数据收集方法在可扩展性、复杂性和数据质量上的问题。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言-动作模型的快速发展，对大规模高质量机器人演示数据的需求日益迫切，但现有遥操作方法存在可扩展性差、设置复杂和数据质量低的问题。

Method: 提出XRoboToolkit框架，支持低延迟立体视觉反馈、基于优化的逆运动学，以及多种跟踪模式（如头部、控制器、手部等），其模块化架构支持跨平台和仿真环境集成。

Result: 通过精确操控任务验证了框架的有效性，并通过训练VLA模型展示了其数据质量。

Conclusion: XRoboToolkit为机器人遥操作提供了高效、高质量的解决方案，支持多样化的机器人平台和任务。

Abstract: The rapid advancement of Vision-Language-Action models has created an urgent
need for large-scale, high-quality robot demonstration datasets. Although
teleoperation is the predominant method for data collection, current approaches
suffer from limited scalability, complex setup procedures, and suboptimal data
quality. This paper presents XRoboToolkit, a cross-platform framework for
extended reality based robot teleoperation built on the OpenXR standard. The
system features low-latency stereoscopic visual feedback, optimization-based
inverse kinematics, and support for diverse tracking modalities including head,
controller, hand, and auxiliary motion trackers. XRoboToolkit's modular
architecture enables seamless integration across robotic platforms and
simulation environments, spanning precision manipulators, mobile robots, and
dexterous hands. We demonstrate the framework's effectiveness through precision
manipulation tasks and validate data quality by training VLA models that
exhibit robust autonomous performance.

</details>


### [143] [HannesImitation: Grasping with the Hannes Prosthetic Hand via Imitation Learning](https://arxiv.org/abs/2508.00491)
*Carlo Alessi,Federico Vasile,Federico Ceola,Giulia Pasquale,Nicolò Boccardo,Lorenzo Natale*

Main category: cs.RO

TL;DR: 论文提出了一种基于模仿学习的HannesImitationPolicy方法，用于控制假肢手Hannes，实现了在非结构化环境中的物体抓取，并展示了其优于基于分割的视觉伺服控制器的性能。


<details>
  <summary>Details</summary>
Motivation: 通过模仿学习减少假肢手用户的认知负担，提升假肢手的灵活性和适应性，使其能在非结构化环境中学习任务。

Method: 使用HannesImitationDataset中的抓取演示数据训练扩散策略，预测手腕方向和手部闭合动作。

Result: 实验表明，该方法在多样化的物体和条件下成功实现了抓取，并在非结构化场景中优于基于分割的视觉伺服控制器。

Conclusion: 模仿学习为假肢手控制提供了新的可能性，能够在不依赖手动标注序列的情况下提升性能。

Abstract: Recent advancements in control of prosthetic hands have focused on increasing
autonomy through the use of cameras and other sensory inputs. These systems aim
to reduce the cognitive load on the user by automatically controlling certain
degrees of freedom. In robotics, imitation learning has emerged as a promising
approach for learning grasping and complex manipulation tasks while simplifying
data collection. Its application to the control of prosthetic hands remains,
however, largely unexplored. Bridging this gap could enhance dexterity
restoration and enable prosthetic devices to operate in more unconstrained
scenarios, where tasks are learned from demonstrations rather than relying on
manually annotated sequences. To this end, we present HannesImitationPolicy, an
imitation learning-based method to control the Hannes prosthetic hand, enabling
object grasping in unstructured environments. Moreover, we introduce the
HannesImitationDataset comprising grasping demonstrations in table, shelf, and
human-to-prosthesis handover scenarios. We leverage such data to train a single
diffusion policy and deploy it on the prosthetic hand to predict the wrist
orientation and hand closure for grasping. Experimental evaluation demonstrates
successful grasps across diverse objects and conditions. Finally, we show that
the policy outperforms a segmentation-based visual servo controller in
unstructured scenarios. Additional material is provided on our project page:
https://hsp-iit.github.io/HannesImitation

</details>


### [144] [OmniUnet: A Multimodal Network for Unstructured Terrain Segmentation on Planetary Rovers Using RGB, Depth, and Thermal Imagery](https://arxiv.org/abs/2508.00580)
*Raul Castilla-Arquillo,Carlos Perez-del-Pulgar,Levin Gerdes,Alfonso Garcia-Cerezo,Miguel A. Olivares-Mendez*

Main category: cs.RO

TL;DR: OmniUnet是一种基于Transformer的神经网络架构，用于RGB-D-T图像语义分割，支持火星探测中的多模态地形感知。


<details>
  <summary>Details</summary>
Motivation: 在非结构化环境中，机器人导航需要多模态感知系统，而火星探测中热成像对地形安全评估尤为重要。

Method: 开发了OmniUnet网络架构，结合RGB、深度和热成像数据，并在半沙漠环境中收集多模态数据集进行训练。

Result: 模型像素准确率达80.37%，在资源受限设备上推理时间为673毫秒，适用于机器人部署。

Conclusion: OmniUnet在多模态地形分割中表现优异，软件和数据集已公开，支持未来研究。

Abstract: Robot navigation in unstructured environments requires multimodal perception
systems that can support safe navigation. Multimodality enables the integration
of complementary information collected by different sensors. However, this
information must be processed by machine learning algorithms specifically
designed to leverage heterogeneous data. Furthermore, it is necessary to
identify which sensor modalities are most informative for navigation in the
target environment. In Martian exploration, thermal imagery has proven valuable
for assessing terrain safety due to differences in thermal behaviour between
soil types. This work presents OmniUnet, a transformer-based neural network
architecture for semantic segmentation using RGB, depth, and thermal (RGB-D-T)
imagery. A custom multimodal sensor housing was developed using 3D printing and
mounted on the Martian Rover Testbed for Autonomy (MaRTA) to collect a
multimodal dataset in the Bardenas semi-desert in northern Spain. This location
serves as a representative environment of the Martian surface, featuring
terrain types such as sand, bedrock, and compact soil. A subset of this dataset
was manually labeled to support supervised training of the network. The model
was evaluated both quantitatively and qualitatively, achieving a pixel accuracy
of 80.37% and demonstrating strong performance in segmenting complex
unstructured terrain. Inference tests yielded an average prediction time of 673
ms on a resource-constrained computer (Jetson Orin Nano), confirming its
suitability for on-robot deployment. The software implementation of the network
and the labeled dataset have been made publicly available to support future
research in multimodal terrain perception for planetary robotics.

</details>


### [145] [On-Device Diffusion Transformer Policy for Efficient Robot Manipulation](https://arxiv.org/abs/2508.00697)
*Yiming Wu,Huan Wang,Zhenghao Chen,Jianxin Pang,Dong Xu*

Main category: cs.RO

TL;DR: LightDP是一种专为移动设备设计的框架，通过网络压缩和减少采样步骤加速扩散策略，实现实时动作预测。


<details>
  <summary>Details</summary>
Motivation: 扩散策略在资源受限的移动平台上应用时存在计算效率低和内存占用大的问题。

Method: LightDP采用网络压缩和采样步骤减少策略，通过统一的剪枝和再训练流程优化模型，并结合一致性蒸馏技术。

Result: 在多个标准数据集上，LightDP实现了实时动作预测，性能与现有扩散策略相当。

Conclusion: LightDP为资源受限环境下扩散策略的实际部署提供了重要进展。

Abstract: Diffusion Policies have significantly advanced robotic manipulation tasks via
imitation learning, but their application on resource-constrained mobile
platforms remains challenging due to computational inefficiency and extensive
memory footprint. In this paper, we propose LightDP, a novel framework
specifically designed to accelerate Diffusion Policies for real-time deployment
on mobile devices. LightDP addresses the computational bottleneck through two
core strategies: network compression of the denoising modules and reduction of
the required sampling steps. We first conduct an extensive computational
analysis on existing Diffusion Policy architectures, identifying the denoising
network as the primary contributor to latency. To overcome performance
degradation typically associated with conventional pruning methods, we
introduce a unified pruning and retraining pipeline, optimizing the model's
post-pruning recoverability explicitly. Furthermore, we combine pruning
techniques with consistency distillation to effectively reduce sampling steps
while maintaining action prediction accuracy. Experimental evaluations on the
standard datasets, \ie, PushT, Robomimic, CALVIN, and LIBERO, demonstrate that
LightDP achieves real-time action prediction on mobile devices with competitive
performance, marking an important step toward practical deployment of
diffusion-based policies in resource-limited environments. Extensive real-world
experiments also show the proposed LightDP can achieve performance comparable
to state-of-the-art Diffusion Policies.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [146] [Formal Power Series Representations in Probability and Expected Utility Theory](https://arxiv.org/abs/2508.00294)
*Arthur Paul Pedersen,Samuel Allen Alexander*

Main category: math.PR

TL;DR: 本文提出了一种新的偏好理论，放弃了传统理论的限制，允许任何满足特定一致性要求的偏好系统扩展为完整系统，且无需传递性、阿基米德性、有界性或连续性。


<details>
  <summary>Details</summary>
Motivation: 传统偏好理论存在过多限制，本文旨在提出一种更通用的理论，放宽这些限制，同时保持一致性。

Method: 通过类比de Finetti的概率基础理论，提出一种新的偏好一致性标准，并证明其扩展性和可表示性。

Result: 任何满足一致性标准的完整偏好系统均可在实数有序域扩展中通过效用表示，扩展了Hölder定理并强化了Hahn嵌入定理。

Conclusion: 本文的理论为偏好分析提供了更灵活的工具，同时保持了数学上的严谨性。

Abstract: We advance a general theory of coherent preference that surrenders
restrictions embodied in orthodox doctrine. This theory enjoys the property
that any preference system admits extension to a complete system of
preferences, provided it satisfies a certain coherence requirement analogous to
the one de Finetti advanced for his foundations of probability. Unlike de
Finetti's theory, the one we set forth requires neither transitivity nor
Archimedeanness nor boundedness nor continuity of preference. This theory also
enjoys the property that any complete preference system meeting the standard of
coherence can be represented by utility in an ordered field extension of the
reals. Representability by utility is a corollary of this paper's central
result, which at once extends H\"older's Theorem and strengthens Hahn's
Embedding Theorem.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [147] [Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web](https://arxiv.org/abs/2507.23585)
*Sophia Liu,Shm Garanganao Almeda*

Main category: cs.HC

TL;DR: 论文提出“超文本摩擦”概念，旨在通过经典超文本原则（摩擦、可追溯性和结构）在算法主导的界面中恢复用户控制权。


<details>
  <summary>Details</summary>
Motivation: 当前算法驱动的界面（如推荐系统和生成式AI工具）牺牲用户控制权以追求参与度和效率，导致用户对内容及其关系的构建失去主导权。

Method: 通过比较分析真实界面（如维基百科与Instagram Explore、Are.na与生成式AI图像工具），研究不同系统如何塑造用户体验、导航和创作。

Result: 超文本系统强调来源、关联思维和用户主导的意义构建，而算法系统则倾向于模糊过程并简化参与。

Conclusion: 论文贡献了（1）界面结构如何影响用户驱动与代理驱动系统中的控制权比较分析，（2）提出超文本价值观作为设计承诺，以在算法主导的网络中恢复用户控制权。

Abstract: Today's algorithm-driven interfaces, from recommendation feeds to GenAI
tools, often prioritize engagement and efficiency at the expense of user
agency. As systems take on more decision-making, users have less control over
what they see and how meaning or relationships between content are constructed.
This paper introduces "Hypertextual Friction," a conceptual design stance that
repositions classical hypertext principles--friction, traceability, and
structure--as actionable values for reclaiming agency in algorithmically
mediated environments. Through a comparative analysis of real-world
interfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image
tools--we examine how different systems structure user experience, navigation,
and authorship. We show that hypertext systems emphasize provenance,
associative thinking, and user-driven meaning-making, while algorithmic systems
tend to obscure process and flatten participation. We contribute: (1) a
comparative analysis of how interface structures shape agency in user-driven
versus agent-driven systems, and (2) a conceptual stance that offers
hypertextual values as design commitments for reclaiming agency in an
increasingly algorithmic web.

</details>


### [148] [A Mixed User-Centered Approach to Enable Augmented Intelligence in Intelligent Tutoring Systems: The Case of MathAIde app](https://arxiv.org/abs/2508.00103)
*Guilherme Guerino,Luiz Rodrigues,Luana Bianchiniand Mariana Alves,Marcelo Marinho,Thomaz Veloso,Valmir Macario,Diego Dermeval,Thales Vieira,Ig Bittencourt,Seiji Isotani*

Main category: cs.HC

TL;DR: 论文探讨了AI在教育中的应用，提出MathAIde系统，结合计算机视觉和AI纠正数学练习，强调教师参与设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 解决AIED中的挑战，如教师角色、AI工具限制和资源可及性，通过增强智能（AuI）提升学习体验。

Method: 设计开发MathAIde系统，包括用户头脑风暴、高保真原型、A/B测试和真实课堂案例研究。

Result: 研究提出教师为中心的AuI设计方法，验证了系统在真实环境中的实用性。

Conclusion: 用户中心设计提升AIED系统的实用性和采用潜力，尤其在资源有限环境中。

Abstract: Integrating Artificial Intelligence in Education (AIED) aims to enhance
learning experiences through technologies like Intelligent Tutoring Systems
(ITS), offering personalized learning, increased engagement, and improved
retention rates. However, AIED faces three main challenges: the critical role
of teachers in the design process, the limitations and reliability of AI tools,
and the accessibility of technological resources. Augmented Intelligence (AuI)
addresses these challenges by enhancing human capabilities rather than
replacing them, allowing systems to suggest solutions. In contrast, humans
provide final assessments, thus improving AI over time. In this sense, this
study focuses on designing, developing, and evaluating MathAIde, an ITS that
corrects mathematics exercises using computer vision and AI and provides
feedback based on photos of student work. The methodology included
brainstorming sessions with potential users, high-fidelity prototyping, A/B
testing, and a case study involving real-world classroom environments for
teachers and students. Our research identified several design possibilities for
implementing AuI in ITSs, emphasizing a balance between user needs and
technological feasibility. Prioritization and validation through prototyping
and testing highlighted the importance of efficiency metrics, ultimately
leading to a solution that offers pre-defined remediation alternatives for
teachers. Real-world deployment demonstrated the usefulness of the proposed
solution. Our research contributes to the literature by providing a usable,
teacher-centered design approach that involves teachers in all design phases.
As a practical implication, we highlight that the user-centered design approach
increases the usefulness and adoption potential of AIED systems, especially in
resource-limited environments.

</details>


### [149] [Your Model Is Unfair, Are You Even Aware? Inverse Relationship Between Comprehension and Trust in Explainability Visualizations of Biased ML Models](https://arxiv.org/abs/2508.00140)
*Zhanna Kaufman,Madeline Endres,Cindy Xiong Bearfield,Yuriy Brun*

Main category: cs.HC

TL;DR: 论文研究了ML模型解释性可视化对用户理解和信任的影响，发现理解与信任呈负相关，且偏见感知是中介因素。


<details>
  <summary>Details</summary>
Motivation: ML系统中的偏见行为普遍存在，影响用户信任，而不同背景的用户对同一系统的看法和信任度不同，因此解释性可视化在理解和信任中起关键作用。

Method: 通过用户研究评估五种最先进的解释性可视化工具（LIME、SHAP、CP、Anchors和ELI5），分析设计特征如何影响非专家用户的理解、偏见感知和信任。

Result: 研究发现理解与信任呈负相关，偏见感知是中介因素；可视化设计可显著提高理解、增加偏见感知并降低信任。

Conclusion: 通过改进模型公平性或调整可视化设计减少偏见感知，可显著提高信任，即使理解保持高水平。研究为促进负责任ML应用提供了系统见解。

Abstract: Systems relying on ML have become ubiquitous, but so has biased behavior
within them. Research shows that bias significantly affects stakeholders' trust
in systems and how they use them. Further, stakeholders of different
backgrounds view and trust the same systems differently. Thus, how ML models'
behavior is explained plays a key role in comprehension and trust. We survey
explainability visualizations, creating a taxonomy of design characteristics.
We conduct user studies to evaluate five state-of-the-art visualization tools
(LIME, SHAP, CP, Anchors, and ELI5) for model explainability, measuring how
taxonomy characteristics affect comprehension, bias perception, and trust for
non-expert ML users. Surprisingly, we find an inverse relationship between
comprehension and trust: the better users understand the models, the less they
trust them. We investigate the cause and find that this relationship is
strongly mediated by bias perception: more comprehensible visualizations
increase people's perception of bias, and increased bias perception reduces
trust. We confirm this relationship is causal: Manipulating explainability
visualizations to control comprehension, bias perception, and trust, we show
that visualization design can significantly (p < 0.001) increase comprehension,
increase perceived bias, and reduce trust. Conversely, reducing perceived model
bias, either by improving model fairness or by adjusting visualization design,
significantly increases trust even when comprehension remains high. Our work
advances understanding of how comprehension affects trust and systematically
investigates visualization's role in facilitating responsible ML applications.

</details>


### [150] [DeformTune: A Deformable XAI Music Prototype for Non-Musicians](https://arxiv.org/abs/2508.00160)
*Ziqing Xu,Nick Bryan-Kinns*

Main category: cs.HC

TL;DR: DeformTune是一种结合可变形触控界面与MeasureVAE模型的AI音乐生成系统，旨在为非音乐专业人士提供更直观的音乐创作体验。初步研究表明，用户面临控制映射不清晰等问题，并提出了增强AI可解释性的设计机会。


<details>
  <summary>Details</summary>
Motivation: 现有AI音乐生成工具依赖文本提示或复杂界面，对非音乐专业人士不友好，因此需要更直观的交互方式。

Method: 结合可变形触控界面与MeasureVAE模型，通过11名无音乐背景的成人参与者进行初步研究，分析其反馈。

Result: 研究发现用户面临控制映射不清晰、表达范围有限等问题，提出了多模态反馈和渐进式交互支持等改进方向。

Conclusion: DeformTune为AI音乐系统的可解释性和新手用户友好性提供了早期见解。

Abstract: Many existing AI music generation tools rely on text prompts, complex
interfaces, or instrument-like controls, which may require musical or technical
knowledge that non-musicians do not possess. This paper introduces DeformTune,
a prototype system that combines a tactile deformable interface with the
MeasureVAE model to explore more intuitive, embodied, and explainable AI
interaction. We conducted a preliminary study with 11 adult participants
without formal musical training to investigate their experience with
AI-assisted music creation. Thematic analysis of their feedback revealed
recurring challenge--including unclear control mappings, limited expressive
range, and the need for guidance throughout use. We discuss several design
opportunities for enhancing explainability of AI, including multimodal feedback
and progressive interaction support. These findings contribute early insights
toward making AI music systems more explainable and empowering for novice
users.

</details>


### [151] [The SPACE of AI: Real-World Lessons on AI's Impact on Developers](https://arxiv.org/abs/2508.00178)
*Brian Houck,Travis Lowdermilk,Cody Beyer,Steven Clarke,Ben Hanrahan*

Main category: cs.HC

TL;DR: AI工具在软件开发中广泛使用，提升生产力但效果因任务复杂性和团队支持而异。


<details>
  <summary>Details</summary>
Motivation: 研究AI工具对开发者生产力和体验的真实影响。

Method: 混合方法研究，包括500多名开发者的调查、访谈和观察。

Result: AI提升效率和满意度，但对协作影响有限；团队文化和支持是关键。

Conclusion: AI是开发者的辅助工具，有效整合需团队文化和组织支持。

Abstract: As artificial intelligence (AI) tools become increasingly embedded in
software development workflows, questions persist about their true impact on
developer productivity and experience. This paper presents findings from a
mixed-methods study examining how developers perceive AI's influence across the
dimensions of the SPACE framework: Satisfaction, Performance, Activity,
Collaboration and Efficiency. Drawing on survey responses from over 500
developers and qualitative insights from interviews and observational studies,
we find that AI is broadly adopted and widely seen as enhancing productivity,
particularly for routine tasks. However, the benefits vary, depending on task
complexity, individual usage patterns, and team-level adoption. Developers
report increased efficiency and satisfaction, with less evidence of impact on
collaboration. Organizational support and peer learning play key roles in
maximizing AI's value. These findings suggest that AI is augmenting developers
rather than replacing them, and that effective integration depends as much on
team culture and support structures as on the tools themselves. We conclude
with practical recommendations for teams, organizations and researchers seeking
to harness AI's potential in software engineering.

</details>


### [152] [What's Behind the Magic? Audiences Seek Artistic Value in Generative AI's Contributions to a Live Dance Performance](https://arxiv.org/abs/2508.00239)
*Jacqueline Elise Bruen,Myounghoon Jeon*

Main category: cs.HC

TL;DR: 研究发现，观众在不知情的情况下更倾向于认可AI生成的艺术作品的艺术价值，强调了社会背景和用户理解在技术解释中的重要性。


<details>
  <summary>Details</summary>
Motivation: 探讨生成式人工智能（GenAI）创作的艺术作品的价值争议，以及观众对其的接受度。

Method: 开发两个版本的舞蹈表演（使用或不使用GenAI），并在观众知情或不知情的情况下调查其感知。

Result: 观众在不知情时更认可AI作品的艺术价值。

Conclusion: 呼吁通过社会背景和用户理解来弥合对GenAI的认知差距。

Abstract: With the development of generative artificial intelligence (GenAI) tools to
create art, stakeholders cannot come to an agreement on the value of these
works. In this study we uncovered the mixed opinions surrounding art made by
AI. We developed two versions of a dance performance augmented by technology
either with or without GenAI. For each version we informed audiences of the
performance's development either before or after a survey on their perceptions
of the performance. There were thirty-nine participants (13 males, 26 female)
divided between the four performances. Results demonstrated that individuals
were more inclined to attribute artistic merit to works made by GenAI when they
were unaware of its use. We present this case study as a call to address the
importance of utilizing the social context and the users' interpretations of
GenAI in shaping a technical explanation, leading to a greater discussion that
can bridge gaps in understanding.

</details>


### [153] [MetaExplainer: A Framework to Generate Multi-Type User-Centered Explanations for AI Systems](https://arxiv.org/abs/2508.00300)
*Shruthi Chari,Oshani Seneviratne,Prithwish Chakraborty,Pablo Meyer,Deborah L. McGuinness*

Main category: cs.HC

TL;DR: MetaExplainer是一个神经符号框架，通过三阶段过程生成用户中心的解释，结合LLM和解释本体，提高AI系统的可解释性和信任度。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统的解释与用户需求存在差距，MetaExplainer旨在通过用户驱动的方法填补这一差距。

Method: 三阶段过程：1) 用LLM分解用户问题；2) 模型解释方法生成建议；3) 合成自然语言解释。

Result: 在问题重构、解释忠实度和上下文利用方面表现优异，用户研究证实其解释的创造性和全面性。

Conclusion: MetaExplainer是一个多功能工具，适用于多种解释类型和领域，有望提升AI的可解释性。

Abstract: Explanations are crucial for building trustworthy AI systems, but a gap often
exists between the explanations provided by models and those needed by users.
To address this gap, we introduce MetaExplainer, a neuro-symbolic framework
designed to generate user-centered explanations. Our approach employs a
three-stage process: first, we decompose user questions into machine-readable
formats using state-of-the-art large language models (LLM); second, we delegate
the task of generating system recommendations to model explainer methods; and
finally, we synthesize natural language explanations that summarize the
explainer outputs. Throughout this process, we utilize an Explanation Ontology
to guide the language models and explainer methods. By leveraging LLMs and a
structured approach to explanation generation, MetaExplainer aims to enhance
the interpretability and trustworthiness of AI systems across various
applications, providing users with tailored, question-driven explanations that
better meet their needs. Comprehensive evaluations of MetaExplainer demonstrate
a step towards evaluating and utilizing current state-of-the-art explanation
frameworks. Our results show high performance across all stages, with a 59.06%
F1-score in question reframing, 70% faithfulness in model explanations, and 67%
context-utilization in natural language synthesis. User studies corroborate
these findings, highlighting the creativity and comprehensiveness of generated
explanations. Tested on the Diabetes (PIMA Indian) tabular dataset,
MetaExplainer supports diverse explanation types, including Contrastive,
Counterfactual, Rationale, Case-Based, and Data explanations. The framework's
versatility and traceability from using ontology to guide LLMs suggest broad
applicability beyond the tested scenarios, positioning MetaExplainer as a
promising tool for enhancing AI explainability across various domains.

</details>


### [154] [How LLMs are Shaping the Future of Virtual Reality](https://arxiv.org/abs/2508.00737)
*Süeda Özkaya,Santiago Berrezueta-Guzman,Stefan Wagner*

Main category: cs.HC

TL;DR: 综述了大型语言模型（LLMs）在虚拟现实（VR）游戏中的应用，探讨了其在叙事生成、NPC互动等方面的潜力与挑战。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs如何提升VR游戏的沉浸感、适应性和智能性，推动更真实的数字体验。

Method: 分析了2018至2025年间62项同行评审研究，总结了LLMs在VR中的关键应用领域和挑战。

Result: LLMs显著增强了VR环境的真实感、创造力和用户参与度，但需解决实时性能、伦理等问题。

Conclusion: 未来研究方向包括多模态AI、情感计算等，以推动智能和包容性VR系统的发展。

Abstract: The integration of Large Language Models (LLMs) into Virtual Reality (VR)
games marks a paradigm shift in the design of immersive, adaptive, and
intelligent digital experiences. This paper presents a comprehensive review of
recent research at the intersection of LLMs and VR, examining how these models
are transforming narrative generation, non-player character (NPC) interactions,
accessibility, personalization, and game mastering. Drawing from an analysis of
62 peer reviewed studies published between 2018 and 2025, we identify key
application domains ranging from emotionally intelligent NPCs and procedurally
generated storytelling to AI-driven adaptive systems and inclusive gameplay
interfaces. We also address the major challenges facing this convergence,
including real-time performance constraints, memory limitations, ethical risks,
and scalability barriers. Our findings highlight that while LLMs significantly
enhance realism, creativity, and user engagement in VR environments, their
effective deployment requires robust design strategies that integrate
multimodal interaction, hybrid AI architectures, and ethical safeguards. The
paper concludes by outlining future research directions in multimodal AI,
affective computing, reinforcement learning, and open-source development,
aiming to guide the responsible advancement of intelligent and inclusive VR
systems.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [155] [Generative Logic: A New Computer Architecture for Deterministic Reasoning and Knowledge Generation](https://arxiv.org/abs/2508.00017)
*Nikolai Sergeev*

Main category: cs.LO

TL;DR: Generative Logic (GL) 是一种确定性架构，通过用户提供的公理化定义生成可验证的证明图，并在原型中成功应用于一阶Peano算术。


<details>
  <summary>Details</summary>
Motivation: 旨在通过系统化的方法探索公理化定义的演绎邻域，生成可重放、可审计的证明图。

Method: 将定义编译为分布式逻辑块网格，通过消息交换和统一规则生成新事实，并导出为可导航的HTML证明图。

Result: 成功自动重建了加法、乘法的结合律、交换律以及分配律等基础算术定律的机器可检查证明。

Conclusion: GL 展示了硬件-软件协同设计的潜力，并可能结合概率模型（如LLMs）用于自动形式化和猜想生成。

Abstract: We present Generative Logic (GL), a deterministic architecture that begins
from user-supplied axiomatic definitions -- written in a minimalist
Mathematical Programming Language (MPL) -- and systematically explores their
deductive neighborhood. Definitions are compiled into a distributed grid of
simple Logic Blocks (LBs) that exchange messages; any time several expressions
unify under an inference rule, a new fact is emitted with full provenance to
its sources, yielding replayable, auditable proof graphs.
  A prototype software implementation instantiates the workflow on first-order
Peano arithmetic. Starting only from the Peano axioms, GL enumerates candidate
implications, applies normalization and type filters, and automatically
reconstructs machine-checkable proofs of foundational arithmetic laws including
associativity and commutativity of addition, associativity and commutativity of
multiplication, and distributivity. Generated proofs export to navigable HTML
so that every inference step can be inspected independently.
  We outline a hardware-software co-design path toward massively parallel
realizations and describe prospective integration with probabilistic models
(e.g., Large Language Models (LLMs)) for autoformalization and conjecture
seeding. The Python and MPL code to reproduce the Peano experiments, along with
the full HTML proof graphs, are available in the project's GitHub repository at
https://github.com/Generative-Logic/GL/tree/35a111ea9ba53afe051703d6050be0c3923e9724
and are permanently archived at https://doi.org/10.5281/zenodo.16408441. We
invite community feedback and collaboration.

</details>


### [156] [Analysing Temporal Reasoning in Description Logics Using Formal Grammars](https://arxiv.org/abs/2508.00575)
*Camille Bourgaux,Anton Gnatenko,Michaël Thomazo*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We establish a correspondence between (fragments of)
$\mathcal{TEL}^\bigcirc$, a temporal extension of the $\mathcal{EL}$
description logic with the LTL operator $\bigcirc^k$, and some specific kinds
of formal grammars, in particular, conjunctive grammars (context-free grammars
equipped with the operation of intersection). This connection implies that
$\mathcal{TEL}^\bigcirc$ does not possess the property of ultimate periodicity
of models, and further leads to undecidability of query answering in
$\mathcal{TEL}^\bigcirc$, closing a question left open since the introduction
of $\mathcal{TEL}^\bigcirc$. Moreover, it also allows to establish decidability
of query answering for some new interesting fragments of
$\mathcal{TEL}^\bigcirc$, and to reuse for this purpose existing tools and
algorithms for conjunctive grammars.

</details>


### [157] [Loop Invariant Generation: A Hybrid Framework of Reasoning optimised LLMs and SMT Solvers](https://arxiv.org/abs/2508.00419)
*Varun Bharti,Shashwat Jha,Dhruv Kumar,Pankaj Jalote*

Main category: cs.LO

TL;DR: 论文探讨了利用推理优化的大型语言模型（LLMs）结合Z3 SMT求解器自动合成循环不变式的方法，在Code2Inv基准测试中实现了100%覆盖率。


<details>
  <summary>Details</summary>
Motivation: 循环不变式对验证程序正确性至关重要，但自动合成仍具挑战性。现有方法仅适用于部分标准基准，因此研究LLMs是否能提升性能。

Method: 将OpenAI的O1、O1-mini和O3-mini模型与Z3 SMT求解器结合，通过生成-检查迭代优化不变式。

Result: 在133个任务中实现100%覆盖率（133/133），优于之前的107/133，且每次仅需1-2次模型提议和14-55秒时间。

Conclusion: LLMs具有潜在逻辑推理能力，可自动化循环不变式合成，且方法可推广至其他命令式语言。

Abstract: Loop invariants are essential for proving the correctness of programs with
loops. Developing loop invariants is challenging, and fully automatic synthesis
cannot be guaranteed for arbitrary programs. Some approaches have been proposed
to synthesize loop invariants using symbolic techniques and more recently using
neural approaches. These approaches are able to correctly synthesize loop
invariants only for subsets of standard benchmarks. In this work, we
investigate whether modern, reasoning-optimized large language models can do
better. We integrate OpenAI's O1, O1-mini, and O3-mini into a tightly coupled
generate-and-check pipeline with the Z3 SMT solver, using solver
counterexamples to iteratively guide invariant refinement. We use Code2Inv
benchmark, which provides C programs along with their formal preconditions and
postconditions. On this benchmark of 133 tasks, our framework achieves 100%
coverage (133 out of 133), outperforming the previous best of 107 out of 133,
while requiring only 1-2 model proposals per instance and 14-55 seconds of
wall-clock time. These results demonstrate that LLMs possess latent logical
reasoning capabilities which can help automate loop invariant synthesis. While
our experiments target C-specific programs, this approach should be
generalizable to other imperative languages.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [158] [Leveraging Operator Learning to Accelerate Convergence of the Preconditioned Conjugate Gradient Method](https://arxiv.org/abs/2508.00101)
*Alena Kopaničáková,Youngkyu Lee,George Em Karniadakis*

Main category: math.NA

TL;DR: 提出了一种基于DeepONet的新放气策略，用于加速预条件共轭梯度法（PCG）在参数化大规模线性系统中的收敛。


<details>
  <summary>Details</summary>
Motivation: 传统放气技术依赖特征向量近似或Krylov子空间回收，而新方法通过算子学习（DeepONet）生成放气子空间，以提高效率和泛化能力。

Method: 采用两种互补方法构建放气算子：1）利用DeepONet学习的基函数近似离散PDE算子的近零空间向量；2）直接使用DeepONet预测的解。还提出了增强收敛的稀疏模式策略。

Result: 数值实验表明，该方法在稳态、瞬态、标量和矢量问题中均有效，且能泛化到不同模型参数和问题分辨率。

Conclusion: 基于DeepONet的放气PCG方法显著提升了收敛速度，并展现出广泛的适用性。

Abstract: We propose a new deflation strategy to accelerate the convergence of the
preconditioned conjugate gradient(PCG) method for solving parametric
large-scale linear systems of equations. Unlike traditional deflation
techniques that rely on eigenvector approximations or recycled Krylov
subspaces, we generate the deflation subspaces using operator learning,
specifically the Deep Operator Network~(DeepONet). To this aim, we introduce
two complementary approaches for assembling the deflation operators. The first
approach approximates near-null space vectors of the discrete PDE operator
using the basis functions learned by the DeepONet. The second approach directly
leverages solutions predicted by the DeepONet. To further enhance convergence,
we also propose several strategies for prescribing the sparsity pattern of the
deflation operator. A comprehensive set of numerical experiments encompassing
steady-state, time-dependent, scalar, and vector-valued problems posed on both
structured and unstructured geometries is presented and demonstrates the
effectiveness of the proposed DeepONet-based deflated PCG method, as well as
its generalization across a wide range of model parameters and problem
resolutions.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [159] [Jet Image Generation in High Energy Physics Using Diffusion Models](https://arxiv.org/abs/2508.00250)
*Victor D. Martinez,Vidya Manian,Sudhir Malik*

Main category: hep-ph

TL;DR: 本文首次将扩散模型应用于生成LHC质子-质子碰撞事件的喷注图像，比较了基于分数的扩散模型和一致性模型的性能，发现一致性模型在生成质量和稳定性上更优。


<details>
  <summary>Details</summary>
Motivation: 研究旨在利用扩散模型生成高能物理实验中的喷注图像，以改进计算效率和生成准确性。

Method: 将喷注的动力学变量映射为二维图像，训练扩散模型学习喷注成分的空间分布，并比较分数扩散模型和一致性模型的性能。

Result: 一致性模型在生成图像的保真度和稳定性上优于分数扩散模型，FID指标证明了其优越性。

Conclusion: 该方法为高能物理研究提供了更高效和准确的工具，一致性模型表现更佳。

Abstract: This article presents, for the first time, the application of diffusion
models for generating jet images corresponding to proton-proton collision
events at the Large Hadron Collider (LHC). The kinematic variables of quark,
gluon, W-boson, Z-boson, and top quark jets from the JetNet simulation dataset
are mapped to two-dimensional image representations. Diffusion models are
trained on these images to learn the spatial distribution of jet constituents.
We compare the performance of score-based diffusion models and consistency
models in accurately generating class-conditional jet images. Unlike approaches
based on latent distributions, our method operates directly in image space. The
fidelity of the generated images is evaluated using several metrics, including
the Fr\'echet Inception Distance (FID), which demonstrates that consistency
models achieve higher fidelity and generation stability compared to score-based
diffusion models. These advancements offer significant improvements in
computational efficiency and generation accuracy, providing valuable tools for
High Energy Physics (HEP) research.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [160] [Data-Driven Motion Planning for Uncertain Nonlinear Systems](https://arxiv.org/abs/2508.00154)
*Babak Esmaeili,Hamidreza Modares,Stefano Di Cairano*

Main category: eess.SY

TL;DR: 提出了一种基于数据驱动的非线性系统运动规划框架，通过构建重叠不变多面体序列实现安全路径规划。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖系统动力学模型，而本文旨在仅通过数据计算安全区域并设计状态反馈控制器。

Method: 围绕随机采样的路径点，识别凸可行区域，通过数据驱动的线性矩阵不等式问题学习椭圆不变集及其局部状态反馈增益，并近似为多面体。

Result: 仿真验证了该方法在复杂非线性系统中实现安全、动态可行路径的有效性。

Conclusion: 该方法无需系统模型，仅依赖数据即可实现安全运动规划。

Abstract: This paper proposes a data-driven motion-planning framework for nonlinear
systems that constructs a sequence of overlapping invariant polytopes. Around
each randomly sampled waypoint, the algorithm identifies a convex admissible
region and solves data-driven linear-matrix-inequality problems to learn
several ellipsoidal invariant sets together with their local state-feedback
gains. The convex hull of these ellipsoids, still invariant under a
piece-wise-affine controller obtained by interpolating the gains, is then
approximated by a polytope. Safe transitions between nodes are ensured by
verifying the intersection of consecutive convex-hull polytopes and introducing
an intermediate node for a smooth transition. Control gains are interpolated in
real time via simplex-based interpolation, keeping the state inside the
invariant polytopes throughout the motion. Unlike traditional approaches that
rely on system dynamics models, our method requires only data to compute safe
regions and design state-feedback controllers. The approach is validated
through simulations, demonstrating the effectiveness of the proposed method in
achieving safe, dynamically feasible paths for complex nonlinear systems.

</details>


### [161] [Learning to optimize with guarantees: a complete characterization of linearly convergent algorithms](https://arxiv.org/abs/2508.00775)
*Andrea Martin,Ian R. Manchester,Luca Furieri*

Main category: eess.SY

TL;DR: 论文提出了一种方法，在保持最坏情况收敛保证的同时，通过修改基线线性收敛算法的更新规则，提升其在特定目标问题上的平均性能。


<details>
  <summary>Details</summary>
Motivation: 在高风险工程应用中，优化算法需具备最坏情况下的理论保证，但为最坏情况设计往往牺牲了实际常见问题实例的性能。本文旨在解决这一问题。

Method: 从基线线性收敛算法出发，推导出所有且仅能保持其收敛性质的更新规则修改，适用于非光滑复合优化问题。

Result: 方法在解决线性方程组和模型预测控制（MPC）问题时表现出有效性。

Conclusion: 该方法成功地在保持最坏情况保证的同时，提升了特定问题的平均性能，适用于多种优化算法。

Abstract: In high-stakes engineering applications, optimization algorithms must come
with provable worst-case guarantees over a mathematically defined class of
problems. Designing for the worst case, however, inevitably sacrifices
performance on the specific problem instances that often occur in practice. We
address the problem of augmenting a given linearly convergent algorithm to
improve its average-case performance on a restricted set of target problems -
for example, tailoring an off-the-shelf solver for model predictive control
(MPC) for an application to a specific dynamical system - while preserving its
worst-case guarantees across the entire problem class. Toward this goal, we
characterize the class of algorithms that achieve linear convergence for
classes of nonsmooth composite optimization problems. In particular, starting
from a baseline linearly convergent algorithm, we derive all - and only - the
modifications to its update rule that maintain its convergence properties. Our
results apply to augmenting legacy algorithms such as gradient descent for
nonconvex, gradient-dominated functions; Nesterov's accelerated method for
strongly convex functions; and projected methods for optimization over
polyhedral feasibility sets. We showcase effectiveness of the approach on
solving optimization problems with tight iteration budgets in application to
ill-conditioned systems of linear equations and MPC for linear systems.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [162] [MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for Document Question-Answering with Hierarchical Index and Multi-Granularity Retrieval](https://arxiv.org/abs/2508.00579)
*Ziyu Gong,Yihua Huang,Chengcheng Mai*

Main category: cs.MM

TL;DR: 提出了一种名为MMRAG-DocQA的多模态RAG模型，通过结合文本和视觉信息解决长文档问答中的幻觉和模态断开问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法（LVLM和RAG）在长文档问答中存在幻觉和模态断开问题，需要一种更有效的解决方案。

Method: 设计了分层索引方法，结合页面内多模态关联和跨页面依赖，提出多粒度语义检索方法。

Result: 在公开数据集MMLongBench-Doc和LongDocURL上验证了MMRAG-DocQA的优越性。

Conclusion: MMRAG-DocQA能有效理解和回答多模态、多页文档问题。

Abstract: The multi-modal long-context document question-answering task aims to locate
and integrate multi-modal evidences (such as texts, tables, charts, images, and
layouts) distributed across multiple pages, for question understanding and
answer generation. The existing methods can be categorized into Large
Vision-Language Model (LVLM)-based and Retrieval-Augmented Generation
(RAG)-based methods. However, the former were susceptible to hallucinations,
while the latter struggled for inter-modal disconnection and cross-page
fragmentation. To address these challenges, a novel multi-modal RAG model,
named MMRAG-DocQA, was proposed, leveraging both textual and visual information
across long-range pages to facilitate accurate question answering. A
hierarchical indexing method with the integration of flattened in-page chunks
and topological cross-page chunks was designed to jointly establish in-page
multi-modal associations and long-distance cross-page dependencies. By means of
joint similarity evaluation and large language model (LLM)-based re-ranking, a
multi-granularity semantic retrieval method, including the page-level parent
page retrieval and document-level summary retrieval, was proposed to foster
multi-modal evidence connection and long-distance evidence integration and
reasoning. Experimental results performed on public datasets, MMLongBench-Doc
and LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in
understanding and answering modality-rich and multi-page documents.

</details>


<div id='math.ST'></div>

# math.ST [[Back]](#toc)

### [163] [Constructive Disintegration and Conditional Modes](https://arxiv.org/abs/2508.00617)
*Nathaël Da Costa,Marvin Pförtner,Jon Cockayne*

Main category: math.ST

TL;DR: 论文探讨了贝叶斯统计中条件化的核心操作——测度分解的构造问题，指出传统机器学习方法中限制概率密度函数的误区，并提供了数学工具用于构造分解。通过实例展示了限制密度与分解密度的显著差异，并讨论了条件模式与分解测度模式的不一致性及其实际影响。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于贝叶斯统计中条件化操作的实际困难，以及传统方法在构造测度分解时的局限性。

Method: 论文提供了一套数学工具，用于构造测度分解，并应用于可微流形上的密度计算。通过实例分析限制密度与分解密度的差异。

Result: 结果显示限制密度与分解密度在特定情况下存在显著差异，且条件模式与分解测度模式不一致。

Conclusion: 结论强调了两种方法在不同建模场景下的实用性，并呼吁根据具体需求选择合适的方法。

Abstract: Conditioning, the central operation in Bayesian statistics, is formalised by
the notion of disintegration of measures. However, due to the implicit nature
of their definition, constructing disintegrations is often difficult. A
folklore result in machine learning conflates the construction of a
disintegration with the restriction of probability density functions onto the
subset of events that are consistent with a given observation. We provide a
comprehensive set of mathematical tools which can be used to construct
disintegrations and apply these to find densities of disintegrations on
differentiable manifolds. Using our results, we provide a disturbingly simple
example in which the restricted density and the disintegration density
drastically disagree. Motivated by applications in approximate Bayesian
inference and Bayesian inverse problems, we further study the modes of
disintegrations. We show that the recently introduced notion of a "conditional
mode" does not coincide in general with the modes of the conditional measure
obtained through disintegration, but rather the modes of the restricted
measure. We also discuss the implications of the discrepancy between the two
measures in practice, advocating for the utility of both approaches depending
on the modelling context.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [164] [GEPAR3D: Geometry Prior-Assisted Learning for 3D Tooth Segmentation](https://arxiv.org/abs/2508.00155)
*Tomasz Szczepański,Szymon Płotka,Michal K. Grzeszczyk,Arleta Adamowicz,Piotr Fudalej,Przemysław Korzeniowski,Tomasz Trzciński,Arkadiusz Sitek*

Main category: eess.IV

TL;DR: GEPAR3D是一种新颖的CBCT牙齿分割方法，结合实例检测和多类分割，显著提高了根尖分割的准确性。


<details>
  <summary>Details</summary>
Motivation: CBCT中牙齿分割（尤其是根尖）对正畸中根吸收评估至关重要，但现有方法难以处理精细结构。

Method: GEPAR3D结合统计形状模型作为几何先验，采用深度分水岭方法建模牙齿为3D能量盆地，优化分割。

Result: 在多个测试集上，GEPAR3D平均DSC达95.0%，召回率95.2%，显著优于其他方法。

Conclusion: GEPAR3D在根尖分割上表现优异，有望提升临床决策准确性，代码和数据集已开源。

Abstract: Tooth segmentation in Cone-Beam Computed Tomography (CBCT) remains
challenging, especially for fine structures like root apices, which is critical
for assessing root resorption in orthodontics. We introduce GEPAR3D, a novel
approach that unifies instance detection and multi-class segmentation into a
single step tailored to improve root segmentation. Our method integrates a
Statistical Shape Model of dentition as a geometric prior, capturing anatomical
context and morphological consistency without enforcing restrictive adjacency
constraints. We leverage a deep watershed method, modeling each tooth as a
continuous 3D energy basin encoding voxel distances to boundaries. This
instance-aware representation ensures accurate segmentation of narrow, complex
root apices. Trained on publicly available CBCT scans from a single center, our
method is evaluated on external test sets from two in-house and two public
medical centers. GEPAR3D achieves the highest overall segmentation performance,
averaging a Dice Similarity Coefficient (DSC) of 95.0% (+2.8% over the
second-best method) and increasing recall to 95.2% (+9.5%) across all test
sets. Qualitative analyses demonstrated substantial improvements in root
segmentation quality, indicating significant potential for more accurate root
resorption assessment and enhanced clinical decision-making in orthodontics. We
provide the implementation and dataset at https://github.com/tomek1911/GEPAR3D.

</details>


### [165] [Weakly Supervised Intracranial Aneurysm Detection and Segmentation in MR angiography via Multi-task UNet with Vesselness Prior](https://arxiv.org/abs/2508.00235)
*Erin Rainville,Amirhossein Rasoulian,Hassan Rivaz,Yiming Xiao*

Main category: eess.IV

TL;DR: 提出了一种弱监督的3D多任务UNet模型，结合血管先验知识，用于颅内动脉瘤的检测和分割，在TOF-MRA中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 颅内动脉瘤（IAs）在影像中难以准确检测和分析，且缺乏标注数据，因此需要开发高效的深度学习算法。

Method: 使用Frangi血管过滤器生成软血管先验，结合多任务UNet进行检测和分割，并在Lausanne数据集上训练和测试。

Result: 在分割（Dice = 0.614，95%HD =1.38mm）和检测（假阳性率 = 1.47，灵敏度 = 92.9%）上优于现有技术。

Conclusion: 该方法在颅内动脉瘤的检测和分割中表现出色，具有较高的临床潜力。

Abstract: Intracranial aneurysms (IAs) are abnormal dilations of cerebral blood vessels
that, if ruptured, can lead to life-threatening consequences. However, their
small size and soft contrast in radiological scans often make it difficult to
perform accurate and efficient detection and morphological analyses, which are
critical in the clinical care of the disorder. Furthermore, the lack of large
public datasets with voxel-wise expert annotations pose challenges for
developing deep learning algorithms to address the issues. Therefore, we
proposed a novel weakly supervised 3D multi-task UNet that integrates
vesselness priors to jointly perform aneurysm detection and segmentation in
time-of-flight MR angiography (TOF-MRA). Specifically, to robustly guide IA
detection and segmentation, we employ the popular Frangi's vesselness filter to
derive soft cerebrovascular priors for both network input and an attention
block to conduct segmentation from the decoder and detection from an auxiliary
branch. We train our model on the Lausanne dataset with coarse ground truth
segmentation, and evaluate it on the test set with refined labels from the same
database. To further assess our model's generalizability, we also validate it
externally on the ADAM dataset. Our results demonstrate the superior
performance of the proposed technique over the SOTA techniques for aneurysm
segmentation (Dice = 0.614, 95%HD =1.38mm) and detection (false positive rate =
1.47, sensitivity = 92.9%).

</details>


### [166] [FMPlug: Plug-In Foundation Flow-Matching Priors for Inverse Problems](https://arxiv.org/abs/2508.00721)
*Yuxiang Wan,Ryan Devera,Wenjie Zhang,Ju Sun*

Main category: eess.IV

TL;DR: FMPlug是一个新颖的插件框架，通过利用观察与目标对象的相似性和生成流的高斯性，增强基础流匹配先验，解决不适定逆问题。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖领域特定或无训练先验，FMPlug通过简单但强大的洞察力，释放领域无关基础模型的潜力。

Method: 引入时间自适应预热策略和尖锐高斯性正则化，优化基础流匹配先验。

Result: 在图像超分辨率和高斯去模糊任务上显著优于现有方法。

Conclusion: FMPlug通过新颖策略和正则化，成功提升了基础模型的性能。

Abstract: We present FMPlug, a novel plug-in framework that enhances foundation
flow-matching (FM) priors for solving ill-posed inverse problems. Unlike
traditional approaches that rely on domain-specific or untrained priors, FMPlug
smartly leverages two simple but powerful insights: the similarity between
observed and desired objects and the Gaussianity of generative flows. By
introducing a time-adaptive warm-up strategy and sharp Gaussianity
regularization, FMPlug unlocks the true potential of domain-agnostic foundation
models. Our method beats state-of-the-art methods that use foundation FM priors
by significant margins, on image super-resolution and Gaussian deblurring.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [167] [funOCLUST: Clustering Functional Data with Outliers](https://arxiv.org/abs/2508.00110)
*Katharine M. Clark,Paul D. McNicholas*

Main category: stat.ML

TL;DR: 提出了一种基于OCLUST算法的功能数据聚类方法，解决了高维和异常值敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 功能数据的高维性和对异常值的敏感性给聚类带来挑战。

Method: 扩展OCLUST算法，创建了一种鲁棒的功能数据聚类和异常值修剪方法。

Result: 在模拟和真实数据集上验证了方法的聚类和异常值识别性能。

Conclusion: 该方法在功能数据聚类和异常值处理中表现出色。

Abstract: Functional data present unique challenges for clustering due to their
infinite-dimensional nature and potential sensitivity to outliers. An extension
of the OCLUST algorithm to the functional setting is proposed to address these
issues. The approach leverages the OCLUST framework, creating a robust method
to cluster curves and trim outliers. The methodology is evaluated on both
simulated and real-world functional datasets, demonstrating strong performance
in clustering and outlier identification.

</details>


### [168] [Sinusoidal Approximation Theorem for Kolmogorov-Arnold Networks](https://arxiv.org/abs/2508.00247)
*Sergei Gleyzer,Hanh Nguyen,Dinesh P. Ramakrishnan,Eric A. F. Reinhardt*

Main category: stat.ML

TL;DR: 论文提出了一种新的Kolmogorov-Arnold网络（KAN）变体，用可学习频率的正弦函数替代传统KAN中的样条函数，并在理论和实验上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统多层感知机（MLP）和KAN在函数逼近能力上存在局限性，本文旨在通过改进KAN的激活函数设计，提升其性能。

Method: 提出了一种新的KAN变体，用加权正弦函数替代内外部函数，并固定相位为线性间隔常数。通过理论证明和数值实验验证其有效性。

Result: 新方法在多元函数逼近任务中优于固定频率傅里叶变换方法，与MLP性能相当。

Conclusion: 改进的KAN在理论和实验上均表现出色，为神经网络设计提供了新的方向。

Abstract: The Kolmogorov-Arnold representation theorem states that any continuous
multivariable function can be exactly represented as a finite superposition of
continuous single variable functions. Subsequent simplifications of this
representation involve expressing these functions as parameterized sums of a
smaller number of unique monotonic functions. These developments led to the
proof of the universal approximation capabilities of multilayer perceptron
networks with sigmoidal activations, forming the alternative theoretical
direction of most modern neural networks.
  Kolmogorov-Arnold Networks (KANs) have been recently proposed as an
alternative to multilayer perceptrons. KANs feature learnable nonlinear
activations applied directly to input values, modeled as weighted sums of basis
spline functions. This approach replaces the linear transformations and
sigmoidal post-activations used in traditional perceptrons. Subsequent works
have explored alternatives to spline-based activations. In this work, we
propose a novel KAN variant by replacing both the inner and outer functions in
the Kolmogorov-Arnold representation with weighted sinusoidal functions of
learnable frequencies. Inspired by simplifications introduced by Lorentz and
Sprecher, we fix the phases of the sinusoidal activations to linearly spaced
constant values and provide a proof of its theoretical validity. We also
conduct numerical experiments to evaluate its performance on a range of
multivariable functions, comparing it with fixed-frequency Fourier transform
methods and multilayer perceptrons (MLPs). We show that it outperforms the
fixed-frequency Fourier transform and achieves comparable performance to MLPs.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [169] [Riemannian Optimization for Distance Geometry: A Study of Convergence, Robustness, and Incoherence](https://arxiv.org/abs/2508.00091)
*Chandler Smith,HanQin Cai,Abiy Tasissa*

Main category: math.OC

TL;DR: 本文提出了一种基于黎曼优化的框架，通过将欧几里得距离几何问题（EDG）转化为半正定Gram矩阵的低秩矩阵补全任务来解决。该方法在非正交基中编码距离测量，并通过黎曼梯度下降实现局部线性收敛。


<details>
  <summary>Details</summary>
Motivation: EDG问题在传感器网络定位、分子构象和流形学习等领域有广泛应用，但现有方法在效率和准确性上存在局限。

Method: 将EDG问题转化为Gram矩阵的低秩补全任务，利用非正交基编码距离测量，并通过黎曼梯度下降优化。

Result: 在伯努利采样模型下，证明了算法的局部线性收敛性，并在合成数据上验证了其性能优于现有方法。

Conclusion: 该方法为EDG问题提供了一种高效且理论保证的解决方案，特别适用于低采样率场景。

Abstract: The problem of recovering a configuration of points from partial pairwise
distances, referred to as the Euclidean Distance Geometry (EDG) problem, arises
in a broad range of applications, including sensor network localization,
molecular conformation, and manifold learning. In this paper, we propose a
Riemannian optimization framework for solving the EDG problem by formulating it
as a low-rank matrix completion task over the space of positive semi-definite
Gram matrices. The available distance measurements are encoded as expansion
coefficients in a non-orthogonal basis, and optimization over the Gram matrix
implicitly enforces geometric consistency through the triangle inequality, a
structure inherited from classical multidimensional scaling. Under a Bernoulli
sampling model for observed distances, we prove that Riemannian gradient
descent on the manifold of rank-$r$ matrices locally converges linearly with
high probability when the sampling probability satisfies $p \geq
\mathcal{O}(\nu^2 r^2 \log(n)/n)$, where $\nu$ is an EDG-specific incoherence
parameter. Furthermore, we provide an initialization candidate using a one-step
hard thresholding procedure that yields convergence, provided the sampling
probability satisfies $p \geq \mathcal{O}(\nu r^{3/2} \log^{3/4}(n)/n^{1/4})$.
A key technical contribution of this work is the analysis of a symmetric linear
operator arising from a dual basis expansion in the non-orthogonal basis, which
requires a novel application of the Hanson--Wright inequality to establish an
optimal restricted isometry property in the presence of coupled terms.
Empirical evaluations on synthetic data demonstrate that our algorithm achieves
competitive performance relative to state-of-the-art methods. Moreover, we
propose a novel notion of matrix incoherence tailored to the EDG setting and
provide robustness guarantees for our method.

</details>


### [170] [Neighbor-Sampling Based Momentum Stochastic Methods for Training Graph Neural Networks](https://arxiv.org/abs/2508.00267)
*Molly Noel,Gabriel Mancino-Ball,Yangyang Xu*

Main category: math.OC

TL;DR: 本文提出了一种基于邻居采样的Adam型随机方法，用于解决非凸GCN训练问题，并通过控制变量技术减少随机误差，实验证明其在大规模图数据集上优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 由于GCN的递归邻域聚合特性，现有高效训练方法缺乏理论保证或缺少现代深度学习算法中的自适应性和动量等关键元素。

Method: 提出几种基于邻居采样的Adam型随机方法，利用控制变量技术减少随机误差。

Result: 在标准假设下，方法具有最优收敛率，实验显示其在大规模图数据集上优于传统NS-based SGD。

Conclusion: 所提方法在理论和实验上均表现出色，尤其适用于大规模图数据。

Abstract: Graph convolutional networks (GCNs) are a powerful tool for graph
representation learning. Due to the recursive neighborhood aggregations
employed by GCNs, efficient training methods suffer from a lack of theoretical
guarantees or are missing important practical elements from modern deep
learning algorithms, such as adaptivity and momentum. In this paper, we present
several neighbor-sampling (NS) based Adam-type stochastic methods for solving a
nonconvex GCN training problem. We utilize the control variate technique
proposed by [1] to reduce the stochastic error caused by neighbor sampling.
Under standard assumptions for Adam-type methods, we show that our methods
enjoy the optimal convergence rate. In addition, we conduct extensive numerical
experiments on node classification tasks with several benchmark datasets. The
results demonstrate superior performance of our methods over classic NS-based
SGD that also uses the control-variate technique, especially for large-scale
graph datasets. Our code is available at https://github.com/RPI-OPT/CV-ADAM-GNN .

</details>


### [171] [Efficient Solving of Large Single Input Superstate Decomposable Markovian Decision Process](https://arxiv.org/abs/2508.00816)
*Youssef Ait El Mahjoub,Jean-Michel Fourneau,Salma Alouah*

Main category: math.OC

TL;DR: 论文提出了一种针对特定结构化MDP（SISDMDP）的高效策略评估方法，结合了单输入分解和单循环递归特性，适用于大规模状态空间和长期优化问题。


<details>
  <summary>Details</summary>
Motivation: 解决大规模状态空间和长期优化MDP问题中策略评估的计算复杂性，尤其是在无限时域设置下。

Method: 定义了SISDMDP，结合单输入分解和单循环递归特性，开发了一种基于结构分解的精确策略评估方法。

Result: 该方法能够高效且精确地评估策略，适用于平均奖励和折扣奖励MDP。

Conclusion: 通过结构分解和集中递归特性，实现了对大规模MDP的高效策略评估，为复杂决策问题提供了可扩展的解决方案。

Abstract: Solving Markov Decision Processes (MDPs) remains a central challenge in
sequential decision-making, especially when dealing with large state spaces and
long-term optimization criteria. A key step in Bellman dynamic programming
algorithms is the policy evaluation, which becomes computationally demanding in
infinite-horizon settings such as average-reward or discounted-reward
formulations. In the context of Markov chains, aggregation and disaggregation
techniques have for a long time been used to reduce complexity by exploiting
structural decompositions. In this work, we extend these principles to a
structured class of MDPs. We define the Single-Input Superstate Decomposable
Markov Decision Process (SISDMDP), which combines Chiu's single-input
decomposition with Robertazzi's single-cycle recurrence property. When a policy
induces this structure, the resulting transition graph can be decomposed into
interacting components with centralized recurrence. We develop an exact and
efficient policy evaluation method based on this structure. This yields a
scalable solution applicable to both average and discounted reward MDPs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [172] [GPT-4.1 Sets the Standard in Automated Experiment Design Using Novel Python Libraries](https://arxiv.org/abs/2508.00033)
*Nuno Fachada,Daniel Fernandes,Carlos M. Fernandes,Bruno D. Ferreira-Saraiva,João P. Matos-Carvalho*

Main category: cs.SE

TL;DR: 该研究系统评估了大型语言模型（LLMs）在生成功能性Python代码方面的能力，特别是在处理不熟悉的API时，发现仅有少数模型（如GPT-4.1）表现稳定。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在科学自动化中生成复杂代码的能力，尤其是在使用不熟悉的Python库时。

Method: 通过零样本提示，测试LLMs在两种复杂任务（数据分析和合成数据生成）中的表现，并定量和定性评估代码的正确性。

Result: 仅有少数模型能稳定生成正确代码，GPT-4.1表现最佳。同时发现第三方库的文档和实现问题。

Conclusion: LLMs在科学自动化中仍有局限，需改进提示设计、库文档和模型能力。

Abstract: Large Language Models (LLMs) have advanced rapidly as tools for automating
code generation in scientific research, yet their ability to interpret and use
unfamiliar Python APIs for complex computational experiments remains poorly
characterized. This study systematically benchmarks a selection of
state-of-the-art LLMs in generating functional Python code for two increasingly
challenging scenarios: conversational data analysis with the \textit{ParShift}
library, and synthetic data generation and clustering using \textit{pyclugen}
and \textit{scikit-learn}. Both experiments use structured, zero-shot prompts
specifying detailed requirements but omitting in-context examples. Model
outputs are evaluated quantitatively for functional correctness and prompt
compliance over multiple runs, and qualitatively by analyzing the errors
produced when code execution fails. Results show that only a small subset of
models consistently generate correct, executable code, with GPT-4.1 standing
out as the only model to always succeed in both tasks. In addition to
benchmarking LLM performance, this approach helps identify shortcomings in
third-party libraries, such as unclear documentation or obscure implementation
bugs. Overall, these findings highlight current limitations of LLMs for
end-to-end scientific automation and emphasize the need for careful prompt
design, comprehensive library documentation, and continued advances in language
model capabilities.

</details>


### [173] [A Survey on Code Generation with LLM-based Agents](https://arxiv.org/abs/2508.00083)
*Yihong Dong,Xue Jiang,Jiaru Qian,Tian Wang,Kechi Zhang,Zhi Jin,Ge Li*

Main category: cs.SE

TL;DR: 本文系统综述了基于大语言模型（LLM）的代码生成代理技术，包括其核心特征、发展历程、技术分类、应用场景、评估工具及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 代码生成代理因其自主性、任务范围扩展和工程实用性增强等特点，正在改变软件开发范式，亟需系统梳理其技术进展和应用潜力。

Method: 通过系统调查，追溯技术发展历程，分类核心技术（单代理与多代理架构），并详细描述其在软件开发生命周期（SDLC）中的应用。

Result: 总结了主流评估基准和工具，分析了当前挑战，并提出了未来研究的长期方向。

Conclusion: 基于LLM的代码生成代理具有广阔应用前景，但需解决基础性挑战以推动其长期发展。

Abstract: Code generation agents powered by large language models (LLMs) are
revolutionizing the software development paradigm. Distinct from previous code
generation techniques, code generation agents are characterized by three core
features. 1) Autonomy: the ability to independently manage the entire workflow,
from task decomposition to coding and debugging. 2) Expanded task scope:
capabilities that extend beyond generating code snippets to encompass the full
software development lifecycle (SDLC). 3) Enhancement of engineering
practicality: a shift in research emphasis from algorithmic innovation toward
practical engineering challenges, such as system reliability, process
management, and tool integration. This domain has recently witnessed rapid
development and an explosion in research, demonstrating significant application
potential. This paper presents a systematic survey of the field of LLM-based
code generation agents. We trace the technology's developmental trajectory from
its inception and systematically categorize its core techniques, including both
single-agent and multi-agent architectures. Furthermore, this survey details
the applications of LLM-based agents across the full SDLC, summarizes
mainstream evaluation benchmarks and metrics, and catalogs representative
tools. Finally, by analyzing the primary challenges, we identify and propose
several foundational, long-term research directions for the future work of the
field.

</details>


### [174] [Benchmarking LLMs for Unit Test Generation from Real-World Functions](https://arxiv.org/abs/2508.00408)
*Dong Huang,Jie M. Zhang,Mark Harman,Qianru Zhang,Mingzhe Du,See-Kiong Ng*

Main category: cs.SE

TL;DR: 论文提出了ULT（UnLeakedTestbench）基准，用于评估大语言模型在单元测试生成中的能力，解决了现有基准的数据污染和结构简单性问题。


<details>
  <summary>Details</summary>
Motivation: 现有基准存在数据污染和结构简单性问题，导致科学结论不可靠，无法反映真实场景。

Method: 通过多阶段筛选构建ULT，包含3,909个高复杂度的Python函数任务，并引入PLT作为对照基准。

Result: ULT的测试生成结果（准确率41.32%、语句覆盖率45.10%等）显著低于TestEval和PLT，表明其更具挑战性。

Conclusion: ULT提供了更真实和具有挑战性的评估标准，有助于更准确地分析大语言模型的测试生成能力。

Abstract: Recently, large language models (LLMs) have shown great promise in automating
unit test generation, significantly reducing the manual effort required by
developers. To effectively evaluate the capabilities of LLMs in this domain, it
is crucial to have a well-designed benchmark that accurately reflects
real-world scenarios and mitigates common pitfalls. Existing LLM test
generation benchmarks are limited by two critical drawbacks: data contamination
and structurally simple function code. As a result, we often cannot rely on the
validity of scientific conclusions drawn from empirical studies using these
limited benchmarks. The empirical evidence presented may be biased due to
contamination and may fail to generalize beyond toy programs due to structural
simplicity.
  To address these problems, we introduce ULT (UnLeakedTestbench), a new
benchmark specifically designed for function-level unit test generation from
real-world Python functions. ULT is constructed through a multi-stage curation
process that ensures high cyclomatic complexity and mitigates test case
contamination. With 3,909 carefully selected function-level tasks, ULT provides
a more realistic and challenging evaluation of LLMs' test generation
capabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT
with leaked tests designed to enable a controlled analysis of memorization
versus reasoning in test generation. Our evaluation results demonstrate that
ULT is significantly more challenging. For example, test cases generated by
LLMs only achieve 41.32\%, 45.10\%, 30.22\%, and 40.21\% for accuracy,
statement coverage, branch coverage, and mutation score on average for all
LLMs, respectively. These results are substantially lower than the
corresponding metrics on TestEval (91.79\%, 92.18\%, 82.04\%, and 49.69\%) and
PLT (47.07\%, 55.13\%, 40.07\%, and 50.80\%).

</details>


### [175] [Accurate and Consistent Graph Model Generation from Text with Large Language Models](https://arxiv.org/abs/2508.00255)
*Boqi Chen,Ou Wei,Bingzhou Zheng,Gunter Mussbacher*

Main category: cs.SE

TL;DR: 提出了一种基于抽象-具体化框架的方法，通过聚合LLM的多个输出来提升生成图模型的一致性和质量。


<details>
  <summary>Details</summary>
Motivation: 解决LLM生成图模型时存在的语法违规、约束不一致和准确性不足的问题。

Method: 采用抽象-具体化框架，先构建概率部分模型，再优化为满足约束的具体模型。

Result: 实验表明，该方法显著提高了生成图模型的一致性和质量。

Conclusion: 该方法有效解决了LLM生成图模型的主要问题，提升了模型的实用性。

Abstract: Graph model generation from natural language description is an important task
with many applications in software engineering. With the rise of large language
models (LLMs), there is a growing interest in using LLMs for graph model
generation. Nevertheless, LLM-based graph model generation typically produces
partially correct models that suffer from three main issues: (1) syntax
violations: the generated model may not adhere to the syntax defined by its
metamodel, (2) constraint inconsistencies: the structure of the model might not
conform to some domain-specific constraints, and (3) inaccuracy: due to the
inherent uncertainty in LLMs, the models can include inaccurate, hallucinated
elements. While the first issue is often addressed through techniques such as
constraint decoding or filtering, the latter two remain largely unaddressed.
Motivated by recent self-consistency approaches in LLMs, we propose a novel
abstraction-concretization framework that enhances the consistency and quality
of generated graph models by considering multiple outputs from an LLM. Our
approach first constructs a probabilistic partial model that aggregates all
candidate outputs and then refines this partial model into the most appropriate
concrete model that satisfies all constraints. We evaluate our framework on
several popular open-source and closed-source LLMs using diverse datasets for
model generation tasks. The results demonstrate that our approach significantly
improves both the consistency and quality of the generated graph models.

</details>


### [176] [SPENCER: Self-Adaptive Model Distillation for Efficient Code Retrieval](https://arxiv.org/abs/2508.00546)
*Wenchao Gu,Zongyi Lyu,Yanlin Wang,Hongyu Zhang,Cuiyun Gao,Michael R. Lyu*

Main category: cs.SE

TL;DR: 论文提出SPENCER框架，结合双编码器和交叉编码器提升代码检索性能，并通过模型蒸馏技术提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有双编码器模型在代码检索中缺乏底层交互，限制了性能。

Method: SPENCER框架结合双编码器和交叉编码器，并引入自适应模型蒸馏技术。

Result: 实验表明，结合双编码器和交叉编码器性能更优，模型蒸馏技术减少70%推理时间且保留98%性能。

Conclusion: SPENCER框架在性能和效率上均优于传统双编码器模型。

Abstract: Code retrieval aims to provide users with desired code snippets based on
users' natural language queries. With the development of deep learning
technologies, adopting pre-trained models for this task has become mainstream.
Considering the retrieval efficiency, most of the previous approaches adopt a
dual-encoder for this task, which encodes the description and code snippet into
representation vectors, respectively. However, the model structure of the
dual-encoder tends to limit the model's performance, since it lacks the
interaction between the code snippet and description at the bottom layer of the
model during training. To improve the model's effectiveness while preserving
its efficiency, we propose a framework, which adopts Self-AdaPtive Model
Distillation for Efficient CodE Retrieval, named SPENCER. SPENCER first adopts
the dual-encoder to narrow the search space and then adopts the cross-encoder
to improve accuracy. To improve the efficiency of SPENCER, we propose a novel
model distillation technique, which can greatly reduce the inference time of
the dual-encoder while maintaining the overall performance. We also propose a
teaching assistant selection strategy for our model distillation, which can
adaptively select the suitable teaching assistant models for different
pre-trained models during the model distillation to ensure the model
performance. Extensive experiments demonstrate that the combination of
dual-encoder and cross-encoder improves overall performance compared to solely
dual-encoder-based models for code retrieval. Besides, our model distillation
technique retains over 98% of the overall performance while reducing the
inference time of the dual-encoder by 70%.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [177] [Agent Network Protocol Technical White Paper](https://arxiv.org/abs/2508.00007)
*Gaowei Chang,Eidan Lin,Chengxuan Yuan,Rizhao Cai,Binbin Chen,Xuan Xie,Yin Zhang*

Main category: cs.NI

TL;DR: ANP提出了一种面向Agentic Web的新一代通信协议，解决现有互联网基础设施对大规模Agent互联与协作的不足。


<details>
  <summary>Details</summary>
Motivation: 现有互联网基础设施主要为人类交互设计，导致Agent间数据孤岛、协作成本高，难以支持大规模互联需求。

Method: ANP采用AI原生设计，兼容现有协议，模块化架构，三层协议系统解决身份认证、动态协商和能力发现。

Result: ANP系统性地解决了Agent身份认证、动态协商和能力互操作性问题。

Conclusion: ANP顺应互联网核心趋势，为Agentic Web提供了高效、可扩展的通信协议。

Abstract: With the development of large models and autonomous decision-making AI,
agents are rapidly becoming the new entities of the internet, following mobile
apps. However, existing internet infrastructure is primarily designed for human
interaction, creating data silos, unfriendly interfaces, and high collaboration
costs among agents, making it difficult to support the needs for large-scale
agent interconnection and collaboration. The internet is undergoing a profound
transformation, showing four core trends: agents replacing traditional
software, universal agent interconnection, native protocol-based connections,
and autonomous agent organization and collaboration. To align with these
trends, Agent Network Protocol (ANP) proposes a new generation of communication
protocols for the Agentic Web. ANP adheres to AI-native design, maintains
compatibility with existing internet protocols, adopts a modular composable
architecture, follows minimalist yet extensible principles, and enables rapid
deployment based on existing infrastructure. Through a three-layer protocol
system--identity and encrypted communication layer, meta-protocol negotiation
layer, and application protocol layer--ANP. systematically solves the problems
of agent identity authentication, dynamic negotiation, and capability discovery
interoperability.

</details>


### [178] [Enabling Immersive XR Collaborations over FTTR Networks (Invited)](https://arxiv.org/abs/2508.00009)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: 本文探讨了FTTR（光纤到房间）在实现室内扩展现实协作中的潜力，提出了预测带宽分配和无缝切换方案，以实现高质量的沉浸式体验。


<details>
  <summary>Details</summary>
Motivation: 研究FTTR作为实现室内扩展现实协作的潜在解决方案，以提升沉浸式体验的质量。

Method: 提出了预测带宽分配和无缝切换的方案。

Result: 展示了通过FTTR可以实现高质量的沉浸式协作体验。

Conclusion: FTTR结合预测带宽分配和无缝切换方案，是提升室内扩展现实协作体验的有效方法。

Abstract: Fiber-To-The-Room is a potential solution to achieve in-premise extended
reality collaborations. This paper explores predictive bandwidth allocation and
seamless handover schemes over FTTR, showing high-quality immersive experience
for in-premise collaborations can be achieved. \c{opyright} 2025 The Author(s).

</details>


### [179] [AoI-Aware Resource Allocation with Deep Reinforcement Learning for HAPS-V2X Networks](https://arxiv.org/abs/2508.00011)
*Ahmet Melih Ince,Ayse Elif Canbilen,Halim Yanikomeroglu*

Main category: cs.NI

TL;DR: 本文提出了一种基于深度确定性策略梯度（DDPG）的强化学习方法，用于优化HAPS支持的V2X网络中的信息新鲜度（AoI）。


<details>
  <summary>Details</summary>
Motivation: 6G网络需要满足高可靠低延迟通信（HRLLC）的需求，尤其是在自动驾驶等安全关键应用中。非地面网络（NTN）的集成可以增强网络冗余，而HAPS因其广覆盖和低延迟特性，特别适合提升通信可靠性和信息新鲜度。

Method: 采用DDPG算法，动态优化HAPS支持的V2X网络中的AoI，通过独立学习实现无需集中协调的资源分配。

Result: 研究表明，HAPS与DDPG结合的方法能有效提升信息新鲜度和网络可靠性，适用于基于车队的自动驾驶系统。

Conclusion: HAPS支持的解决方案与DDPG结合，为AoI感知的资源分配提供了高效途径，尤其在基础设施受限地区表现突出。

Abstract: Sixth-generation (6G) networks are designed to meet the hyper-reliable and
low-latency communication (HRLLC) requirements of safety-critical applications
such as autonomous driving. Integrating non-terrestrial networks (NTN) into the
6G infrastructure brings redundancy to the network, ensuring continuity of
communications even under extreme conditions. In particular, high-altitude
platform stations (HAPS) stand out for their wide coverage and low latency
advantages, supporting communication reliability and enhancing information
freshness, especially in rural areas and regions with infrastructure
constraints. In this paper, we present reinforcement learning-based approaches
using deep deterministic policy gradient (DDPG) to dynamically optimize the
age-of-information (AoI) in HAPS-enabled vehicle-to-everything (V2X) networks.
The proposed method improves information freshness and overall network
reliability by enabling independent learning without centralized coordination.
The findings reveal the potential of HAPS-supported solutions, combined with
DDPG-based learning, for efficient AoI-aware resource allocation in
platoon-based autonomous vehicle systems.

</details>


### [180] [Scalable Spectrum Availability Prediction using a Markov Chain Framework and ITU-R Propagation Models](https://arxiv.org/abs/2508.00028)
*Abir Ray*

Main category: cs.NI

TL;DR: 本文提出了一种结合马尔可夫链模型和ITU-R传播模型的可扩展频谱可用性预测框架，用于动态频谱共享系统。


<details>
  <summary>Details</summary>
Motivation: 频谱资源在时间和空间上常未被充分利用，动态频谱访问策略允许次级用户利用空闲频率，但需预测频谱可用性以避免干扰。

Method: 结合两态马尔可夫链模型（捕捉时间占用模式）和ITU-R传播模型（P.528和P.2108，考虑路径损耗和杂波效应），预测频谱机会。

Result: 该方法能高效预测时空频谱可用性，计算成本低，适用于实时频谱管理。

Conclusion: 框架灵活，可适应不同频段和场景，为认知无线电网络等动态频谱共享系统提供有效支持。

Abstract: Spectrum resources are often underutilized across time and space, motivating
dynamic spectrum access strategies that allow secondary users to exploit unused
frequencies. A key challenge is predicting when and where spectrum will be
available (i.e., unused by primary licensed users) in order to enable proactive
and interference-free access. This paper proposes a scalable framework for
spectrum availability prediction that combines a two-state Markov chain model
of primary user activity with high-fidelity propagation models from the ITU-R
(specifically Recommendations P.528 and P.2108). The Markov chain captures
temporal occupancy patterns, while the propagation models incorporate path loss
and clutter effects to determine if primary signals exceed interference
thresholds at secondary user locations. By integrating these components, the
proposed method can predict spectrum opportunities both in time and space with
improved accuracy. We develop the system model and algorithm for the approach,
analyze its scalability and computational efficiency, and discuss assumptions,
limitations, and potential applications. The framework is flexible and can be
adapted to various frequency bands and scenarios. The results and analysis show
that the proposed approach can effectively identify available spectrum with low
computational cost, making it suitable for real-time spectrum management in
cognitive radio networks and other dynamic spectrum sharing systems.

</details>


### [181] [Quality-of-Service Aware LLM Routing for Edge Computing with Multiple Experts](https://arxiv.org/abs/2508.00234)
*Jin Yang,Qiong Wu,Zhiying Feng,Zhi Zhou,Deke Guo,Xu Chen*

Main category: cs.NI

TL;DR: 论文提出了一种基于深度强化学习（DRL）的QoS感知LLM路由框架，以解决边缘LLM服务的异构性、请求干扰和动态负载问题，显著提升了服务质量和资源效率。


<details>
  <summary>Details</summary>
Motivation: 由于云LLM服务的高延迟、不稳定性和隐私问题，边缘部署LLM成为趋势，但现有路由算法无法同时处理LLM服务的异构性、请求干扰和动态负载，导致长期稳定的QoS难以保障。

Method: 提出了一种DRL框架，结合动态状态抽象技术（使用异构图注意力网络HAN）和动作影响估计器，以及定制的奖励函数，以优化路由决策。

Result: 实验表明，该算法在Poisson和真实工作负载下显著提升了平均QoS和计算资源效率。

Conclusion: 该研究为边缘LLM服务提供了一种高效的路由解决方案，能够在动态环境中维持高QoS。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities,
leading to a significant increase in user demand for LLM services. However,
cloud-based LLM services often suffer from high latency, unstable
responsiveness, and privacy concerns. Therefore, multiple LLMs are usually
deployed at the network edge to boost real-time responsiveness and protect data
privacy, particularly for many emerging smart mobile and IoT applications.
Given the varying response quality and latency of LLM services, a critical
issue is how to route user requests from mobile and IoT devices to an
appropriate LLM service (i.e., edge LLM expert) to ensure acceptable
quality-of-service (QoS). Existing routing algorithms fail to simultaneously
address the heterogeneity of LLM services, the interference among requests, and
the dynamic workloads necessary for maintaining long-term stable QoS. To meet
these challenges, in this paper we propose a novel deep reinforcement learning
(DRL)-based QoS-aware LLM routing framework for sustained high-quality LLM
services. Due to the dynamic nature of the global state, we propose a dynamic
state abstraction technique to compactly represent global state features with a
heterogeneous graph attention network (HAN). Additionally, we introduce an
action impact estimator and a tailored reward function to guide the DRL agent
in maximizing QoS and preventing latency violations. Extensive experiments on
both Poisson and real-world workloads demonstrate that our proposed algorithm
significantly improves average QoS and computing resource efficiency compared
to existing baselines.

</details>


### [182] [Large AI Model-Enabled Secure Communications in Low-Altitude Wireless Networks: Concepts, Perspectives and Case Study](https://arxiv.org/abs/2508.00256)
*Chuang Zhang,Geng Sun,Jiacheng Wang,Yijing Lin,Weijie Yuan,Sinem Coleri,Dusit Niyato,Tony Q. S. Quek*

Main category: cs.NI

TL;DR: 本文探讨了低空无线网络（LAWNs）的安全挑战，并提出了一种基于大型人工智能模型（LAMs）的优化框架，通过增强状态特征和设计内在奖励来提升安全通信任务的强化学习性能。


<details>
  <summary>Details</summary>
Motivation: 低空无线网络（LAWNs）因其低空操作、频繁移动和依赖非授权频谱等特点，面临独特的安全挑战，传统AI方法存在局限性，需要更先进的解决方案。

Method: 提出了一种基于LAMs的优化框架，利用大型语言模型（LLMs）生成增强状态特征，并设计内在奖励，以提升强化学习性能。

Result: 通过案例研究验证了所提框架的有效性，仿真结果表明其在安全通信任务中的优越性能。

Conclusion: LAMs在LAWNs安全通信中具有潜力，未来可进一步探索其应用方向。

Abstract: Low-altitude wireless networks (LAWNs) have the potential to revolutionize
communications by supporting a range of applications, including urban parcel
delivery, aerial inspections and air taxis. However, compared with traditional
wireless networks, LAWNs face unique security challenges due to low-altitude
operations, frequent mobility and reliance on unlicensed spectrum, making it
more vulnerable to some malicious attacks. In this paper, we investigate some
large artificial intelligence model (LAM)-enabled solutions for secure
communications in LAWNs. Specifically, we first explore the amplified security
risks and important limitations of traditional AI methods in LAWNs. Then, we
introduce the basic concepts of LAMs and delve into the role of LAMs in
addressing these challenges. To demonstrate the practical benefits of LAMs for
secure communications in LAWNs, we propose a novel LAM-based optimization
framework that leverages large language models (LLMs) to generate enhanced
state features on top of handcrafted representations, and to design intrinsic
rewards accordingly, thereby improving reinforcement learning performance for
secure communication tasks. Through a typical case study, simulation results
validate the effectiveness of the proposed framework. Finally, we outline
future directions for integrating LAMs into secure LAWN applications.

</details>


### [183] [Towards Reliable AI in 6G: Detecting Concept Drift in Wireless Network](https://arxiv.org/abs/2508.00042)
*Athanasios Tziouvaras,Carolina Fortuna,George Floros,Kostas Kolomvatsos,Panagiotis Sarigiannidis,Marko Grobelnik,Blaž Bertalanič*

Main category: cs.NI

TL;DR: 论文提出两种无监督、模型无关的批量概念漂移检测器，用于AI原生6G网络中模型性能的自动维护，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 无线环境的非静态特性导致概念漂移，影响AI模型性能，现有方法难以通用或高效应对。

Method: 引入两种基于期望效用评分的无监督概念漂移检测器，无需部署后真实标签即可触发模型重训练。

Result: 在真实无线用例中，新方法比传统检测器性能提升20-40个百分点，F1分数达0.94和1.00，误报率降低20个百分点。

Conclusion: 新方法有效解决了AI原生6G网络中的概念漂移问题，显著提升了模型维护的自动化水平。

Abstract: AI-native 6G networks promise unprecedented automation and performance by
embedding machine-learning models throughout the radio access and core segments
of the network. However, the non-stationary nature of wireless environments due
to infrastructure changes, user mobility, and emerging traffic patterns,
induces concept drifts that can quickly degrade these model accuracies.
Existing methods in general are very domain specific, or struggle with certain
type of concept drift. In this paper, we introduce two unsupervised,
model-agnostic, batch concept drift detectors. Both methods compute an
expected-utility score to decide when concept drift occurred and if model
retraining is warranted, without requiring ground-truth labels after
deployment. We validate our framework on two real-world wireless use cases in
outdoor fingerprinting for localization and for link-anomaly detection, and
demonstrate that both methods are outperforming classical detectors such as
ADWIN, DDM, CUSUM by 20-40 percentage points. Additionally, they achieve an
F1-score of 0.94 and 1.00 in correctly triggering retraining alarm, thus
reducing the false alarm rate by up to 20 percentage points compared to the
best classical detectors.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [184] [ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism](https://arxiv.org/abs/2508.00554)
*Li Zhao,Rui Sun,Zuoyou Jiang,Bo Yang,Yuxiao Bai,Mengting Chen,Xinyang Wang,Jing Li,Zuo Bai*

Main category: q-fin.TR

TL;DR: 提出一种基于多代理系统的金融交易方法，通过内部竞争机制提升抗市场噪声能力，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）在金融交易中对市场噪声高度敏感的问题。

Method: 设计包含数据团队和研究团队的多代理系统，引入实时评估和排名机制，仅采纳表现最优代理的输出。

Result: 实验表明，该系统在多种评估指标上显著优于现有多代理系统和传统量化投资方法。

Conclusion: 该系统通过动态适应和内部竞争机制，有效提升了交易性能和抗噪声能力。

Abstract: In financial trading, large language model (LLM)-based agents demonstrate
significant potential. However, the high sensitivity to market noise undermines
the performance of LLM-based trading systems. To address this limitation, we
propose a novel multi-agent system featuring an internal competitive mechanism
inspired by modern corporate management structures. The system consists of two
specialized teams: (1) Data Team - responsible for processing and condensing
massive market data into diversified text factors, ensuring they fit the
model's constrained context. (2) Research Team - tasked with making
parallelized multipath trading decisions based on deep research methods. The
core innovation lies in implementing a real-time evaluation and ranking
mechanism within each team, driven by authentic market feedback. Each agent's
performance undergoes continuous scoring and ranking, with only outputs from
top-performing agents being adopted. The design enables the system to
adaptively adjust to dynamic environment, enhances robustness against market
noise and ultimately delivers superior trading performance. Experimental
results demonstrate that our proposed system significantly outperforms
prevailing multiagent systems and traditional quantitative investment methods
across diverse evaluation metrics.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [185] [SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware Video Generation](https://arxiv.org/abs/2508.00782)
*Kien T. Pham,Yingqing He,Yazhou Xing,Qifeng Chen,Long Chen*

Main category: cs.GR

TL;DR: SpA2V是一个音频驱动视频生成的框架，通过利用音频中的空间和语义线索生成语义和空间对齐的视频。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注语义信息，忽略了音频中的空间属性（如位置和运动方向），而人类可以自然识别这些信息。SpA2V旨在填补这一空白。

Method: SpA2V分为两个阶段：1）音频引导的视频规划，利用MLLM构建视频场景布局（VSL）；2）布局引导的视频生成，将VSL作为条件输入预训练的扩散模型。

Result: 实验表明，SpA2V能够生成与输入音频在语义和空间上对齐的逼真视频。

Conclusion: SpA2V通过利用音频的空间线索，显著提升了视频生成的准确性和空间一致性。

Abstract: Audio-driven video generation aims to synthesize realistic videos that align
with input audio recordings, akin to the human ability to visualize scenes from
auditory input. However, existing approaches predominantly focus on exploring
semantic information, such as the classes of sounding sources present in the
audio, limiting their ability to generate videos with accurate content and
spatial composition. In contrast, we humans can not only naturally identify the
semantic categories of sounding sources but also determine their deeply encoded
spatial attributes, including locations and movement directions. This useful
information can be elucidated by considering specific spatial indicators
derived from the inherent physical properties of sound, such as loudness or
frequency. As prior methods largely ignore this factor, we present SpA2V, the
first framework explicitly exploits these spatial auditory cues from audios to
generate videos with high semantic and spatial correspondence. SpA2V decomposes
the generation process into two stages: 1) Audio-guided Video Planning: We
meticulously adapt a state-of-the-art MLLM for a novel task of harnessing
spatial and semantic cues from input audio to construct Video Scene Layouts
(VSLs). This serves as an intermediate representation to bridge the gap between
the audio and video modalities. 2) Layout-grounded Video Generation: We develop
an efficient and effective approach to seamlessly integrate VSLs as conditional
guidance into pre-trained diffusion models, enabling VSL-grounded video
generation in a training-free manner. Extensive experiments demonstrate that
SpA2V excels in generating realistic videos with semantic and spatial alignment
to the input audios.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [186] [Online Fine-Tuning of Carbon Emission Predictions using Real-Time Recurrent Learning for State Space Models](https://arxiv.org/abs/2508.00804)
*Julian Lemmel,Manuel Kranzl,Adam Lamine,Philipp Neubauer,Radu Grosu,Sophie Neubauer*

Main category: cs.CE

TL;DR: 提出了一种基于实时循环学习的结构化状态空间模型（SSM）在线微调方法，通过动态更新模型参数提升预测精度。


<details>
  <summary>Details</summary>
Motivation: SSM通常在离线训练后静态部署，无法适应动态环境。本研究旨在通过在线调整参数提升模型在实时数据中的表现。

Method: 采用实时循环学习技术，在线更新线性循环单元SSM的参数。

Result: 实验表明，该方法在汽车硬件采集的小型碳排放数据集上持续降低在线推理的预测误差。

Conclusion: 该方法适用于动态且资源受限的环境，展现了在线自适应SSM的潜力。

Abstract: This paper introduces a new approach for fine-tuning the predictions of
structured state space models (SSMs) at inference time using real-time
recurrent learning. While SSMs are known for their efficiency and long-range
modeling capabilities, they are typically trained offline and remain static
during deployment. Our method enables online adaptation by continuously
updating model parameters in response to incoming data. We evaluate our
approach for linear-recurrent-unit SSMs using a small carbon emission dataset
collected from embedded automotive hardware. Experimental results show that our
method consistently reduces prediction error online during inference,
demonstrating its potential for dynamic, resource-constrained environments.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [187] [Activation-Guided Local Editing for Jailbreaking Attacks](https://arxiv.org/abs/2508.00555)
*Jiecong Wang,Haoran Li,Hao Peng,Ziqian Zeng,Zihao Wang,Haohua Du,Zhengtao Yu*

Main category: cs.CR

TL;DR: 提出了一种两阶段的对抗攻击框架AGILE，结合场景生成和隐藏状态引导的细粒度编辑，显著提升了攻击成功率和可迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有越狱方法存在不连贯、不可读或依赖人工的问题，需要一种更高效且可扩展的解决方案。

Method: 两阶段框架：首先生成场景并重写恶意查询，然后利用隐藏状态信息进行细粒度编辑。

Result: 攻击成功率提升37.74%，在黑盒模型中表现优异，对现有防御机制仍有效。

Conclusion: AGILE框架展示了当前防御机制的局限性，为未来防御开发提供了重要参考。

Abstract: Jailbreaking is an essential adversarial technique for red-teaming these
models to uncover and patch security flaws. However, existing jailbreak methods
face significant drawbacks. Token-level jailbreak attacks often produce
incoherent or unreadable inputs and exhibit poor transferability, while
prompt-level attacks lack scalability and rely heavily on manual effort and
human ingenuity. We propose a concise and effective two-stage framework that
combines the advantages of these approaches. The first stage performs a
scenario-based generation of context and rephrases the original malicious query
to obscure its harmful intent. The second stage then utilizes information from
the model's hidden states to guide fine-grained edits, effectively steering the
model's internal representation of the input from a malicious toward a benign
one. Extensive experiments demonstrate that this method achieves
state-of-the-art Attack Success Rate, with gains of up to 37.74% over the
strongest baseline, and exhibits excellent transferability to black-box models.
Our analysis further demonstrates that AGILE maintains substantial
effectiveness against prominent defense mechanisms, highlighting the
limitations of current safeguards and providing valuable insights for future
defense development. Our code is available at
https://github.com/yunsaijc/AGILE.

</details>


### [188] [Demo: TOSense -- What Did You Just Agree to?](https://arxiv.org/abs/2508.00659)
*Xinzhang Chen,Hassan Ali,Arash Shaghaghi,Salil S. Kanhere,Sanjay Jha*

Main category: cs.CR

TL;DR: TOSense是一个Chrome扩展，通过自然语言问答帮助用户理解冗长晦涩的服务条款（ToS），结合爬虫和轻量级语言模型，无需人工标注即可评估答案准确性。


<details>
  <summary>Details</summary>
Motivation: 解决用户因ToS冗长晦涩导致的信息不对称和法律风险问题。

Method: 结合爬虫（tos-crawl）提取ToS内容，以及MiniLM和BART-encoder的轻量级语言模型管道进行语义检索和答案验证。

Result: 在五大平台（Apple、Google、X、Microsoft、Netflix）上测试，准确率最高达44.5%。

Conclusion: TOSense通过自动化工具显著提升了用户对ToS的理解能力，展示了实际应用的潜力。

Abstract: Online services often require users to agree to lengthy and obscure Terms of
Service (ToS), leading to information asymmetry and legal risks. This paper
proposes TOSense-a Chrome extension that allows users to ask questions about
ToS in natural language and get concise answers in real time. The system
combines (i) a crawler "tos-crawl" that automatically extracts ToS content, and
(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval
and BART-encoder for answer relevance verification. To avoid expensive manual
annotation, we present a novel Question Answering Evaluation Pipeline (QEP)
that generates synthetic questions and verifies the correctness of answers
using clustered topic matching. Experiments on five major platforms, Apple,
Google, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of
TOSense (with up to 44.5% accuracy) across varying number of topic clusters.
During the demonstration, we will showcase TOSense in action. Attendees will be
able to experience seamless extraction, interactive question answering, and
instant indexing of new sites.

</details>


### [189] [CyGATE: Game-Theoretic Cyber Attack-Defense Engine for Patch Strategy Optimization](https://arxiv.org/abs/2508.00478)
*Yuning Jiang,Nay Oo,Qiaoran Meng,Lu Lin,Dusit Niyato,Zehui Xiong,Hoon Wei Lim,Biplab Sikdar*

Main category: cs.CR

TL;DR: CyGATE是一个基于博弈论的框架，利用大型语言模型（LLMs）和检索增强生成（RAG）优化网络攻防中的策略选择和补丁优先级。


<details>
  <summary>Details</summary>
Motivation: 现有博弈论模型依赖静态假设且缺乏实时威胁情报整合，限制了适应性。

Method: 将网络冲突建模为部分可观测随机博弈（POSG），攻防双方通过信念状态应对不确定性。

Result: 在动态补丁调度场景中，CyGATE能有效优先处理高风险漏洞，提升适应性、战略预见性和资源效率。

Conclusion: CyGATE为多代理场景提供了灵活架构，增强了网络防御的动态性和效率。

Abstract: Modern cyber attacks unfold through multiple stages, requiring defenders to
dynamically prioritize mitigations under uncertainty. While game-theoretic
models capture attacker-defender interactions, existing approaches often rely
on static assumptions and lack integration with real-time threat intelligence,
limiting their adaptability. This paper presents CyGATE, a game-theoretic
framework modeling attacker-defender interactions, using large language models
(LLMs) with retrieval-augmented generation (RAG) to enhance tactic selection
and patch prioritization. Applied to a two-agent scenario, CyGATE frames cyber
conflicts as a partially observable stochastic game (POSG) across Cyber Kill
Chain stages. Both agents use belief states to navigate uncertainty, with the
attacker adapting tactics and the defender re-prioritizing patches based on
evolving risks and observed adversary behavior. The framework's flexible
architecture enables extension to multi-agent scenarios involving coordinated
attackers, collaborative defenders, or complex enterprise environments with
multiple stakeholders. Evaluated in a dynamic patch scheduling scenario, CyGATE
effectively prioritizes high-risk vulnerabilities, enhancing adaptability
through dynamic threat integration, strategic foresight by anticipating
attacker moves under uncertainty, and efficiency by optimizing resource use.

</details>


### [190] [Preliminary Investigation into Uncertainty-Aware Attack Stage Classification](https://arxiv.org/abs/2508.00368)
*Alessandro Gaudenzi,Lorenzo Nodari,Lance Kaplan,Alessandra Russo,Murat Sensoy,Federico Cerutti*

Main category: cs.CR

TL;DR: 论文提出一种基于证据深度学习（EDL）的分类方法，用于推断APT攻击阶段，并处理不确定性和分布外输入。


<details>
  <summary>Details</summary>
Motivation: APT攻击的多阶段性和复杂性使得传统检测系统难以准确推断攻击阶段，而有效的应对策略需要针对不同阶段定制。

Method: 采用EDL方法，通过输出Dirichlet分布参数建模预测不确定性，不仅能预测攻击阶段，还能识别不确定或分布外输入。

Result: 模拟环境中的实验表明，该方法能准确推断攻击阶段并有效检测分布外输入。

Conclusion: 不确定性感知模型在动态对抗环境中用于阶段性威胁检测是可行的。

Abstract: Advanced Persistent Threats (APTs) represent a significant challenge in
cybersecurity due to their prolonged, multi-stage nature and the sophistication
of their operators. Traditional detection systems typically focus on
identifying malicious activity in binary terms (benign or malicious) without
accounting for the progression of an attack. However, effective response
strategies depend on accurate inference of the attack's current stage, as
countermeasures must be tailored to whether an adversary is in the early
reconnaissance phase or actively conducting exploitation or exfiltration. This
work addresses the problem of attack stage inference under uncertainty, with a
focus on robustness to out-of-distribution (OOD) inputs. We propose a
classification approach based on Evidential Deep Learning (EDL), which models
predictive uncertainty by outputting parameters of a Dirichlet distribution
over possible stages. This allows the system not only to predict the most
likely stage of an attack but also to indicate when it is uncertain or the
input lies outside the training distribution. Preliminary experiments in a
simulated environment demonstrate that the proposed model can accurately infer
the stage of an attack with calibrated confidence while effectively detecting
OOD inputs, which may indicate changes in the attackers' tactics. These results
support the feasibility of deploying uncertainty-aware models for staged threat
detection in dynamic and adversarial environments.

</details>


### [191] [LeakSealer: A Semisupervised Defense for LLMs Against Prompt Injection and Leakage Attacks](https://arxiv.org/abs/2508.00602)
*Francesco Panebianco,Stefano Bonfanti,Francesco Trovò,Michele Carminati*

Main category: cs.CR

TL;DR: 论文提出了一种分析LLM历史交互数据的方法和LeakSealer框架，用于检测和防御安全威胁（如越狱攻击和敏感信息泄露）。


<details>
  <summary>Details</summary>
Motivation: 随着LLM的广泛应用，其面临的安全威胁（如越狱攻击和数据泄露）日益严重，亟需有效的检测和防御方法。

Method: 1. 分析历史交互数据生成主题分类的使用地图；2. 提出LeakSealer框架，结合静态分析和动态防御（HITL流程）。

Result: LeakSealer在静态设置中表现最佳（ToxicChat数据集），动态设置中PII泄露检测的AUPRC达0.97，显著优于基线。

Conclusion: LeakSealer能有效检测和防御LLM的安全威胁，为实际部署提供了实用工具。

Abstract: The generalization capabilities of Large Language Models (LLMs) have led to
their widespread deployment across various applications. However, this
increased adoption has introduced several security threats, notably in the
forms of jailbreaking and data leakage attacks. Additionally, Retrieval
Augmented Generation (RAG), while enhancing context-awareness in LLM responses,
has inadvertently introduced vulnerabilities that can result in the leakage of
sensitive information. Our contributions are twofold. First, we introduce a
methodology to analyze historical interaction data from an LLM system, enabling
the generation of usage maps categorized by topics (including adversarial
interactions). This approach further provides forensic insights for tracking
the evolution of jailbreaking attack patterns. Second, we propose LeakSealer, a
model-agnostic framework that combines static analysis for forensic insights
with dynamic defenses in a Human-In-The-Loop (HITL) pipeline. This technique
identifies topic groups and detects anomalous patterns, allowing for proactive
defense mechanisms. We empirically evaluate LeakSealer under two scenarios: (1)
jailbreak attempts, employing a public benchmark dataset, and (2) PII leakage,
supported by a curated dataset of labeled LLM interactions. In the static
setting, LeakSealer achieves the highest precision and recall on the ToxicChat
dataset when identifying prompt injection. In the dynamic setting, PII leakage
detection achieves an AUPRC of $0.97$, significantly outperforming baselines
such as Llama Guard.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [192] [Modelling Program Spaces in Program Synthesis with Constraints](https://arxiv.org/abs/2508.00005)
*Tilman Hinnerichs,Bart Swinkels,Jaap de Jong,Reuben Gardos Reid,Tudor Magirescu,Neil Yorke-Smith,Sebastijan Dumancic*

Main category: cs.PL

TL;DR: 论文提出了一种利用语法约束来优化程序合成的方法，通过BART求解器高效传播和解决约束，显著减少程序空间和枚举时间。


<details>
  <summary>Details</summary>
Motivation: 程序合成的核心挑战是处理庞大的程序空间，传统方法未充分利用约束来去除无用程序。

Method: 引入语法约束建模程序空间，开发BART求解器高效传播和解决约束。

Result: 约束消除了高达99%的程序空间，显著减少枚举时间。

Conclusion: 语法约束是优化程序合成的有效工具，BART求解器展示了其高效性。

Abstract: A core challenge in program synthesis is taming the large space of possible
programs. Since program synthesis is essentially a combinatorial search, the
community has sought to leverage powerful combinatorial constraint solvers.
Here, constraints are used to express the program semantics, but not as a
potentially potent tool to remove unwanted programs. Recent inductive logic
programming approaches introduce constraints on the program's syntax to be
synthesized. These syntactic constraints allow for checking and propagating a
constraint without executing the program, and thus for arbitrary operators. In
this work, we leverage syntactic constraints to model program spaces, defining
not just solutions that are feasible, but also ones that are likely useful. To
demonstrate this idea, we introduce BART, a solver that efficiently propagates
and solves these constraints. We evaluate BART on program space enumeration
tasks, finding that the constraints eliminate up to 99 percent of the program
space, and that modeling program spaces significantly reduces enumeration time.

</details>


### [193] [Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations](https://arxiv.org/abs/2508.00534)
*Mikel Vandeloise*

Main category: cs.PL

TL;DR: 本文通过系统文献综述，探讨了多范式语言的分类问题，提出了基于数学框架的重构方法。


<details>
  <summary>Details</summary>
Motivation: 多范式语言的兴起挑战了传统分类方法，导致互操作性缺陷等实际问题。

Method: 基于74项主要研究的综合分析，评估现有分类形式及其局限性，并提出基于数学框架的重构方法。

Result: 现有分类法缺乏概念粒度与统一形式基础，重构方法利用类型理论、范畴论和统一编程理论（UTP）提供形式化保证。

Conclusion: 文献反映了从分类向形式化重构框架的转变，本文提出了统一研究的议程。

Abstract: The rise of multi-paradigm languages challenges traditional classification
methods, leading to practical software engineering issues like interoperability
defects. This systematic literature review (SLR) maps the formal foundations of
programming paradigms. Our objective is twofold: (1) to assess the state of the
art of classification formalisms and their limitations, and (2) to identify the
conceptual primitives and mathematical frameworks for a more powerful,
reconstructive approach.
  Based on a synthesis of 74 primary studies, we find that existing taxonomies
lack conceptual granularity, a unified formal basis, and struggle with hybrid
languages. In response, our analysis reveals a strong convergence toward a
compositional reconstruction of paradigms. This approach identifies a minimal
set of orthogonal, atomic primitives and leverages mathematical frameworks,
predominantly Type theory, Category theory and Unifying Theories of Programming
(UTP), to formally guarantee their compositional properties.
  We conclude that the literature reflects a significant intellectual shift
away from classification towards these promising formal, reconstructive
frameworks. This review provides a map of this evolution and proposes a
research agenda for their unification.

</details>


### [194] [Automated Type Annotation in Python Using Large Language Models](https://arxiv.org/abs/2508.00422)
*Varun Bharti,Shashwat Jha,Dhruv Kumar,Pankaj Jalote*

Main category: cs.PL

TL;DR: LLMs用于生成Python类型注释，通过生成-检查-修复流程，无需任务特定微调即可与传统深度学习竞争。


<details>
  <summary>Details</summary>
Motivation: 手动生成类型注释易出错且耗时，传统自动化方法存在局限性，探索LLMs的潜力。

Method: 使用LLM生成类型注释，结合Concrete Syntax Tree和Mypy验证，迭代修复错误。

Result: GPT 4.1mini和O3Mini表现最佳，准确率分别达70.5%和79.1%，平均修复次数低于一次。

Conclusion: 通用和推理优化的LLMs能有效生成一致的类型注释，适用于其他可选类型语言。

Abstract: Type annotations in Python enhance maintainability and error detection.
However, generating these annotations manually is error prone and requires
extra effort. Traditional automation approaches like static analysis, machine
learning, and deep learning struggle with limited type vocabularies, behavioral
over approximation, and reliance on large labeled datasets. In this work, we
explore the use of LLMs for generating type annotations in Python. We develop a
generate check repair pipeline: the LLM proposes annotations guided by a
Concrete Syntax Tree representation, a static type checker (Mypy) verifies
them, and any errors are fed back for iterative refinement. We evaluate four
LLM variants: GPT 4oMini, GPT 4.1mini (general-purpose), and O3Mini, O4Mini
(reasoning optimized), on 6000 code snippets from the ManyTypes4Py benchmark.
We first measure the proportion of code snippets annotated by LLMs for which
MyPy reported no errors (i.e., consistent results): GPT 4oMini achieved
consistency on 65.9% of cases (34.1% inconsistent), while GPT 4.1mini, O3Mini,
and O4Mini each reached approximately 88.6% consistency (around 11.4%
failures). To measure annotation quality, we then compute exact-match and
base-type match accuracies over all 6000 snippets: GPT 4.1mini and O3Mini
perform the best, achieving up to 70.5% exact match and 79.1% base type
accuracy, requiring under one repair iteration on average. Our results
demonstrate that general-purpose and reasoning optimized LLMs, without any task
specific fine tuning or additional training can be effective in generating
consistent type annotations.They perform competitively with traditional deep
learning techniques which require large labeled dataset for training. While our
work focuses on Python, the pipeline can be extended to other optionally typed
imperative languages like Ruby

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [195] [Towards a Measure Theory of Semantic Information](https://arxiv.org/abs/2508.00525)
*George M. Coghill*

Main category: cs.IT

TL;DR: 本文批判了Floridi的强语义信息理论，提出了一种基于单位圆的新方法，解决了Bar-Hillel-Carnap悖论，并展示了其效用。


<details>
  <summary>Details</summary>
Motivation: 解决Bar-Hillel-Carnap悖论，即矛盾被赋予最大信息量的问题，同时改进Floridi的理论。

Method: 基于单位圆和冯·诺依曼的量子概率理论，构建了一个新的信息度量空间。

Result: 新方法满足了Floridi的要求，消除了悖论，且矛盾和对立消息具有相同的信息量。

Conclusion: 新方法不仅解决了悖论，还展示了矛盾消息的信息量等价性，具有实际应用价值。

Abstract: A classic account of the quantification of semantic information is that of
Bar-Hiller and Carnap. Their account proposes an inverse relation between the
informativeness of a statement and its probability. However, their approach
assigns the maximum informativeness to a contradiction: which Floridi refers to
as the Bar-Hillel-Carnap paradox. He developed a novel theory founded on a
distance metric and parabolic relation, designed to remove this paradox.
Unfortunately is approach does not succeed in that aim.
  In this paper I critique Floridi's theory of strongly semantic information on
its own terms and show where it succeeds and fails. I then present a new
approach based on the unit circle (a relation that has been the basis of
theories from basic trigonometry to quantum theory). This is used, by analogy
with von Neumann's quantum probability to construct a measure space for
informativeness that meets all the requirements stipulated by Floridi and
removes the paradox. In addition, while contradictions and tautologies have
zero informativeness, it is found that messages which are contradictory to each
other are equally informative. The utility of this is explained by means of an
example.

</details>


### [196] [Information-Theoretic Decentralized Secure Aggregation with Collusion Resilience](https://arxiv.org/abs/2508.00596)
*Xiang Zhang,Zhou Li,Shuangyang Li,Kai Wan,Derrick Wing Kwan Ng,Giuseppe Caire*

Main category: cs.IT

TL;DR: 该论文研究了去中心化联邦学习中的安全聚合问题，从信息论角度确定了通信和密钥使用的最优界限。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注协议设计和计算保证，缺乏对去中心化环境下安全聚合的基本信息论限制的理解。

Method: 研究了一个由K个完全连接用户组成的网络，每个用户持有私有输入，目标是安全计算所有输入的总和。

Result: 确定了最优速率区域，表明每个用户必须传输至少一个符号、持有至少一个密钥符号，且所有用户必须共同持有至少K-1个独立密钥符号。

Conclusion: 结果为分布式学习系统中设计可证明安全和通信高效的协议提供了理论基础。

Abstract: In decentralized federated learning (FL), multiple clients collaboratively
learn a shared machine learning (ML) model by leveraging their privately held
datasets distributed across the network, through interactive exchange of the
intermediate model updates. To ensure data security, cryptographic techniques
are commonly employed to protect model updates during aggregation. Despite
growing interest in secure aggregation, existing works predominantly focus on
protocol design and computational guarantees, with limited understanding of the
fundamental information-theoretic limits of such systems. Moreover, optimal
bounds on communication and key usage remain unknown in decentralized settings,
where no central aggregator is available. Motivated by these gaps, we study the
problem of decentralized secure aggregation (DSA) from an information-theoretic
perspective. Specifically, we consider a network of $K$ fully-connected users,
each holding a private input -- an abstraction of local training data -- who
aim to securely compute the sum of all inputs. The security constraint requires
that no user learns anything beyond the input sum, even when colluding with up
to $T$ other users. We characterize the optimal rate region, which specifies
the minimum achievable communication and secret key rates for DSA. In
particular, we show that to securely compute one symbol of the desired input
sum, each user must (i) transmit at least one symbol to others, (ii) hold at
least one symbol of secret key, and (iii) all users must collectively hold no
fewer than $K - 1$ independent key symbols. Our results establish the
fundamental performance limits of DSA, providing insights for the design of
provably secure and communication-efficient protocols in distributed learning
systems.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [197] [Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine Learning](https://arxiv.org/abs/2508.00024)
*Sebastián Andrés Cajas Ordóñez,Luis Fernando Torres Torres,Mario Bifulco,Carlos Andrés Durán,Cristian Bosch,Ricardo Simón Carbajo*

Main category: quant-ph

TL;DR: 提出一种结合类平衡k均值蒸馏与预训练Vision Transformer嵌入的量子-经典混合方法，显著提升量子支持向量机性能。


<details>
  <summary>Details</summary>
Motivation: 解决量子支持向量机因高维量子态和硬件限制带来的可扩展性问题。

Method: 采用嵌入感知的量子-经典混合流程，结合类平衡k均值蒸馏和预训练ViT嵌入。

Result: 在Fashion-MNIST和MNIST上分别实现8.02%和4.42%的准确率提升，CNN特征则表现下降。

Conclusion: ViT嵌入与量子特征空间存在协同效应，为可扩展量子机器学习提供了实用路径。

Abstract: Quantum Support Vector Machines face scalability challenges due to
high-dimensional quantum states and hardware limitations. We propose an
embedding-aware quantum-classical pipeline combining class-balanced k-means
distillation with pretrained Vision Transformer embeddings. Our key finding:
ViT embeddings uniquely enable quantum advantage, achieving up to 8.02%
accuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST,
while CNN features show performance degradation. Using 16-qubit tensor network
simulation via cuTensorNet, we provide the first systematic evidence that
quantum kernel advantage depends critically on embedding choice, revealing
fundamental synergy between transformer attention and quantum feature spaces.
This provides a practical pathway for scalable quantum machine learning that
leverages modern neural architectures.

</details>


### [198] [Quantum Semi-Random Forests for Qubit-Efficient Recommender Systems](https://arxiv.org/abs/2508.00027)
*Azadeh Alavi,Fatemeh Kouchmeshki,Abdolrahman Alavi,Yongli Ren,Jiayang Niu*

Main category: quant-ph

TL;DR: 提出了一种三阶段混合机器学习算法，通过压缩标签配置文件、在固定量子比特预算下优化特征选择，并使用仅需5个量子比特的量子半随机森林（QsRF）进行推荐评分，性能与现有方法相当。


<details>
  <summary>Details</summary>
Motivation: 解决现代推荐系统中稀疏语义标签映射到量子比特时需求过高的问题，以适应当前NISQ设备的限制。

Method: 采用三阶段算法：1) 使用SVD和k-means压缩标签；2) 通过QAOA优化特征选择；3) 用5量子比特的QsRF进行推荐评分。

Result: 在ICM-150/500数据集上，100-tree QsRF的性能与全特征基线方法相当。

Conclusion: 该方法在有限量子比特资源下实现了高性能推荐，适用于NISQ设备。

Abstract: Modern recommenders describe each item with hundreds of sparse semantic tags,
yet most quantum pipelines still map one qubit per tag, demanding well beyond
one hundred qubits, far out of reach for current noisy-intermediate-scale
quantum (NISQ) devices and prone to deep, error-amplifying circuits. We close
this gap with a three-stage hybrid machine learning algorithm that compresses
tag profiles, optimizes feature selection under a fixed qubit budget via QAOA,
and scores recommendations with a Quantum semi-Random Forest (QsRF) built on
just five qubits, while performing similarly to the state-of-the-art methods.
Leveraging SVD sketching and k-means, we learn a 1000-atom dictionary ($>$97 \%
variance), then solve a 2020 QUBO via depth-3 QAOA to select 5 atoms. A
100-tree QsRF trained on these codes matches full-feature baselines on
ICM-150/500.

</details>


### [199] [Hybrid Quantum Classical Surrogate for Real Time Inverse Finite Element Modeling in Digital Twins](https://arxiv.org/abs/2508.00029)
*Azadeh Alavi,Sanduni Jayasinghe,Mojtaba Mahmoodian,Sam Mazaheri,John Thangarajah,Sujeeva Setunge*

Main category: quant-ph

TL;DR: 提出了一种混合量子经典多层感知器（QMLP）框架，用于解决大规模结构健康监测中的计算复杂性问题，并通过实验验证其优越性能。


<details>
  <summary>Details</summary>
Motivation: 大规模土木结构的意外故障可能导致重大经济和安全问题，传统有限元建模在实时监测中面临高计算成本和复杂逆向分析的挑战。

Method: 采用对称正定矩阵和多项式特征嵌入传感器数据，结合参数化量子电路和经典神经网络进行推断。

Result: 在桥梁实验中，QMLP的均方误差为0.0000000000316，显著优于纯经典方法。

Conclusion: 量子增强方法为实时结构健康监测提供了高效、可扩展的解决方案，推动了数字孪生技术的发展。

Abstract: Large-scale civil structures, such as bridges, pipelines, and offshore
platforms, are vital to modern infrastructure, where unexpected failures can
cause significant economic and safety repercussions. Although finite element
(FE) modeling is widely used for real-time structural health monitoring (SHM),
its high computational cost and the complexity of inverse FE analysis, where
low dimensional sensor data must map onto high-dimensional displacement or
stress fields pose ongoing challenges. Here, we propose a hybrid quantum
classical multilayer perceptron (QMLP) framework to tackle these issues and
facilitate swift updates to digital twins across a range of structural
applications.
  Our approach embeds sensor data using symmetric positive definite (SPD)
matrices and polynomial features, yielding a representation well suited to
quantum processing. A parameterized quantum circuit (PQC) transforms these
features, and the resultant quantum outputs feed into a classical neural
network for final inference. By fusing quantum capabilities with classical
modeling, the QMLP handles large scale inverse FE mapping while preserving
computational viability.
  Through extensive experiments on a bridge, we demonstrate that the QMLP
achieves a mean squared error (MSE) of 0.0000000000316, outperforming purely
classical baselines with a large margin. These findings confirm the potential
of quantum-enhanced methods for real time SHM, establishing a pathway toward
more efficient, scalable digital twins that can robustly monitor and diagnose
structural integrity in near real time.

</details>


### [200] [Dimension reduction with structure-aware quantum circuits for hybrid machine learning](https://arxiv.org/abs/2508.00048)
*Ammar Daskin*

Main category: quant-ph

TL;DR: 论文提出了一种基于Schmidt分解和量子电路的混合机器学习模型，用于数据压缩和降噪。


<details>
  <summary>Details</summary>
Motivation: 通过Schmidt分解和SVD降噪方法，结合量子电路实现数据的高效压缩，减少大规模模型的参数数量。

Method: 利用Schmidt分解和SVD递归处理向量，设计基于k值的量子电路，结合经典神经网络构建混合模型。

Result: 实验证实量子电路能成功压缩数据，提供有效的k秩近似。

Conclusion: 该方法展示了量子电路在数据压缩和降噪中的潜力，为大规模模型训练提供了新思路。

Abstract: Schmidt decomposition of a vector can be understood as writing the singular
value decomposition (SVD) in vector form. A vector can be written as a linear
combination of tensor product of two dimensional vectors by recursively
applying Schmidt decompositions via SVD to all subsystems. Given a vector
expressed as a linear combination of tensor products, using only the $k$
principal terms yields a $k$-rank approximation of the vector. Therefore,
writing a vector in this reduced form allows to retain most important parts of
the vector while removing small noises from it, analogous to SVD-based
denoising.
  In this paper, we show that quantum circuits designed based on a value $k$
(determined from the tensor network decomposition of the mean vector of the
training sample) can approximate the reduced-form representations of entire
datasets. We then employ this circuit ansatz with a classical neural network
head to construct a hybrid machine learning model. Since the output of the
quantum circuit for an $2^n$ dimensional vector is an $n$ dimensional
probability vector, this provides an exponential compression of the input and
potentially can reduce the number of learnable parameters for training
large-scale models. We use datasets provided in the Python scikit-learn module
for the experiments. The results confirm the quantum circuit is able to
compress data successfully to provide effective $k$-rank approximations to the
classical processing component.

</details>


<div id='cs.OS'></div>

# cs.OS [[Back]](#toc)

### [201] [Composable OS Kernel Architectures for Autonomous Intelligence](https://arxiv.org/abs/2508.00604)
*Rajpreet Singh,Vidhi Kothari*

Main category: cs.OS

TL;DR: 提出了一种新型操作系统内核架构，将静态资源管理的内核转变为自适应、AI集成的平台，支持智能系统的需求。


<details>
  <summary>Details</summary>
Motivation: 随着智能系统在边缘设备、云基础设施和嵌入式实时环境中的普及，传统内核架构已无法满足智能应用的动态需求。

Method: 1. 将可加载内核模块（LKMs）设计为面向AI的计算单元；2. 扩展Linux内核为AI原生环境，支持深度学习推理和实时调度；3. 引入神经符号内核设计，结合范畴论和同伦类型论。

Result: 实现了操作系统对智能应用认知需求的主动预测和自适应能力。

Conclusion: 该研究为智能系统提供了一种高效、灵活的内核架构，推动了操作系统与AI的深度融合。

Abstract: As intelligent systems permeate edge devices, cloud infrastructure, and
embedded real-time environments, this research proposes a new OS kernel
architecture for intelligent systems, transforming kernels from static resource
managers to adaptive, AI-integrated platforms. Key contributions include: (1)
treating Loadable Kernel Modules (LKMs) as AI-oriented computation units for
fast sensory and cognitive processing in kernel space; (2) expanding the Linux
kernel into an AI-native environment with built-in deep learning inference,
floating-point acceleration, and real-time adaptive scheduling for efficient ML
workloads; and (3) introducing a Neurosymbolic kernel design leveraging
Category Theory and Homotopy Type Theory to unify symbolic reasoning and
differentiable logic within OS internals. Together, these approaches enable
operating systems to proactively anticipate and adapt to the cognitive needs of
autonomous intelligent applications.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [202] [Reinitializing weights vs units for maintaining plasticity in neural networks](https://arxiv.org/abs/2508.00212)
*J. Fernando Hernandez-Garcia,Shibhansh Dohare,Jun Luo,Rich S. Sutton*

Main category: cs.NE

TL;DR: 论文比较了两种防止神经网络丧失可塑性的方法：重新初始化单元与重新初始化权重，并提出了一种新的选择性权重重新初始化算法。实验表明，在特定情况下（如网络单元较少或包含层归一化时），权重重新初始化更有效。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络在长时间训练非静态数据时丧失可塑性的问题，设计持续学习系统。

Method: 提出选择性权重重新初始化算法，并与持续反向传播和ReDo算法进行比较。

Result: 在单元较少或包含层归一化的网络中，权重重新初始化更有效；在足够大的网络中，两种方法效果相当。

Conclusion: 权重重新初始化在更广泛的情况下能保持网络的可塑性。

Abstract: Loss of plasticity is a phenomenon in which a neural network loses its
ability to learn when trained for an extended time on non-stationary data. It
is a crucial problem to overcome when designing systems that learn continually.
An effective technique for preventing loss of plasticity is reinitializing
parts of the network. In this paper, we compare two different reinitialization
schemes: reinitializing units vs reinitializing weights. We propose a new
algorithm, which we name \textit{selective weight reinitialization}, for
reinitializing the least useful weights in a network. We compare our algorithm
to continual backpropagation and ReDo, two previously proposed algorithms that
reinitialize units in the network. Through our experiments in continual
supervised learning problems, we identify two settings when reinitializing
weights is more effective at maintaining plasticity than reinitializing units:
(1) when the network has a small number of units and (2) when the network
includes layer normalization. Conversely, reinitializing weights and units are
equally effective at maintaining plasticity when the network is of sufficient
size and does not include layer normalization. We found that reinitializing
weights maintains plasticity in a wider variety of settings than reinitializing
units.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [203] [Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving](https://arxiv.org/abs/2508.00589)
*Stefan Englmeier,Max A. Büttner,Katharina Winter,Fabian B. Flohr*

Main category: cs.CV

TL;DR: 论文提出了一种基于上下文感知的运动检索框架，用于在自动驾驶数据集中高效检索罕见的人类行为场景，以支持自动驾驶系统的针对性评估。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统需在安全关键场景中可靠运行，尤其是涉及弱势道路使用者（VRUs）的复杂行为。然而，在大规模数据集中检索这些罕见行为具有挑战性。

Method: 结合SMPL运动序列和视频帧，将其编码到与自然语言对齐的多模态嵌入空间，通过文本查询实现人类行为及其上下文的可扩展检索。

Result: 提出的方法在WayMoCo数据集上的运动-上下文检索任务中，准确率比现有最优模型高出27.5%。

Conclusion: 该框架为自动驾驶系统在多样化人类行为场景中的评估提供了有效工具，并展示了在检索任务中的显著性能提升。

Abstract: Autonomous driving systems must operate reliably in safety-critical
scenarios, particularly those involving unusual or complex behavior by
Vulnerable Road Users (VRUs). Identifying these edge cases in driving datasets
is essential for robust evaluation and generalization, but retrieving such rare
human behavior scenarios within the long tail of large-scale datasets is
challenging. To support targeted evaluation of autonomous driving systems in
diverse, human-centered scenarios, we propose a novel context-aware motion
retrieval framework. Our method combines Skinned Multi-Person Linear
(SMPL)-based motion sequences and corresponding video frames before encoding
them into a shared multimodal embedding space aligned with natural language.
Our approach enables the scalable retrieval of human behavior and their context
through text queries. This work also introduces our dataset WayMoCo, an
extension of the Waymo Open Dataset. It contains automatically labeled motion
and scene context descriptions derived from generated pseudo-ground-truth SMPL
sequences and corresponding image data. Our approach outperforms
state-of-the-art models by up to 27.5% accuracy in motion-context retrieval,
when evaluated on the WayMoCo dataset.

</details>


### [204] [Punching Bag vs. Punching Person: Motion Transferability in Videos](https://arxiv.org/abs/2508.00085)
*Raiyaan Abdullah,Jared Claypoole,Michael Cogswell,Ajay Divakaran,Yogesh Rawat*

Main category: cs.CV

TL;DR: 论文探讨了动作识别模型在跨多样上下文中的高级运动概念迁移能力，发现模型在新情境下性能显著下降，并提出了改进方法。


<details>
  <summary>Details</summary>
Motivation: 研究动作识别模型是否能有效迁移高级运动概念到不同情境，即使是在相似分布中。

Method: 引入了一个运动迁移性框架，使用三个数据集（Syn-TA、Kinetics400-TA、Something-Something-v2-TA）评估了13个先进模型。

Result: 模型在新情境下性能显著下降，多模态模型对细粒度动作识别表现更差，大模型在空间线索主导时表现更好但时间推理能力不足。

Conclusion: 研究为动作识别中的运动迁移性评估提供了重要基准，并提出了改进方向。

Abstract: Action recognition models demonstrate strong generalization, but can they
effectively transfer high-level motion concepts across diverse contexts, even
within similar distributions? For example, can a model recognize the broad
action "punching" when presented with an unseen variation such as "punching
person"? To explore this, we introduce a motion transferability framework with
three datasets: (1) Syn-TA, a synthetic dataset with 3D object motions; (2)
Kinetics400-TA; and (3) Something-Something-v2-TA, both adapted from natural
video datasets. We evaluate 13 state-of-the-art models on these benchmarks and
observe a significant drop in performance when recognizing high-level actions
in novel contexts. Our analysis reveals: 1) Multimodal models struggle more
with fine-grained unknown actions than with coarse ones; 2) The bias-free
Syn-TA proves as challenging as real-world datasets, with models showing
greater performance drops in controlled settings; 3) Larger models improve
transferability when spatial cues dominate but struggle with intensive temporal
reasoning, while reliance on object and background cues hinders generalization.
We further explore how disentangling coarse and fine motions can improve
recognition in temporally challenging datasets. We believe this study
establishes a crucial benchmark for assessing motion transferability in action
recognition. Datasets and relevant code:
https://github.com/raiyaan-abdullah/Motion-Transfer.

</details>


### [205] [On the Risk of Misleading Reports: Diagnosing Textual Biases in Multimodal Clinical AI](https://arxiv.org/abs/2508.00171)
*David Restrepo,Ira Ktena,Maria Vakalopoulou,Stergios Christodoulidis,Enzo Ferrante*

Main category: cs.CV

TL;DR: 论文提出了一种名为选择性模态转移（SMS）的方法，用于量化视觉语言模型（VLMs）在二元分类任务中对图像和文本模态的依赖程度，揭示了模型对文本输入的显著依赖。


<details>
  <summary>Details</summary>
Motivation: 临床决策依赖于医学图像和相关临床报告的综合分析，但现有视觉语言模型可能偏向一种模态，忽视视觉线索。

Method: 通过交换样本中的图像或文本来暴露模态特定偏差，评估了六种开源VLMs在两种医学影像数据集上的表现。

Result: 发现模型对文本输入的依赖显著，即使存在互补的视觉信息。定性注意力分析进一步证实图像内容常被文本细节掩盖。

Conclusion: 强调了设计和评估真正整合视觉和文本线索的多模态医学模型的重要性。

Abstract: Clinical decision-making relies on the integrated analysis of medical images
and the associated clinical reports. While Vision-Language Models (VLMs) can
offer a unified framework for such tasks, they can exhibit strong biases toward
one modality, frequently overlooking critical visual cues in favor of textual
information. In this work, we introduce Selective Modality Shifting (SMS), a
perturbation-based approach to quantify a model's reliance on each modality in
binary classification tasks. By systematically swapping images or text between
samples with opposing labels, we expose modality-specific biases. We assess six
open-source VLMs-four generalist models and two fine-tuned for medical data-on
two medical imaging datasets with distinct modalities: MIMIC-CXR (chest X-ray)
and FairVLMed (scanning laser ophthalmoscopy). By assessing model performance
and the calibration of every model in both unperturbed and perturbed settings,
we reveal a marked dependency on text input, which persists despite the
presence of complementary visual information. We also perform a qualitative
attention-based analysis which further confirms that image content is often
overshadowed by text details. Our findings highlight the importance of
designing and evaluating multimodal medical models that genuinely integrate
visual and textual cues, rather than relying on single-modality signals.

</details>


### [206] [Exploring the Feasibility of Deep Learning Techniques for Accurate Gender Classification from Eye Images](https://arxiv.org/abs/2508.00135)
*Basna Mohammed Salih Hasan,Ramadhan J. Mstafa*

Main category: cs.CV

TL;DR: 论文提出了一种基于眼部周围区域的性别分类CNN模型，在CVBL和(Female and Male)数据集上分别达到99%和96%的准确率。


<details>
  <summary>Details</summary>
Motivation: 性别分类在安全、人机交互等领域至关重要，但化妆品和伪装等因素会影响分类准确性，因此研究专注于利用眼部周围区域进行性别分类。

Method: 使用卷积神经网络(CNN)模型，基于眼部周围区域的彩色图像提取特征进行性别分类，并在CVBL和(Female and Male)数据集上验证性能。

Result: 模型在CVBL数据集上达到99%的准确率，在(Female and Male)数据集上达到96%的准确率，参数量较少(7,235,089)。

Conclusion: 模型在性别分类中表现出色，适用于安全和监控等实际应用。

Abstract: Gender classification has emerged as a crucial aspect in various fields,
including security, human-machine interaction, surveillance, and advertising.
Nonetheless, the accuracy of this classification can be influenced by factors
such as cosmetics and disguise. Consequently, our study is dedicated to
addressing this concern by concentrating on gender classification using color
images of the periocular region. The periocular region refers to the area
surrounding the eye, including the eyelids, eyebrows, and the region between
them. It contains valuable visual cues that can be used to extract key features
for gender classification. This paper introduces a sophisticated Convolutional
Neural Network (CNN) model that utilizes color image databases to evaluate the
effectiveness of the periocular region for gender classification. To validate
the model's performance, we conducted tests on two eye datasets, namely CVBL
and (Female and Male). The recommended architecture achieved an outstanding
accuracy of 99% on the previously unused CVBL dataset while attaining a
commendable accuracy of 96% with a small number of learnable parameters
(7,235,089) on the (Female and Male) dataset. To ascertain the effectiveness of
our proposed model for gender classification using the periocular region, we
evaluated its performance through an extensive range of metrics and compared it
with other state-of-the-art approaches. The results unequivocally demonstrate
the efficacy of our model, thereby suggesting its potential for practical
application in domains such as security and surveillance.

</details>


### [207] [Fine-grained Spatiotemporal Grounding on Egocentric Videos](https://arxiv.org/abs/2508.00518)
*Shuo Liang,Yiwu Zhong,Zi-Yuan Hu,Yeyao Tao,Liwei Wang*

Main category: cs.CV

TL;DR: 该论文提出了EgoMask，首个用于自我中心视频的像素级时空基准，通过自动标注管道构建，并展示了现有模型在该基准上的表现不佳，但通过微调可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 自我中心视频在增强现实和机器人等应用中的重要性日益增长，但现有研究主要集中在外部中心视频，自我中心视频的研究相对不足。

Method: 论文通过系统分析自我中心与外部中心视频的差异，提出EgoMask基准及其自动标注管道，并构建了大规模训练数据集EgoMask-Train。

Result: 实验表明，现有最先进的时空定位模型在EgoMask上表现不佳，但在EgoMask-Train上微调后性能显著提升，同时不影响在外部中心数据集上的表现。

Conclusion: 该研究为推进自我中心视频理解提供了关键资源和见解，代码已开源。

Abstract: Spatiotemporal video grounding aims to localize target entities in videos
based on textual queries. While existing research has made significant progress
in exocentric videos, the egocentric setting remains relatively underexplored,
despite its growing importance in applications such as augmented reality and
robotics. In this work, we conduct a systematic analysis of the discrepancies
between egocentric and exocentric videos, revealing key challenges such as
shorter object durations, sparser trajectories, smaller object sizes, and
larger positional shifts. To address these challenges, we introduce EgoMask,
the first pixel-level benchmark for fine-grained spatiotemporal grounding in
egocentric videos. It is constructed by our proposed automatic annotation
pipeline, which annotates referring expressions and object masks across short-,
medium-, and long-term videos. Additionally, we create EgoMask-Train, a
large-scale training dataset to facilitate model development. Experiments
demonstrate that the state-of-the-art spatiotemporal grounding models perform
poorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields
significant improvements, while preserving performance on exocentric datasets.
Our work thus provides essential resources and insights for advancing
egocentric video understanding. Our code is available at
https://github.com/LaVi-Lab/EgoMask .

</details>


### [208] [Controllable Pedestrian Video Editing for Multi-View Driving Scenarios via Motion Sequence](https://arxiv.org/abs/2508.00299)
*Danzhen Fu,Jiagao Hu,Daiguo Zhou,Fei Wang,Zepeng Wang,Wenhua Liao*

Main category: cs.CV

TL;DR: 提出了一种可控的行人视频编辑框架，用于增强自动驾驶训练数据中的危险场景表示。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶系统中的行人检测模型因训练数据中危险场景不足而缺乏鲁棒性。

Method: 通过视频修复和人体运动控制技术，在多视角驾驶场景中实现行人编辑，包括插入、替换和移除。

Result: 实验表明，该方法能高质量地完成行人编辑，具有视觉真实感、时空一致性和多视角一致性。

Conclusion: 该方法为多视角行人视频生成提供了鲁棒且通用的解决方案，适用于数据增强和场景模拟。

Abstract: Pedestrian detection models in autonomous driving systems often lack
robustness due to insufficient representation of dangerous pedestrian scenarios
in training datasets. To address this limitation, we present a novel framework
for controllable pedestrian video editing in multi-view driving scenarios by
integrating video inpainting and human motion control techniques. Our approach
begins by identifying pedestrian regions of interest across multiple camera
views, expanding detection bounding boxes with a fixed ratio, and resizing and
stitching these regions into a unified canvas while preserving cross-view
spatial relationships. A binary mask is then applied to designate the editable
area, within which pedestrian editing is guided by pose sequence control
conditions. This enables flexible editing functionalities, including pedestrian
insertion, replacement, and removal. Extensive experiments demonstrate that our
framework achieves high-quality pedestrian editing with strong visual realism,
spatiotemporal coherence, and cross-view consistency. These results establish
the proposed method as a robust and versatile solution for multi-view
pedestrian video generation, with broad potential for applications in data
augmentation and scenario simulation in autonomous driving.

</details>


### [209] [GV-VAD : Exploring Video Generation for Weakly-Supervised Video Anomaly Detection](https://arxiv.org/abs/2508.00312)
*Suhang Cai,Xiaohao Peng,Chong Wang,Xiaojie Cai,Jiangbo Qian*

Main category: cs.CV

TL;DR: 论文提出了一种基于生成视频增强的弱监督视频异常检测框架（GV-VAD），通过文本条件视频生成模型生成可控且物理合理的合成视频，低成本扩充训练数据，并采用合成样本损失缩放策略优化训练效果。


<details>
  <summary>Details</summary>
Motivation: 解决视频异常检测中真实异常数据稀缺、标注成本高的问题，提升模型的性能和泛化能力。

Method: 利用文本条件视频生成模型生成合成视频，结合合成样本损失缩放策略优化训练。

Result: 在UCF-Crime数据集上表现优于现有方法。

Conclusion: GV-VAD框架通过合成视频增强训练数据，显著提升了视频异常检测的性能。

Abstract: Video anomaly detection (VAD) plays a critical role in public safety
applications such as intelligent surveillance. However, the rarity,
unpredictability, and high annotation cost of real-world anomalies make it
difficult to scale VAD datasets, which limits the performance and
generalization ability of existing models. To address this challenge, we
propose a generative video-enhanced weakly-supervised video anomaly detection
(GV-VAD) framework that leverages text-conditioned video generation models to
produce semantically controllable and physically plausible synthetic videos.
These virtual videos are used to augment training data at low cost. In
addition, a synthetic sample loss scaling strategy is utilized to control the
influence of generated synthetic samples for efficient training. The
experiments show that the proposed framework outperforms state-of-the-art
methods on UCF-Crime datasets. The code is available at
https://github.com/Sumutan/GV-VAD.git.

</details>


### [210] [Advancing Welding Defect Detection in Maritime Operations via Adapt-WeldNet and Defect Detection Interpretability Analysis](https://arxiv.org/abs/2508.00381)
*Kamal Basha S,Athira Nambiar*

Main category: cs.CV

TL;DR: 本文提出Adapt-WeldNet框架和DDIA方法，提升焊接缺陷检测的性能和可解释性，适用于海洋和离岸环境。


<details>
  <summary>Details</summary>
Motivation: 传统无损检测方法难以检测细微或内部缺陷，现有神经网络方法缺乏可解释性，存在安全隐患。

Method: Adapt-WeldNet系统评估预训练架构、迁移学习策略和自适应优化器；DDIA框架结合XAI技术和专家验证。

Result: 优化了缺陷检测性能，提升了系统透明度和可信度。

Conclusion: 该工作增强了焊接缺陷检测系统的信任、安全性和可靠性。

Abstract: Weld defect detection is crucial for ensuring the safety and reliability of
piping systems in the oil and gas industry, especially in challenging marine
and offshore environments. Traditional non-destructive testing (NDT) methods
often fail to detect subtle or internal defects, leading to potential failures
and costly downtime. Furthermore, existing neural network-based approaches for
defect classification frequently rely on arbitrarily selected pretrained
architectures and lack interpretability, raising safety concerns for
deployment. To address these challenges, this paper introduces
``Adapt-WeldNet", an adaptive framework for welding defect detection that
systematically evaluates various pre-trained architectures, transfer learning
strategies, and adaptive optimizers to identify the best-performing model and
hyperparameters, optimizing defect detection and providing actionable insights.
Additionally, a novel Defect Detection Interpretability Analysis (DDIA)
framework is proposed to enhance system transparency. DDIA employs Explainable
AI (XAI) techniques, such as Grad-CAM and LIME, alongside domain-specific
evaluations validated by certified ASNT NDE Level II professionals.
Incorporating a Human-in-the-Loop (HITL) approach and aligning with the
principles of Trustworthy AI, DDIA ensures the reliability, fairness, and
accountability of the defect detection system, fostering confidence in
automated decisions through expert validation. By improving both performance
and interpretability, this work enhances trust, safety, and reliability in
welding defect detection systems, supporting critical operations in offshore
and marine environments.

</details>


### [211] [$MV_{Hybrid}$: Improving Spatial Transcriptomics Prediction with Hybrid State Space-Vision Transformer Backbone in Pathology Vision Foundation Models](https://arxiv.org/abs/2508.00383)
*Won June Cho,Hongjun Yoon,Daeky Jeong,Hyeongyeol Lim,Yosep Chong*

Main category: cs.CV

TL;DR: 论文提出了一种混合架构$MV_{Hybrid}$，结合状态空间模型（SSMs）和ViT，用于预测病理图像中的基因表达，性能优于现有ViT模型。


<details>
  <summary>Details</summary>
Motivation: 空间转录组学成本高且技术复杂，限制了临床应用。从常规病理图像预测基因表达是一种实用替代方案，但现有ViT模型性能不足。

Method: 提出$MV_{Hybrid}$架构，结合SSMs和ViT，利用负实特征值初始化SSMs以增强低频模式捕捉能力。在相同数据集上预训练并评估多种架构。

Result: $MV_{Hybrid}$在LOSO评估中相关性比ViT高57%，性能下降减少43%，在分类、检索和生存预测任务中表现优于ViT。

Conclusion: $MV_{Hybrid}$作为新一代病理视觉基础模型，具有优越性能和鲁棒性，有望推动精准肿瘤学应用。

Abstract: Spatial transcriptomics reveals gene expression patterns within tissue
context, enabling precision oncology applications such as treatment response
prediction, but its high cost and technical complexity limit clinical adoption.
Predicting spatial gene expression (biomarkers) from routine histopathology
images offers a practical alternative, yet current vision foundation models
(VFMs) in pathology based on Vision Transformer (ViT) backbones perform below
clinical standards. Given that VFMs are already trained on millions of diverse
whole slide images, we hypothesize that architectural innovations beyond ViTs
may better capture the low-frequency, subtle morphological patterns correlating
with molecular phenotypes. By demonstrating that state space models initialized
with negative real eigenvalues exhibit strong low-frequency bias, we introduce
$MV_{Hybrid}$, a hybrid backbone architecture combining state space models
(SSMs) with ViT. We compare five other different backbone architectures for
pathology VFMs, all pretrained on identical colorectal cancer datasets using
the DINOv2 self-supervised learning method. We evaluate all pretrained models
using both random split and leave-one-study-out (LOSO) settings of the same
biomarker dataset. In LOSO evaluation, $MV_{Hybrid}$ achieves 57% higher
correlation than the best-performing ViT and shows 43% smaller performance
degradation compared to random split in gene expression prediction,
demonstrating superior performance and robustness, respectively. Furthermore,
$MV_{Hybrid}$ shows equal or better downstream performance in classification,
patch retrieval, and survival prediction tasks compared to that of ViT, showing
its promise as a next-generation pathology VFM backbone. Our code is publicly
available at: https://github.com/deepnoid-ai/MVHybrid.

</details>


### [212] [Decouple before Align: Visual Disentanglement Enhances Prompt Tuning](https://arxiv.org/abs/2508.00395)
*Fei Zhang,Tianfei Zhou,Jiangchao Yao,Ya Zhang,Ivor W. Tsang,Yanfeng Wang*

Main category: cs.CV

TL;DR: 本文提出DAPT框架，解决提示调优中的视觉与文本模态信息不对称问题，通过解耦和对齐视觉模态的前景与背景，提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决提示调优中视觉模态比文本模态传递更多上下文信息导致的信息不对称问题。

Method: 提出DAPT框架，先解耦视觉模态为前景和背景表示，再分别与文本对齐，并引入视觉拉推正则化以增强关注。

Result: 在少样本学习、基础到新类泛化和数据高效学习中表现优异。

Conclusion: DAPT通过解耦和对齐视觉模态，有效解决了信息不对称问题，提升了模型性能。

Abstract: Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm,
has showcased remarkable effectiveness in improving the task-specific
transferability of vision-language models. This paper delves into a previously
overlooked information asymmetry issue in PT, where the visual modality mostly
conveys more context than the object-oriented textual modality.
Correspondingly, coarsely aligning these two modalities could result in the
biased attention, driving the model to merely focus on the context area. To
address this, we propose DAPT, an effective PT framework based on an intuitive
decouple-before-align concept. First, we propose to explicitly decouple the
visual modality into the foreground and background representation via
exploiting coarse-and-fine visual segmenting cues, and then both of these
decoupled patterns are aligned with the original foreground texts and the
hand-crafted background classes, thereby symmetrically strengthening the modal
alignment. To further enhance the visual concentration, we propose a visual
pull-push regularization tailored for the foreground-background patterns,
directing the original visual representation towards unbiased attention on the
region-of-interest object. We demonstrate the power of architecture-free DAPT
through few-shot learning, base-to-novel generalization, and data-efficient
learning, all of which yield superior performance across prevailing benchmarks.
Our code will be released at https://github.com/Ferenas/DAPT.

</details>


### [213] [DC-AE 1.5: Accelerating Diffusion Model Convergence with Structured Latent Space](https://arxiv.org/abs/2508.00413)
*Junyu Chen,Dongyun Zou,Wenkun He,Junsong Chen,Enze Xie,Song Han,Han Cai*

Main category: cs.CV

TL;DR: DC-AE 1.5是一种新型深度压缩自编码器家族，通过结构化潜在空间和增强扩散训练策略，解决了高分辨率扩散模型中潜在通道增加导致的收敛慢和生成质量下降问题。


<details>
  <summary>Details</summary>
Motivation: 高分辨率扩散模型中，增加自编码器的潜在通道数虽能提升重建质量，但会导致扩散模型收敛变慢，进而影响生成质量。这一问题限制了潜在扩散模型的质量上限，并阻碍了更高空间压缩比自编码器的应用。

Method: 提出了两种创新方法：1) 结构化潜在空间，通过训练在潜在空间中施加通道级结构，前部通道捕捉对象结构，后部通道捕捉图像细节；2) 增强扩散训练，通过额外的扩散训练目标加速收敛。

Result: 在ImageNet 512x512上，DC-AE-1.5-f64c128比DC-AE-f32c32生成质量更高，且速度快4倍。

Conclusion: DC-AE 1.5通过结构化潜在空间和增强扩散训练策略，显著提升了扩散模型的收敛速度和生成质量，为高分辨率图像生成提供了更高效的解决方案。

Abstract: We present DC-AE 1.5, a new family of deep compression autoencoders for
high-resolution diffusion models. Increasing the autoencoder's latent channel
number is a highly effective approach for improving its reconstruction quality.
However, it results in slow convergence for diffusion models, leading to poorer
generation quality despite better reconstruction quality. This issue limits the
quality upper bound of latent diffusion models and hinders the employment of
autoencoders with higher spatial compression ratios. We introduce two key
innovations to address this challenge: i) Structured Latent Space, a
training-based approach to impose a desired channel-wise structure on the
latent space with front latent channels capturing object structures and latter
latent channels capturing image details; ii) Augmented Diffusion Training, an
augmented diffusion training strategy with additional diffusion training
objectives on object latent channels to accelerate convergence. With these
techniques, DC-AE 1.5 delivers faster convergence and better diffusion scaling
results than DC-AE. On ImageNet 512x512, DC-AE-1.5-f64c128 delivers better
image generation quality than DC-AE-f32c32 while being 4x faster. Code:
https://github.com/dc-ai-projects/DC-Gen.

</details>


### [214] [Contact-Aware Amodal Completion for Human-Object Interaction via Multi-Regional Inpainting](https://arxiv.org/abs/2508.00427)
*Seunggeun Chi,Enna Sachdeva,Pin-Hao Huang,Kwonjoon Lee*

Main category: cs.CV

TL;DR: 提出了一种结合物理先验知识和多区域修复技术的新方法，用于动态场景中的遮挡物体补全，显著提升了生成结果的准确性和真实性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在动态场景中因对人类-物体交互（HOI）理解有限，难以生成合理的遮挡补全结果。

Method: 利用人类拓扑和接触信息的物理约束，定义主次区域，并在扩散模型中采用定制化的去噪策略进行多区域修复。

Result: 实验表明，该方法在HOI场景中显著优于现有方法，且无需真实接触标注也能保持鲁棒性。

Conclusion: 该方法使机器感知更接近人类对动态环境的理解，适用于3D重建和新视角/姿态合成等任务。

Abstract: Amodal completion, which is the process of inferring the full appearance of
objects despite partial occlusions, is crucial for understanding complex
human-object interactions (HOI) in computer vision and robotics. Existing
methods, such as those that use pre-trained diffusion models, often struggle to
generate plausible completions in dynamic scenarios because they have a limited
understanding of HOI. To solve this problem, we've developed a new approach
that uses physical prior knowledge along with a specialized multi-regional
inpainting technique designed for HOI. By incorporating physical constraints
from human topology and contact information, we define two distinct regions:
the primary region, where occluded object parts are most likely to be, and the
secondary region, where occlusions are less probable. Our multi-regional
inpainting method uses customized denoising strategies across these regions
within a diffusion model. This improves the accuracy and realism of the
generated completions in both their shape and visual detail. Our experimental
results show that our approach significantly outperforms existing methods in
HOI scenarios, moving machine perception closer to a more human-like
understanding of dynamic environments. We also show that our pipeline is robust
even without ground-truth contact annotations, which broadens its applicability
to tasks like 3D reconstruction and novel view/pose synthesis.

</details>


### [215] [Reducing the gap between general purpose data and aerial images in concentrated solar power plants](https://arxiv.org/abs/2508.00440)
*M. A. Pérez-Cutiño,J. Valverde,J. Capitán,J. M. Díaz-Báñez*

Main category: cs.CV

TL;DR: 论文提出AerialCSP虚拟数据集，用于模拟CSP工厂的航拍图像，以减少对大量手动标注数据的依赖，并提升模型在真实场景中的故障检测能力。


<details>
  <summary>Details</summary>
Motivation: CSP工厂的航拍图像具有高反射面和领域特定元素，现有通用数据集训练的模型难以泛化，而手动标注数据成本高、耗时长。

Method: 创建AerialCSP虚拟数据集，模拟真实CSP工厂的航拍图像，用于模型预训练，并评估多种模型在该数据集上的表现。

Result: 预训练在AerialCSP上的模型显著提高了真实场景中故障检测的准确性，尤其是对小而罕见的缺陷。

Conclusion: AerialCSP为CSP相关视觉任务提供了高质量合成数据，减少了手动标注需求，提升了模型性能。

Abstract: In the context of Concentrated Solar Power (CSP) plants, aerial images
captured by drones present a unique set of challenges. Unlike urban or natural
landscapes commonly found in existing datasets, solar fields contain highly
reflective surfaces, and domain-specific elements that are uncommon in
traditional computer vision benchmarks. As a result, machine learning models
trained on generic datasets struggle to generalize to this setting without
extensive retraining and large volumes of annotated data. However, collecting
and labeling such data is costly and time-consuming, making it impractical for
rapid deployment in industrial applications.
  To address this issue, we propose a novel approach: the creation of
AerialCSP, a virtual dataset that simulates aerial imagery of CSP plants. By
generating synthetic data that closely mimic real-world conditions, our
objective is to facilitate pretraining of models before deployment,
significantly reducing the need for extensive manual labeling. Our main
contributions are threefold: (1) we introduce AerialCSP, a high-quality
synthetic dataset for aerial inspection of CSP plants, providing annotated data
for object detection and image segmentation; (2) we benchmark multiple models
on AerialCSP, establishing a baseline for CSP-related vision tasks; and (3) we
demonstrate that pretraining on AerialCSP significantly improves real-world
fault detection, particularly for rare and small defects, reducing the need for
extensive manual labeling. AerialCSP is made publicly available at
https://mpcutino.github.io/aerialcsp/.

</details>


### [216] [TopoTTA: Topology-Enhanced Test-Time Adaptation for Tubular Structure Segmentation](https://arxiv.org/abs/2508.00442)
*Jiale Zhou,Wenhan Wang,Shikun Li,Xiaolei Qu,Xin Guo,Yizhong Liu,Wenzhong Tang,Xun Lin,Yefeng Zheng*

Main category: cs.CV

TL;DR: TopoTTA是一种针对管状结构分割（TSS）的测试时自适应框架，通过两阶段方法解决领域偏移问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 管状结构分割对领域偏移特别敏感，传统的分割方法在未见目标域中性能下降严重。

Method: TopoTTA分为两阶段：第一阶段使用TopoMDCs增强拓扑表示；第二阶段通过TopoHG策略和伪标签对齐提升拓扑连续性。

Result: 在四个场景和十个数据集上，TopoTTA平均提升了31.81%的clDice分数。

Conclusion: TopoTTA是一种有效的、即插即用的解决方案，适用于CNN-based TSS模型。

Abstract: Tubular structure segmentation (TSS) is important for various applications,
such as hemodynamic analysis and route navigation. Despite significant progress
in TSS, domain shifts remain a major challenge, leading to performance
degradation in unseen target domains. Unlike other segmentation tasks, TSS is
more sensitive to domain shifts, as changes in topological structures can
compromise segmentation integrity, and variations in local features
distinguishing foreground from background (e.g., texture and contrast) may
further disrupt topological continuity. To address these challenges, we propose
Topology-enhanced Test-Time Adaptation (TopoTTA), the first test-time
adaptation framework designed specifically for TSS. TopoTTA consists of two
stages: Stage 1 adapts models to cross-domain topological discrepancies using
the proposed Topological Meta Difference Convolutions (TopoMDCs), which enhance
topological representation without altering pre-trained parameters; Stage 2
improves topological continuity by a novel Topology Hard sample Generation
(TopoHG) strategy and prediction alignment on hard samples with pseudo-labels
in the generated pseudo-break regions. Extensive experiments across four
scenarios and ten datasets demonstrate TopoTTA's effectiveness in handling
topological distribution shifts, achieving an average improvement of 31.81% in
clDice. TopoTTA also serves as a plug-and-play TTA solution for CNN-based TSS
models.

</details>


### [217] [Graph Lineages and Skeletal Graph Products](https://arxiv.org/abs/2508.00197)
*Eric Mjolsness,Cory B. Scott*

Main category: cs.CV

TL;DR: 论文提出了一种结构化图谱系（graph lineages）的定义，用于描述层次化增长的图结构，并推导了其代数类型理论，适用于层次化模型架构和局部算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是为机器学习和计算科学中的数学模型架构提供一种层次化增长的图结构描述方法，以支持高效的多尺度分析和操作。

Method: 方法包括定义层次化增长的图谱系，推导其代数操作（如乘积、和等），并引入空间高效的一元操作（如加厚和升级）。

Result: 结果表明，这些操作具有类似但不同于标准操作的代数和范畴论性质，并能逼近连续极限对象。

Conclusion: 结论是该方法适用于定义层次化模型架构（如深度神经网络和多网格数值方法），并支持局部采样和优化算法。

Abstract: Graphs, and sequences of growing graphs, can be used to specify the
architecture of mathematical models in many fields including machine learning
and computational science. Here we define structured graph "lineages" (ordered
by level number) that grow in a hierarchical fashion, so that: (1) the number
of graph vertices and edges increases exponentially in level number; (2)
bipartite graphs connect successive levels within a graph lineage and, as in
multigrid methods, can constrain matrices relating successive levels; (3) using
prolongation maps within a graph lineage, process-derived distance measures
between graphs at successive levels can be defined; (4) a category of "graded
graphs" can be defined, and using it low-cost "skeletal" variants of standard
algebraic graph operations and type constructors (cross product, box product,
disjoint sum, and function types) can be derived for graded graphs and hence
hierarchical graph lineages; (5) these skeletal binary operators have similar
but not identical algebraic and category-theoretic properties to their standard
counterparts; (6) graph lineages and their skeletal product constructors can
approach continuum limit objects. Additional space-efficient unary operators on
graded graphs are also derived: thickening, which creates a graph lineage of
multiscale graphs, and escalation to a graph lineage of search frontiers
(useful as a generalization of adaptive grids and in defining "skeletal"
functions). The result is an algebraic type theory for graded graphs and
(hierarchical) graph lineages. The approach is expected to be well suited to
defining hierarchical model architectures - "hierarchitectures" - and local
sampling, search, or optimization algorithms on them. We demonstrate such
application to deep neural networks (including visual and feature scale spaces)
and to multigrid numerical methods.

</details>


### [218] [SAM-PTx: Text-Guided Fine-Tuning of SAM with Parameter-Efficient, Parallel-Text Adapters](https://arxiv.org/abs/2508.00213)
*Shayan Jalilian,Abdul Bais*

Main category: cs.CV

TL;DR: SAM-PTx通过轻量级适配器设计，将CLIP文本嵌入注入SAM的图像编码器，提升语义引导的分割性能。


<details>
  <summary>Details</summary>
Motivation: 探索语义文本提示在分割任务中的潜力，弥补传统空间提示的不足。

Method: 提出Parallel-Text适配器，仅修改MLP并行分支，保留注意力路径，使用冻结CLIP文本嵌入作为语义指导。

Result: 在COD10K、COCO和ADE20K数据集上，语义提示显著优于纯空间提示基线。

Conclusion: 语义条件集成是高效适应SAM架构的实用且可扩展方法。

Abstract: The Segment Anything Model (SAM) has demonstrated impressive generalization
in prompt-based segmentation. Yet, the potential of semantic text prompts
remains underexplored compared to traditional spatial prompts like points and
boxes. This paper introduces SAM-PTx, a parameter-efficient approach for
adapting SAM using frozen CLIP-derived text embeddings as class-level semantic
guidance. Specifically, we propose a lightweight adapter design called
Parallel-Text that injects text embeddings into SAM's image encoder, enabling
semantics-guided segmentation while keeping most of the original architecture
frozen. Our adapter modifies only the MLP-parallel branch of each transformer
block, preserving the attention pathway for spatial reasoning. Through
supervised experiments and ablations on the COD10K dataset as well as low-data
subsets of COCO and ADE20K, we show that incorporating fixed text embeddings as
input improves segmentation performance over purely spatial prompt baselines.
To our knowledge, this is the first work to use text prompts for segmentation
on the COD10K dataset. These results suggest that integrating semantic
conditioning into SAM's architecture offers a practical and scalable path for
efficient adaptation with minimal computational complexity.

</details>


### [219] [Object-Centric Cropping for Visual Few-Shot Classification](https://arxiv.org/abs/2508.00218)
*Aymane Abdali,Bartosz Boguslawski,Lucas Drumetz,Vincent Gripon*

Main category: cs.CV

TL;DR: 在少样本图像分类中，通过引入目标局部位置信息显著提升性能，且仅需少量标注或完全无监督方法即可实现。


<details>
  <summary>Details</summary>
Motivation: 解决少样本图像分类中因图像模糊（如多目标或复杂背景）导致的性能下降问题。

Method: 利用Segment Anything Model（仅需标注目标的一个像素）或完全无监督的前景目标提取方法。

Result: 在多个基准测试中显著提升了分类性能。

Conclusion: 局部位置信息的引入是提升少样本分类性能的有效方法，且可通过轻量级标注或无监督方式实现。

Abstract: In the domain of Few-Shot Image Classification, operating with as little as
one example per class, the presence of image ambiguities stemming from multiple
objects or complex backgrounds can significantly deteriorate performance. Our
research demonstrates that incorporating additional information about the local
positioning of an object within its image markedly enhances classification
across established benchmarks. More importantly, we show that a significant
fraction of the improvement can be achieved through the use of the Segment
Anything Model, requiring only a pixel of the object of interest to be pointed
out, or by employing fully unsupervised foreground object extraction methods.

</details>


### [220] [LesiOnTime -- Joint Temporal and Clinical Modeling for Small Breast Lesion Segmentation in Longitudinal DCE-MRI](https://arxiv.org/abs/2508.00496)
*Mohammed Kamran,Maria Bernathova,Raoul Varga,Christian Singer,Zsuzsanna Bago-Horvath,Thomas Helbich,Georg Langs,Philipp Seeböck*

Main category: cs.CV

TL;DR: LesiOnTime是一种新颖的3D分割方法，通过结合纵向影像和BI-RADS评分，模拟临床诊断流程，显著提升了小病灶分割的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法主要针对大病灶，忽略了纵向和临床信息，而这些信息对早期癌症检测至关重要。

Method: 提出Temporal Prior Attention（TPA）块动态整合历史与当前扫描信息，以及BI-RADS一致性正则化（BCR）损失，将领域知识嵌入训练过程。

Result: 在DCE-MRI数据集上，LesiOnTime比现有单时间点和纵向基线方法Dice分数提高了5%。

Conclusion: 研究表明，结合时间和临床背景对乳腺癌筛查中的早期病灶分割至关重要。

Abstract: Accurate segmentation of small lesions in Breast Dynamic Contrast-Enhanced
MRI (DCE-MRI) is critical for early cancer detection, especially in high-risk
patients. While recent deep learning methods have advanced lesion segmentation,
they primarily target large lesions and neglect valuable longitudinal and
clinical information routinely used by radiologists. In real-world screening,
detecting subtle or emerging lesions requires radiologists to compare across
timepoints and consider previous radiology assessments, such as the BI-RADS
score. We propose LesiOnTime, a novel 3D segmentation approach that mimics
clinical diagnostic workflows by jointly leveraging longitudinal imaging and
BIRADS scores. The key components are: (1) a Temporal Prior Attention (TPA)
block that dynamically integrates information from previous and current scans;
and (2) a BI-RADS Consistency Regularization (BCR) loss that enforces latent
space alignment for scans with similar radiological assessments, thus embedding
domain knowledge into the training process. Evaluated on a curated in-house
longitudinal dataset of high-risk patients with DCE-MRI, our approach
outperforms state-of-the-art single-timepoint and longitudinal baselines by 5%
in terms of Dice. Ablation studies demonstrate that both TPA and BCR contribute
complementary performance gains. These results highlight the importance of
incorporating temporal and clinical context for reliable early lesion
segmentation in real-world breast cancer screening. Our code is publicly
available at https://github.com/cirmuw/LesiOnTime

</details>


### [221] [Steering Guidance for Personalized Text-to-Image Diffusion Models](https://arxiv.org/abs/2508.00319)
*Sunghyun Park,Seokeon Choi,Hyoungwoo Park,Sungrack Yun*

Main category: cs.CV

TL;DR: 提出了一种个性化引导方法，通过未学习的弱模型和动态权重插值，平衡目标分布对齐与文本编辑能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如CFG和AG）在少量图像微调时无法平衡目标分布对齐与原始模型知识保留。

Method: 利用未学习的弱模型和动态权重插值，在推理时控制未学习程度。

Result: 实验表明，该方法能提升文本对齐和目标分布保真度，且无需额外计算开销。

Conclusion: 个性化引导方法有效解决了现有方法的局限性，实现了更好的平衡。

Abstract: Personalizing text-to-image diffusion models is crucial for adapting the
pre-trained models to specific target concepts, enabling diverse image
generation. However, fine-tuning with few images introduces an inherent
trade-off between aligning with the target distribution (e.g., subject
fidelity) and preserving the broad knowledge of the original model (e.g., text
editability). Existing sampling guidance methods, such as classifier-free
guidance (CFG) and autoguidance (AG), fail to effectively guide the output
toward well-balanced space: CFG restricts the adaptation to the target
distribution, while AG compromises text alignment. To address these
limitations, we propose personalization guidance, a simple yet effective method
leveraging an unlearned weak model conditioned on a null text prompt. Moreover,
our method dynamically controls the extent of unlearning in a weak model
through weight interpolation between pre-trained and fine-tuned models during
inference. Unlike existing guidance methods, which depend solely on guidance
scales, our method explicitly steers the outputs toward a balanced latent space
without additional computational overhead. Experimental results demonstrate
that our proposed guidance can improve text alignment and target distribution
fidelity, integrating seamlessly with various fine-tuning strategies.

</details>


### [222] [Wukong Framework for Not Safe For Work Detection in Text-to-Image systems](https://arxiv.org/abs/2508.00591)
*Mingrui Liu,Sixiao Zhang,Cheng Long*

Main category: cs.CV

TL;DR: 论文提出Wukong框架，利用扩散模型早期去噪步骤的中间输出和预训练参数，高效检测NSFW内容，性能优于文本过滤器，接近图像过滤器。


<details>
  <summary>Details</summary>
Motivation: 现有NSFW内容检测方法（文本过滤器和图像过滤器）存在效率低或易受攻击的问题，需更高效准确的解决方案。

Method: 基于扩散模型的早期去噪步骤和U-Net的交叉注意力层，设计Wukong框架，实现早期检测。

Result: Wukong在效率和准确性上显著优于文本过滤器，与图像过滤器相当。

Conclusion: Wukong为NSFW内容检测提供了高效准确的解决方案，适用于现代T2I系统。

Abstract: Text-to-Image (T2I) generation is a popular AI-generated content (AIGC)
technology enabling diverse and creative image synthesis. However, some outputs
may contain Not Safe For Work (NSFW) content (e.g., violence), violating
community guidelines. Detecting NSFW content efficiently and accurately, known
as external safeguarding, is essential. Existing external safeguards fall into
two types: text filters, which analyze user prompts but overlook T2I
model-specific variations and are prone to adversarial attacks; and image
filters, which analyze final generated images but are computationally costly
and introduce latency. Diffusion models, the foundation of modern T2I systems
like Stable Diffusion, generate images through iterative denoising using a
U-Net architecture with ResNet and Transformer blocks. We observe that: (1)
early denoising steps define the semantic layout of the image, and (2)
cross-attention layers in U-Net are crucial for aligning text and image
regions. Based on these insights, we propose Wukong, a transformer-based NSFW
detection framework that leverages intermediate outputs from early denoising
steps and reuses U-Net's pre-trained cross-attention parameters. Wukong
operates within the diffusion process, enabling early detection without waiting
for full image generation. We also introduce a new dataset containing prompts,
seeds, and image-specific NSFW labels, and evaluate Wukong on this and two
public benchmarks. Results show that Wukong significantly outperforms
text-based safeguards and achieves comparable accuracy of image filters, while
offering much greater efficiency.

</details>


### [223] [Backdoor Attacks on Deep Learning Face Detection](https://arxiv.org/abs/2508.00620)
*Quentin Le Roux,Yannick Teglia,Teddy Furon,Philippe Loubet-Moundi*

Main category: cs.CV

TL;DR: 论文研究了人脸检测系统中的漏洞，提出了一种名为“人脸生成攻击”的方法，首次展示了针对地标回归任务的攻击，并提出了缓解措施。


<details>
  <summary>Details</summary>
Motivation: 在无约束环境下，人脸检测系统面临光照、姿态等挑战，需要依赖边界框和地标坐标回归。然而，这些任务可能受到攻击，因此研究其安全漏洞至关重要。

Method: 论文提出了“人脸生成攻击”和“地标偏移攻击”，通过生成对抗样本或操纵地标坐标，攻击人脸检测系统的回归任务。

Result: 实验首次证明了地标回归任务可被后门攻击，展示了攻击的有效性。

Conclusion: 论文揭示了人脸检测系统的安全漏洞，并提出了相应的防御措施，为未来研究提供了方向。

Abstract: Face Recognition Systems that operate in unconstrained environments capture
images under varying conditions,such as inconsistent lighting, or diverse face
poses. These challenges require including a Face Detection module that
regresses bounding boxes and landmark coordinates for proper Face Alignment.
This paper shows the effectiveness of Object Generation Attacks on Face
Detection, dubbed Face Generation Attacks, and demonstrates for the first time
a Landmark Shift Attack that backdoors the coordinate regression task performed
by face detectors. We then offer mitigations against these vulnerabilities.

</details>


### [224] [CLIPTime: Time-Aware Multimodal Representation Learning from Images and Text](https://arxiv.org/abs/2508.00447)
*Anju Rani,Daniel Ortiz-Arroyo,Petar Durdevic*

Main category: cs.CV

TL;DR: CLIPTime是一个基于CLIP架构的多模态、多任务框架，旨在通过图像和文本输入预测真菌生长的发育阶段和时间戳，无需显式时间输入。


<details>
  <summary>Details</summary>
Motivation: 理解生物生长的时间动态对微生物学、农业等领域至关重要，但现有视觉语言模型在捕捉时间进展方面效果有限。

Method: CLIPTime通过联合视觉-文本嵌入学习，实现时间感知推理，并引入合成真菌生长数据集进行训练和评估。

Result: 实验表明，CLIPTime能有效建模生物进展，并生成可解释的时间相关输出。

Conclusion: CLIPTime展示了视觉语言模型在生物监测应用中的潜力。

Abstract: Understanding the temporal dynamics of biological growth is critical across
diverse fields such as microbiology, agriculture, and biodegradation research.
Although vision-language models like Contrastive Language Image Pretraining
(CLIP) have shown strong capabilities in joint visual-textual reasoning, their
effectiveness in capturing temporal progression remains limited. To address
this, we propose CLIPTime, a multimodal, multitask framework designed to
predict both the developmental stage and the corresponding timestamp of fungal
growth from image and text inputs. Built upon the CLIP architecture, our model
learns joint visual-textual embeddings and enables time-aware inference without
requiring explicit temporal input during testing. To facilitate training and
evaluation, we introduce a synthetic fungal growth dataset annotated with
aligned timestamps and categorical stage labels. CLIPTime jointly performs
classification and regression, predicting discrete growth stages alongside
continuous timestamps. We also propose custom evaluation metrics, including
temporal accuracy and regression error, to assess the precision of time-aware
predictions. Experimental results demonstrate that CLIPTime effectively models
biological progression and produces interpretable, temporally grounded outputs,
highlighting the potential of vision-language models in real-world biological
monitoring applications.

</details>


### [225] [Guiding Diffusion-Based Articulated Object Generation by Partial Point Cloud Alignment and Physical Plausibility Constraints](https://arxiv.org/abs/2508.00558)
*Jens U. Kreber,Joerg Stueckler*

Main category: cs.CV

TL;DR: PhysNAP是一种基于扩散模型的新方法，用于生成与部分点云对齐且物理合理的铰接物体。


<details>
  <summary>Details</summary>
Motivation: 铰接物体是日常环境中重要的可交互对象，但现有方法在物理合理性和点云对齐方面存在不足。

Method: 使用带符号距离函数（SDFs）表示部件形状，通过点云对齐损失和非穿透性、移动性约束指导扩散过程。

Result: 在PartNet-Mobility数据集上评估，PhysNAP在约束一致性和生成能力之间取得了平衡。

Conclusion: PhysNAP能够生成物理更合理的铰接物体，并在点云对齐方面表现优于无指导的扩散模型。

Abstract: Articulated objects are an important type of interactable objects in everyday
environments. In this paper, we propose PhysNAP, a novel diffusion model-based
approach for generating articulated objects that aligns them with partial point
clouds and improves their physical plausibility. The model represents part
shapes by signed distance functions (SDFs). We guide the reverse diffusion
process using a point cloud alignment loss computed using the predicted SDFs.
Additionally, we impose non-penetration and mobility constraints based on the
part SDFs for guiding the model to generate more physically plausible objects.
We also make our diffusion approach category-aware to further improve point
cloud alignment if category information is available. We evaluate the
generative ability and constraint consistency of samples generated with PhysNAP
using the PartNet-Mobility dataset. We also compare it with an unguided
baseline diffusion model and demonstrate that PhysNAP can improve constraint
consistency and provides a tradeoff with generative ability.

</details>


### [226] [D3: Training-Free AI-Generated Video Detection Using Second-Order Features](https://arxiv.org/abs/2508.00701)
*Chende Zheng,Ruiqi suo,Chenhao Lin,Zhengyu Zhao,Le Yang,Shuai Liu,Minghui Yang,Cong Wang,Chao Shen*

Main category: cs.CV

TL;DR: 论文提出了一种基于二阶动力学分析的无训练检测方法D3，用于识别AI生成视频中的时间伪影，并在多个数据集上验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有检测方法对合成视频中时间伪影的探索不足，导致检测效果有限。

Method: 通过牛顿力学下的二阶动力学分析建立理论框架，提出基于二阶时间差异的无训练检测方法D3。

Result: D3在4个开源数据集上表现优异，例如在Gen-Video上比之前最佳方法提高了10.39%的平均精度。

Conclusion: D3具有高效的计算性能和鲁棒性，为合成视频检测提供了新思路。

Abstract: The evolution of video generation techniques, such as Sora, has made it
increasingly easy to produce high-fidelity AI-generated videos, raising public
concern over the dissemination of synthetic content. However, existing
detection methodologies remain limited by their insufficient exploration of
temporal artifacts in synthetic videos. To bridge this gap, we establish a
theoretical framework through second-order dynamical analysis under Newtonian
mechanics, subsequently extending the Second-order Central Difference features
tailored for temporal artifact detection. Building on this theoretical
foundation, we reveal a fundamental divergence in second-order feature
distributions between real and AI-generated videos. Concretely, we propose
Detection by Difference of Differences (D3), a novel training-free detection
method that leverages the above second-order temporal discrepancies. We
validate the superiority of our D3 on 4 open-source datasets (Gen-Video,
VideoPhy, EvalCrafter, VidProM), 40 subsets in total. For example, on GenVideo,
D3 outperforms the previous best method by 10.39% (absolute) mean Average
Precision. Additional experiments on time cost and post-processing operations
demonstrate D3's exceptional computational efficiency and strong robust
performance. Our code is available at https://github.com/Zig-HS/D3.

</details>


### [227] [Is It Really You? Exploring Biometric Verification Scenarios in Photorealistic Talking-Head Avatar Videos](https://arxiv.org/abs/2508.00748)
*Laura Pedrouzo-Rodriguez,Pedro Delgado-DeRobles,Luis F. Gomez,Ruben Tolosana,Ruben Vera-Rodriguez,Aythami Morales,Julian Fierrez*

Main category: cs.CV

TL;DR: 论文探讨了在虚拟化身中利用面部运动模式作为行为生物特征进行身份验证的可行性，并提出了一个轻量级的时空图卷积网络架构。


<details>
  <summary>Details</summary>
Motivation: 随着逼真虚拟化身的普及，其带来的安全风险（如冒充攻击）日益严重，需要新的生物特征验证方法。

Method: 使用GAGAvatar生成真实和冒充的虚拟化身视频数据集，并提出基于面部关键点的时空图卷积网络架构。

Result: 实验表明，面部运动特征可实现有效的身份验证，AUC值接近80%。

Conclusion: 研究强调了在虚拟化身通信系统中开发更先进行为生物特征防御的紧迫性，并提供了公开的基准和系统。

Abstract: Photorealistic talking-head avatars are becoming increasingly common in
virtual meetings, gaming, and social platforms. These avatars allow for more
immersive communication, but they also introduce serious security risks. One
emerging threat is impersonation: an attacker can steal a user's
avatar-preserving their appearance and voice-making it nearly impossible to
detect its fraudulent usage by sight or sound alone. In this paper, we explore
the challenge of biometric verification in such avatar-mediated scenarios. Our
main question is whether an individual's facial motion patterns can serve as
reliable behavioral biometrics to verify their identity when the avatar's
visual appearance is a facsimile of its owner. To answer this question, we
introduce a new dataset of realistic avatar videos created using a
state-of-the-art one-shot avatar generation model, GAGAvatar, with genuine and
impostor avatar videos. We also propose a lightweight, explainable
spatio-temporal Graph Convolutional Network architecture with temporal
attention pooling, that uses only facial landmarks to model dynamic facial
gestures. Experimental results demonstrate that facial motion cues enable
meaningful identity verification with AUC values approaching 80%. The proposed
benchmark and biometric system are available for the research community in
order to bring attention to the urgent need for more advanced behavioral
biometric defenses in avatar-based communication systems.

</details>


### [228] [Sample-Aware Test-Time Adaptation for Medical Image-to-Image Translation](https://arxiv.org/abs/2508.00766)
*Irene Iele,Francesco Di Feola,Valerio Guarrasi,Paolo Soda*

Main category: cs.CV

TL;DR: 提出了一种动态测试时间适应（TTA）框架，用于医学图像转换任务，通过重建模块和动态适应块解决分布外样本的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 现有图像转换技术在处理分布外样本时性能下降，需要一种动态适应方法。

Method: 引入重建模块量化域偏移，动态适应块选择性调整预训练模型的内部特征。

Result: 在低剂量CT去噪和T1到T2 MRI转换任务中表现优于基线模型和现有TTA方法。

Conclusion: 动态样本特定调整是提高模型鲁棒性的有效途径。

Abstract: Image-to-image translation has emerged as a powerful technique in medical
imaging, enabling tasks such as image denoising and cross-modality conversion.
However, it suffers from limitations in handling out-of-distribution samples
without causing performance degradation. To address this limitation, we propose
a novel Test-Time Adaptation (TTA) framework that dynamically adjusts the
translation process based on the characteristics of each test sample. Our
method introduces a Reconstruction Module to quantify the domain shift and a
Dynamic Adaptation Block that selectively modifies the internal features of a
pretrained translation model to mitigate the shift without compromising the
performance on in-distribution samples that do not require adaptation. We
evaluate our approach on two medical image-to-image translation tasks: low-dose
CT denoising and T1 to T2 MRI translation, showing consistent improvements over
both the baseline translation model without TTA and prior TTA methods. Our
analysis highlights the limitations of the state-of-the-art that uniformly
apply the adaptation to both out-of-distribution and in-distribution samples,
demonstrating that dynamic, sample-specific adjustment offers a promising path
to improve model resilience in real-world scenarios. The code is available at:
https://github.com/cosbidev/Sample-Aware_TTA.

</details>


### [229] [SU-ESRGAN: Semantic and Uncertainty-Aware ESRGAN for Super-Resolution of Satellite and Drone Imagery with Fine-Tuning for Cross Domain Evaluation](https://arxiv.org/abs/2508.00750)
*Prerana Ramkumar*

Main category: cs.CV

TL;DR: SU-ESRGAN是一种针对卫星图像的SR框架，结合了ESRGAN、DeepLabv3分割损失和蒙特卡洛dropout，提供语义一致性和像素级不确定性。


<details>
  <summary>Details</summary>
Motivation: 解决GAN在卫星图像超分辨率中缺乏语义一致性和像素置信度的问题，适用于关键遥感应用。

Method: 集成ESRGAN、DeepLabv3分割损失和蒙特卡洛dropout，生成像素级不确定性图。

Result: 在PSNR、SSIM和LPIPS指标上与基线ESRGAN相当，适用于无人机和卫星系统。

Conclusion: SU-ESRGAN在跨域应用中表现良好，强调领域感知训练的重要性。

Abstract: Generative Adversarial Networks (GANs) have achieved realistic
super-resolution (SR) of images however, they lack semantic consistency and
per-pixel confidence, limiting their credibility in critical remote sensing
applications such as disaster response, urban planning and agriculture. This
paper introduces Semantic and Uncertainty-Aware ESRGAN (SU-ESRGAN), the first
SR framework designed for satellite imagery to integrate the ESRGAN,
segmentation loss via DeepLabv3 for class detail preservation and Monte Carlo
dropout to produce pixel-wise uncertainty maps. The SU-ESRGAN produces results
(PSNR, SSIM, LPIPS) comparable to the Baseline ESRGAN on aerial imagery. This
novel model is valuable in satellite systems or UAVs that use wide
field-of-view (FoV) cameras, trading off spatial resolution for coverage. The
modular design allows integration in UAV data pipelines for on-board or
post-processing SR to enhance imagery resulting due to motion blur, compression
and sensor limitations. Further, the model is fine-tuned to evaluate its
performance on cross domain applications. The tests are conducted on two drone
based datasets which differ in altitude and imaging perspective. Performance
evaluation of the fine-tuned models show a stronger adaptation to the Aerial
Maritime Drone Dataset, whose imaging characteristics align with the training
data, highlighting the importance of domain-aware training in SR-applications.

</details>


<div id='physics.ed-ph'></div>

# physics.ed-ph [[Back]](#toc)

### [230] [Advancing Quantum Information Science Pre-College Education: The Case for Learning Sciences Collaboration](https://arxiv.org/abs/2508.00668)
*Raquel Coelho,Roy Pea,Christian Schunn,Jinglei Cheng,Junyu Liu*

Main category: physics.ed-ph

TL;DR: 论文探讨如何通过跨学科合作（学习科学）为年轻学习者提供量子信息科学教育支持，提出设计研究和知识表征框架作为关键贡献。


<details>
  <summary>Details</summary>
Motivation: 解决年轻学习者如何参与量子信息科学这一全新领域的挑战。

Method: 结合学习科学的设计研究方法和知识表征框架，推动量子信息科学教育的发展。

Result: 提出跨学科合作的双向模式，既能支持量子概念学习，也能提升复杂领域教学的理解。

Conclusion: 跨学科合作的理论与实践价值值得投入，尽管存在挑战。

Abstract: As quantum information science advances and the need for pre-college
engagement grows, a critical question remains: How can young learners be
prepared to participate in a field so radically different from what they have
encountered before? This paper argues that meeting this challenge will require
strong interdisciplinary collaboration with the Learning Sciences (LS), a field
dedicated to understanding how people learn and designing theory-guided
environments to support learning. Drawing on lessons from previous STEM
education efforts, we discuss two key contributions of the learning sciences to
quantum information science (QIS) education. The first is design-based
research, the signature methodology of learning sciences, which can inform the
development, refinement, and scaling of effective QIS learning experiences. The
second is a framework for reshaping how learners reason about, learn and
participate in QIS practices through shifts in knowledge representations that
provide new forms of engagement and associated learning. We call for a two-way
partnership between quantum information science and the learning sciences, one
that not only supports learning in quantum concepts and practices but also
improves our understanding of how to teach and support learning in highly
complex domains. We also consider potential questions involved in bridging
these disciplinary communities and argue that the theoretical and practical
benefits justify the effort.

</details>
