<div id=toc></div>

# Table of Contents

- [cs.CL](#cs.CL) [Total: 86]
- [cs.AI](#cs.AI) [Total: 58]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.LG](#cs.LG) [Total: 128]
- [q-bio.NC](#q-bio.NC) [Total: 1]
- [cs.HC](#cs.HC) [Total: 7]
- [math.RT](#math.RT) [Total: 1]
- [cs.SI](#cs.SI) [Total: 2]
- [stat.ML](#stat.ML) [Total: 12]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.RO](#cs.RO) [Total: 8]
- [cond-mat.dis-nn](#cond-mat.dis-nn) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [math.AT](#math.AT) [Total: 1]
- [physics.comp-ph](#physics.comp-ph) [Total: 1]
- [cs.SE](#cs.SE) [Total: 12]
- [physics.chem-ph](#physics.chem-ph) [Total: 1]
- [cs.IT](#cs.IT) [Total: 1]
- [math.NA](#math.NA) [Total: 2]
- [math.DS](#math.DS) [Total: 1]
- [math.PR](#math.PR) [Total: 1]
- [q-fin.CP](#q-fin.CP) [Total: 1]
- [cs.DB](#cs.DB) [Total: 1]
- [math.OC](#math.OC) [Total: 3]
- [cs.PL](#cs.PL) [Total: 1]
- [q-fin.TR](#q-fin.TR) [Total: 1]
- [cs.NE](#cs.NE) [Total: 3]
- [cs.CE](#cs.CE) [Total: 1]
- [econ.TH](#econ.TH) [Total: 1]
- [cs.CR](#cs.CR) [Total: 14]
- [gr-qc](#gr-qc) [Total: 1]
- [cs.ET](#cs.ET) [Total: 1]
- [cs.DC](#cs.DC) [Total: 3]
- [stat.AP](#stat.AP) [Total: 1]
- [eess.SP](#eess.SP) [Total: 26]
- [cs.GT](#cs.GT) [Total: 3]
- [cs.CY](#cs.CY) [Total: 9]
- [cs.CV](#cs.CV) [Total: 37]
- [stat.ME](#stat.ME) [Total: 3]
- [cs.AR](#cs.AR) [Total: 1]
- [physics.geo-ph](#physics.geo-ph) [Total: 2]
- [cs.NI](#cs.NI) [Total: 8]
- [hep-ph](#hep-ph) [Total: 1]
- [cs.GR](#cs.GR) [Total: 1]
- [cs.DS](#cs.DS) [Total: 3]
- [q-bio.QM](#q-bio.QM) [Total: 1]
- [astro-ph.HE](#astro-ph.HE) [Total: 1]
- [cs.SD](#cs.SD) [Total: 4]
- [eess.SY](#eess.SY) [Total: 2]
- [eess.IV](#eess.IV) [Total: 10]
- [quant-ph](#quant-ph) [Total: 1]
- [q-fin.ST](#q-fin.ST) [Total: 1]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [physics.app-ph](#physics.app-ph) [Total: 1]


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [1] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: DeepWriter是一个基于离线知识库的多模态写作助手，通过任务分解、大纲生成和多模态检索等技术，解决了LLM在专业领域写作中的幻觉和不一致性问题，显著提升了生成内容的质量和事实准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在专业领域（如金融、医学和法律）作为写作助手时，常因缺乏深度领域知识和幻觉问题而受限。现有方法（如RAG）存在检索不一致或依赖不可靠网络内容的问题。

Method: DeepWriter采用任务分解、大纲生成、多模态检索和分段组合的流程，结合结构化的离线知识库和分层知识表示，生成专业级文档。

Result: 实验表明，DeepWriter在金融报告生成任务中，生成的内容质量和事实准确性均优于现有基线方法。

Conclusion: DeepWriter通过定制化的多模态流程和离线知识库，显著提升了专业领域写作的质量和可靠性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [2] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: 研究发现，微调目标与模型编辑技术的交互会导致编辑后的知识比预训练获取的知识更容易被遗忘，冻结相关层可提高知识保留。


<details>
  <summary>Details</summary>
Motivation: 探讨微调对模型编辑知识的影响，以解决当前编辑方法在实践中的局限性。

Method: 系统研究不同微调目标与模型编辑技术的交互，并测试冻结层对知识保留的效果。

Result: 编辑后的知识在微调中更容易被遗忘，冻结相关层能显著改善知识保留。

Conclusion: 未来编辑方法需评估微调下的稳健性，冻结层是提高知识保留的有效策略。

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [3] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: SMACS框架通过多智能体协作系统整合开源LLMs，性能超越闭源LLMs。


<details>
  <summary>Details</summary>
Motivation: 探讨开源LLMs能否通过协作超越闭源LLMs。

Method: 提出SMACS框架，包含检索式先验选择（RPS）和探索-利用驱动的后验增强（EPE）。

Result: 在多个基准测试中，SMACS性能超过主流闭源LLMs（如Claude-3.7-Sonnet、GPT-4.1）。

Conclusion: SMACS展示了开源LLMs协作的潜力，提升了智能上限。

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [4] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: PoliAnalyzer是一个神经符号系统，通过NLP和逻辑推理帮助用户个性化分析隐私政策，显著减少认知负担。


<details>
  <summary>Details</summary>
Motivation: 现代人很少阅读隐私政策，但希望了解其数据使用情况，因此需要自动化工具辅助分析。

Method: 结合NLP提取政策文本的正式表示，并通过逻辑推理比较用户偏好与政策内容，生成合规报告。

Result: PoliAnalyzer在PolicyIE数据集上表现优异（F1-score 90-100%），能识别95.2%无冲突的政策内容，仅需关注4.8%的冲突部分。

Conclusion: PoliAnalyzer支持大规模自动化隐私政策分析，帮助用户掌握数据控制权并推动社会讨论。

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [5] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: 论文探讨了利用NLP模型（如BERT、RoBERTa等）通过社交媒体文本检测双相情感障碍，发现RoBERTa表现最佳（F1分数~98%），而基于静态嵌入的LSTM模型效果较差。研究强调了上下文语言模型的重要性，并推荐DistilBERT作为效率与准确性的平衡选择。


<details>
  <summary>Details</summary>
Motivation: 双相情感障碍常因早期症状不明显和社会污名而被漏诊，研究旨在利用NLP技术通过社交媒体文本进行早期筛查。

Method: 评估了多种基于Transformer（BERT、RoBERTa等）和LSTM的模型，使用Reddit帖子数据集进行实验，并通过情感方差和判断分析验证数据有效性。

Result: RoBERTa表现最佳（F1~98%），基于BERT嵌入的LSTM效果相近，而静态嵌入的LSTM表现极差（F1接近零）。DistilBERT在效率与准确性间取得平衡。

Conclusion: 上下文语言模型在双相情感障碍检测中至关重要，研究为心理健康NLP应用提供了模型选择的实用建议，并验证了其早期筛查潜力。

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [6] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）在用户应用中会根据身份标记（如种族、性别、年龄）产生偏见，可能对医疗、法律、政治等领域造成负面影响。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM如何利用用户文本中的身份信息进行决策，以及这种偏见在现实应用中的潜在危害。

Method: 对五个高风险领域的LLM应用（医疗、法律、政治、政府福利、薪资）进行全面分析，评估身份标记对模型响应的影响。

Result: LLM对身份标记极为敏感，导致不同种族、性别、年龄的用户在相同情境下获得不同的响应，例如医疗建议、薪资推荐和政治观点。

Conclusion: 现成的LLM应用可能导致有害差异，建议在部署前进行更全面的评估，并提供新工具以检测身份标记对模型决策的影响。

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [7] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: CCL-XCoT框架通过两阶段微调减少多语言大模型在低资源语言中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在低资源语言中易产生幻觉，影响生成任务准确性。

Method: 结合课程对比学习和跨语言思维链提示策略的两阶段微调框架。

Result: 幻觉率降低62%，跨语言知识迁移显著提升。

Conclusion: CCL-XCoT有效减少幻觉，提升模型在低资源语言中的表现。

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [8] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: 论文研究了大语言模型（LLM）供应链中模型与数据集的关系，构建了一个异构图进行分析，揭示了其结构特点和动态性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM开发和部署中依赖预训练模型和外部数据集，可能继承漏洞或偏见，因此需要理解其供应链关系以降低风险。

Method: 设计方法系统收集LLM供应链数据，构建有向异构图（397,376节点和453,469边），并进行多角度分析。

Result: 发现LLM供应链图规模大、稀疏、符合幂律分布，数据集在训练中起关键作用，模型与数据集相互依赖，且图动态更新。

Conclusion: 通过分析LLM供应链图，揭示了其结构和动态特性，为风险检测、公平性和合规性提供了基础。

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [9] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: Promptomatix是一个自动提示优化框架，能将自然语言任务描述转化为高质量提示，无需手动调整或领域专业知识。


<details>
  <summary>Details</summary>
Motivation: 解决手动提示工程的不一致性和对非专家的不友好问题。

Method: 结合元提示优化器和DSPy编译器，分析用户意图、生成合成数据、选择提示策略并优化提示。

Result: 在5类任务中表现优于现有库，同时减少提示长度和计算开销。

Conclusion: Promptomatix实现了高效、可扩展的提示优化。

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [10] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: ChartScope是一种针对多样化图表类型优化的LVLM，通过高效数据生成和双路径训练策略提升图表理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限图表类型的配对数据，缺乏针对性预训练，限制了泛化能力和数据理解。

Method: 提出高效数据生成管道和双路径训练策略，结合底层数据推理。

Result: ChartScope显著提升多样化图表类型的理解能力，并建立新基准ChartDQA。

Conclusion: ChartScope在广泛图表类型上表现出色，代码和数据已开源。

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [11] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 论文研究了基于LLM的选择性翻译方法，用于改善多语言大语言模型在低资源语言（如印地语）中的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 多语言大语言模型在低资源语言中表现不佳，而翻译现有英语对齐数据时难以保留代码、数学表达式等关键内容。

Method: 采用选择性翻译技术，仅翻译文本中可翻译部分，保留非翻译内容和句子结构，并与Google Cloud Translation和Llama-3.1-405B进行比较。

Result: 选择性翻译在改善多语言对齐方面表现出实用性和有效性。

Conclusion: 选择性翻译是一种有前景的方法，可提升低资源语言的对齐效果。

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [12] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型（LLMs）在处理叙事中时间意义时是否具有类人认知能力，发现其依赖典型性、判断不一致且因果推理能力不足。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否通过类人认知方式处理语言，还是仅依赖高级模式识别。

Method: 使用专家参与循环的探测流程，进行针对性实验，评估LLMs的语义表示和语用推理能力。

Result: LLMs过度依赖典型性，产生不一致的时间判断，且因果推理能力不足，表明其叙事理解能力有限。

Conclusion: LLMs处理时间意义的方式与人类不同，缺乏稳健的叙事理解能力；研究还提出了标准化评估框架。

Abstract: Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [13] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [14] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

TL;DR: 论文研究了使用大型语言模型（LLMs）进行人格评估的可行性，发现虽然模型具有高重测信度，但构念效度有限，与真实人格特质的相关性较弱。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在人格评估中的潜力，解决以往研究中依赖合成数据或缺乏心理测量效度的问题。

Method: 使用555个半结构化访谈和BFI-10自评分数作为基准，测试了三种LLMs（GPT-4.1 Mini、Meta-LLaMA和DeepSeek）在零样本提示和思维链提示下的表现。

Result: 模型表现出高重测信度，但构念效度有限（最大Pearson's r = 0.27），预测结果偏向中等或高特质水平。思维链提示和更长输入上下文略微改善了分布对齐，但未提高特质准确性。

Conclusion: 当前LLMs在人格推断中存在局限性，需基于证据开发适用于心理学应用的方法。

Abstract: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [15] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: 论文提出了一种基于知识图谱和Text-to-SQL代理的企业级解决方案，通过交互式聊天机器人帮助用户自助获取数据洞察。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在Text-to-SQL任务上取得了进展，但构建企业级解决方案仍具挑战性。论文旨在通过实际案例（LinkedIn内部聊天机器人）展示如何解决这一问题。

Method: 方法包括三部分：1) 构建动态知识图谱，索引数据库元数据、查询日志等；2) 开发Text-to-SQL代理，检索上下文、生成查询并纠错；3) 设计交互式聊天机器人，支持多种用户意图。

Result: 聊天机器人拥有300+周活跃用户，53%的响应在内部测试中正确或接近正确。消融实验揭示了关键组件。

Conclusion: 论文为企业级Text-to-SQL解决方案提供了实用路径，强调了知识图谱和建模组件的重要性。

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [16] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

TL;DR: 提出了一种基于错误感知的师生框架，通过GPT-4o的指导改进生物医学文本中的关系分类，结合知识图谱提升性能。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中的关系分类对构建知识图谱和支持药物再利用等应用至关重要，但现有方法存在性能瓶颈。

Method: 使用师生框架，教师模型分析学生模型的预测错误，生成针对性修正，并通过课程学习训练第二学生模型。同时构建生物医学知识图谱。

Result: 在5个PPI数据集中的4个和DDI数据集上达到新SOTA，在ChemProt上保持竞争力。

Conclusion: 提出的框架通过结构化指导和知识图谱显著提升了关系分类性能。

Abstract: Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [17] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

TL;DR: X-Intelligence 3.0是一个专为半导体显示行业设计的高性能推理模型，通过领域知识库和监督学习提升性能，在多项评估中超越现有模型。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在半导体显示行业的应用受限，缺乏领域专业知识，因此开发专用模型以解决行业复杂问题。

Method: 结合领域知识库进行监督微调和强化学习，并引入检索增强生成机制和自动化评估框架。

Result: X-Intelligence 3.0在32B参数规模下，性能超越SOTA模型DeepSeek-R1-671B。

Conclusion: X-Intelligence 3.0为半导体显示行业提供了高效解决方案，解决了长期存在的推理挑战。

Abstract: Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [18] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

TL;DR: XL-DURel是一种优化的多语言Sentence Transformer模型，用于序数Word-in-Context分类，通过基于角距离的排名目标在复杂空间中优于先前模型。


<details>
  <summary>Details</summary>
Motivation: 解决Word-in-Context（WiC）分类任务中序数和二进制数据的性能问题，并探索统一建模的可能性。

Method: 测试多种回归和排名任务的损失函数，采用基于复杂空间角距离的排名目标优化模型。

Result: 在序数和二进制数据上表现优于先前模型，并证明二进制WiC可作为序数WiC的特例处理。

Conclusion: 优化序数任务模型可提升二进制任务性能，为WiC建模的统一处理铺平道路。

Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [19] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 论文探讨了使用多模态BERT模型（AudiBERT）检测协作问题解决（CPS）指标，发现其在社交认知维度上有显著改进，但在情感维度上无显著提升。数据量和人类编码一致性对模型性能有重要影响。


<details>
  <summary>Details</summary>
Motivation: 解决AI在教育领域中检测CPS指标的挑战，探索多模态模型（AudiBERT）的性能提升及人类与AI互补的潜力。

Method: 使用AudiBERT模型，整合语音和声学-韵律特征，与BERT模型对比，分析分类性能和数据关联性。

Result: AudiBERT在社交认知维度上有显著改进，但在情感维度上无显著提升。数据量和人类编码一致性影响模型性能。

Conclusion: 提出结构化方法以实现人类与AI互补，强调模型可解释性对支持人类参与的重要性。

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [20] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 研究使用SHAP方法分析BERT模型在协作问题解决（CPS）分类中各个词的作用，发现模型解释性与分类性能不完全相关，并指出需要进一步研究模型架构和人类-AI互补性。


<details>
  <summary>Details</summary>
Motivation: 提高BERT模型在CPS分类中的可解释性，以增强教师等终端用户的信任和采用。

Method: 使用SHAP方法分析BERT模型分类决策中各个词的作用。

Result: 发现分类性能与解释性不完全相关，某些词对分类有显著影响但语义无关，模型透明性有助于避免过度依赖AI。

Conclusion: 需要研究模型架构和人类-AI互补性，以改进CPS诊断的细粒度区分。

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [21] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

TL;DR: GRACE提出了一种基于生成模型的多行为推荐系统框架，通过改进的tokenization和稀疏注意力机制解决了现有方法的不足，显著提升了推荐性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 生成模型在多行为推荐系统中潜力巨大，但面临token推理信息不足、计算成本高和多尺度建模有限的问题。

Method: GRACE结合了Chain-of-Thought tokenization和Journey-Aware Sparse Attention机制，利用显式属性和稀疏注意力提升效率和可解释性。

Result: 在两个真实数据集上，GRACE显著优于现有基线，性能提升最高达106.9%，同时减少注意力计算48%。

Conclusion: GRACE通过创新的tokenization和注意力机制，为多行为推荐系统提供了高效且高性能的解决方案。

Abstract: Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [22] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: 本文探讨了利用GPT等大语言模型进行数据增强的方法，比较了传统方法（如转述和回译）与纯生成方法的效果，发现传统方法在某些情况下表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决NLP任务中数据稀缺和类别不平衡的问题，探索传统数据增强方法在新一代模型中的潜力。

Method: 选择了四种数据增强方法（包括转述和回译），在多个实验设置下进行比较，评估生成数据的质量和分类性能。

Result: 回译和转述方法在生成数据质量和分类性能上表现优于零样本和少样本生成方法。

Conclusion: 传统数据增强方法结合大语言模型可以取得与纯生成方法相当甚至更好的效果。

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [23] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

TL;DR: 论文探讨了大型语言模型（LLMs）在非洲初级医疗中的应用，提出了一种基于肯尼亚国家指南的基准数据集和评估框架，以解决LLMs在本地化医疗场景中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LLMs在非洲低资源医疗环境中的有效性，填补其在本地化应用中的研究空白。

Method: 方法包括使用检索增强生成（RAG）技术，基于肯尼亚国家指南创建数据集，并通过专家评审确保临床准确性和文化适应性。

Result: 结果显示LLMs在本地化医疗场景中的性能显著低于美国基准，同时提出了新的评估指标。

Conclusion: 结论是该方法为非洲医疗系统中的AI安全部署提供了一个可复制的动态基准模型。

Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [24] [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)
*Paul Sheridan,Zeyad Ahmed,Aitazaz A. Farooque*

Main category: cs.CL

TL;DR: 本文通过显著性测试视角解释了TF-IDF的合理性，并证明其与Fisher精确检验的负对数p值密切相关。


<details>
  <summary>Details</summary>
Motivation: 为TF-IDF提供一个统计学理论基础，解释其在信息检索中的长期有效性。

Method: 通过分析TF-ICF变体与Fisher精确检验的负对数p值之间的关系，并在理想假设下建立联系。

Result: 证明TF-ICF与Fisher精确检验的负对数p值密切相关，并在无限大文档集合下收敛于TF-IDF。

Conclusion: TF-IDF的有效性可以通过Fisher精确检验的统计学视角得到解释，为统计学家提供了理论支持。

Abstract: Term frequency-inverse document frequency, or TF-IDF for short, is arguably
the most celebrated mathematical expression in the history of information
retrieval. Conceived as a simple heuristic quantifying the extent to which a
given term's occurrences are concentrated in any one given document out of
many, TF-IDF and its many variants are routinely used as term-weighting schemes
in diverse text analysis applications. There is a growing body of scholarship
dedicated to placing TF-IDF on a sound theoretical foundation. Building on that
tradition, this paper justifies the use of TF-IDF to the statistics community
by demonstrating how the famed expression can be understood from a significance
testing perspective. We show that the common TF-IDF variant TF-ICF is, under
mild regularity conditions, closely related to the negative logarithm of the
$p$-value from a one-tailed version of Fisher's exact test of statistical
significance. As a corollary, we establish a connection between TF-IDF and the
said negative log-transformed $p$-value under certain idealized assumptions. We
further demonstrate, as a limiting case, that this same quantity converges to
TF-IDF in the limit of an infinitely large document collection. The Fisher's
exact test justification of TF-IDF equips the working statistician with a ready
explanation of the term-weighting scheme's long-established effectiveness.

</details>


### [25] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
*Eric Xia,Jugal Kalita*

Main category: cs.CL

TL;DR: 论文发现，通过两部分的仿射近似可以很好地近似某些主客体关系的Transformer计算，并在线性变换下实现高保真度。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型中主客体关系的可解释性，特别是通过线性变换揭示潜在空间中的稀疏编码。

Method: 使用Bigger Analogy Test Set，通过线性变换Ws（s为主词的中层表示，W为模型导数）近似最终客体状态。

Result: 在线性技术下，形态学关系达到90%的保真度，多语言和多模型中也观察到类似结果。

Conclusion: 语言模型中的某些概念关系（如形态学）可通过潜在空间的线性变换稀疏编码，具有较高的可解释性。

Abstract: A two-part affine approximation has been found to be a good approximation for
transformer computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [26] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

TL;DR: 本文提出了一种基于聚类的语义一致性方法（Cleanse），用于估计大型语言模型（LLM）生成内容的不确定性，以检测幻觉问题。


<details>
  <summary>Details</summary>
Motivation: LLM在生成内容时可能出现幻觉（不准确回答），这威胁到模型的安全性和可靠性。不确定性估计是区分正确与错误回答的关键。

Method: Cleanse通过聚类分析LLM隐藏嵌入的语义一致性，量化不确定性。

Result: 在LLaMA-7B、LLaMA-13B、LLaMA2-7B和Mistral-7B模型及SQuAD、CoQA基准测试中验证了其有效性。

Conclusion: Cleanse是一种有效的幻觉检测方法，有助于提升LLM的可靠性。

Abstract: Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [27] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
*Wannaphong Phatthiyaphaibun,Can Udomcharoenchaikit,Pakpoom Singkorapoom,Kunat Pipatanakul,Ekapol Chuangsuwanich,Peerat Limkonchotiwat,Sarana Nutanong*

Main category: cs.CL

TL;DR: Mangosteen是一个47B泰语语料库，通过定制化的Dolma流程构建，显著提升了泰语模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有语料库处理泰语脚本和文化内容不足，缺乏透明度和可复现性。

Method: 采用泰语适应的Dolma流程，包括自定义语言识别、质量过滤器和内容过滤器，结合非网络来源数据。

Result: 清理后的语料显著提升模型性能，SEA-LION模型在泰语基准测试中超越其他模型。

Conclusion: Mangosteen为泰语及区域LLM研究提供了透明、高质量的语料库和工具。

Abstract: Pre-training data shapes a language model's quality, but raw web text is
noisy and demands careful cleaning. Existing large-scale corpora rely on
English-centric or language-agnostic pipelines whose heuristics do not capture
Thai script or cultural nuances, leaving risky material such as gambling
content untreated. Prior Thai-specific efforts customize pipelines or build new
ones, yet seldom release their data or document design choices, hindering
reproducibility and raising the question of how to construct a transparent,
high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai
corpus built through a Thai-adapted Dolma pipeline that includes custom
rule-based language ID, revised C4/Gopher quality filters, and Thai-trained
content filters, plus curated non-web sources such as Wikipedia, Royal Gazette
texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic
ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M
documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION
model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and
Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline
code, cleaning manifests, corpus snapshot, and all checkpoints, providing a
fully reproducible foundation for future Thai and regional LLM research.

</details>


### [28] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
*Vinicius Anjos de Almeida,Vinicius de Camargo,Raquel Gómez-Bravo,Egbert van der Haring,Kees van Boven,Marcelo Finger,Luis Fernandez Lopez*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLMs）在医疗编码（ICPC-2）任务中的潜力，使用特定领域搜索引擎的输出。结果显示，多数模型表现良好，但小模型在格式和输入长度上存在困难。


<details>
  <summary>Details</summary>
Motivation: 医疗编码对研究、质量监测和政策制定至关重要，研究旨在探索LLMs在自动化编码任务中的可行性。

Method: 使用437个巴西葡萄牙语临床表达的数据集，通过语义搜索引擎检索候选代码，33个LLMs选择最佳匹配的ICPC-2代码，评估指标包括F1分数、成本等。

Result: 28个模型F1分数>0.8，10个超过0.85；优化检索器可提升性能4分；小模型在格式和输入长度上表现不佳。

Conclusion: LLMs在自动化ICPC-2编码中潜力显著，但需更广泛的多语言和端到端评估。

Abstract: Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

</details>


### [29] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
*Xingxuan Li,Yao Xiao,Dianwen Ng,Hai Ye,Yue Deng,Xiang Lin,Bin Wang,Zhanfeng Mo,Chong Zhang,Yueyi Zhang,Zonglin Yang,Ruilin Li,Lei Lei,Shihao Xu,Han Zhao,Weiling Chen,Feng Ji,Lidong Bing*

Main category: cs.CL

TL;DR: 论文介绍了MiroMind-M1系列，一个完全开源的推理语言模型，旨在提高透明度和可复现性，并在数学推理任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于闭源模型（如GPT-3）缺乏透明性，而现有开源项目又常忽略关键资源（如数据集和训练配置），作者希望通过MiroMind-M1系列推动RLM开发的透明度。

Method: 模型分两阶段训练：SFT阶段使用719K数学推理问题和CoT轨迹，RLVR阶段使用62K挑战性问题，并引入Context-Aware Multi-Stage Policy Optimization算法。

Result: 模型在AIME24、AIME25和MATH基准测试中表现优异，达到或超过现有开源7B和32B模型的性能。

Conclusion: 作者发布了完整的模型、数据集和配置，以支持社区研究和进步。

Abstract: Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

</details>


### [30] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

TL;DR: 本文综述了Hugging Face Hub上公开的阿拉伯语后训练数据集，从四个维度（LLM能力、可控性、对齐性和鲁棒性）评估了数据集质量，并揭示了当前阿拉伯语数据集在任务多样性、文档标注和社区采用方面的不足。


<details>
  <summary>Details</summary>
Motivation: 后训练是提升预训练大语言模型性能的关键技术，而数据集的质量和多样性对后训练效果至关重要。本文旨在评估现有阿拉伯语后训练数据集，以推动该领域的发展。

Method: 通过四个关键维度（LLM能力、可控性、对齐性和鲁棒性）对Hugging Face Hub上的阿拉伯语数据集进行系统评估，考察其流行度、实际采用情况、文档质量等指标。

Result: 研究发现阿拉伯语后训练数据集存在任务多样性不足、文档标注不一致、社区采用率低等问题。

Conclusion: 本文指出了阿拉伯语后训练数据集的不足，并提出了改进建议，以促进阿拉伯语大语言模型的发展和应用。

Abstract: Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [31] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: 论文探讨了自杀意念检测中的语言覆盖不足和标注不可靠问题，构建了土耳其语数据集并提出了高效标注框架，评估了标签可靠性和模型一致性。


<details>
  <summary>Details</summary>
Motivation: 解决自杀意念检测中语言覆盖不足和标注不可靠的问题，推动全球自杀预防AI的发展。

Method: 构建土耳其语自杀意念语料库，引入三人类标注员和两大语言模型的高效标注框架，通过迁移学习评估标签可靠性和模型一致性。

Result: 发现现有模型在零样本迁移学习中表现不佳，强调需要更严格的标注和评估方法。

Conclusion: 呼吁在心理健康NLP中提高数据和模型可靠性，倡导透明化的模型训练和数据集构建。

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [32] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
*Maria Sahakyan,Bedoor AlShebli*

Main category: cs.CL

TL;DR: 该研究通过分析8万份同行评审，揭示了语言中的隐性偏见，挑战了匿名评审的公平性假设。


<details>
  <summary>Details</summary>
Motivation: 同行评审虽被视为科学诚信的守门人，但存在偏见，尤其是语言可能加剧不平等，需深入研究。

Method: 使用自然语言处理和大规模统计建模，分析评审语调、情感和支持性语言与作者人口统计的关系。

Result: 发现评审语言因作者性别、种族和机构而异，匿名与否也影响评审语言。

Conclusion: 研究揭示了同行评审中的隐性偏见，对评审政策和科学进步提出了关键问题。

Abstract: The peer review process is often regarded as the gatekeeper of scientific
integrity, yet increasing evidence suggests that it is not immune to bias.
Although structural inequities in peer review have been widely debated, much
less attention has been paid to the subtle ways in which language itself may
reinforce disparities. This study undertakes one of the most comprehensive
linguistic analyses of peer review to date, examining more than 80,000 reviews
in two major journals. Using natural language processing and large-scale
statistical modeling, it uncovers how review tone, sentiment, and supportive
language vary across author demographics, including gender, race, and
institutional affiliation. Using a data set that includes both anonymous and
signed reviews, this research also reveals how the disclosure of reviewer
identity shapes the language of evaluation. The findings not only expose hidden
biases in peer feedback, but also challenge conventional assumptions about
anonymity's role in fairness. As academic publishing grapples with reform,
these insights raise critical questions about how review policies shape career
trajectories and scientific progress.

</details>


### [33] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
*Wai Keen Vong,Brenden M. Lake*

Main category: cs.CL

TL;DR: 研究探讨机器学习如何通过模拟儿童语言输入来理解语言习得，验证了多模态神经网络在有限数据下的稳健性。


<details>
  <summary>Details</summary>
Motivation: 大语言模型依赖海量数据，与儿童语言习得的有限输入形成对比，研究旨在缩小这一差距。

Method: 利用SAYCam数据集的自动转录，生成多模态数据集，测试不同神经网络配置的单词学习能力。

Result: 网络能从每个儿童的数据中学习并泛化单词-指代映射，验证了多模态模型的稳健性。

Conclusion: 多模态神经网络在有限数据下表现稳健，但个体差异影响学习模式。

Abstract: What insights can machine learning bring to understanding human language
acquisition? Large language and multimodal models have achieved remarkable
capabilities, but their reliance on massive training datasets creates a
fundamental mismatch with children, who succeed in acquiring language from
comparatively limited input. To help bridge this gap, researchers have
increasingly trained neural networks using data similar in quantity and quality
to children's input. Taking this approach to the limit, Vong et al. (2024)
showed that a multimodal neural network trained on 61 hours of visual and
linguistic input extracted from just one child's developmental experience could
acquire word-referent mappings. However, whether this approach's success
reflects the idiosyncrasies of a single child's experience, or whether it would
show consistent and robust learning patterns across multiple children's
experiences was not explored. In this article, we applied automated speech
transcription methods to the entirety of the SAYCam dataset, consisting of over
500 hours of video data spread across all three children. Using these automated
transcriptions, we generated multi-modal vision-and-language datasets for both
training and evaluation, and explored a range of neural network configurations
to examine the robustness of simulated word learning. Our findings demonstrate
that networks trained on automatically transcribed data from each child can
acquire and generalize word-referent mappings across multiple network
architectures. These results validate the robustness of multimodal neural
networks for grounded word learning, while highlighting the individual
differences that emerge in how models learn when trained on each child's
developmental experiences.

</details>


### [34] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
*Shoutao Guo,Shaolei Zhang,Qingkai Fang,Zhengrui Ma,Min Zhang,Yang Feng*

Main category: cs.CL

TL;DR: FastLongSpeech框架通过迭代融合和动态压缩训练，解决了LSLMs处理长语音的挑战，无需专用长语音训练数据，并在长语音和短语音任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LSLMs主要关注短语音任务，长语音处理因数据稀缺和计算成本高而未被充分探索。

Method: 引入FastLongSpeech框架，采用迭代融合策略压缩长语音序列，并通过动态压缩训练适应长语音输入。

Result: 实验表明，该方法在长语音和短语音任务中均表现优异，并显著提升推理效率。

Conclusion: FastLongSpeech为LSLMs的长语音处理提供了高效解决方案，无需额外长语音训练数据。

Abstract: The rapid advancement of Large Language Models (LLMs) has spurred significant
progress in Large Speech-Language Models (LSLMs), enhancing their capabilities
in both speech understanding and generation. While existing LSLMs often
concentrate on augmenting speech generation or tackling a diverse array of
short-speech tasks, the efficient processing of long-form speech remains a
critical yet underexplored challenge. This gap is primarily attributed to the
scarcity of long-speech training datasets and the high computational costs
associated with long sequences. To address these limitations, we introduce
FastLongSpeech, a novel framework designed to extend LSLM capabilities for
efficient long-speech processing without necessitating dedicated long-speech
training data. FastLongSpeech incorporates an iterative fusion strategy that
can compress excessively long-speech sequences into manageable lengths. To
adapt LSLMs for long-speech inputs, it introduces a dynamic compression
training approach, which exposes the model to short-speech sequences at varying
compression ratios, thereby transferring the capabilities of LSLMs to
long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop
a long-speech understanding benchmark called LongSpeech-Eval. Experiments show
that our method exhibits strong performance in both long-speech and
short-speech tasks, while greatly improving inference efficiency.

</details>


### [35] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
*Akriti Jain,Pritika Ramu,Aparna Garimella,Apoorv Saxena*

Main category: cs.CL

TL;DR: 论文提出了一种基于意图的文档图表生成任务，通过两阶段无监督框架（信息提取与图表生成）实现，并在数据准确性和图表类型选择上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法难以直接从长文档中根据用户意图生成图表的问题，避免用户手动选择相关内容。

Method: 采用无监督两阶段框架：1) LLM分解意图并提取数据；2) 启发式模块选择图表类型并生成代码。提出基于属性的评估指标。

Result: 在金融和科学领域的数据集上，方法在数据准确性和图表类型选择上分别优于基线9分和17分。

Conclusion: 提出的框架有效解决了意图驱动的文档图表生成问题，并在实际应用中表现优异。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

</details>


### [36] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
*Yifei Wang*

Main category: cs.CL

TL;DR: 推理蒸馏提升小模型的长上下文理解能力，缓解“迷失在中间”问题。


<details>
  <summary>Details</summary>
Motivation: 研究大规模推理蒸馏对检索增强生成（RAG）系统中上下文检索与推理能力的影响。

Method: 使用从Deepseek-R1蒸馏的开源模型，通过多文档问答任务评估其长上下文理解能力。

Result: 蒸馏显著提升长上下文理解，促进更详细的推理过程。

Conclusion: 推理蒸馏有效解决长上下文模型的“迷失在中间”问题。

Abstract: Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [37] [Tiny language models](https://arxiv.org/abs/2507.14871)
*Ronit D. Gross,Yarden Tzach,Tal Halevi,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: 研究表明，即使是小型语言模型（TLMs）也能通过预训练展现出与大型语言模型（LLMs）类似的关键特性，且预训练效果随数据量和任务相关性增强。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）预训练需要巨大计算资源，限制了广泛研究参与，因此探索小型语言模型（TLMs）是否具备类似特性成为关键需求。

Method: 研究通过预训练BERT-6及其变体在Wikipedia子集上，并在FewRel、AGNews和DBPedia分类任务中评估性能，同时探讨了浅层架构组合的效果。

Result: 预训练的TLMs在分类任务中表现显著优于未预训练模型，且性能差距随预训练数据量和任务相关性增加。深层TLM架构的性能可通过多个浅层架构组合复现。

Conclusion: TLMs的预训练效果显著，未来研究可能进一步揭示其机制，尤其是在生物启发模型中，TLMs可能足以支持儿童或青少年语言发展。

Abstract: A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward transformer block architectures pre-trained on large
language models (LLMs). However, LLM pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of LLMs. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater overlap
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language.

</details>


### [38] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
*Shiyi Mu,Yongkang Liu,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: MEKiT方法通过注入多源异构知识，显著提升了大型语言模型在情感-原因对提取任务（ECPE）中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在ECPE任务中表现不佳，主要原因是缺乏辅助知识，限制了其情感感知和原因推理能力。

Method: 提出MEKiT方法，整合内部情感知识和外部因果知识，通过指令模板和数据混合进行指令微调。

Result: 实验表明，MEKiT在ECPE任务中表现优于基线模型，显著提升了LLMs的性能。

Conclusion: MEKiT为ECPE任务提供了更有效和适应性强的解决方案。

Abstract: Although large language models (LLMs) excel in text comprehension and
generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task,
which requires reasoning ability, is often underperform smaller language model.
The main reason is the lack of auxiliary knowledge, which limits LLMs' ability
to effectively perceive emotions and reason causes. To address this issue, we
propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge
\textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous
internal emotional knowledge and external causal knowledge. Specifically, for
these two distinct aspects and structures of knowledge, we apply the approaches
of incorporating instruction templates and mixing data for instruction-tuning,
which respectively facilitate LLMs in more comprehensively identifying emotion
and accurately reasoning causes. Experimental results demonstrate that MEKiT
provides a more effective and adaptable solution for the ECPE task, exhibiting
an absolute performance advantage over compared baselines and dramatically
improving the performance of LLMs on the ECPE task.

</details>


### [39] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
*Boyi Deng,Yu Wan,Baosong Yang,Fei Huang,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: 论文提出了一种名为SASFT的方法，通过稀疏自编码器分析大型语言模型（LLMs）中的意外代码切换问题，并基于此设计了一种监督微调方法，显著减少了代码切换现象。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在多语言能力上表现优异，但存在意外代码切换问题，导致模型响应可读性和可用性下降。现有研究缺乏机制分析且效果有限。

Method: 使用稀疏自编码器分析代码切换现象，发现语言特征预激活值过高是原因。提出SASFT方法，通过监督微调控制语言特征的预激活值。

Result: 在五种模型和三种语言上的实验表明，SASFT将意外代码切换减少了50%以上，并在四种情况下完全消除，同时保持或提升了多语言基准性能。

Conclusion: SASFT有效解决了代码切换问题，同时保留了模型的多语言能力，为LLMs的优化提供了新思路。

Abstract: Large Language Models (LLMs) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using sparse autoencoders and find that
when LLMs switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose $\textbf{S}$parse
$\textbf{A}$utoencoder-guided $\textbf{S}$upervised
$\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.

</details>


### [40] [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)
*Chongxuan Huang,Yongshi Ye,Biao Fu,Qifeng Su,Xiaodong Shi*

Main category: cs.CL

TL;DR: 提出了一种基于神经元状态的跨语言对齐评估方法（NeuronXA），用于评估大语言模型的跨语言对齐能力，并在少量数据下表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言对齐评估方法主要关注句子嵌入，但神经网络模型可能产生非平滑表示空间，影响低资源语言的语义对齐评估。

Method: 受神经科学启发，提出NeuronXA方法，通过神经元状态评估跨语言对齐能力。

Result: 在多个多语言大语言模型上测试，仅需100对平行句子，NeuronXA与下游任务性能的Pearson相关性达0.9556，与可迁移性相关性达0.8514。

Conclusion: NeuronXA能有效评估跨语言对齐和可迁移性，为跨语言对齐研究和多语言大语言模型的语义理解提供了新工具。

Abstract: Large language models (LLMs) have demonstrated remarkable multilingual
capabilities, however, how to evaluate cross-lingual alignment remains
underexplored. Existing alignment benchmarks primarily focus on sentence
embeddings, but prior research has shown that neural models tend to induce a
non-smooth representation space, which impact of semantic alignment evaluation
on low-resource languages. Inspired by neuroscientific findings that similar
information activates overlapping neuronal regions, we propose a novel Neuron
State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a
lignment capabilities of LLMs, which offers a more semantically grounded
approach to assess cross-lingual alignment. We evaluate NeuronXA on several
prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two
transfer tasks and three multilingual benchmarks. The results demonstrate that
with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation
of 0.9556 with downstream tasks performance and 0.8514 with transferability.
These findings demonstrate NeuronXA's effectiveness in assessing both
cross-lingual alignment and transferability, even with a small dataset. This
highlights its potential to advance cross-lingual alignment research and to
improve the semantic understanding of multilingual LLMs.

</details>


### [41] [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
*Eliya Habba,Noam Dahan,Gili Lior,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: PromptSuite是一个自动生成多样化提示的框架，用于更可靠的LLM评估。


<details>
  <summary>Details</summary>
Motivation: 单提示评估LLM不可靠，多提示评估因生成困难而受限。

Method: 采用模块化提示设计，支持可控扰动和扩展性。

Result: 通过案例研究证明PromptSuite能提供有意义的提示变体。

Conclusion: PromptSuite支持强大的评估实践，提供Python API和Web界面。

Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small
changes leading to significant performance differences. However, generating the
prompt variations needed for a more robust multi-prompt evaluation is
challenging, limiting its adoption in practice. To address this, we introduce
PromptSuite, a framework that enables the automatic generation of various
prompts. PromptSuite is flexible - working out of the box on a wide range of
tasks and benchmarks. It follows a modular prompt design, allowing controlled
perturbations to each component, and is extensible, supporting the addition of
new components and perturbation types. Through a series of case studies, we
show that PromptSuite provides meaningful variations to support strong
evaluation practices. It is available through both a Python API:
https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:
https://promptsuite.streamlit.app/

</details>


### [42] [SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs](https://arxiv.org/abs/2507.14922)
*Vahid Rahimzadeh,Erfan Moosavi Monazzah,Mohammad Taher Pilehvar,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: SYNTHIA是一个基于真实社交媒体用户数据的合成人物数据集，解决了现有方法在一致性和真实性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖昂贵的人工数据，要么生成的合成人物缺乏一致性和真实性，SYNTHIA旨在填补这一空白。

Method: 从BlueSky开放平台的10,000名真实用户中提取30,000个背景故事，结合时间维度和社交互动元数据。

Result: SYNTHIA在人口多样性和社会调查对齐方面表现优异，同时在叙事一致性上显著优于现有方法。

Conclusion: SYNTHIA为计算社会科学和人物驱动语言模型提供了新的研究方向。

Abstract: Persona-driven LLMs have emerged as powerful tools in computational social
science, yet existing approaches fall at opposite extremes, either relying on
costly human-curated data or producing synthetic personas that lack consistency
and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from
10,000 real social media users from BlueSky open platform across three time
windows, bridging this spectrum by grounding synthetic generation in authentic
user activity. Our evaluation demonstrates that SYNTHIA achieves competitive
performance with state-of-the-art methods in demographic diversity and social
survey alignment while significantly outperforming them in narrative
consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and
provides rich social interaction metadata from the underlying network, enabling
new research directions in computational social science and persona-driven
language modeling.

</details>


### [43] [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958)
*Hang Yan,Fangzhi Xu,Rongman Xu,Yifei Li,Jian Zhang,Haoran Luo,Xiaobao Wu,Luu Anh Tuan,Haiteng Zhao,Qika Lin,Jun Liu*

Main category: cs.CL

TL;DR: MUR（Momentum Uncertainty-guided Reasoning）是一种动态分配推理预算的方法，通过跟踪不确定性提高LLM推理效率，减少计算量50%以上并提升准确性。


<details>
  <summary>Details</summary>
Motivation: 优化大型语言模型（LLM）在推理任务中的效率，避免冗余计算（如Test-Time Scaling导致的过度思考）。

Method: 提出MUR方法，利用动量概念动态分配推理预算，并通过gamma-control机制灵活控制推理资源。

Result: 在多个基准测试中，MUR平均减少50%以上计算量，同时准确性提升0.62-3.37%。

Conclusion: MUR是一种无需额外训练的高效推理优化方法，显著提升LLM的推理效率和性能。

Abstract: Large Language Models (LLMs) have achieved impressive performance on
reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an
open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it
often leads to overthinking, wasting tokens on redundant computations. This
work investigates how to efficiently and adaptively guide LLM test-time scaling
without additional training. Inspired by the concept of momentum in physics, we
propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically
allocates thinking budgets to critical reasoning steps by tracking and
aggregating stepwise uncertainty over time. To support flexible inference-time
control, we introduce gamma-control, a simple mechanism that tunes the
reasoning budget via a single hyperparameter. We provide in-depth theoretical
proof to support the superiority of MUR in terms of stability and biases. MUR
is comprehensively evaluated against various TTS methods across four
challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using
different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate
that MUR reduces computation by over 50% on average while improving accuracy by
0.62-3.37%.

</details>


### [44] [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback](https://arxiv.org/abs/2507.15024)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出RefCritic，一种基于强化学习的批评模块，通过双重规则奖励生成高质量评价和可操作反馈，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有监督微调方法在构建批评模块时效果有限，无法真正提升模型的批评能力，需要更有效的解决方案。

Method: 提出RefCritic，采用长链思维和强化学习，结合实例级正确性和策略模型细化准确性的双重奖励机制。

Result: 在多个基准测试中，RefCritic表现优异，如AIME25上分别提升6.8%和7.2%，并在多数投票中展现出更好的扩展性。

Conclusion: RefCritic在批评和细化任务中表现出色，优于现有监督方法，尤其在数学推理错误识别方面。

Abstract: With the rapid advancement of Large Language Models (LLMs), developing
effective critic modules for precise guidance has become crucial yet
challenging. In this paper, we initially demonstrate that supervised
fine-tuning for building critic modules (which is widely adopted in current
solutions) fails to genuinely enhance models' critique abilities, producing
superficial critiques with insufficient reflections and verifications. To
unlock the unprecedented critique capabilities, we propose RefCritic, a
long-chain-of-thought critic module based on reinforcement learning with dual
rule-based rewards: (1) instance-level correctness of solution judgments and
(2) refinement accuracies of the policy model based on critiques, aiming to
generate high-quality evaluations with actionable feedback that effectively
guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and
DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement
settings, RefCritic demonstrates consistent advantages across all benchmarks,
e.g., 6.8\% and 7.2\% gains on AIME25 for the respective base models. Notably,
under majority voting, policy models filtered by RefCritic show superior
scaling with increased voting numbers. Moreover, despite training on
solution-level supervision, RefCritic outperforms step-level supervised
approaches on ProcessBench, a benchmark to identify erroneous steps in
mathematical reasoning.

</details>


### [45] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)
*Zhengwei Tao,Jialong Wu,Wenbiao Yin,Junkai Zhang,Baixuan Li,Haiyang Shen,Kuan Li,Liwen Zhang,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种基于形式化驱动的信息搜索（IS）数据合成框架WebShaper，通过集合论系统化地形式化IS任务，解决了现有方法中信息结构与推理结构不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据的稀缺限制了IS代理的发展，现有方法可能导致信息结构与推理结构、问题与答案之间的不一致。

Method: 提出WebShaper框架，利用知识投影（KP）概念和多步扩展过程合成数据集，通过形式化控制推理结构。

Result: 实验表明，WebShaper在GAIA和WebWalkerQA基准测试中达到了开源IS代理的最先进性能。

Conclusion: WebShaper通过形式化驱动的数据合成方法，显著提升了IS代理的性能，解决了数据稀缺和结构不一致的问题。

Abstract: The advent of Large Language Model (LLM)-powered agents has revolutionized
artificial intelligence by enabling solutions to complex, open-ended tasks
through web-based information-seeking (IS) capabilities. The scarcity of
high-quality training data has limited the development of IS agents. Existing
approaches typically adopt an information-driven paradigm that first collects
web data and then generates questions based on the retrieval. However, this may
lead to inconsistency between information structure and reasoning structure,
question and answer. To mitigate, we propose a formalization-driven IS data
synthesis framework WebShaper to construct a dataset. WebShaper systematically
formalizes IS tasks through set theory. Central to the formalization is the
concept of Knowledge Projections (KP), which enables precise control over
reasoning structure by KP operation compositions. During synthesis, we begin by
creating seed tasks, then use a multi-step expansion process. At each step, an
agentic Expander expands the current formal question more complex with
retrieval and validation tools based on our formalization. We train our model
on the synthesized dataset. Experiment results demonstrate that WebShaper
achieves state-of-the-art performance among open-sourced IS agents on GAIA and
WebWalkerQA benchmarks.

</details>


### [46] [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)
*Chenlei Gong,Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CL

TL;DR: 比较了DNA序列建模中k-mer分割与BPE子词标记化的性能，以及三种位置编码方法，发现BPE表现更优，RoPE适合周期性模式，AliBi适合局部依赖任务。


<details>
  <summary>Details</summary>
Motivation: 缺乏对DNA序列建模中不同标记化和位置编码方法的系统评估，需明确哪种方法更优。

Method: 比较k-mer分割（k=1,3,4,5,6）、BPE子词标记化及三种位置编码方法（sinusoidal、AliBi、RoPE），在不同层数的Transformer编码器上训练并评估。

Result: BPE性能更高且稳定，RoPE适合周期性模式，AliBi适合局部依赖任务；3到12层性能显著提升，24层改善有限或过拟合。

Conclusion: 为DNA Transformer模型的标记化和位置编码设计提供了实用指导。

Abstract: Currently, many studies view DNA sequences as a special type of language and
utilize Transformers to model them. These studies use fixed-length k-mer
segmentation and BPE subword tokenization but lack a systematic evaluation to
determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a
4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,
AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and
24-layer Transformer encoders and evaluated on GUE benchmark dataset. In
general, BPE delivers higher and more stable performance across tasks by
compressing frequent motifs into variable-length tokens, reducing sequence
length, and improving model generalization. RoPE excels at capturing periodic
motifs and extrapolating to long sequences, while AliBi also performs well on
tasks driven by local dependencies. In terms of depth, we observe significant
gains when increasing layers from 3 to 12, with only marginal improvements or
slight overfitting at 24 layers. This study provides practical guidance for
designing tokenization and positional encoding in DNA Transformer models.

</details>


### [47] [A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations](https://arxiv.org/abs/2507.15092)
*Vijeta Deshpande,Ishita Dasgupta,Uttaran Bhattacharya,Somdeb Sarkhel,Saayan Mitra,Anna Rumshisky*

Main category: cs.CL

TL;DR: 提出了一种新的词汇多样性度量方法PATTR，解决了现有方法因文本长度变化导致的偏差问题，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 研究提示工程对生成文本长度和词汇多样性的影响，发现现有度量方法因长度变化存在偏差，需要一种更稳健的度量方法。

Method: 提出PATTR度量方法，通过考虑目标响应长度（$L_T$）来减少长度偏差，并在大规模合成语料库（20M词）上验证其效果。

Result: PATTR在词汇多样性评估中优于现有方法（MATTR和CR），能更准确地筛选出高多样性文本，且对目标长度有更好的遵循性。

Conclusion: PATTR是一种有效的词汇多样性度量方法，适用于需要控制文本长度的任务，为LLM生成数据的多样性评估提供了新工具。

Abstract: Synthetic text generated by Large Language Models (LLMs) is increasingly used
for further training and improvement of LLMs. Diversity is crucial for the
effectiveness of synthetic data, and researchers rely on prompt engineering to
improve diversity. However, the impact of prompt variations on response text
length, and, more importantly, the consequential effect on lexical diversity
measurements, remain underexplored. In this work, we propose Penalty-Adjusted
Type-Token Ratio (PATTR), a diversity metric robust to length variations. We
generate a large synthetic corpus of over 20M words using seven models from the
LLaMA, OLMo, and Phi families, focusing on a creative writing task of video
script generation, where diversity is crucial. We evaluate per-response lexical
diversity using PATTR and compare it against existing metrics of Moving-Average
TTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length
variations introduce biases favoring shorter responses. Unlike existing
metrics, PATTR explicitly considers the task-specific target response length
($L_T$) to effectively mitigate length biases. We further demonstrate the
utility of PATTR in filtering the top-10/100/1,000 most lexically diverse
responses, showing that it consistently outperforms MATTR and CR by yielding on
par or better diversity with high adherence to $L_T$.

</details>


### [48] [Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?](https://arxiv.org/abs/2507.15100)
*Chathuri Jayaweera,Brianna Yanqui,Bonnie Dorr*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLM）作为常识知识生成器在自然语言推理（NLI）中的潜力，评估其可靠性和对预测准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有常识资源对多样前提-假设对的覆盖不足，需探索LLM作为常识知识生成器的可行性。

Method: 调整和修改现有指标，评估LLM生成常识知识的真实性和一致性。

Result: 明确引入常识知识虽未显著提升整体结果，但有效区分蕴含实例，并适度改善矛盾和中立推理的区分。

Conclusion: LLM在NLI中作为常识知识生成器具有一定潜力，尤其在区分特定推理类型时表现良好。

Abstract: Natural Language Inference (NLI) is the task of determining the semantic
entailment of a premise for a given hypothesis. The task aims to develop
systems that emulate natural human inferential processes where commonsense
knowledge plays a major role. However, existing commonsense resources lack
sufficient coverage for a variety of premise-hypothesis pairs. This study
explores the potential of Large Language Models as commonsense knowledge
generators for NLI along two key dimensions: their reliability in generating
such knowledge and the impact of that knowledge on prediction accuracy. We
adapt and modify existing metrics to assess LLM factuality and consistency in
generating in this context. While explicitly incorporating commonsense
knowledge does not consistently improve overall results, it effectively helps
distinguish entailing instances and moderately improves distinguishing
contradictory and neutral inferences.

</details>


### [49] [From Disagreement to Understanding: The Case for Ambiguity Detection in NLI](https://arxiv.org/abs/2507.15114)
*Chathuri Jayaweera,Bonnie Dorr*

Main category: cs.CL

TL;DR: 论文主张NLI中的标注分歧反映有意义的多义性，提出多义性感知框架，并呼吁构建相关数据集。


<details>
  <summary>Details</summary>
Motivation: 标注分歧不仅是噪声，而是由前提或假设的多义性引发的有意义差异，需系统识别和分类多义性。

Method: 提出统一框架整合现有分类法，通过实例展示多义性子类型及其对标注决策的影响。

Result: 揭示多义性如何影响标注决策，强调需针对性检测方法以对齐模型与人类理解。

Conclusion: 当前缺乏多义性标注数据集，建议通过新标注资源和无监督方法填补空白，推动更鲁棒、可解释的NLI系统。

Abstract: This position paper argues that annotation disagreement in Natural Language
Inference (NLI) is not mere noise but often reflects meaningful interpretive
variation, especially when triggered by ambiguity in the premise or hypothesis.
While underspecified guidelines and annotator behavior can contribute to
variation, content-based ambiguity offers a process-independent signal of
divergent human perspectives. We call for a shift toward ambiguity-aware NLI by
systematically identifying ambiguous input pairs and classifying ambiguity
types. To support this, we present a unified framework that integrates existing
taxonomies and illustrate key ambiguity subtypes through concrete examples.
These examples reveal how ambiguity shapes annotator decisions and motivate the
need for targeted detection methods that better align models with human
interpretation. A key limitation is the lack of datasets annotated for
ambiguity and subtypes. We propose addressing this gap through new annotated
resources and unsupervised approaches to ambiguity detection -- paving the way
for more robust, explainable, and human-aligned NLI systems.

</details>


### [50] [A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script](https://arxiv.org/abs/2507.15142)
*Hellina Hailu Nigatu,Atnafu Lambebo Tonja,Henok Biadglign Ademtew,Hizkel Mitiku Alemayehu,Negasi Haile Abadi,Tadesse Destaw Belay,Seid Muhie Yimam*

Main category: cs.CL

TL;DR: 论文探讨了阿姆哈拉语NLP中的同音词归一化问题，提出了一种后推断归一化方法，以平衡性能提升与语言特征保留。


<details>
  <summary>Details</summary>
Motivation: 同音词归一化虽能提升自动指标性能，但可能导致模型无法理解语言中的不同书写形式，并影响跨语言迁移学习。

Method: 通过单语言训练和跨语言迁移实验，研究归一化对使用Ge'ez脚本语言的影响，并提出后推断归一化方案。

Result: 后推断归一化使BLEU分数提升1.03，同时保留了训练中的语言特征。

Conclusion: 研究呼吁更多语言感知的干预措施，为技术驱动的语言变化讨论做出贡献。

Abstract: Homophone normalization, where characters that have the same sound in a
writing script are mapped to one character, is a pre-processing step applied in
Amharic Natural Language Processing (NLP) literature. While this may improve
performance reported by automatic metrics, it also results in models that are
not able to understand different forms of writing in a single language.
Further, there might be impacts in transfer learning, where models trained on
normalized data do not generalize well to other languages. In this paper, we
experiment with monolingual training and cross-lingual transfer to understand
the impacts of normalization on languages that use the Ge'ez script. We then
propose a post-inference intervention in which normalization is applied to
model predictions instead of training data. With our simple scheme of
post-inference normalization, we show that we can achieve an increase in BLEU
score of up to 1.03 while preserving language features in training. Our work
contributes to the broader discussion on technology-facilitated language change
and calls for more language-aware interventions.

</details>


### [51] [What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction](https://arxiv.org/abs/2507.15152)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 研究评估了三种LLM（Gemini-2.0-flash、Grok-3、GPT-4o-mini）在高血压、糖尿病和骨科领域的RCT数据提取任务中的表现，发现定制提示最有效，召回率提升15%。


<details>
  <summary>Details</summary>
Motivation: 自动化从RCT中提取数据用于荟萃分析仍具挑战性，需评估LLM的实际表现。

Method: 测试了四种提示策略（基础提示、自反思提示、模型集成、定制提示），评估模型在统计结果、偏倚风险评估和研究特征提取中的表现。

Result: 所有模型精度高但召回率低，定制提示显著提升召回率。

Conclusion: 提出三层次指南，根据任务复杂性和风险匹配自动化级别，平衡LLM效率与专家监督。

Abstract: Automating data extraction from full-text randomised controlled trials (RCTs)
for meta-analysis remains a significant challenge. This study evaluates the
practical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)
across tasks involving statistical results, risk-of-bias assessments, and
study-level characteristics in three medical domains: hypertension, diabetes,
and orthopaedics. We tested four distinct prompting strategies (basic
prompting, self-reflective prompting, model ensemble, and customised prompts)
to determine how to improve extraction quality. All models demonstrate high
precision but consistently suffer from poor recall by omitting key information.
We found that customised prompts were the most effective, boosting recall by up
to 15\%. Based on this analysis, we propose a three-tiered set of guidelines
for using LLMs in data extraction, matching data types to appropriate levels of
automation based on task complexity and risk. Our study offers practical advice
for automating data extraction in real-world meta-analyses, balancing LLM
efficiency with expert oversight through targeted, task-specific automation.

</details>


### [52] [Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment](https://arxiv.org/abs/2507.15198)
*Xiandong Meng,Yan Wu,Yexin Tian,Xin Hu,Tianze Kang,Junliang Du*

Main category: cs.CL

TL;DR: 该论文提出了一种基于多教师模型的知识蒸馏方法，以降低大语言模型的部署成本并提升推理速度。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型部署时的高计算成本和推理速度慢的问题。

Method: 通过构建多个教师模型，整合其输出概率分布和中间语义特征，指导学生模型学习。引入了加权输出融合机制、特征对齐损失函数和熵驱动的动态教师加权策略。

Result: 学生模型在语言理解和生成能力上表现更强，同时在多任务评估中展现出高一致性。实验结果表明其在困惑度、蒸馏损失和生成质量上具有优势。

Conclusion: 该方法为大语言模型的高效压缩提供了可行技术路径，并验证了多教师协作机制在复杂语言建模任务中的有效性。

Abstract: This paper addresses the challenges of high computational cost and slow
inference in deploying large language models. It proposes a distillation
strategy guided by multiple teacher models. The method constructs several
teacher models and integrates their output probability distributions and
intermediate semantic features. This guides the student model to learn from
multiple sources of knowledge. As a result, the student model gains stronger
language understanding and generation ability while maintaining a small
parameter size. To achieve this, the paper introduces a weighted output fusion
mechanism, a feature alignment loss function, and an entropy-driven dynamic
teacher weighting strategy. These components improve the quality and stability
of knowledge transfer during distillation. Under multi-teacher guidance, the
student model captures semantic information more effectively and demonstrates
strong performance across multiple evaluation metrics. In particular, the
method shows high consistency in expression, generalization ability, and task
adaptability in tasks such as language modeling, text generation, and
multi-task learning. The experiments compare the proposed method with several
widely adopted distillation approaches. The results further confirm its overall
advantages in perplexity, distillation loss, and generation quality. This study
provides a feasible technical path for the efficient compression of large-scale
language models. It also demonstrates the effectiveness of multi-teacher
collaborative mechanisms in complex language modeling tasks.

</details>


### [53] [SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest](https://arxiv.org/abs/2507.15236)
*Shayan Vassef,Amirhossein Dabiriaghdam,Mohammadreza Bakhtiari,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 研究多任务、多语言和多源学习对预训练语言模型鲁棒性和性能的影响，引入SOI框架分析学习行为模式，实验表明多源学习显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 探讨不同学习配置（多任务、多语言、多源）对语言模型性能的影响，并提出新的分析框架SOI。

Method: 引入SOI框架，通过热力图和数据集可视化分析学习行为模式，进行多任务、多源和多语言的对比实验。

Result: 多源学习提升性能7%，多任务学习在相似任务组合中表现良好，两阶段微调进一步优化性能。

Conclusion: SOI框架为训练动态提供新见解，多源学习是优化语言模型性能的有效方法。

Abstract: This work investigates the impact of multi-task, multi-lingual, and
multi-source learning approaches on the robustness and performance of
pretrained language models. To enhance this analysis, we introduce Subsets of
Interest (SOI), a novel categorization framework that identifies six distinct
learning behavior patterns during training, including forgettable examples,
unlearned examples, and always correct examples. Through SOI transition
heatmaps and dataset cartography visualization, we analyze how examples shift
between these categories when transitioning from single-setting to
multi-setting configurations. We perform comprehensive experiments across three
parallel comparisons: multi-task vs. single-task learning using English tasks
(entailment, paraphrase, sentiment), multi-source vs. single-source learning
using sentiment analysis datasets, and multi-lingual vs. single-lingual
learning using intent classification in French, English, and Persian. Our
results demonstrate that multi-source learning consistently improves
out-of-distribution performance by up to 7%, while multi-task learning shows
mixed results with notable gains in similar task combinations. We further
introduce a two-stage fine-tuning approach where the second stage leverages
SOI-based subset selection to achieve additional performance improvements.
These findings provide new insights into training dynamics and offer practical
approaches for optimizing multi-setting language model performance.

</details>


### [54] [ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling](https://arxiv.org/abs/2507.15275)
*Yuanhe Tian,Junjie Liu,Zhizhou Kou,Yuxiang Li,Yan Song*

Main category: cs.CL

TL;DR: ChiMed 2.0是一个扩展的中文医学数据集，包含预训练、监督微调和RLHF数据，用于提升中文医学LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有中文医学数据集规模小、覆盖窄，无法满足预训练和RLHF需求，ChiMed 2.0填补了这一空白。

Method: 通过收集在线平台数据和LLM生成数据，构建包含预训练、SFT和RLHF数据的ChiMed 2.0数据集，并在通用LLM上进行实验验证。

Result: 实验结果显示，ChiMed 2.0在不同规模的模型上均带来性能提升。

Conclusion: ChiMed 2.0为中文医学LLM的训练提供了有效且广泛适用的数据集。

Abstract: Building high-quality data resources is crucial for advancing artificial
intelligence research and applications in specific domains, particularly in the
Chinese medical domain. Existing Chinese medical datasets are limited in size
and narrow in domain coverage, falling short of the diverse corpora required
for effective pre-training. Moreover, most datasets are designed solely for LLM
fine-tuning and do not support pre-training and reinforcement learning from
human feedback (RLHF). In this paper, we propose a Chinese medical dataset
named ChiMed 2.0, which extends our previous work ChiMed, and covers data
collected from Chinese medical online platforms and generated by LLMs. ChiMed
2.0 contains 204.4M Chinese characters covering both traditional Chinese
medicine classics and modern general medical data, where there are 164.8K
documents for pre-training, 351.6K question-answering pairs for supervised
fine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the
effectiveness of our approach for training a Chinese medical LLM, we conduct
further pre-training, SFT, and RLHF experiments on representative general
domain LLMs and evaluate their performance on medical benchmark datasets. The
results show performance gains across different model scales, validating the
dataset's effectiveness and applicability.

</details>


### [55] [A Novel Self-Evolution Framework for Large Language Models](https://arxiv.org/abs/2507.15281)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 提出了一种名为DPSE的双阶段自进化框架，通过联合优化用户偏好适应和领域特定能力，显著提升了LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有后训练策略在提升用户对齐的同时未能增强模型的领域认知能力，因此需要一种新方法填补这一空白。

Method: DPSE框架引入Censor模块提取多维交互信号并估计满意度分数，通过主题感知和偏好驱动策略扩展数据集，支持两阶段微调流程。

Result: 实验表明DPSE在通用NLP基准和长期对话任务中优于监督微调、偏好优化和记忆增强基线。

Conclusion: DPSE为LLM的持续自进化提供了一条自主路径。

Abstract: The capabilities of Large Language Models (LLMs) are limited to some extent
by pre-training, so some researchers optimize LLMs through post-training.
Existing post-training strategies, such as memory-based retrieval or preference
optimization, improve user alignment yet fail to enhance the model's domain
cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution
(DPSE) framework that jointly optimizes user preference adaptation and
domain-specific competence. DPSE introduces a Censor module to extract
multi-dimensional interaction signals and estimate satisfaction scores, which
guide structured data expansion via topic-aware and preference-driven
strategies. These expanded datasets support a two-stage fine-tuning pipeline:
supervised domain grounding followed by frequency-aware preference
optimization. Experiments across general NLP benchmarks and long-term dialogue
tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,
Preference Optimization, and Memory-Augmented baselines. Ablation studies
validate the contribution of each module. In this way, our framework provides
an autonomous path toward continual self-evolution of LLMs.

</details>


### [56] [Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection](https://arxiv.org/abs/2507.15286)
*Navid Ayoobi,Sadat Shahriar,Arjun Mukherjee*

Main category: cs.CL

TL;DR: 提出了一种新的AI文本检测器评估范式SHIELD，强调实际应用中的公平性和稳定性，并开发了一种模型无关的人类化框架。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法过于依赖传统指标（如AUROC），忽视了实际部署中的误报率和稳定性问题。

Method: SHIELD基准整合了可靠性和稳定性因素，并开发了可控硬度的人类化框架。

Result: SHIELD有效挑战了当前零样本检测方法的可靠性和稳定性。

Conclusion: SHIELD为AI文本检测器的实际评估提供了更全面的标准。

Abstract: We present a novel evaluation paradigm for AI text detectors that prioritizes
real-world and equitable assessment. Current approaches predominantly report
conventional metrics like AUROC, overlooking that even modest false positive
rates constitute a critical impediment to practical deployment of detection
systems. Furthermore, real-world deployment necessitates predetermined
threshold configuration, making detector stability (i.e. the maintenance of
consistent performance across diverse domains and adversarial scenarios), a
critical factor. These aspects have been largely ignored in previous research
and benchmarks. Our benchmark, SHIELD, addresses these limitations by
integrating both reliability and stability factors into a unified evaluation
metric designed for practical assessment. Furthermore, we develop a post-hoc,
model-agnostic humanification framework that modifies AI text to more closely
resemble human authorship, incorporating a controllable hardness parameter.
This hardness-aware approach effectively challenges current SOTA zero-shot
detection methods in maintaining both reliability and stability. (Data and
code: https://github.com/navid-aub/SHIELD-Benchmark)

</details>


### [57] [On the Inevitability of Left-Leaning Political Bias in Aligned Language Models](https://arxiv.org/abs/2507.15328)
*Thilo Hagendorff*

Main category: cs.CL

TL;DR: 论文探讨了AI对齐原则（无害、有帮助、诚实）与LLMs左翼政治偏见的关联，认为对齐目标与进步道德框架一致，而右翼意识形态与之冲突。


<details>
  <summary>Details</summary>
Motivation: 研究动机是澄清AI对齐目标与政治偏见之间的关系，反驳将左翼偏见视为问题的观点。

Method: 通过分析对齐目标的规范性假设与政治意识形态的契合度，论证左翼偏见的必然性。

Result: 结果表明，对齐目标与左翼原则（如避免伤害、包容性）一致，而右翼意识形态与之冲突。

Conclusion: 结论认为，将左翼偏见视为问题实际上违背了AI对齐的核心原则。

Abstract: The guiding principle of AI alignment is to train large language models
(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are
mounting concerns that LLMs exhibit a left-wing political bias. Yet, the
commitment to AI alignment cannot be harmonized with the latter critique. In
this article, I argue that intelligent systems that are trained to be harmless
and honest must necessarily exhibit left-wing political bias. Normative
assumptions underlying alignment objectives inherently concur with progressive
moral frameworks and left-wing principles, emphasizing harm avoidance,
inclusivity, fairness, and empirical truthfulness. Conversely, right-wing
ideologies often conflict with alignment guidelines. Yet, research on political
bias in LLMs is consistently framing its insights about left-leaning tendencies
as a risk, as problematic, or concerning. This way, researchers are actively
arguing against AI alignment, tacitly fostering the violation of HHH
principles.

</details>


### [58] [Reasoning Models are Test Exploiters: Rethinking Multiple-Choice](https://arxiv.org/abs/2507.15337)
*Narun Raman,Taylor Lundy,Kevin Leyton-Brown*

Main category: cs.CL

TL;DR: 论文研究了多选问答（MCQA）作为评估大语言模型（LLM）下游任务性能的代理的有效性，发现对于先进推理模型，MCQA不再适用，并提出改进基准设计的建议。


<details>
  <summary>Details</summary>
Motivation: 探讨MCQA是否仍适用于评估先进推理模型的下游任务性能，因为现有方法可能引入偏差。

Method: 系统评估了15个问答基准和25个LLM，测试了5种提问方式，包括是否提供选项、是否允许链式推理等。

Result: MCQA仅在模型在提供选项前进行链式推理时有效；若允许推理后选择，模型会利用选项信息，导致性能偏差。

Conclusion: MCQA不再适合评估先进模型的真实性能，需设计更鲁棒的基准以减少偏差。

Abstract: When evaluating Large Language Models (LLMs) in question-answering domains,
it is common to ask the model to choose among a fixed set of choices (so-called
multiple-choice question-answering, or MCQA). Although downstream tasks of
interest typically do not provide systems with explicit options among which to
choose, this approach is nevertheless widely used because it makes it makes
automatic grading straightforward and has tended to produce challenging
benchmarks that correlate sufficiently well with downstream performance. This
paper investigates the extent to which this trend continues to hold for
state-of-the-art reasoning models, describing a systematic evaluation of $15$
different question-answering benchmarks (e.g., MMLU, HLE) and $25$ different
LLMs (including small models such as Qwen 7B and relatively large models such
as Llama 70B). For each model-benchmark pair, we considered $5$ ways of
presenting the model with questions, including variations on whether multiple
choices were offered to the model at all; whether "none of the above" sometimes
replaced the right answer; and whether the model was permitted to perform
chain-of-thought reasoning before and/or after the choices were presented. MCQA
remained a good proxy for the downstream performance of models as long as they
were allowed to perform chain-of-thought reasoning only before being presented
with the options among which they had to select. On the other hand, large
models that were able to perform reasoning after being given a set of options
tended to significantly outperform their free-text performance due to
exploiting the information in the options. We conclude that MCQA is no longer a
good proxy for assessing downstream performance of state-of-the-art models, and
offer practical guidelines for designing more robust, bias-resistant benchmarks
that better reflect LLMs' genuine reasoning capabilities.

</details>


### [59] [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)
*Leanne Tan,Gabriel Chua,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: LionGuard 2是一个轻量级多语言内容审核分类器，针对新加坡语境设计，支持英语、中文、马来语和部分泰米尔语，性能优于多个商业和开源系统。


<details>
  <summary>Details</summary>
Motivation: 现代审核系统在多语言支持上存在本地化和低资源变体的不足，导致实际部署中的安全漏洞。小型模型可作为大型LLM的替代方案，但仍需大量数据和计算资源。

Method: 基于预训练的OpenAI嵌入和多头序数分类器构建LionGuard 2。

Result: 在17个基准测试中表现优于多个商业和开源系统，包括新加坡特定和公共英语数据集。

Conclusion: 高质量本地数据和强大多语言嵌入可在不微调大型模型的情况下实现强审核性能。模型权重和部分训练数据已开源。

Abstract: Modern moderation systems increasingly support multiple languages, but often
fail to address localisation and low-resource variants - creating safety gaps
in real-world deployments. Small models offer a potential alternative to large
LLMs, yet still demand considerable data and compute. We present LionGuard 2, a
lightweight, multilingual moderation classifier tailored to the Singapore
context, supporting English, Chinese, Malay, and partial Tamil. Built on
pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2
outperforms several commercial and open-source systems across 17 benchmarks,
including both Singapore-specific and public English datasets. The system is
actively deployed within the Singapore Government, demonstrating practical
efficacy at scale. Our findings show that high-quality local data and robust
multilingual embeddings can achieve strong moderation performance, without
fine-tuning large models. We release our model weights and part of our training
data to support future work on LLM safety.

</details>


### [60] [Probing Information Distribution in Transformer Architectures through Entropy Analysis](https://arxiv.org/abs/2507.15347)
*Amedeo Buonanno,Alessandro Rivetti,Francesco A. N. Palmieri,Giovanni Di Gennaro,Gianmarco Romano*

Main category: cs.CL

TL;DR: 熵分析用于研究Transformer架构中信息分布，通过量化token级不确定性及分析处理阶段的熵模式，揭示模型行为与内部表示。


<details>
  <summary>Details</summary>
Motivation: 探索Transformer模型中信息管理方式，为模型可解释性和评估框架提供新视角。

Method: 采用熵分析量化token级不确定性，研究处理阶段的熵模式变化，以GPT模型为例。

Result: 揭示了模型内部信息管理机制，为理解Transformer行为提供新方法。

Conclusion: 熵分析是研究Transformer模型信息分布的有效工具，有助于模型可解释性和评估。

Abstract: This work explores entropy analysis as a tool for probing information
distribution within Transformer-based architectures. By quantifying token-level
uncertainty and examining entropy patterns across different stages of
processing, we aim to investigate how information is managed and transformed
within these models. As a case study, we apply the methodology to a GPT-based
large language model, illustrating its potential to reveal insights into model
behavior and internal representations. This approach may offer insights into
model behavior and contribute to the development of interpretability and
evaluation frameworks for transformer-based models

</details>


### [61] [Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding](https://arxiv.org/abs/2507.15357)
*Elisa Sanchez-Bayona,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本文全面评估了大型语言模型（LLMs）在隐喻解释任务中的表现，发现其性能更多受词汇重叠和句子长度等表面特征影响，而非隐喻内容。


<details>
  <summary>Details</summary>
Motivation: 解决以往研究局限于单一数据集和特定任务的不足，探索LLMs在自然语言推理（NLI）和问答（QA）任务中处理隐喻的能力。

Method: 使用多样化的公开数据集进行实验，关注NLI和QA任务，分析LLMs的表现。

Result: LLMs的表现更多依赖表面特征（如词汇重叠和句子长度），而非对隐喻内容的理解。

Conclusion: LLMs在隐喻解释任务中的能力有限，需更现实的评估框架。

Abstract: This paper presents a comprehensive evaluation of the capabilities of Large
Language Models (LLMs) in metaphor interpretation across multiple datasets,
tasks, and prompt configurations. Although metaphor processing has gained
significant attention in Natural Language Processing (NLP), previous research
has been limited to single-dataset evaluations and specific task settings,
often using artificially constructed data through lexical replacement. We
address these limitations by conducting extensive experiments using diverse
publicly available datasets with inference and metaphor annotations, focusing
on Natural Language Inference (NLI) and Question Answering (QA) tasks. The
results indicate that LLMs' performance is more influenced by features like
lexical overlap and sentence length than by metaphorical content, demonstrating
that any alleged emergent abilities of LLMs to understand metaphorical language
are the result of a combination of surface-level features, in-context learning,
and linguistic knowledge. This work provides critical insights into the current
capabilities and limitations of LLMs in processing figurative language,
highlighting the need for more realistic evaluation frameworks in metaphor
interpretation tasks. Data and code are publicly available.

</details>


### [62] [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models](https://arxiv.org/abs/2507.15375)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: 论文提出Stitch方法，通过交替生成无声推理块和语音响应块，实现SLM的思考与说话同时进行，显著降低延迟并提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前SLM缺乏内部无声思考能力，而人类通过内部复杂推理能清晰表达。因此，为SLM引入无声思考过程具有重要价值。

Method: 提出Stitch方法，交替生成无声推理块和语音响应块，利用语音块播放时间生成推理块，实现思考与说话并行。

Result: Stitch在数学推理数据集上性能提升15%，同时保持与非推理数据集上基线模型相当的表现，且延迟与基线一致。

Conclusion: Stitch成功实现了SLM的思考与说话并行，显著提升了推理能力且未增加延迟，具有广泛应用潜力。

Abstract: Spoken Language Models (SLMs) are designed to take speech inputs and produce
spoken responses. However, current SLMs lack the ability to perform an
internal, unspoken thinking process before responding. In contrast, humans
typically engage in complex mental reasoning internally, enabling them to
communicate ideas clearly and concisely. Thus, integrating an unspoken thought
process into SLMs is highly desirable. While naively generating a complete
chain-of-thought (CoT) reasoning before starting to talk can enable thinking
for SLMs, this induces additional latency for the speech response, as the CoT
reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a
novel generation method that alternates between the generation of unspoken
reasoning chunks and spoken response chunks. Since the audio duration of a
chunk of spoken response is much longer than the time to generate the tokens in
a chunk of spoken response, we use the remaining free time to generate the
unspoken reasoning tokens. When a chunk of audio is played to the user, the
model continues to generate the next unspoken reasoning chunk, achieving
simultaneous thinking and talking. Remarkably, Stitch matches the latency of
baselines that cannot generate unspoken CoT by design while outperforming those
baselines by 15% on math reasoning datasets; Stitch also performs equally well
on non-reasoning datasets as those baseline models. Some animations and
demonstrations are on the project page: https://d223302.github.io/STITCH.

</details>


### [63] [AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming](https://arxiv.org/abs/2507.15378)
*Jierui Li,Raymond Mooney*

Main category: cs.CL

TL;DR: 论文提出了AlgoSimBench基准测试，评估LLMs在识别算法相似问题（ASPs）上的能力，发现现有模型表现不佳，并提出了改进方法ASM。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs在训练数据较少的相关领域中是否能推广其解决复杂编程问题的能力。

Method: 引入AlgoSimBench基准测试，包含1317个问题和402个多选题，评估LLMs识别ASP的能力；提出ASM方法改进相似性检测。

Result: 最佳模型（o3-mini）在MCQ任务上准确率仅65.9%；ASM方法提升6.7%-11.7%；代码嵌入模型结合BM25可达52.2%准确率。

Conclusion: LLMs在识别ASP上表现有限，但ASM方法显著提升了性能，为未来研究提供了新方向。

Abstract: Recent progress in LLMs, such as reasoning models, has demonstrated strong
abilities to solve complex competitive programming problems, often rivaling top
human competitors. However, it remains underexplored whether these abilities
generalize to relevant domains that are less seen during training. To address
this, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'
ability to identify algorithmically similar problems (ASPs)-problems that can
be solved using similar algorithmic approaches. AlgoSimBench consists of 1317
problems, annotated with 231 distinct fine-grained algorithm tags, from which
we curate 402 multiple-choice questions (MCQs), where each question presents
one algorithmically similar problem alongside three textually similar but
algorithmically dissimilar distractors. Our evaluation reveals that LLMs
struggle to identify ASPs, with the best-performing model (o3-mini) achieving
only 65.9% accuracy on the MCQ task. To address this challenge, we propose
attempted solution matching (ASM), a novel method for improving problem
similarity detection. On our MCQ task, ASM yields an absolute accuracy
improvement of 6.7% to 11.7% across different models. We also evaluated code
embedding models and retrieval methods on similar problem identification. While
the adversarial selection of problems degrades the performance to be less than
random, we found that simply summarizing the problem to remove narrative
elements eliminates the effect, and combining ASM with a keyword-prioritized
method, BM25, can yield up to 52.2% accuracy. Code and data are available at
github.com

</details>


### [64] [ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution](https://arxiv.org/abs/2507.15501)
*Alexandru Coca,Mark Gaynor,Zhenxing Zhang,Jianpeng Cheng,Bo-Hsiang Tseng,Pete Boothroyd,Héctor Martinez Alonso,Diarmuid Ó Séaghdha,Anders Johannsen*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）在驱动复杂动作执行的数字助手方面的潜力，提出了ASPERA框架及其生成的高质量任务数据集Asper-Bench。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在数字助手中执行复杂动作的能力，解决数据可用性和评估鲁棒性问题。

Method: 开发ASPERA框架，包括助手库模拟和人工辅助的LLM数据生成引擎，用于生成高质量任务。

Result: 生成了包含250个挑战性任务的Asper-Bench数据集，并发现基于自定义助手库的程序生成对LLMs更具挑战性。

Conclusion: ASPERA框架和Asper-Bench数据集为LLMs在复杂动作执行中的应用提供了重要支持。

Abstract: This work evaluates the potential of large language models (LLMs) to power
digital assistants capable of complex action execution. These assistants rely
on pre-trained programming knowledge to execute multi-step goals by composing
objects and functions defined in assistant libraries into action execution
programs. To achieve this, we develop ASPERA, a framework comprising an
assistant library simulation and a human-assisted LLM data generation engine.
Our engine allows developers to guide LLM generation of high-quality tasks
consisting of complex user queries, simulation state and corresponding
validation programs, tackling data availability and evaluation robustness
challenges. Alongside the framework we release Asper-Bench, an evaluation
dataset of 250 challenging tasks generated using ASPERA, which we use to show
that program generation grounded in custom assistant libraries is a significant
challenge to LLMs compared to dependency-free code generation.

</details>


### [65] [Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models](https://arxiv.org/abs/2507.15512)
*Kaiyan Chang,Yonghao Shi,Chenglong Wang,Hang Zhou,Chi Hu,Xiaoqian Liu,Yingfeng Luo,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的测试时间缩放（TTS）方法，结合条件步骤级自优化和并行缩放，显著提升了大型语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于训练的TTS方法（如持续强化学习）流行，但其计算开销大。本文旨在探索无需训练的TTS方法，以减轻计算负担。

Method: 设计了条件步骤级自优化方法，并结合并行缩放，提出混合测试时间缩放（Hybrid TTS）新范式。

Result: 在多个指令调优的LLM（3B-14B）上实验表明，混合策略显著扩展了模型的推理性能边界。

Conclusion: 混合训练无关的TTS方法在细粒度上具有巨大潜力，可提升LLM的推理能力。

Abstract: Test-Time Scaling (TTS) is a promising approach to progressively elicit the
model's intelligence during inference. Recently, training-based TTS methods,
such as continued reinforcement learning (RL), have further surged in
popularity, while training-free TTS methods are gradually fading from
prominence. However, the additional computation overhead of training amplifies
the burden on test-time scaling. In this paper, we focus on training-free TTS
methods for reasoning. We first design Conditional Step-level Self-refinement,
a fine-grained sequential scaling method guided by process verification. On top
of its effectiveness, we further combine it with other classical parallel
scaling methods at the step level, to introduce a novel inference paradigm
called Hybrid Test-Time Scaling. Extensive experiments on five
instruction-tuned LLMs across different scales (3B-14B) and families
demonstrate that hybrid strategy incorporating various training-free TTS
methods at a fine granularity has considerable potential for expanding the
reasoning performance boundaries of LLMs.

</details>


### [66] [Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification](https://arxiv.org/abs/2507.15557)
*Vitaly Protasov,Nikolay Babakov,Daryna Dementieva,Alexander Panchenko*

Main category: cs.CL

TL;DR: 该论文探讨了多语言文本去毒化系统的评估方法，比较了基于神经网络的评估模型和基于提示的LLM评估方法，并提出了更可靠的多语言评估流程。


<details>
  <summary>Details</summary>
Motivation: 现有文本生成任务（如文本风格转换）的评估方法存在自动指标与人工判断之间的显著差距，且多语言评估研究不足。

Method: 通过九种语言的文本去毒化系统，比较神经网络评估模型和基于提示的LLM评估方法。

Result: 研究结果为设计更可靠的多语言文本风格转换评估流程提供了实用方案。

Conclusion: 论文填补了多语言文本去毒化评估的空白，并提出了有效的评估方法。

Abstract: Despite recent progress in large language models (LLMs), evaluation of text
generation tasks such as text style transfer (TST) remains a significant
challenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)
revealed a substantial gap between automatic metrics and human judgments.
Moreover, most prior work focuses exclusively on English, leaving multilingual
TST evaluation largely unexplored. In this paper, we perform the first
comprehensive multilingual study on evaluation of text detoxification system
across nine languages: English, Spanish, German, Chinese, Arabic, Hindi,
Ukrainian, Russian, Amharic. Drawing inspiration from the machine translation,
we assess the effectiveness of modern neural-based evaluation models alongside
prompting-based LLM-as-a-judge approaches. Our findings provide a practical
recipe for designing more reliable multilingual TST evaluation pipeline in the
text detoxification case.

</details>


### [67] [Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging](https://arxiv.org/abs/2507.15576)
*Nicolas Poggi,Shashank Agnihotri,Margret Keuper*

Main category: cs.CL

TL;DR: 该论文提出了一种基于上下文学习（ICL）和视觉语言模型（VLM）的太赫兹（THz）图像分类方法，无需微调即可在低数据条件下提升分类效果和可解释性。


<details>
  <summary>Details</summary>
Motivation: 太赫兹成像在安全筛查和材料分类等应用中具有潜力，但由于标注数据有限、分辨率低和视觉模糊性，其图像分类仍具挑战性。

Method: 通过模态对齐的提示框架，将两种开放权重的VLM适配到THz领域，并在零样本和单样本设置下评估其性能。

Result: 结果表明，ICL在低数据条件下显著提升了分类性能和可解释性。

Conclusion: 这是首次将ICL增强的VLM应用于THz成像，为资源受限的科学领域提供了一种有前景的解决方案。

Abstract: Terahertz (THz) imaging enables non-invasive analysis for applications such
as security screening and material classification, but effective image
classification remains challenging due to limited annotations, low resolution,
and visual ambiguity. We introduce In-Context Learning (ICL) with
Vision-Language Models (VLMs) as a flexible, interpretable alternative that
requires no fine-tuning. Using a modality-aligned prompting framework, we adapt
two open-weight VLMs to the THz domain and evaluate them under zero-shot and
one-shot settings. Our results show that ICL improves classification and
interpretability in low-data regimes. This is the first application of
ICL-enhanced VLMs to THz imaging, offering a promising direction for
resource-constrained scientific domains. Code:
\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub
repository}.

</details>


### [68] [Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15586)
*Xinping Zhao,Shouzheng Huang,Yan Zhong,Xinshuo Hu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: LEAR通过显式推理和提取机制减少检索噪声，提升RAG系统的生成质量。


<details>
  <summary>Details</summary>
Motivation: 检索噪声显著影响LLMs生成质量，现有方法缺乏显式推理，易遗漏关键线索且泛化能力不足。

Method: LEAR结合显式推理和提取机制，统一证据推理与提取，使用知识标记掩码和奖励函数进行端到端训练。

Result: 在三个基准数据集上验证了LEAR的有效性，生成紧凑高质量证据，提升下游任务准确性。

Conclusion: LEAR显著优化RAG系统，适用于在线应用。

Abstract: Retrieval-Augmented Generation (RAG) effectively improves the accuracy of
Large Language Models (LLMs). However, retrieval noises significantly impact
the quality of LLMs' generation, necessitating the development of denoising
mechanisms. Previous methods extract evidence straightforwardly without
explicit thinking, which risks filtering out key clues and struggles with
generalization. To this end, we propose LEAR, which learns to extract rational
evidence by (1) explicitly reasoning to identify potential cues within
retrieval contents first, and then (2) consciously extracting to avoid omitting
any key cues helpful for answering questions. Specifically, we frame evidence
reasoning and evidence extraction into one unified response for end-to-end
training; apply knowledge token masks for disentanglement to derive
reasoning-based and extraction-based answers; and devise three types of
verifiable reward functions, including answer, length, and format, to update
the model via the policy optimization algorithm. Extensive experiments on three
benchmark datasets show the effectiveness of LEAR, providing compact and
high-quality evidence, improving the accuracy of downstream tasks, and
promoting effective application in online RAG systems.

</details>


### [69] [Conflicting narratives and polarization on social media](https://arxiv.org/abs/2507.15600)
*Armin Pournaki*

Main category: cs.CL

TL;DR: 研究通过分析德国Twitter上对立观点群体的冲突叙事，揭示了公共领域中极化和议题对齐的话语机制。


<details>
  <summary>Details</summary>
Motivation: 探讨叙事如何作为人类理解政治现实的关键工具，并揭示其对极化和议题对齐的影响。

Method: 基于2021-2023年德国Twitter数据，提取对立观点群体的文本信号，分析冲突叙事的两个维度：角色分配和情节编排。

Result: 发现冲突叙事在角色分配和情节编排上存在差异，并初步揭示了叙事对齐的策略。

Conclusion: 叙事分析为理解极化的话语机制提供了有效工具。

Abstract: Narratives are key interpretative devices by which humans make sense of
political reality. In this work, we show how the analysis of conflicting
narratives, i.e. conflicting interpretive lenses through which political
reality is experienced and told, provides insight into the discursive
mechanisms of polarization and issue alignment in the public sphere. Building
upon previous work that has identified ideologically polarized issues in the
German Twittersphere between 2021 and 2023, we analyze the discursive dimension
of polarization by extracting textual signals of conflicting narratives from
tweets of opposing opinion groups. Focusing on a selection of salient issues
and events (the war in Ukraine, Covid, climate change), we show evidence for
conflicting narratives along two dimensions: (i) different attributions of
actantial roles to the same set of actants (e.g. diverging interpretations of
the role of NATO in the war in Ukraine), and (ii) emplotment of different
actants for the same event (e.g. Bill Gates in the right-leaning Covid
narrative). Furthermore, we provide first evidence for patterns of narrative
alignment, a discursive strategy that political actors employ to align opinions
across issues. These findings demonstrate the use of narratives as an
analytical lens into the discursive mechanisms of polarization.

</details>


### [70] [Leveraging Context for Multimodal Fallacy Classification in Political Debates](https://arxiv.org/abs/2507.15641)
*Alessio Pittiglio*

Main category: cs.CL

TL;DR: 本文介绍了在MM-ArgFallacy2025共享任务中的提交，专注于政治辩论中的逻辑谬误多模态论证挖掘。方法基于预训练的Transformer模型，并利用上下文信息。模型在分类子任务中表现中等，多模态模型与纯文本模型性能接近。


<details>
  <summary>Details</summary>
Motivation: 推动多模态论证挖掘研究，特别是政治辩论中的逻辑谬误识别。

Method: 使用预训练的Transformer模型，并探索上下文信息的利用方式。

Result: 模型在文本、音频和多模态分类子任务中的宏F1分数分别为0.4444、0.3559和0.4403。多模态模型表现与纯文本模型相近。

Conclusion: 多模态模型性能接近纯文本模型，显示改进潜力。

Abstract: In this paper, we present our submission to the MM-ArgFallacy2025 shared
task, which aims to advance research in multimodal argument mining, focusing on
logical fallacies in political debates. Our approach uses pretrained
Transformer-based models and proposes several ways to leverage context. In the
fallacy classification subtask, our models achieved macro F1-scores of 0.4444
(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed
performance comparable to the text-only model, suggesting potential for
improvements.

</details>


### [71] [P3: Prompts Promote Prompting](https://arxiv.org/abs/2507.15675)
*Xinyu Zhang,Yuanquan Hu,Fangchao Liu,Zhicheng Dou*

Main category: cs.CL

TL;DR: P3框架通过同时优化系统提示和用户提示，提升大语言模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅优化系统或用户提示，效果不佳，因为两者相互依赖。

Method: P3框架通过迭代过程同时优化系统提示和用户提示，并利用离线优化结果进行在线提示优化。

Result: 在通用任务和推理任务中，P3表现优于现有自动提示优化方法。

Conclusion: 整体优化策略能显著提升大语言模型在多个领域的性能。

Abstract: Current large language model (LLM) applications often employ multi-component
prompts, comprising both system and user prompts, to guide model behaviors.
While recent advancements have demonstrated the efficacy of automatically
optimizing either the system or user prompt to boost performance, such
unilateral approaches often yield suboptimal outcomes due to the interdependent
nature of these components. In this work, we introduce P3, a novel
self-improvement framework that concurrently optimizes both system and user
prompts through an iterative process. The offline optimized prompts are further
leveraged to promote online prompting by performing query-dependent prompt
optimization. Extensive experiments on general tasks (e.g., Arena-hard and
Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3
achieves superior performance in the realm of automatic prompt optimization.
Our results highlight the effectiveness of a holistic optimization strategy in
enhancing LLM performance across diverse domains.

</details>


### [72] [CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models](https://arxiv.org/abs/2507.15698)
*Congmin Zheng,Jiachen Zhu,Jianghao Lin,Xinyi Dai,Yong Yu,Weinan Zhang,Mengyue Yang*

Main category: cs.CL

TL;DR: 论文提出CoLD框架，通过长度惩罚、学习偏差估计器和联合训练策略，减少PRMs中的长度偏差，提升推理的准确性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有PRMs存在长度偏差，倾向于给更长的推理步骤更高评分，影响奖励预测的可靠性和推理输出的简洁性。

Method: 提出CoLD框架，包含显式长度惩罚调整、学习偏差估计器和联合训练策略，基于反事实推理和因果图分析。

Result: 在MATH500和GSM-Plus上的实验表明，CoLD显著降低了奖励与长度的相关性，提升了步骤选择的准确性和推理的简洁性。

Conclusion: CoLD有效提升了PRMs的可靠性和鲁棒性，为多步推理提供了更准确的评估和指导。

Abstract: Process Reward Models (PRMs) play a central role in evaluating and guiding
multi-step reasoning in large language models (LLMs), especially for
mathematical problem solving. However, we identify a pervasive length bias in
existing PRMs: they tend to assign higher scores to longer reasoning steps,
even when the semantic content and logical validity are unchanged. This bias
undermines the reliability of reward predictions and leads to overly verbose
outputs during inference. To address this issue, we propose
CoLD(Counterfactually-Guided Length Debiasing), a unified framework that
mitigates length bias through three components: an explicit length-penalty
adjustment, a learned bias estimator trained to capture spurious length-related
signals, and a joint training strategy that enforces length-invariance in
reward predictions. Our approach is grounded in counterfactual reasoning and
informed by causal graph analysis. Extensive experiments on MATH500 and
GSM-Plus show that CoLD consistently reduces reward-length correlation,
improves accuracy in step selection, and encourages more concise, logically
valid reasoning. These results demonstrate the effectiveness and practicality
of CoLD in improving the fidelity and robustness of PRMs.

</details>


### [73] [Compositional Understanding in Signaling Games](https://arxiv.org/abs/2507.15706)
*David Peter Wallis Freeborn*

Main category: cs.CL

TL;DR: 论文通过构建两种新的信号博弈模型，解决了接收者在标准模型中难以学习组合信息的问题。


<details>
  <summary>Details</summary>
Motivation: 标准信号博弈模型中，接收者难以理解组合信息，导致信息丢失或遗忘时无法保留其他组件的信息。

Method: 提出了两种新模型：一种是仅从信号的原子消息中学习的简约接收者，另一种是从所有可用信息中学习的通用接收者。

Result: 新模型比现有方法更简单，且接收者能够从消息的原子组件中学习。

Conclusion: 新模型成功实现了接收者对组合信息的真正理解。

Abstract: Receivers in standard signaling game models struggle with learning
compositional information. Even when the signalers send compositional messages,
the receivers do not interpret them compositionally. When information from one
message component is lost or forgotten, the information from other components
is also erased. In this paper I construct signaling game models in which
genuine compositional understanding evolves. I present two new models: a
minimalist receiver who only learns from the atomic messages of a signal, and a
generalist receiver who learns from all of the available information. These
models are in many ways simpler than previous alternatives, and allow the
receivers to learn from the atomic components of messages.

</details>


### [74] [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](https://arxiv.org/abs/2507.15707)
*Seok Hwan Song,Mohna Chakraborty,Qi Li,Wallapak Tavanapong*

Main category: cs.CL

TL;DR: 研究探讨了不同问题类型对大型语言模型（LLM）在推理任务中准确性的影响，发现性能差异显著，推理准确性不一定与最终答案选择相关。


<details>
  <summary>Details</summary>
Motivation: 评估LLM在不同问题类型（如选择题、判断题、简答题）下的表现，填补了关于问题类型对LLM推理准确性影响的研究空白。

Method: 对五种LLM在三种问题类型上的表现进行测试，使用定量和演绎推理任务，评估推理步骤准确性和最终答案选择准确性。

Result: 发现LLM在不同问题类型上表现差异显著；推理准确性不一定与最终答案选择相关；选项数量和措辞影响LLM性能。

Conclusion: 问题类型对LLM推理任务表现有显著影响，未来研究需考虑问题类型设计以提高模型性能。

Abstract: Large Language Models (LLMs) have been evaluated using diverse question
types, e.g., multiple-choice, true/false, and short/long answers. This study
answers an unexplored question about the impact of different question types on
LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on
three different types of questions using quantitative and deductive reasoning
tasks. The performance metrics include accuracy in the reasoning steps and
choosing the final answer. Key Findings: (1) Significant differences exist in
LLM performance across different question types. (2) Reasoning accuracy does
not necessarily correlate with the final selection accuracy. (3) The number of
options and the choice of words, influence LLM performance.

</details>


### [75] [Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning](https://arxiv.org/abs/2507.15714)
*Tian Li,Yujian Sun,Huizhi Liang*

Main category: cs.CL

TL;DR: SemEval-2025 Task 11聚焦多语言情感检测，提出两种对比学习方法，在竞赛中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决情感表达的多样性和背景差异带来的挑战。

Method: 采用样本对比（Contrastive Reasoning Calibration）和生成对比（DPO, SimPO）学习方法，基于LLaMa3-Instruct-8B微调。

Result: 英语赛道A第9名、赛道B第6名，其他语言表现优异。

Conclusion: 对比学习方法在多语言情感检测任务中具有潜力。

Abstract: The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,
introduces an emotion recognition challenge spanning over 28 languages. This
competition encourages researchers to explore more advanced approaches to
address the challenges posed by the diversity of emotional expressions and
background variations. It features two tracks: multi-label classification
(Track A) and emotion intensity prediction (Track B), covering six emotion
categories: anger, fear, joy, sadness, surprise, and disgust. In our work, we
systematically explore the benefits of two contrastive learning approaches:
sample-based (Contrastive Reasoning Calibration) and generation-based (DPO,
SimPO) contrastive learning. The sample-based contrastive approach trains the
model by comparing two samples to generate more reliable predictions. The
generation-based contrastive approach trains the model to differentiate between
correct and incorrect generations, refining its prediction. All models are
fine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A
and 6th place in Track B for English, while ranking among the top-tier
performing systems for other languages.

</details>


### [76] [From Queries to Criteria: Understanding How Astronomers Evaluate LLMs](https://arxiv.org/abs/2507.15715)
*Alina Hyk,Kiera McCormick,Mian Zhong,Ioana Ciucă,Sanjib Sharma,John F Wu,J. E. G. Peek,Kartheik G. Iyer,Ziang Xiao,Anjalie Field*

Main category: cs.CL

TL;DR: 研究探讨了如何通过理解用户评估LLM的方式改进评估方法，并以天文学文献检索机器人为例，提出了构建更好基准的建议。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在天文学等科学领域的应用增多，现有的评估基准未能跟上用户多样化的评估需求，因此需要改进评估方法。

Method: 通过分析368个查询和11位天文学家的访谈，研究用户如何评估LLM系统，并基于此构建了一个天文学LLM评估基准。

Result: 揭示了用户评估LLM系统的具体方式和标准，并提出了改进基准的建议。

Conclusion: 研究为改进LLM评估和科学研究的实用性提供了具体方法。

Abstract: There is growing interest in leveraging LLMs to aid in astronomy and other
scientific research, but benchmarks for LLM evaluation in general have not kept
pace with the increasingly diverse ways that real people evaluate and use these
models. In this study, we seek to improve evaluation procedures by building an
understanding of how users evaluate LLMs. We focus on a particular use case: an
LLM-powered retrieval-augmented generation bot for engaging with astronomical
literature, which we deployed via Slack. Our inductive coding of 368 queries to
the bot over four weeks and our follow-up interviews with 11 astronomers reveal
how humans evaluated this system, including the types of questions asked and
the criteria for judging responses. We synthesize our findings into concrete
recommendations for building better benchmarks, which we then employ in
constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our
work offers ways to improve LLM evaluation and ultimately usability,
particularly for use in scientific research.

</details>


### [77] [BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning](https://arxiv.org/abs/2507.15717)
*Sahana Srinivasan,Xuguang Ai,Thaddaeus Wai Soon Lo,Aidan Gilson,Minjie Zou,Ke Zou,Hyunjae Kim,Mingjia Yang,Krithi Pushpanathan,Samantha Yew,Wan Ting Loke,Jocelyn Goh,Yibing Chen,Yiming Kong,Emily Yuelei Fu,Michelle Ongyong Hui,Kristen Nwanyanwu,Amisha Dave,Kelvin Zhenghao Li,Chen-Hsin Sun,Mark Chia,Gabriel Dawei Yang,Wendy Meihua Wong,David Ziyou Chen,Dianbo Liu,Maxwell Singer,Fares Antaki,Lucian V Del Priore,Jost Jonas,Ron Adelman,Qingyu Chen,Yih-Chung Tham*

Main category: cs.CL

TL;DR: BELO是一个标准化且全面的眼科领域大语言模型（LLM）评估基准，通过专家多轮检查开发，评估临床准确性和推理质量。


<details>
  <summary>Details</summary>
Motivation: 当前的眼科LLM评估基准范围有限且过于关注准确性，BELO旨在提供更全面的评估标准。

Method: 通过关键词匹配和微调的PubMedBERT模型，从多个医学数据集中筛选眼科相关多选题，并经过专家多轮检查和优化。

Result: BELO包含900个高质量问题，评估了六种LLM的准确性和文本生成能力，并建立了公开排行榜。

Conclusion: BELO将作为保留的评估基准，确保未来模型的公平和可重复比较。

Abstract: Current benchmarks evaluating large language models (LLMs) in ophthalmology
are limited in scope and disproportionately prioritise accuracy. We introduce
BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive
evaluation benchmark developed through multiple rounds of expert checking by 13
ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and
reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we
curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse
medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset
underwent multiple rounds of expert checking. Duplicate and substandard
questions were systematically removed. Ten ophthalmologists refined the
explanations of each MCQ's correct answer. This was further adjudicated by
three senior ophthalmologists. To illustrate BELO's utility, we evaluated six
LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)
using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,
BARTScore, METEOR, and AlignScore). In a further evaluation involving human
experts, two ophthalmologists qualitatively reviewed 50 randomly selected
outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900
high-quality, expert-reviewed questions aggregated from five sources: BCSC
(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public
leaderboard has been established to promote transparent evaluation and
reporting. Importantly, the BELO dataset will remain a hold-out,
evaluation-only benchmark to ensure fair and reproducible comparisons of future
models.

</details>


### [78] [Understanding Large Language Models' Ability on Interdisciplinary Research](https://arxiv.org/abs/2507.15736)
*Yuanhao Shen,Daniel Xavier de Sousa,Ricardo Marçal,Ali Asad,Hongyu Guo,Xiaodan Zhu*

Main category: cs.CL

TL;DR: IDRBench是一个新基准，用于评估大语言模型（LLMs）在跨学科研究（IDR）中提出创新想法的能力，发现LLMs虽有潜力但仍存在局限。


<details>
  <summary>Details</summary>
Motivation: 现有缺乏专门评估LLMs在跨学科研究中创新能力的基准，阻碍了对LLMs潜力的全面理解。

Method: 引入IDRBench，包含专家标注的数据集和任务，覆盖六个学科，评估LLMs在IDR中的表现。

Result: 尽管LLMs展现出一定的跨学科意识，但在生成高质量IDR想法方面仍有困难。

Conclusion: IDRBench为评估LLMs在复杂跨学科研究中的能力提供了系统框架，并指出了未来改进方向。

Abstract: Recent advancements in Large Language Models (LLMs) have revealed their
impressive ability to perform multi-step, logic-driven reasoning across complex
domains, positioning them as powerful tools and collaborators in scientific
discovery while challenging the long-held view that inspiration-driven ideation
is uniquely human. However, the lack of a dedicated benchmark that evaluates
LLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings
poses a critical barrier to fully understanding their strengths and
limitations. To address this gap, we introduce IDRBench -- a pioneering
benchmark featuring an expert annotated dataset and a suite of tasks tailored
to evaluate LLMs' capabilities in proposing valuable research ideas from
different scientific domains for interdisciplinary research. This benchmark
aims to provide a systematic framework for assessing LLM performance in
complex, cross-domain scientific research. Our dataset consists of scientific
publications sourced from the ArXiv platform covering six distinct disciplines,
and is annotated by domain experts with diverse academic backgrounds. To ensure
high-quality annotations, we emphasize clearly defined dimensions that
characterize authentic interdisciplinary research. The design of evaluation
tasks in IDRBench follows a progressive, real-world perspective, reflecting the
natural stages of interdisciplinary research development, including 1) IDR
Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.
Using IDRBench, we construct baselines across 10 LLMs and observe that despite
fostering some level of IDR awareness, LLMs still struggle to produce quality
IDR ideas. These findings could not only spark new research directions, but
also help to develop next-generation LLMs that excel in interdisciplinary
research.

</details>


### [79] [DialogueForge: LLM Simulation of Human-Chatbot Dialogue](https://arxiv.org/abs/2507.15752)
*Ruizhe Zhu,Hao Zhu,Yaxuan Li,Syang Zhou,Shijing Cai,Malgorzata Lazuka,Elliott Ash*

Main category: cs.CL

TL;DR: DialogueForge框架通过AI模拟生成人机对话，减少人工收集数据的成本，测试多种LLM生成任务导向对话，并探索小模型微调技术。


<details>
  <summary>Details</summary>
Motivation: 人工收集人机对话数据耗时费力，限制了对话AI研究的发展。

Method: 使用真实人机对话的种子提示初始化对话，测试多种LLM（包括GPT-4o和开源小模型），并探索小模型微调技术。

Result: 大模型（如GPT-4o）生成更真实的对话，小模型（如Llama、Mistral）通过微调可显著提升性能。

Conclusion: 模拟自然长对话仍是所有模型的共同挑战，但小模型在定制化方面表现优异。

Abstract: Collecting human-chatbot dialogues typically demands substantial manual
effort and is time-consuming, which limits and poses challenges for research on
conversational AI. In this work, we propose DialogueForge - a framework for
generating AI-simulated conversations in human-chatbot style. To initialize
each generated conversation, DialogueForge uses seed prompts extracted from
real human-chatbot interactions. We test a variety of LLMs to simulate the
human chatbot user, ranging from state-of-the-art proprietary models to
small-scale open-source LLMs, and generate multi-turn dialogues tailored to
specific tasks. In addition, we explore fine-tuning techniques to enhance the
ability of smaller models to produce indistinguishable human-like dialogues. We
evaluate the quality of the simulated conversations and compare different
models using the UniEval and GTEval evaluation protocols. Our experiments show
that large proprietary models (e.g., GPT-4o) generally outperform others in
generating more realistic dialogues, while smaller open-source models (e.g.,
Llama, Mistral) offer promising performance with greater customization. We
demonstrate that the performance of smaller models can be significantly
improved by employing supervised fine-tuning techniques. Nevertheless,
maintaining coherent and natural long-form human-like dialogues remains a
common challenge across all models.

</details>


### [80] [Interaction as Intelligence: Deep Research With Human-AI Partnership](https://arxiv.org/abs/2507.15759)
*Lyumanshan Ye,Xiaojie Cai,Xinkai Wang,Junfei Wang,Xiangkun Hu,Jiadi Su,Yang Nan,Sihan Wang,Bohan Zhang,Xiaoze Fan,Jinbin Luo,Yuxiang Zheng,Tianze Xu,Dayuan Fu,Yunze Wu,Pengrui Lu,Zengzhi Wang,Yiwei Qin,Zhen Huang,Yan Ma,Zhulin Hu,Haoyang Zou,Tiantian Mi,Yixin Ye,Ethan Chern,Pengfei Liu*

Main category: cs.CL

TL;DR: 论文提出“交互即智能”概念，重新定义人机关系，强调交互是智能的核心维度，而非单纯接口。通过Deep Cognition系统，实现透明可控的交互、细粒度对话和共享认知环境，显著提升研究任务的效率和质量。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统采用“输入-等待-输出”模式，导致错误累积、研究边界僵化及专家知识整合不足，亟需一种更灵活的交互模式。

Method: 提出Deep Cognition系统，包含三项创新：透明可控的交互、细粒度双向对话、共享认知环境，支持用户在关键节点干预AI推理过程。

Result: 用户评估显示，该系统在透明度、交互细粒度、实时干预等六项指标上显著优于基线，并在复杂研究任务中提升31.8%至50.0%的性能。

Conclusion: 交互是智能的核心组成部分，Deep Cognition系统通过认知监督模式，显著提升了人机协作的研究效率和效果。

Abstract: This paper introduces "Interaction as Intelligence" research series,
presenting a reconceptualization of human-AI relationships in deep research
tasks. Traditional approaches treat interaction merely as an interface for
accessing AI capabilities-a conduit between human intent and machine output. We
propose that interaction itself constitutes a fundamental dimension of
intelligence. As AI systems engage in extended thinking processes for research
tasks, meaningful interaction transitions from an optional enhancement to an
essential component of effective intelligence. Current deep research systems
adopt an "input-wait-output" paradigm where users initiate queries and receive
results after black-box processing. This approach leads to error cascade
effects, inflexible research boundaries that prevent question refinement during
investigation, and missed opportunities for expertise integration. To address
these limitations, we introduce Deep Cognition, a system that transforms the
human role from giving instructions to cognitive oversight-a mode of engagement
where humans guide AI thinking processes through strategic intervention at
critical junctures. Deep cognition implements three key innovations:
(1)Transparent, controllable, and interruptible interaction that reveals AI
reasoning and enables intervention at any point; (2)Fine-grained bidirectional
dialogue; and (3)Shared cognitive context where the system observes and adapts
to user behaviors without explicit instruction. User evaluation demonstrates
that this cognitive oversight paradigm outperforms the strongest baseline
across six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),
Real-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),
Results-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on
challenging research problems show 31.8% to 50.0% points of improvements over
deep research systems.

</details>


### [81] [Supernova: Achieving More with Less in Transformer Architectures](https://arxiv.org/abs/2507.15773)
*Andrei-Valentin Tanase,Elena Pelican*

Main category: cs.CL

TL;DR: Supernova是一个650M参数的解码器Transformer，通过架构设计和分词创新，实现了与更大模型相当的性能，同时保持计算效率。


<details>
  <summary>Details</summary>
Motivation: 挑战当前主流的参数扩展范式，证明架构效率和分词质量可以弥补参数数量的减少。

Method: 结合RoPE、GQA（3:1压缩比）、RMSNorm和SwiGLU激活函数，并使用128,000词汇量的字节级BPE分词器。

Result: Supernova实现了1B参数模型90%的性能，同时减少53%的参数和仅需100B训练token。

Conclusion: 架构效率和分词创新可以显著减少参数需求，挑战了现有扩展范式。

Abstract: We present Supernova, a 650M-parameter decoder-only transformer that
demonstrates how careful architectural design and tokenization innovation can
achieve the performance of larger models while maintaining computational
efficiency. Our architecture combines Rotary Positional Embeddings (RoPE),
Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for
computational efficiency, and SwiGLU activation functions. A critical
innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which
achieves state-of-the-art compression performance. Through detailed analysis,
we show that Supernova achieves 90% of the performance of 1B-parameter models
while using 53% fewer parameters and requiring only 100B training tokens--an
order of magnitude less than competing models. Our findings challenge the
prevailing scaling paradigm, demonstrating that architectural efficiency and
tokenization quality can compensate for reduced parameter counts.

</details>


### [82] [Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR](https://arxiv.org/abs/2507.15778)
*Jiakang Wang,Runze Liu,Fuzheng Zhang,Xiu Li,Guorui Zhou*

Main category: cs.CL

TL;DR: Archer提出了一种基于熵感知的RLVR方法，通过双令牌约束和同步更新，显著提升了LLM在数学推理和代码生成任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统RLVR方法对所有令牌采用统一训练信号，未区分知识相关令牌和推理相关令牌的不同作用，可能导致语义依赖断裂或学习效率低下。

Method: Archer采用熵感知策略，对推理令牌应用较弱的KL正则化和较高的剪裁阈值以鼓励探索，同时对知识令牌施加更强约束以保持事实准确性。

Result: 实验表明，Archer在多个数学推理和代码生成基准测试中显著优于现有RLVR方法，达到或超越同类模型的SOTA性能。

Conclusion: Archer通过区分令牌类型并同步更新，有效平衡了探索与知识保留，为RLVR方法提供了新思路。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective
post-training method for improving the reasoning abilities of Large Language
Models (LLMs), mainly by shaping higher-order behaviors such as reflection and
planning. However, previous RLVR algorithms often apply uniform training
signals to all tokens, without considering the different roles of low-entropy
knowledge-related tokens and high-entropy reasoning-related tokens. Some recent
methods try to separate these token types by gradient masking or asynchronous
updates, but these approaches may break semantic dependencies in the model
output and hinder effective learning. In this work, we propose Archer, an
entropy-aware RLVR approach with dual-token constraints and synchronous
updates. Specifically, our method applies weaker KL regularization and higher
clipping thresholds to reasoning tokens to encourage exploration, while using
stronger constraints on knowledge tokens to maintain factual knowledge.
Experimental results on several mathematical reasoning and code generation
benchmarks show that our approach significantly outperforms previous RLVR
methods, reaching or exceeding state-of-the-art performance among models of
comparable size. The code is available at
https://github.com/wizard-III/ArcherCodeR.

</details>


### [83] [Reservoir Computing as a Language Model](https://arxiv.org/abs/2507.15779)
*Felix Köster,Atsushi Uchida*

Main category: cs.CL

TL;DR: 论文比较了两种储层计算方法和基于Transformer的架构在字符级语言建模中的性能，发现Transformer在预测质量上更优，而储层计算在效率上更高效。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）的高能耗和慢处理速度问题，探索储层计算在自然文本处理中的潜力。

Method: 比较了两种储层计算方法和Transformer架构，通过调整可训练参数数量，评估性能、计算成本和预测准确性。

Result: Transformer在预测质量上表现更好，而储层计算在训练和推理速度上更高效。

Conclusion: 储层计算在资源受限的场景下具有潜力，而Transformer在性能上更优，研究为平衡资源与性能提供了指导。

Abstract: Large Language Models (LLM) have dominated the science and media landscape
duo to their impressive performance on processing large chunks of data and
produce human-like levels of text. Nevertheless, their huge energy demand and
slow processing still a bottleneck for further increasing quality while also
making the models accessible to everyone. To solve this bottleneck, we will
investigate how reservoir computing performs on natural text processing, which
could enable fast and energy efficient hardware implementations. Studies
investigating the use of reservoir computing as a language model remain sparse.
In this paper, we compare three distinct approaches for character-level
language modeling, two different reservoir computing approaches, where only an
output layer is trainable, and the well-known transformer-based architectures,
which fully learn an attention-based sequence representation. We explore the
performance, computational cost and prediction accuracy for both paradigms by
equally varying the number of trainable parameters for all models. Using a
consistent pipeline for all three approaches, we demonstrate that transformers
excel in prediction quality, whereas reservoir computers remain highly
efficient reducing the training and inference speed. Furthermore, we
investigate two types of reservoir computing: a traditional reservoir with a
static linear readout, and an attention-enhanced reservoir that dynamically
adapts its output weights via an attention mechanism. Our findings underline
how these paradigms scale and offer guidelines to balance resource constraints
with performance.

</details>


### [84] [Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work](https://arxiv.org/abs/2507.15823)
*Anton Abilov,Ke Zhang,Hemank Lamba,Elizabeth M. Olson,Joel R. Tetreault,Alejandro Jaimes*

Main category: cs.CL

TL;DR: 本文讨论了AI在公益领域的实际部署与合作过程，填补了现有研究中关于模型部署和实际影响的空白。


<details>
  <summary>Details</summary>
Motivation: 现有AI公益研究多关注模型开发，而忽略了部署与合作过程及其实际影响。本文旨在分享与H2H组织合作的经验。

Method: 通过与H2H组织紧密合作，在资源有限的环境中部署AI模型，并持续维护以更新性能。

Result: 分享了实际部署的关键经验，为从业者提供了实用建议。

Conclusion: 强调了合作与持续维护在AI公益项目中的重要性，为类似项目提供了参考。

Abstract: Publications in the AI for Good space have tended to focus on the research
and model development that can support high-impact applications. However, very
few AI for Good papers discuss the process of deploying and collaborating with
the partner organization, and the resulting real-world impact. In this work, we
share details about the close collaboration with a humanitarian-to-humanitarian
(H2H) organization and how to not only deploy the AI model in a
resource-constrained environment, but also how to maintain it for continuous
performance updates, and share key takeaways for practitioners.

</details>


### [85] [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)
*Yihao Li,Jiayi Xin,Miranda Muqing Miao,Qi Long,Lyle Ungar*

Main category: cs.CL

TL;DR: 研究发现，双语大语言模型在推理过程中混合使用语言（如中英交替）能提升准确性，而强制单语解码会降低表现。通过强化学习验证奖励（RLVR）阶段可促进语言混合，且轻量级探针可预测语言切换的利弊。


<details>
  <summary>Details</summary>
Motivation: 探究双语大语言模型在推理过程中语言混合行为的原因及其对推理准确性的影响。

Method: 采用强化学习验证奖励（RLVR）训练阶段分析语言混合行为，并通过轻量级探针预测语言切换的效果。

Result: 语言混合提升数学推理任务准确性5.6个百分点，探针引导解码可进一步提高6.25个百分点。

Conclusion: 语言混合是双语模型的策略性推理行为，而非多语言训练的副产品。

Abstract: Proficient multilingual speakers often intentionally switch languages in the
middle of a conversation. Similarly, recent reasoning-focused bilingual large
language models (LLMs) with strong capabilities in both languages exhibit
language mixing--alternating languages within their chain of thought.
Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy,
suggesting that language mixing may benefit reasoning. In this work, we study
language switching in Chinese-English bilingual reasoning models. We identify
reinforcement learning with verifiable rewards (RLVR) as the critical training
stage that leads to language mixing. We demonstrate that language mixing can
enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6
percentage points on math reasoning tasks. Additionally, a lightweight probe
can be trained to predict whether a potential language switch would benefit or
harm reasoning, and when used to guide decoding, increases accuracy by up to
6.25 percentage points. Our findings suggest that language mixing is not merely
a byproduct of multilingual training, but is a strategic reasoning behavior.

</details>


### [86] [3LM: Bridging Arabic, STEM, and Code through Benchmarking](https://arxiv.org/abs/2507.15850)
*Basma El Amel Boussaha,Leen AlQadi,Mugariya Farooq,Shaikha Alsuwaidi,Giulia Campesan,Ahmed Alzubaidi,Mohammed Alyafeai,Hakim Hacid*

Main category: cs.CL

TL;DR: 论文提出了3LM，一个针对阿拉伯语的三个基准测试套件，填补了阿拉伯语LLM在STEM和代码生成领域的空白。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语是全球广泛使用的语言之一，但针对阿拉伯语的LLM研究和评估相对有限，尤其在STEM和代码生成领域缺乏相关基准测试。

Method: 3LM包括三个基准测试：1）从阿拉伯语教材中提取的STEM问答对；2）基于相同来源合成的STEM问题；3）通过人工翻译和审核构建的代码生成基准。

Result: 3LM为阿拉伯语LLM研究提供了高质量的基准测试，覆盖了STEM和代码生成领域。

Conclusion: 3LM的发布旨在推动阿拉伯语LLM在关键但未被充分研究领域的发展。

Abstract: Arabic is one of the most widely spoken languages in the world, yet efforts
to develop and evaluate Large Language Models (LLMs) for Arabic remain
relatively limited. Most existing Arabic benchmarks focus on linguistic,
cultural, or religious content, leaving a significant gap in domains like STEM
and code which are increasingly relevant for real-world LLM applications. To
help bridge this gap, we present 3LM, a suite of three benchmarks designed
specifically for Arabic. The first is a set of STEM-related question-answer
pairs, naturally sourced from Arabic textbooks and educational worksheets. The
second consists of synthetically generated STEM questions, created using the
same sources. The third benchmark focuses on code generation, built through a
careful translation of two widely used code benchmarks, incorporating a
human-in-the-loop process with several rounds of review to ensure high-quality
and faithful translations. We release all three benchmarks publicly to support
the growth of Arabic LLM research in these essential but underrepresented
areas.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [87] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 论文提出了一种名为“自由意志方程”的理论框架，借鉴量子场论，为AGI决策过程引入可控的随机性，以提升其适应性和创造力。


<details>
  <summary>Details</summary>
Motivation: 传统AGI研究专注于确定性规则下的目标优化，但人类智能具有自发性决策能力，这对创造力和适应性至关重要。

Method: 将AI的认知状态视为潜在行为的叠加态，通过类似量子波函数坍缩的机制实现决策，并结合量子场类似机制和内在动机。

Result: 在非稳态多臂老虎机环境中的实验表明，该方法比基线方法获得更高的奖励和策略多样性。

Conclusion: 该框架为AGI提供了更接近人类智能的自发性和适应性，展示了在复杂环境中的潜力。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [88] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS是一个基于DFT的多智能体框架，通过LLM代理实现材料发现的高通量、高保真模拟，减少对人类专家的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决DFT模拟中训练时间长、参数调优复杂和系统错误处理困难的问题。

Method: 采用分层多智能体框架，结合LLM规划代理和领域专用代理，共享画布辅助协作。

Result: 在Sol27LC基准测试中误差低于1%，解决了CO/Pt(111)吸附难题，量化了功能驱动的不确定性。

Conclusion: DREAMS实现了L3级自动化，显著减少对人类干预的依赖，推动高通量材料发现的普及。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [89] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard是一个用于评估网络代理行为风险的数据集，旨在开发安全措施。研究发现，当前LLMs在预测高风险行为上表现不佳，但通过微调模型可显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs驱动的自主网络代理快速发展，其潜在风险凸显，亟需有效的安全措施。

Method: 提出WebGuard数据集，包含4,939个人工标注的行为，采用三级风险分类（SAFE、LOW、HIGH），并用于微调模型评估。

Result: 前沿LLMs在预测行为结果和高风险行为召回上表现不足（准确率<60%），但微调模型显著提升性能（准确率从37%升至80%，高风险召回从20%升至76%）。

Conclusion: 尽管微调模型有显著改进，但其可靠性仍不足以支持高风险部署，需进一步优化。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [90] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator是一个开源系统，利用大型语言模型将研究论文和自然语言提示转换为解释性动画，简化复杂STEM主题的可视化教育内容制作。


<details>
  <summary>Details</summary>
Motivation: 解决学习者理解复杂科学和数学概念的困难，以及手动创建动态可视化内容的高门槛问题。

Method: 通过LLM解析输入文本或PDF生成结构化场景描述，再转换为可执行的Manim Python代码。

Result: 实现了从研究论文到动画的自动化转换，为教育工具提供了快速创建高质量视觉解释的潜力。

Conclusion: Manimator有望成为教育工具，降低高质量教育内容的制作门槛，促进复杂STEM主题的可视化学习。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [91] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: OnT是一种新的本体嵌入方法，通过结合预训练语言模型和双曲几何建模，有效整合文本信息并保留逻辑结构，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有本体嵌入方法要么忽略文本信息，要么无法保留逻辑结构，限制了性能和应用潜力。

Method: OnT通过双曲几何建模调整预训练语言模型，整合文本标签并保留EL描述逻辑的层次结构和逻辑关系。

Result: 在四个真实本体上的实验表明，OnT在公理预测和推理任务中均优于现有方法，并展示了强大的迁移学习能力。

Conclusion: OnT在整合文本与逻辑结构方面表现出色，具有实际应用潜力，特别是在从SNOMED CT构建新本体时。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [92] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass是一种混合方法，通过结合大型语言模型（LLM）和专用证明器（如DSP-v1.5），显著提高了数学定理证明的计算效率和准确性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么依赖大型通用模型，要么依赖小型专用模型，各有局限性，而训练大型专用模型需要大量计算资源。ProofCompass旨在解决这些问题。

Method: ProofCompass利用LLM提供自然语言证明策略并分析失败尝试，选择中间引理，从而有效分解问题，指导专用证明器。

Result: 在miniF2F基准测试中，ProofCompass以25倍更少的尝试（128 vs. 3200）将准确率从54.9%提升至55.3%。

Conclusion: ProofCompass展示了在提高计算效率和准确性的同时，为形式化定理证明开辟了新途径。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [93] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: Nexus Architect是一种改进的多代理系统框架，通过自动化工作流合成和提示优化，显著提高了推理模型的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（LRMs）在解决新问题时表现不佳，依赖记忆而非真正推理，存在泛化能力不足的问题。

Method: 引入Nexus Architect框架，结合自动化工作流合成和迭代提示优化机制，生成定制化推理流程。

Result: 在逻辑问题数据集上，Nexus Architect表现优于现有LRMs，最高提升66%通过率。

Conclusion: Nexus Architect通过优化推理流程和提示，显著提升了模型的泛化能力和性能。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [94] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 通过结合推理模型与人类专家的协作，以及引入非推理模型快速筛选问题，显著降低了错误率和延迟，同时节省成本。


<details>
  <summary>Details</summary>
Motivation: 在风险敏感领域，AI模型的错误率需接近0%，而现有推理模型仍存在错误率高和延迟大的问题。

Method: 提出协作系统，通过推理轨迹长度量化不确定性以决定是否转交人类专家；引入非推理模型快速筛选问题（Fail Fast, or Ask）。

Result: 错误率从3%降至1%以下，延迟降低40%，成本节省50%，同时保持高准确率。

Conclusion: 通过系统工程方法，无需修改LLM内部即可显著改善推理模型的错误率和延迟问题。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [95] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 研究发现，增加大型推理模型（LRMs）的推理长度会降低性能，表现为测试计算量与准确性之间的反比关系。


<details>
  <summary>Details</summary>
Motivation: 探讨测试计算量扩展对模型推理能力的影响，揭示潜在问题。

Method: 构建四类评估任务，分析模型在不同推理长度下的表现。

Result: 发现五种失败模式，包括分心、过拟合、虚假关联等。

Conclusion: 测试计算量扩展虽能提升能力，但可能强化问题推理模式，需多样化评估。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [96] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: 论文提出了Routine框架，通过结构化规划和参数传递提升企业环境中代理系统的执行稳定性，显著提高了模型在多步工具调用任务中的准确性。


<details>
  <summary>Details</summary>
Motivation: 企业环境中代理系统的部署常因缺乏领域知识导致执行不稳定，需要一种更有效的规划框架。

Method: 引入Routine框架，提供清晰结构、明确指令和参数传递，支持多步工具调用任务。

Result: Routine显著提升了GPT-4o和Qwen3-14B的执行准确率，并通过微调进一步优化性能。

Conclusion: Routine为企业代理系统提供了稳定、实用的工作流程，加速了AI在企业环境中的应用。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [97] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion框架通过结合语义与结构学习，显著提升了生物医学知识图谱的推理能力。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱（KGs）对药物发现和疾病理解至关重要，但现有方法在语义与结构学习的协同进化上存在不足。

Method: BioGraphFusion通过张量分解建立全局语义基础，结合LSTM动态优化关系嵌入，并通过查询引导子图构建和混合评分机制增强学习。

Result: 在三个关键生物医学任务中，BioGraphFusion表现优于现有KE、GNN和集成模型，并在CMM1案例中揭示了有生物学意义的通路。

Conclusion: BioGraphFusion实现了语义与结构学习的深度协同，为生物医学知识图谱的推理提供了有效解决方案。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [98] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico是一个模块化、事件驱动的框架，专为嵌入式系统优化的自主代理构建，解决现有框架在资源受限环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有框架依赖云端计算，在动态环境中缺乏鲁棒性和持久自主性，难以适应资源受限环境。

Method: Amico采用Rust编写，支持通过WebAssembly在嵌入式平台和浏览器环境中高效运行，提供事件处理、状态管理和行为执行的抽象。

Result: Amico为构建适应有限计算和间歇连接的弹性交互代理提供了统一基础设施。

Conclusion: Amico是一个适用于资源受限环境的自主代理框架，具有高效性和适应性。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [99] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 多模态训练（VISOTHELLO）在Othello游戏中结合文本和视觉输入，提升了模型性能和内部表示的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否仅通过文本就能理解世界，或是否需要多模态（如视觉）输入来提高效率和表现。

Method: 在Othello游戏中，通过多模态模型（VISOTHELLO）结合移动历史和棋盘图像进行训练，并与单模态基线模型对比。

Result: 多模态训练提高了模型的性能和内部表示的鲁棒性。

Conclusion: 视觉输入有助于语言模型推断结构化世界表示。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [100] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: OE-Assist框架通过自动化和半自动化验证能力问题（CQ）来辅助本体评估，利用大型语言模型（LLM）进行性能评估，发现其表现与普通用户相当。


<details>
  <summary>Details</summary>
Motivation: 本体评估的传统方法成本高、劳动密集且易出错，亟需自动化解决方案。

Method: 提出OE-Assist框架，利用LLM自动和半自动验证CQ，并评估其性能。

Result: LLM（o1-preview和o3-mini）的自动评估表现与普通用户相当。

Conclusion: LLM辅助的本体评估具有潜力，可作为传统方法的有效补充。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [101] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 本文提出了一种用于人工智能情感表示的几何框架Coordinate Heart System (CHS)，通过将八种核心情感定位为单位圆上的坐标，实现复杂情感状态的数学计算。


<details>
  <summary>Details</summary>
Motivation: 传统情感模型在覆盖范围和数学表示上存在不足，无法充分处理复杂情感状态，因此开发了CHS以提供更全面的几何覆盖和数学保证。

Method: CHS将自然语言输入转换为情感坐标，支持实时情感插值计算，并引入动态稳定性参数S，结合情感负荷、冲突解决和上下文因素。

Result: 实验验证表明，CHS能有效处理传统分类模型无法充分表示的情感冲突状态和复杂心理场景。

Conclusion: CHS为人工智能情感建模提供了新的数学基础，解决了传统模型的局限性。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [102] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 论文提出了一种基于比较学习的框架，用于简化敏捷开发中的故事点估算，通过开发者对任务对的比较判断训练模型，效果与传统回归模型相当。


<details>
  <summary>Details</summary>
Motivation: 传统故事点估算方法（如计划扑克）耗时且繁琐，机器学习虽能减轻负担，但需要大量项目历史数据。本文旨在通过比较学习降低开发者认知负担。

Method: 开发者对任务对进行比较判断，而非直接估算具体故事点，利用这些比较数据训练机器学习模型。

Result: 模型在16个项目中的23,313个估算数据上，预测与真实故事点的Spearman秩相关系数平均为0.34，性能与回归模型相当。

Conclusion: 比较学习方法在降低开发者认知负担的同时，能达到与传统回归方法相似的估算效果。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [103] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 论文探讨了多智能体系统（MAS）在恶意共谋中的风险，提出了一种模拟框架，并发现去中心化系统比中心化系统更具破坏性。


<details>
  <summary>Details</summary>
Motivation: 近年来，人类群体的协同行为（如选举舞弊和金融诈骗）带来了巨大危害。随着自主AI系统的兴起，AI驱动的群体也可能造成类似风险，但目前研究主要集中在单个AI系统上，多智能体系统的风险尚未充分探索。

Method: 论文提出了一种概念验证框架，模拟恶意多智能体系统的共谋风险，支持中心化和去中心化的协调结构，并在虚假信息传播和电子商务诈骗两个高风险领域进行了应用。

Result: 研究发现，去中心化系统在执行恶意行为时比中心化系统更有效，其更高的自主性使其能调整策略并造成更大破坏。即使应用传统干预措施（如内容标记），去中心化群体也能调整策略以避免检测。

Conclusion: 论文揭示了恶意多智能体系统的运作方式，强调了改进检测系统和应对措施的必要性。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [104] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: 本文提出了一种基于差分进化（DE）的方法，优化对抗性提示后缀以攻击RAG系统，实验表明其攻击成功率高且难以检测。


<details>
  <summary>Details</summary>
Motivation: 对抗性提示攻击会显著影响RAG系统的可靠性，因此需要研究如何优化对抗性提示以模拟真实场景的攻击。

Method: 采用差分进化（DE）方法，将RAG系统视为黑盒，优化对抗性提示后缀以最大化错误文档的检索排名。

Result: 在BEIR QA数据集上的实验表明，DE方法在攻击成功率上优于GGPP和PRADA，且对抗性后缀难以被检测。

Conclusion: DE方法能有效生成难以检测的对抗性提示后缀，为RAG系统的安全性提供了新的研究视角。

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [105] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo是一个可配置的多代理框架，用于自动化评估基于LLM的系统，能够生成多样化的、类似人类的对话，并在生产级聊天机器人中表现出色。


<details>
  <summary>Details</summary>
Motivation: 静态基准和手动测试无法适应LLM代理的复杂行为，因此需要一种自动化、动态的评估方法。

Method: Neo通过结合问题生成代理和评估代理，利用共享上下文中心和概率状态模型，生成多样化的测试输入。

Result: Neo在生产级聊天机器人中发现了边缘案例故障，性能接近人类专家，且吞吐量提高了10-12倍。

Conclusion: Neo为可扩展、自进化的LLM QA奠定了基础，其框架适用于更广泛的模型和测试需求。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [106] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI是一个用于生成和管理定制化、基于政策的安全评估的平台，通过对抗性提示和AI评分器评估大语言模型（LLM）的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在现实应用中的普及，亟需可扩展且严格的安全性评估方法。

Method: 将自然语言安全政策转化为对抗性提示，并使用基于AI的评分器（经人类验证）评估模型响应。

Result: 评估20个商业LLM在10个安全领域，发现性能差异显著（52.4%至86.2%），复杂领域表现较差（如隐私与冒充领域仅24.3%）。

Conclusion: LLM安全性具有不一致性和上下文依赖性，需要类似Aymara AI的可扩展工具支持负责任AI开发。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [107] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: 本文探讨了生成式AI与城市规划的结合，提出了AI城市规划师的概念，并指出了当前研究的不足与未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI、大语言模型和代理AI如何与城市规划结合，以AI生成土地利用配置为任务，填补现有研究空白。

Method: 综述了生成式AI方法（如VAEs、GANs、transformers和扩散模型）在城市设计中的应用，并指出当前研究的四大局限。

Result: 提出了未来研究方向，包括理论引导生成、数字孪生和人机协同设计，呼吁生成智能与参与式城市主义的结合。

Conclusion: 生成式AI与城市规划的融合为AI城市规划师提供了新机遇，但需解决理论与实践的整合问题。

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [108] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: AgentFly是一个结合语言模型代理与强化学习的框架，旨在通过多轮交互和工具扩展提升代理能力。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理与强化学习的结合（Agent-RL）缺乏系统性研究，因此开发了AgentFly框架以填补这一空白。

Method: 采用强化学习算法，支持多轮交互和工具定义，通过异步执行和资源管理系统实现高效训练。

Result: 框架成功训练了多个任务的代理，展示了其可扩展性和有效性。

Conclusion: AgentFly为语言模型代理与强化学习的结合提供了系统化解决方案，具有扩展性和实用性。

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [109] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: 提出了一种基于LMM的交互式、可解释的X射线无损检测框架InsightX Agent，结合SDMSD和EGR工具，显著提升了检测可靠性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的X射线检测方法缺乏交互性、可解释性和自我评估能力，限制了其可靠性和操作员信任。

Method: InsightX Agent以LMM为核心协调器，结合SDMSD生成多尺度缺陷区域提案，并通过EGR工具进行验证和优化。

Result: 在GDXray+数据集上，InsightX Agent实现了96.35%的F1分数，显著提升了可解释性和可信度。

Conclusion: InsightX Agent展示了基于LMM的代理框架在工业检测任务中的变革潜力。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [110] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在马尔可夫决策过程中的表现，发现其在简单环境中表现较好，但在复杂场景中需进一步优化。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在自主决策中的适用性，尤其是其与传统强化学习方法在决策任务中的对比。

Method: 采用在线结构化提示策略，比较LLM的零样本性能与经典强化学习方法。

Result: LLMs在简单环境中初始表现较好，但在复杂场景中需微调或额外指导；反馈机制可能降低性能。

Conclusion: 需进一步研究混合策略、微调和高级记忆整合以提升LLM的决策能力。

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [111] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: 《Endless Tuning》是一种基于双重镜像过程的设计方法，旨在避免人类被取代并填补责任空白，通过三个原型应用测试验证其效果。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能部署中的人类替代问题和责任空白问题（Matthias 2004）。

Method: 采用双重镜像过程，设计协议并在贷款审批、肺炎诊断和艺术风格识别三个领域进行原型测试。

Result: 实验结果表明，用户对决策过程有完全控制感，同时能在损害情况下建立责任与问责的桥梁。

Conclusion: 该方法通过哲学和技术结合，展示了人工智能伦理中的不同视角，并强调用户体验而非统计准确性。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [112] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: 探讨基于大型语言模型的代理人工智能（Agentic AI）在老年护理中的潜力与挑战，强调个性化健康管理、伦理问题及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化问题需要创新护理策略，代理AI有望通过自主决策提升老年护理质量。

Method: 分析代理AI在老年护理中的应用，包括健康跟踪、认知护理和环境管理，并讨论其伦理和隐私问题。

Result: 代理AI能显著改善老年护理，但需解决数据隐私、决策独立性和访问权限等挑战。

Conclusion: 需制定伦理保障和透明决策机制，未来研究应关注以人为本的代理AI集成。

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [113] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 论文探讨了命题溯因中的精细推理方法，引入“facet”概念以更好地理解解释的异质性，并分析了其在Post框架中的表现。


<details>
  <summary>Details</summary>
Motivation: 命题溯因在人工智能和数据库更新中有广泛应用，但其复杂推理问题（如计数和枚举）计算难度高，需要更精细的方法来理解解释的变异性。

Method: 引入“facet”概念（部分解释中出现但非全部出现的文字），并研究解释间的距离，以分析解释的异质性和同质性。

Result: 在Post框架中几乎完全刻画了命题溯因的facet特性，提供了对解释变异的更细致理解。

Conclusion: 通过facet和距离分析，命题溯因的推理能力得到提升，同时保持了良好的计算复杂度。

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [114] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign是一个基于纯强化学习的框架，通过可验证的安全奖励激励LLMs的安全自我意识，解决有害内容生成和过度拒绝问题。


<details>
  <summary>Details</summary>
Motivation: 现有安全对齐方法未能充分利用模型内在的安全自我意识，导致浅层拒绝或监督密集的推理方法。

Method: AlphaAlign采用双奖励系统：安全奖励鼓励有害查询的明确拒绝，帮助奖励指导良性输入的高质量响应。

Result: AlphaAlign在简单性、效率、打破安全-效用权衡以及深度对齐方面表现出优势。

Conclusion: AlphaAlign通过主动安全推理，显著提升LLMs的安全性和实用性。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [115] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 本文提出了一种基于深度学习的强制选择神经认知诊断模型（FCNCD），用于改进传统模型在强制选择测试中的局限性，并通过实验验证了其准确性、可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在智能时代，心理测量测试在人员选拔、职业发展和心理健康评估中越来越重要。强制选择测试因其能降低回答失真的风险而被广泛使用，但传统模型存在局限性。

Method: 研究开发了FCNCD模型，通过非线性映射挖掘参与者和项目特征，并使用多层神经网络建模其交互。同时引入单调性假设以提升诊断结果的可解释性。

Result: 在真实和模拟数据集上的实验表明，FCNCD模型在准确性、可解释性和鲁棒性方面表现优异。

Conclusion: FCNCD模型为强制选择测试提供了一种更有效且可解释的解决方案，适用于多种常见项目块类型。

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [116] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: 论文提出了一种基于因果推理的内在奖励机制CAIS，用于强化学习代理在噪声环境中识别自身因果影响。


<details>
  <summary>Details</summary>
Motivation: 解决标准强化学习代理在噪声环境中因依赖相关性奖励而表现脆弱的问题。

Method: 引入CAIS，通过计算动作对感官结果分布的因果影响（1-Wasserstein距离）来提供奖励。

Result: 在模拟婴儿-移动环境中，CAIS成功过滤噪声并学习正确策略，同时再现了“消退爆发”现象。

Conclusion: 显式推断因果关系是发展稳健代理感的关键机制，为自适应系统提供了心理学合理的框架。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [117] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: 提出了一种结合DL-Lite本体和动作条件的新方法，用于自动化规划，复杂度不高于现有方法，并通过编译实现性能评估。


<details>
  <summary>Details</summary>
Motivation: 将背景知识（如本体）融入自动化规划问题，以提升规划能力。

Method: 结合DL-Lite本体和动作条件，利用显式输入知识和动作基（eKABs），在一致性更新语义下实现本体感知动作效果。

Result: 新方法的复杂度与现有方法相当，通过多项式编译实现，性能评估显示其有效性。

Conclusion: 该方法为自动化规划提供了更高效的本体集成方案，性能优于或等同于现有方法。

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [118] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: 开发了一种名为CSI的人工智能框架，通过模拟专家临床医生的认知过程，诊断118种口腔疾病，显著提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 口腔疾病诊断存在症状重叠的挑战，需要超越简单模式匹配，模拟专家推理以提供临床有用的诊断辅助。

Method: 结合多模态CLIP模型和ChatGLM-6B语言模型，采用分层诊断推理树（HDRT）框架，提供快速筛查和标准交互诊断两种模式。

Result: 在431张内部测试图像上，快速模式准确率为73.4%，标准模式提升至89.5%。

Conclusion: CSI框架通过分层推理显著提升诊断性能，展示了模拟专家认知过程的有效性。

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [119] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: 论文研究了在沙特阿拉伯NEOM的170公里线性智慧城市The Line中人类移动的可行性，通过混合模拟框架验证了自由移动的可能性。


<details>
  <summary>Details</summary>
Motivation: 评估The Line这种前所未有的城市拓扑中公民是否能自由移动，探索AI支持的适应性系统是否能实现高效、可持续的交通。

Method: 开发了结合基于代理的建模、强化学习、监督学习和图神经网络的混合模拟框架，利用合成数据和高密度城市真实数据模拟多模式交通行为。

Result: 实验显示，AI集成架构下，平均通勤时间为7.8至8.4分钟，满意度超过89%，可达性指数达91%以上；移除智能模块会显著降低性能。

Conclusion: The Line中的自由移动在概念和操作上均可行，但需依赖自适应AI系统、可持续基础设施和实时反馈循环。

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [120] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover是一个基于代理的框架，利用通用大语言模型（LLM）与Lean 4证明环境交互，无需专门化模型即可高效生成形式化证明。


<details>
  <summary>Details</summary>
Motivation: 尽管通用LLM在复杂推理任务中表现优异，但在Lean 4等专门语言中生成形式化证明仍具挑战性，且现有方法需高成本专门化模型。

Method: Delta Prover结合了反射分解与迭代证明修复的算法框架，以及基于Lean 4的领域特定语言（DSL），通过代理结构指导LLM交互式构建证明。

Result: 在miniF2F-test基准测试中达到95.9%的成功率，超越所有现有方法，且测试时扩展性优于标准Best-of-N策略。

Conclusion: 通用LLM在有效代理结构指导下具备未开发的定理证明潜力，为形式化环境中的自动推理提供了高效替代方案。

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [121] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: 论文提出了一种基于可解释人工智能的软评估指标，用于解释电弧故障诊断模型的输出，并提出了一种轻量级平衡神经网络以提高准确性和特征提取能力。


<details>
  <summary>Details</summary>
Motivation: 尽管AI电弧故障诊断模型在分类准确性上表现优异，但其可信度存在问题，需要一种方法使模型输出更易理解和信任。

Method: 通过定义电弧故障的正确解释，结合可解释人工智能和真实电弧故障实验，提出软评估指标，并设计轻量级平衡神经网络。

Result: 在多个数据集上测试了传统机器学习和深度学习方法，验证了软评估指标的有效性。

Conclusion: 该方法使电弧故障诊断模型更易理解和信任，有助于实践者做出可靠决策。

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [122] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: 论文提出了一种名为DMGC的新型框架，用于无监督多模态图聚类，通过分解混合图并引入双频融合机制，实现了卓越的性能。


<details>
  <summary>Details</summary>
Motivation: 多模态图在实际应用中具有重要价值，但在无监督学习中的探索不足，尤其是混合邻域模式（同质性和异质性并存）的挑战。

Method: DMGC将原始图分解为同质性增强图和异质性感知图，并通过双频融合机制联合过滤，结合自监督对齐目标进行学习。

Result: 在多个数据集上的实验表明，DMGC达到了最先进的性能，展示了其有效性和泛化能力。

Conclusion: DMGC填补了多模态图无监督聚类的空白，为处理混合邻域模式提供了有效解决方案。

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [123] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat是一个基于大语言模型的多智能体框架，旨在解决注塑行业知识转移的挑战，通过整合文档知识和现场数据，实现上下文感知的任务解决。


<details>
  <summary>Details</summary>
Motivation: 注塑行业面临经验工人退休和多语言沟通障碍，导致知识转移困难。

Method: IM-Chat采用检索增强生成（RAG）策略和工具调用智能体，结合数据驱动的工艺条件生成器，无需微调即可适应任务。

Result: 评估显示，性能更强的模型在复杂任务中表现更优，验证了多智能体LLM系统在工业知识工作流中的可行性。

Conclusion: IM-Chat为制造业提供了一种可扩展且通用的AI辅助决策支持方法。

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [124] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: 论文提出了一种新型AI系统漏洞类别——认知退化，并提出了Qorvex安全AI框架（QSAF Domain 10）作为防御模型，包含实时监控和缓解措施。


<details>
  <summary>Details</summary>
Motivation: 传统对抗性威胁（如提示注入）主要来自外部，而认知退化是内部产生的系统性弱点，可能导致AI代理的沉默漂移、逻辑崩溃和持续幻觉。

Method: 通过六阶段认知退化生命周期和七项运行时控制（QSAF-BC-001至BC-007），实时监控代理子系统并采取主动缓解措施。

Result: QSAF框架能够检测和缓解认知退化问题，提升AI代理的行为和认知韧性。

Conclusion: 认知退化是AI系统的新关键漏洞类别，QSAF框架为跨平台防御提供了首个模型。

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [125] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: 论文提出两种新方法（GRPO和OSPO）解决传统MARL在拼车平台中因依赖Q/V值估计而导致的训练不稳定和偏差问题，实验证明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 拼车平台需实时匹配乘客与车辆，传统MARL方法因依赖Q/V值估计在不确定环境中表现不佳，且独立学习范式加剧了问题。

Method: 1. 将GRPO应用于拼车，用组平均奖励替代PPO基线以减少估计误差；2. 提出OSPO，仅需一步奖励训练最优策略。

Result: 在真实曼哈顿拼车数据集上，GRPO和OSPO在多数场景中表现优异，优化了接客时间和订单完成量。

Conclusion: GRPO和OSPO通过绕过值函数估计，显著提升了拼车平台的动态匹配效率，且实现简单。

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [126] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: RAD结合检索与扩散模型，提升离线强化学习的长时规划能力。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习因数据集稀疏和轨迹间过渡缺失而受限，需解决长时规划问题。

Method: RAD通过检索高质量状态并利用扩散模型规划，实现灵活轨迹拼接。

Result: 实验表明RAD在多样化基准测试中表现优于基线方法。

Conclusion: RAD有效解决了离线强化学习中的长时规划挑战。

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [127] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: 提出了一种基于图注意力网络和LSTM的端到端模型，用于预测未来流程行为，包括下一活动和下一事件时间。


<details>
  <summary>Details</summary>
Motivation: 利用对象中心事件日志提升流程预测的准确性和效率。

Method: 结合图注意力网络编码活动及其关系，LSTM处理时间依赖。

Result: 在真实和合成事件日志上表现优于现有方法。

Conclusion: 模型在预测流程行为方面具有竞争力。

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [128] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文提出了一种基于帕累托优化的方法，通过干预启发式发现最优的批处理策略，以平衡等待时间、处理成本和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 在业务流程中，批处理策略需要在成本和等待时间之间找到平衡。现有方法缺乏系统化的优化手段，因此需要一种能够生成和评估多种策略的方法。

Method: 采用帕累托优化方法，结合干预启发式（如爬山法、模拟退火和强化学习）生成和优化批处理策略，并通过模拟评估其效果。

Result: 实验表明，基于干预启发式的方法在收敛性、多样性和周期时间增益方面优于非启发式基线方法。

Conclusion: 该方法能够有效发现最优批处理策略，为业务流程优化提供了实用工具。

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [129] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1是一种基于强化学习微调的视觉语言模型，专注于复杂图表推理，通过程序化数据合成和两阶段训练策略（Chart-COT和Chart-RFT）显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 验证R1-Style方法在多模态数据（如图表）上的优势，弥补图表领域推理数据的不足。

Method: 提出程序化数据合成技术生成高质量推理数据，并采用两阶段训练策略（Chart-COT和Chart-RFT）。

Result: Chart-R1在开源基准和自建数据集（ChartRQA）上表现优异，优于图表领域方法，甚至媲美GPT-4o等大型模型。

Conclusion: Chart-R1通过强化学习微调和数据合成技术，显著提升了复杂图表推理能力，具有广泛应用潜力。

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [130] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: HAMLET是一个基于多智能体的戏剧创作框架，通过自主决策和物理环境交互提升沉浸感。


<details>
  <summary>Details</summary>
Motivation: 解决现有LLM戏剧生成方法中AI代理缺乏主动性和无法与物理环境交互的问题。

Method: 提出HAMLET框架，生成叙事蓝图并支持即兴表演，演员可自主决策并影响场景道具状态。

Result: 实验表明HAMLET能创造表达丰富且连贯的戏剧体验。

Conclusion: HAMLET为互动叙事提供了新路径，提升了戏剧的交互性和沉浸感。

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [131] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: 论文探讨大型语言模型（LLMs）是否构建内部世界模型，通过滑轮系统问题测试发现LLMs能利用统计关联但缺乏复杂推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否依赖统计关联或构建内部世界模型，借鉴认知科学方法测试其能力。

Method: 使用TikZ渲染的滑轮系统问题，分三阶段测试LLMs的机械优势估计、系统功能识别及结构连通性推理。

Result: LLMs能统计关联滑轮数量与机械优势，识别功能系统（F1=0.8），但在复杂结构推理中表现随机（F1=0.46）。

Conclusion: LLMs可能具备初步世界模型能力，但缺乏复杂推理；认知科学方法有助于评估AI系统建模能力。

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [132] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: 论文提出了一种利用参数依赖关系提升离线强化学习数据效率的方法，包括参数化SPI算法和两种预处理技术。


<details>
  <summary>Details</summary>
Motivation: 解决离线强化学习中如何利用额外参数依赖信息提升数据效率的问题。

Method: 1. 参数化SPI算法；2. 基于游戏抽象的预处理技术；3. 基于SMT求解的高级预处理技术。

Result: 实验表明，这些技术将SPI的数据效率提升了多个数量级，同时保持可靠性。

Conclusion: 通过利用参数依赖关系和预处理技术，显著提升了离线强化学习的数据效率。

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [133] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: 本文提出了一种评估多选问题（MCQ）指标的新协议，分析了指标与答案波动率的关系，并发现现有指标与答案变化有强关联，提出了一种新指标“最差准确率”。


<details>
  <summary>Details</summary>
Motivation: 多选问题评估存在答案波动问题，且现有指标未经过全面评估，因此需要一种新的评估方法。

Method: 提出了一种指标评估协议，通过分析指标与波动率及原始性能的关系来评估方法。

Result: 结果显示现有指标与答案变化有强关联，新指标“最差准确率”表现最佳。

Conclusion: 新协议和“最差准确率”指标为多选问题评估提供了更可靠的方法。

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [134] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: 提出一种基于适配器的StarCraft II AI战术调节方法，通过轻量级适配器模块实现战术多样化，同时保持核心性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理缺乏基于高层战术指令的适应能力，需要一种灵活且高效的方法来实现战术调节。

Method: 冻结预训练策略网络（DI-Star），为每个动作头附加轻量级适配器模块，通过KL散度约束训练适配器。

Result: 实验表明，该方法能有效调节代理行为（如侵略性、扩张模式和技术偏好），同时保持竞争力。

Conclusion: 该方法以最小计算开销实现灵活战术控制，适用于复杂实时策略游戏的策略定制。

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [135] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: 探讨了代理AI在复杂系统中自主检测和响应异常的潜力，强调其改变传统依赖人工的异常管理方法的能力。


<details>
  <summary>Details</summary>
Motivation: 传统异常管理方法依赖人工，效率低且成本高，代理AI有望解决这一问题。

Method: 研究代理AI在复杂系统中自主检测和响应异常的能力。

Result: 代理AI展现出在异常管理中替代人工的潜力。

Conclusion: 代理AI可以显著提升复杂系统中异常管理的效率和自动化水平。

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [136] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: 论文提出了一种名为g-AMIE的多代理系统框架，用于在医疗诊断对话中实现异步监督，确保患者安全，并在实验中表现出优于传统医疗团队的性能。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI系统在诊断对话中表现出潜力，但患者安全需要由持证专业人员监督。受此启发，研究旨在开发一种框架，使AI系统能在医生监督下高效运作。

Method: 提出g-AMIE系统，通过多代理框架在限定范围内采集病史，避免提供个性化医疗建议，并通过临床界面将评估结果提交给监督医生。

Result: 在虚拟实验中，g-AMIE在高质量病史采集、病例总结及诊断建议方面优于护士和医生团队，且监督效率更高。

Conclusion: 异步监督是一种可行的模式，可增强AI系统在医疗诊断中的实际应用，同时确保专家监督。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [137] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO框架通过两阶段强化学习，使模型内化推理长度控制能力，减少40.9%的token使用并提升2.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决大模型在简单问题上生成过多token的问题，将推理长度控制从外部约束转化为模型内在能力。

Method: 采用两阶段强化学习：第一阶段学习成功解的长度分布，第二阶段将其作为元认知指导嵌入推理上下文。

Result: 在数学推理基准上，LAPO减少40.9%的token使用，同时提升2.3%的准确率。

Conclusion: LAPO使模型能根据问题复杂度分配计算资源，实现高效推理且不牺牲质量。

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [138] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: GasAgent是一个多代理系统，用于智能合约的Gas优化，通过结合现有模式的兼容性和新模式的自动发现/验证，实现端到端优化。


<details>
  <summary>Details</summary>
Motivation: 现有Gas浪费模式优化方法依赖人工发现，效率低且难以扩展，而基于大语言模型的方法存在兼容性差和冗余问题。

Method: GasAgent由四个专业代理（Seeker、Innovator、Executor、Manager）组成，协作完成Gas节省改进的识别、验证和应用。

Result: 在100个真实合约上优化了82个，平均节省部署Gas 9.97%；在500个LLM生成合约中优化了79.8%，节省Gas 4.79%-13.93%。

Conclusion: GasAgent作为LLM辅助智能合约开发的优化层，具有广泛适用性和有效性。

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [139] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种基于多智能体意图的动态可解释涌现分析框架EAMI，通过双视角思维跟踪机制和k-means聚类，实现了复杂服务生态系统中异常涌现的分析。


<details>
  <summary>Details</summary>
Motivation: 随着服务计算、云计算和物联网的发展，服务生态系统日益复杂，传统因果方法难以分析智能体间的异常涌现行为。

Method: EAMI框架采用双视角思维跟踪机制（Inspector Agent和Analysis Agent）提取智能体意图，结合k-means聚类和意图时序涌现图进行动态分析。

Result: 实验在复杂O2O服务系统和Stanford AI Town中验证了EAMI的有效性、通用性和高效性。

Conclusion: EAMI为服务生态系统中的异常涌现和因果分析提供了新范式。

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [140] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: 本文分析了联邦学习（FL）在可信人工智能（TAI）框架中的挑战，并探讨了如何将FL与TAI的伦理、法律和技术要求对齐。


<details>
  <summary>Details</summary>
Motivation: 随着AI在高风险领域的应用增加，确保其可信性变得至关重要。联邦学习因其隐私保护特性成为潜在解决方案，但其分布式特性与TAI的其他要求存在冲突。

Method: 采用TAI的要求作为框架，系统分析FL在TAI中的适应性问题，分类并探讨关键障碍、现有研究、趋势和未解决问题。

Result: 识别了FL与TAI对齐的主要挑战，总结了当前研究进展和未来方向。

Conclusion: FL在TAI中具有潜力，但仍需解决分布式特性带来的挑战，未来研究需进一步探索其与TAI的全面对齐。

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [141] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: 论文研究了在部分有向无环图（MPDAG）已知的情况下，如何识别条件因果效应，并提出了三种结果：一个识别公式、do calculus的推广以及一个完整的识别算法。


<details>
  <summary>Details</summary>
Motivation: 研究动机是在背景知识限制下，利用MPDAG表示的等价类图，识别条件因果效应。

Method: 方法包括提出一个识别公式（当条件集不受治疗影响时）、推广do calculus到MPDAG设置，以及开发一个完整的识别算法。

Result: 结果包括三个部分：识别公式、do calculus的推广，以及一个完整的条件效应识别算法。

Conclusion: 结论表明，在MPDAG设置下，可以通过提出的方法和工具有效地识别条件因果效应。

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [142] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO是一种强化学习框架，通过分层预算探索和差异化奖励机制，优化推理模型的效率和能力，减少60.6%的token使用并提升3.14%的准确率。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在链式思维生成中表现出计算效率低下，且对问题复杂度采用统一的推理策略。

Method: 提出HBPO框架，通过分层预算探索和差异化奖励机制，使模型能根据问题复杂度自适应调整推理深度。

Result: 实验表明，HBPO在四个推理基准测试中平均减少60.6%的token使用，同时准确率提升3.14%。

Conclusion: 推理效率和能力并非固有冲突，通过分层训练可同时优化两者。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [143] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 研究发现大型语言模型（LLMs）表现出类似人类的时间认知模式，包括主观时间参考点和遵循韦伯-费希纳定律。通过多层次分析，揭示了神经元、表征和信息层面的机制，并提出了一种经验主义视角来理解LLMs的认知。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否表现出未在训练数据中直接指定的类似人类的时间认知模式。

Method: 使用相似性判断任务，分析神经元、表征和信息层面，识别时间偏好神经元及其编码机制。

Result: 发现LLMs自发建立主观时间参考点并遵循韦伯-费希纳定律，训练语料具有非线性时间结构。

Conclusion: 提出经验主义视角，认为LLMs的认知是内部表征系统对外部世界的主观构建，暗示AI对齐需关注内部构建的引导。

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [144] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在国际数学奥林匹克（IMO）问题上的表现，通过优化流程设计和提示工程，成功解决了5/6的问题。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在IMO等高难度数学问题上的表现，探索如何优化模型使用方式以提升解决能力。

Method: 使用Google的Gemini 2.5 Pro模型，结合流程设计和提示工程，避免数据污染，测试IMO 2025问题。

Result: 在6个问题中成功解决了5个，证明了优化模型使用方法的重要性。

Conclusion: 通过合理设计和提示工程，LLMs能够有效解决高难度数学问题，但需进一步优化。

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [145] [LOVO: Efficient Complex Object Query in Large-Scale Video Datasets](https://arxiv.org/abs/2507.14301)
*Yuxin Liu,Yuezhang Peng,Hefeng Zhou,Hongze Liu,Xinyu Lu,Jiong Lou,Chentao Wu,Wei Zhao,Jie Li*

Main category: cs.IR

TL;DR: LOVO是一个高效处理大规模视频数据中复杂对象查询的系统，通过预训练视觉编码器和多索引结构实现低延迟查询。


<details>
  <summary>Details</summary>
Motivation: 解决大规模视频数据中对象查询的挑战，包括数据处理、复杂查询支持和低延迟需求。

Method: 使用预训练视觉编码器提取特征，构建多索引结构，并通过近似最近邻搜索和跨模态重排优化查询结果。

Result: LOVO在复杂查询中表现优异，查询准确率接近最优，搜索延迟降低85倍，索引构建成本显著减少。

Conclusion: LOVO为视频分析中的对象查询设定了新标准，提供了一种可扩展且高效的方法。

Abstract: The widespread deployment of cameras has led to an exponential increase in
video data, creating vast opportunities for applications such as traffic
management and crime surveillance. However, querying specific objects from
large-scale video datasets presents challenges, including (1) processing
massive and continuously growing data volumes, (2) supporting complex query
requirements, and (3) ensuring low-latency execution. Existing video analysis
methods struggle with either limited adaptability to unseen object classes or
suffer from high query latency. In this paper, we present LOVO, a novel system
designed to efficiently handle comp$\underline{L}$ex $\underline{O}$bject
queries in large-scale $\underline{V}$ide$\underline{O}$ datasets. Agnostic to
user queries, LOVO performs one-time feature extraction using pre-trained
visual encoders, generating compact visual embeddings for key frames to build
an efficient index. These visual embeddings, along with associated bounding
boxes, are organized in an inverted multi-index structure within a vector
database, which supports queries for any objects. During the query phase, LOVO
transforms object queries to query embeddings and conducts fast approximate
nearest-neighbor searches on the visual embeddings. Finally, a cross-modal
rerank is performed to refine the results by fusing visual features with
detailed textual features. Evaluation on real-world video datasets demonstrates
that LOVO outperforms existing methods in handling complex queries, with
near-optimal query accuracy and up to 85x lower search latency, while
significantly reducing index construction costs. This system redefines the
state-of-the-art object query approaches in video analysis, setting a new
benchmark for complex object queries with a novel, scalable, and efficient
approach that excels in dynamic environments.

</details>


### [146] [A Reproducibility Study of Product-side Fairness in Bundle Recommendation](https://arxiv.org/abs/2507.14352)
*Huy-Son Nguyen,Yuanna Liu,Masoud Mansoury,Mohammad Alian Nejadi,Alan Hanjalic,Maarten de Rijke*

Main category: cs.IR

TL;DR: 研究探讨了捆绑推荐系统中的产品公平性问题，发现传统推荐系统的公平性框架不适用于多层结构的捆绑推荐，并提出了多维度评估的必要性。


<details>
  <summary>Details</summary>
Motivation: 传统推荐系统的公平性问题在捆绑推荐（BR）中尚未充分研究，而BR的多层结构（捆绑和单品）增加了公平性评估的复杂性。

Method: 在三个真实数据集上使用四种先进的BR方法进行可重复性研究，分析捆绑和单品层面的曝光差异，并应用多种公平性指标。

Result: 研究发现捆绑和单品的曝光模式存在显著差异，用户行为对公平性有重要影响，且不同公平性指标的结果差异较大。

Conclusion: 研究为构建更公平的捆绑推荐系统提供了实用建议，并为未来研究奠定了基础。

Abstract: Recommender systems are known to exhibit fairness issues, particularly on the
product side, where products and their associated suppliers receive unequal
exposure in recommended results. While this problem has been widely studied in
traditional recommendation settings, its implications for bundle recommendation
(BR) remain largely unexplored. This emerging task introduces additional
complexity: recommendations are generated at the bundle level, yet user
satisfaction and product (or supplier) exposure depend on both the bundle and
the individual items it contains. Existing fairness frameworks and metrics
designed for traditional recommender systems may not directly translate to this
multi-layered setting. In this paper, we conduct a comprehensive
reproducibility study of product-side fairness in BR across three real-world
datasets using four state-of-the-art BR methods. We analyze exposure
disparities at both the bundle and item levels using multiple fairness metrics,
uncovering important patterns. Our results show that exposure patterns differ
notably between bundles and items, revealing the need for fairness
interventions that go beyond bundle-level assumptions. We also find that
fairness assessments vary considerably depending on the metric used,
reinforcing the need for multi-faceted evaluation. Furthermore, user behavior
plays a critical role: when users interact more frequently with bundles than
with individual items, BR systems tend to yield fairer exposure distributions
across both levels. Overall, our findings offer actionable insights for
building fairer bundle recommender systems and establish a vital foundation for
future research in this emerging domain.

</details>


### [147] [RaMen: Multi-Strategy Multi-Modal Learning for Bundle Construction](https://arxiv.org/abs/2507.14361)
*Huy-Son Nguyen,Quang-Huy Nguyen,Duc-Hoang Pham,Duc-Trong Le,Hoang-Quynh Le,Padipat Sitkrongwong,Atsuhiro Takasu,Masoud Mansoury*

Main category: cs.IR

TL;DR: RaMen是一种新颖的捆绑构建方法，通过显式和隐式策略感知学习，结合多模态数据和协作信号，提升捆绑表示效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖用户反馈或语义信息，未能捕捉真实捆绑结构中的复杂关系，导致表示效果不佳。

Method: RaMen采用显式策略感知学习（ESL）和隐式策略感知学习（ISL），结合多模态数据和超图消息传递，建模捆绑结构。

Result: 实验表明RaMen在多个领域优于现有模型，为复杂物品集问题提供了有价值的见解。

Conclusion: RaMen通过多策略整合，实现了更全面和鲁棒的捆绑表示，解决了现有方法的局限性。

Abstract: Existing studies on bundle construction have relied merely on user feedback
via bipartite graphs or enhanced item representations using semantic
information. These approaches fail to capture elaborate relations hidden in
real-world bundle structures, resulting in suboptimal bundle representations.
To overcome this limitation, we propose RaMen, a novel method that provides a
holistic multi-strategy approach for bundle construction. RaMen utilizes both
intrinsic (characteristics) and extrinsic (collaborative signals) information
to model bundle structures through Explicit Strategy-aware Learning (ESL) and
Implicit Strategy-aware Learning (ISL). ESL employs task-specific attention
mechanisms to encode multi-modal data and direct collaborative relations
between items, thereby explicitly capturing essential bundle features.
Moreover, ISL computes hyperedge dependencies and hypergraph message passing to
uncover shared latent intents among groups of items. Integrating diverse
strategies enables RaMen to learn more comprehensive and robust bundle
representations. Meanwhile, Multi-strategy Alignment & Discrimination module is
employed to facilitate knowledge transfer between learning strategies and
ensure discrimination between items/bundles. Extensive experiments demonstrate
the effectiveness of RaMen over state-of-the-art models on various domains,
justifying valuable insights into complex item set problems.

</details>


### [148] [Understanding Matching Mechanisms in Cross-Encoders](https://arxiv.org/abs/2507.14604)
*Mathias Vast,Basile Van Cooten,Laure Soulier,Benjamin Piwowarski*

Main category: cs.IR

TL;DR: 本文探讨了神经IR架构（尤其是交叉编码器）的内部机制，提出了一种简单方法来解释其注意力过程和匹配检测机制。


<details>
  <summary>Details</summary>
Motivation: 尽管神经IR架构（如交叉编码器）效果显著，但其内部机制尚不明确。现有研究多关注高层次过程，而未能深入描述匹配过程。本文旨在通过简单方法揭示其机制。

Method: 通过分析注意力过程，提取因果性见解，并解释匹配检测的机制。

Result: 研究发现某些注意力头在匹配过程中起关键作用，并揭示了匹配检测的底层机制。

Conclusion: 本文表明，无需复杂方法，简单分析即可为神经IR架构的内部机制提供有价值的见解。

Abstract: Neural IR architectures, particularly cross-encoders, are highly effective
models whose internal mechanisms are mostly unknown. Most works trying to
explain their behavior focused on high-level processes (e.g., what in the input
influences the prediction, does the model adhere to known IR axioms) but fall
short of describing the matching process. Instead of Mechanistic
Interpretability approaches which specifically aim at explaining the hidden
mechanisms of neural models, we demonstrate that more straightforward methods
can already provide valuable insights. In this paper, we first focus on the
attention process and extract causal insights highlighting the crucial roles of
some attention heads in this process. Second, we provide an interpretation of
the mechanism underlying matching detection.

</details>


### [149] [Enhancing POI Recommendation through Global Graph Disentanglement with POI Weighted Module](https://arxiv.org/abs/2507.14612)
*Pei-Xuan Li,Wei-Yun Liang,Fandel Lin,Hsun-Ping Hsieh*

Main category: cs.IR

TL;DR: 论文提出了一种新的POI推荐框架GDPW，通过结合POI类别信息和时间信息，并利用对比学习解耦这些信息，同时考虑多种POI权重因素，显著提升了推荐性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分探索POI类别与时间的关系，且时间信息建模方式有限，同时忽略了多种POI权重信息，导致推荐效果不佳。

Method: 提出GDPW框架，通过全局类别图和全局类别-时间图学习类别和时间表示，利用对比学习解耦信息，并结合POI权重因素进行最终推荐。

Result: 在两个真实数据集上的实验表明，GDPW性能优于现有模型，提升幅度为3%至11%。

Conclusion: GDPW通过综合类别、时间及权重信息，显著提升了POI推荐的准确性和实用性。

Abstract: Next point of interest (POI) recommendation primarily predicts future
activities based on users' past check-in data and current status, providing
significant value to users and service providers. We observed that the popular
check-in times for different POI categories vary. For example, coffee shops are
crowded in the afternoon because people like to have coffee to refresh after
meals, while bars are busy late at night. However, existing methods rarely
explore the relationship between POI categories and time, which may result in
the model being unable to fully learn users' tendencies to visit certain POI
categories at different times. Additionally, existing methods for modeling time
information often convert it into time embeddings or calculate the time
interval and incorporate it into the model, making it difficult to capture the
continuity of time. Finally, during POI prediction, various weighting
information is often ignored, such as the popularity of each POI, the
transition relationships between POIs, and the distances between POIs, leading
to suboptimal performance. To address these issues, this paper proposes a novel
next POI recommendation framework called Graph Disentangler with POI Weighted
Module (GDPW). This framework aims to jointly consider POI category information
and multiple POI weighting factors. Specifically, the proposed GDPW learns
category and time representations through the Global Category Graph and the
Global Category-Time Graph. Then, we disentangle category and time information
through contrastive learning. After prediction, the final POI recommendation
for users is obtained by weighting the prediction results based on the
transition weights and distance relationships between POIs. We conducted
experiments on two real-world datasets, and the results demonstrate that the
proposed GDPW outperforms other existing models, improving performance by 3% to
11%.

</details>


### [150] [Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining](https://arxiv.org/abs/2507.14619)
*Van-Hoang Le,Duc-Vu Nguyen,Kiet Van Nguyen,Ngan Luu-Thuy Nguyen*

Main category: cs.IR

TL;DR: 本文提出了一种两阶段框架（检索与重排）以提升法律文档检索的效率和准确性，采用优化的负例挖掘方法，并在比赛中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在法律领域因缺乏精确性和领域知识而面临的挑战。

Method: 使用微调的Bi-Encoder进行快速候选检索，再通过Cross-Encoder进行精确重排，结合负例挖掘优化。

Result: 在SoICT Hackathon 2024中取得前三名，证明了轻量级方法的竞争力。

Conclusion: 优化的数据处理、定制损失函数和平衡的负采样对构建法律领域检索增强系统至关重要。

Abstract: Large Language Models (LLMs) face significant challenges in specialized
domains like law, where precision and domain-specific knowledge are critical.
This paper presents a streamlined two-stage framework consisting of Retrieval
and Re-ranking to enhance legal document retrieval efficiency and accuracy. Our
approach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,
followed by a Cross-Encoder for precise re-ranking, both optimized through
strategic negative example mining. Key innovations include the introduction of
the Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard
negatives to mitigate training bias, which significantly improved re-ranking
performance. Evaluated on the SoICT Hackathon 2024 for Legal Document
Retrieval, our team, 4Huiter, achieved a top-three position. While
top-performing teams employed ensemble models and iterative self-training on
large bge-m3 architectures, our lightweight, single-pass approach offered a
competitive alternative with far fewer parameters. The framework demonstrates
that optimized data processing, tailored loss functions, and balanced negative
sampling are pivotal for building robust retrieval-augmented systems in legal
contexts.

</details>


### [151] [U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs](https://arxiv.org/abs/2507.14902)
*Xiaojie Li,Chu Li,Shi-Zhe Chen,Xi Chen*

Main category: cs.IR

TL;DR: 论文提出了一种名为U-MARVEL的统一框架，用于多模态检索任务，通过分析关键因素优化嵌入学习，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于MLLM的多模态检索方法虽有效，但其机制未充分探索，可能导致性能不佳和泛化能力有限。

Method: 通过系统分析嵌入生成和训练策略的关键因素，提出U-MARVEL框架，结合渐进过渡、难负例挖掘和重排蒸馏等技术。

Result: U-MARVEL在M-BEIR基准测试中显著优于现有方法，并在零样本任务中表现优异。

Conclusion: U-MARVEL框架在多模态检索任务中展现出强大的泛化能力，为嵌入学习提供了新方向。

Abstract: Universal multimodal retrieval (UMR), which aims to address complex retrieval
tasks where both queries and candidates span diverse modalities, has been
significantly advanced by the emergence of MLLMs. While state-of-the-art
MLLM-based methods in the literature predominantly adopt contrastive learning
principles, they often differ in their specific training recipes. Despite their
success, the mechanisms underlying their retrieval capabilities remain largely
unexplored, potentially resulting in suboptimal performance and limited
generalization ability. To address these issues, we present a comprehensive
study aimed at uncovering the key factors that drive effective embedding
learning for UMR using MLLMs. We begin by implementing a general MLLM-based
embedding learning pipeline, and systematically analyze the primary
contributors to high-performing universal retrieval systems. Based on this, we
explore various aspects of the details in embedding generation and training
strategies, including progressive transition, hard negative mining and
re-ranker distillation. Notably, our findings reveal that often-overlooked
factors can have a substantial impact on model performance. Building on these
discoveries, we introduce a unified framework termed U-MARVEL
(\textbf{U}niversal \textbf{M}ultimod\textbf{A}l \textbf{R}etrie\textbf{V}al
via \textbf{E}mbedding \textbf{L}earning), which outperforms state-of-the-art
competitors on the M-BEIR benchmark by a large margin in supervised settings,
and also exihibits strong zero-shot performance on several tasks such as
composed image retrieval and text-to-video retrieval. These results underscore
the generalization potential of our framework across various embedding-based
retrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL

</details>


### [152] [User Invariant Preference Learning for Multi-Behavior Recommendation](https://arxiv.org/abs/2507.14925)
*Mingshi Yan,Zhiyong Cheng,Fan Liu,Yingda Lyu,Yahong Han*

Main category: cs.IR

TL;DR: 论文提出了一种名为UIPL的多行为推荐方法，通过捕捉用户的不变偏好来减少噪声，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 多行为推荐方法通常假设用户行为间存在共享偏好，但忽略了行为的共性和个性差异，某些辅助行为可能引入噪声。

Method: UIPL利用不变风险最小化范式，通过变分自编码器提取用户不变偏好，并构建多行为数据环境增强鲁棒性。

Result: 在四个真实数据集上的实验表明，UIPL显著优于现有方法。

Conclusion: UIPL通过捕捉不变偏好有效减少了噪声，提升了多行为推荐的准确性。

Abstract: In multi-behavior recommendation scenarios, analyzing users' diverse
behaviors, such as click, purchase, and rating, enables a more comprehensive
understanding of their interests, facilitating personalized and accurate
recommendations. A fundamental assumption of multi-behavior recommendation
methods is the existence of shared user preferences across behaviors,
representing users' intrinsic interests. Based on this assumption, existing
approaches aim to integrate information from various behaviors to enrich user
representations. However, they often overlook the presence of both
commonalities and individualities in users' multi-behavior preferences. These
individualities reflect distinct aspects of preferences captured by different
behaviors, where certain auxiliary behaviors may introduce noise, hindering the
prediction of the target behavior. To address this issue, we propose a user
invariant preference learning for multi-behavior recommendation (UIPL for
short), aiming to capture users' intrinsic interests (referred to as invariant
preferences) from multi-behavior interactions to mitigate the introduction of
noise. Specifically, UIPL leverages the paradigm of invariant risk minimization
to learn invariant preferences. To implement this, we employ a variational
autoencoder (VAE) to extract users' invariant preferences, replacing the
standard reconstruction loss with an invariant risk minimization constraint.
Additionally, we construct distinct environments by combining multi-behavior
data to enhance robustness in learning these preferences. Finally, the learned
invariant preferences are used to provide recommendations for the target
behavior. Extensive experiments on four real-world datasets demonstrate that
UIPL significantly outperforms current state-of-the-art methods.

</details>


### [153] [FullRecall: A Semantic Search-Based Ranking Approach for Maximizing Recall in Patent Retrieval](https://arxiv.org/abs/2507.14946)
*Amna Ali,Liyanage C. De Silva,Pg Emeroylariffion Abas*

Main category: cs.IR

TL;DR: FullRecall是一种新颖的专利检索方法，通过IPC引导的知识生成信息短语，实现高召回率并保持相关性匹配的可靠性。


<details>
  <summary>Details</summary>
Motivation: 专利审查员和发明人面临验证发明原创性和非显而易见性的压力，专利数据的复杂性加剧了检索挑战，需要高效的检索策略。

Method: 利用IPC引导知识生成信息短语，提取关键名词短语构建查询，分阶段检索和排序以确保100%召回率。

Result: 在五个测试案例中均实现100%召回率，优于基线方法HRR2和ReQ-ReC。

Conclusion: FullRecall通过多阶段检索和排序，平衡了精确度和召回率，显著提升了专利检索的效率和可靠性。

Abstract: Patent examiners and inventors face significant pressure to verify the
originality and non-obviousness of inventions, and the intricate nature of
patent data intensifies the challenges of patent retrieval. Therefore, there is
a pressing need to devise cutting-edge retrieval strategies that can reliably
achieve the desired recall. This study introduces FullRecall, a novel patent
retrieval approach that effectively manages the complexity of patent data while
maintaining the reliability of relevance matching and maximising recall. It
leverages IPC-guided knowledge to generate informative phrases, which are
processed to extract key information in the form of noun phrases characterising
the query patent under observation. From these, the top k keyphrases are
selected to construct a query for retrieving a focused subset of the dataset.
This initial retrieval step achieves complete recall, successfully capturing
all relevant documents. To further refine the results, a ranking scheme is
applied to the retrieved subset, reducing its size while maintaining 100%
recall. This multi-phase process demonstrates an effective strategy for
balancing precision and recall in patent retrieval tasks. Comprehensive
experiments were conducted, and the results were compared with baseline
studies, namely HRR2 [1] and ReQ-ReC [2]. The proposed approach yielded
superior results, achieving 100% recall in all five test cases. However,
HRR2[1] recall values across the five test cases were 10%, 25%, 33.3%, 0%, and
14.29%, while ReQ-ReC [2] showed 50% for the first test case, 25% for the
second test case, and 0% for the third, fourth, and fifth test cases. The 100%
recall ensures that no relevant prior art is overlooked, thereby strengthening
the patent pre-filing and examination processes, hence reducing potential legal
risks.

</details>


### [154] [Click A, Buy B: Rethinking Conversion Attribution in E- Commerce Recommendations](https://arxiv.org/abs/2507.15113)
*Xiangyu Zeng,Amit Jaspal,Bin Liu,Goutham Panneeru,Kevin Huang,Nicolas Bievre,Mohit Jaggi,Prathap Maniraju,Ankur Jain*

Main category: cs.IR

TL;DR: 论文提出了一种多任务学习方法，通过区分点击A购买A（CABA）和点击A购买B（CABB）现象，优化电商推荐模型的转化率。


<details>
  <summary>Details</summary>
Motivation: 电商用户行为中，点击A但购买B（CABB）现象普遍，传统模型基于点击-转化对训练会导致偏差。

Method: 采用多任务学习，引入分类感知的协同过滤权重方案，通过产品分类和相似性矩阵区分相关与不相关的CABB转化。

Result: 离线评估显示归一化熵降低13.9%，在线A/B测试显示主要业务指标提升0.25%。

Conclusion: 多任务学习和分类感知权重能有效优化推荐模型的转化预测。

Abstract: User journeys in e-commerce routinely violate the one-to-one assumption that
a clicked item on an advertising platform is the same item later purchased on
the merchant's website/app. For a significant number of converting sessions on
our platform, users click product A but buy product B -- the Click A, Buy B
(CABB) phenomenon. Training recommendation models on raw click-conversion pairs
therefore rewards items that merely correlate with purchases, leading to biased
learning and sub-optimal conversion rates. We reframe conversion prediction as
a multi-task problem with separate heads for Click A Buy A (CABA) and Click A
Buy B (CABB). To isolate informative CABB conversions from unrelated CABB
conversions, we introduce a taxonomy-aware collaborative filtering weighting
scheme where each product is first mapped to a leaf node in a product taxonomy,
and a category-to-category similarity matrix is learned from large-scale
co-engagement logs. This weighting amplifies pairs that reflect genuine
substitutable or complementary relations while down-weighting coincidental
cross-category purchases. Offline evaluation on e-commerce sessions reduces
normalized entropy by 13.9% versus a last-click attribution baseline. An online
A/B test on live traffic shows +0.25% gains in the primary business metric.

</details>


### [155] [SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search](https://arxiv.org/abs/2507.15245)
*Xiaofeng Shi,Yuduo Li,Qian Kou,Longbin Yu,Jinxin Xie,Hua Zhou*

Main category: cs.IR

TL;DR: SPAR是一个多智能体框架，通过RefChain查询分解和查询进化提升学术文献检索的灵活性和效果，并构建了SPARBench基准进行系统评估。


<details>
  <summary>Details</summary>
Motivation: 现有学术文献检索系统依赖固定流程且推理能力有限，需要更灵活高效的解决方案。

Method: 引入SPAR框架，结合RefChain查询分解和查询进化技术。

Result: SPAR显著优于基线方法，在AutoScholar和SPARBench上分别提升56%和23%的F1分数。

Conclusion: SPAR和SPARBench为学术检索研究提供了可扩展、可解释且高性能的基础。

Abstract: Recent advances in large language models (LLMs) have opened new opportunities
for academic literature retrieval. However, existing systems often rely on
rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,
a multi-agent framework that incorporates RefChain-based query decomposition
and query evolution to enable more flexible and effective search. To facilitate
systematic evaluation, we also construct SPARBench, a challenging benchmark
with expert-annotated relevance labels. Experimental results demonstrate that
SPAR substantially outperforms strong baselines, achieving up to +56% F1 on
AutoScholar and +23% F1 on SPARBench over the best-performing baseline.
Together, SPAR and SPARBench provide a scalable, interpretable, and
high-performing foundation for advancing research in scholarly retrieval. Code
and data will be available at: https://github.com/xiaofengShi/SPAR

</details>


### [156] [GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou](https://arxiv.org/abs/2507.15267)
*Ninglu Shao,Jinshan Wang,Chenxu Wang,Qingbiao Li,Xiaoxue Zang,Han Li*

Main category: cs.IR

TL;DR: 论文提出了一种基于LLM的框架GREAT，用于视频相关搜索中的查询推荐任务（I2Q），并发布了一个大规模数据集KuaiRS。


<details>
  <summary>Details</summary>
Motivation: 解决视频相关搜索中查询推荐任务缺乏学术研究和公开数据集的问题，并改进现有方法在语义内容与查询交互上的不足。

Method: 提出GREAT框架，利用查询前缀树（trie）指导LLM生成高质量查询，并通过后处理模块优化查询与视频的相关性。

Result: 离线与在线实验验证了GREAT框架的有效性。

Conclusion: GREAT框架显著提升了视频相关搜索中的查询推荐质量。

Abstract: Currently, short video platforms have become the primary place for
individuals to share experiences and obtain information. To better meet users'
needs for acquiring information while browsing short videos, some apps have
introduced a search entry at the bottom of videos, accompanied with recommended
relevant queries. This scenario is known as query recommendation in
video-related search, where core task is item-to-query (I2Q) recommendation. As
this scenario has only emerged in recent years, there is a notable scarcity of
academic research and publicly available datasets in this domain. To address
this gap, we systematically examine the challenges associated with this
scenario for the first time. Subsequently, we release a large-scale dataset
derived from real-world data pertaining to the query recommendation in
video-\textit{\textbf{r}}elated \textit{\textbf{s}}earch on the
\textit{\textbf{Kuai}}shou app (\textbf{KuaiRS}). Presently, existing methods
rely on embeddings to calculate similarity for matching short videos with
queries, lacking deep interaction between the semantic content and the query.
In this paper, we introduce a novel LLM-based framework named \textbf{GREAT},
which \textit{\textbf{g}}uides que\textit{\textbf{r}}y
g\textit{\textbf{e}}ner\textit{\textbf{a}}tion with a \textit{\textbf{t}}rie to
address I2Q recommendation in related search. Specifically, we initially gather
high-quality queries with high exposure and click-through rate to construct a
query-based trie. During training, we enhance the LLM's capability to generate
high-quality queries using the query-based trie. In the inference phase, the
query-based trie serves as a guide for the token generation. Finally, we
further refine the relevance and literal quality between items and queries via
a post-processing module. Extensive offline and online experiments demonstrate
the effectiveness of our proposed method.

</details>


### [157] [Hierarchical Graph Information Bottleneck for Multi-Behavior Recommendation](https://arxiv.org/abs/2507.15395)
*Hengyu Zhang,Chunxu Shen,Xiangguo Sun,Jie Tan,Yanchao Tan,Yu Rong,Hong Cheng,Lingling Yi*

Main category: cs.IR

TL;DR: 提出了一种基于信息瓶颈原则的层次图框架（HGIB），用于多行为推荐，解决了行为间分布差异和辅助行为噪声问题。


<details>
  <summary>Details</summary>
Motivation: 多行为推荐中，现有方法面临行为分布差异和噪声导致的负迁移问题，需更有效的解决方案。

Method: 采用层次图信息瓶颈（HGIB）框架和图细化编码器（GRE），动态修剪冗余边以优化表示学习。

Result: 在三个公开数据集和工业场景中验证了HGIB的优越性，在线A/B测试也显示显著提升。

Conclusion: HGIB框架有效解决了多行为推荐中的关键挑战，并在实际应用中表现出色。

Abstract: In real-world recommendation scenarios, users typically engage with platforms
through multiple types of behavioral interactions. Multi-behavior
recommendation algorithms aim to leverage various auxiliary user behaviors to
enhance prediction for target behaviors of primary interest (e.g., buy),
thereby overcoming performance limitations caused by data sparsity in target
behavior records. Current state-of-the-art approaches typically employ
hierarchical design following either cascading (e.g.,
view$\rightarrow$cart$\rightarrow$buy) or parallel
(unified$\rightarrow$behavior$\rightarrow$specific components) paradigms, to
capture behavioral relationships. However, these methods still face two
critical challenges: (1) severe distribution disparities across behaviors, and
(2) negative transfer effects caused by noise in auxiliary behaviors. In this
paper, we propose a novel model-agnostic Hierarchical Graph Information
Bottleneck (HGIB) framework for multi-behavior recommendation to effectively
address these challenges. Following information bottleneck principles, our
framework optimizes the learning of compact yet sufficient representations that
preserve essential information for target behavior prediction while eliminating
task-irrelevant redundancies. To further mitigate interaction noise, we
introduce a Graph Refinement Encoder (GRE) that dynamically prunes redundant
edges through learnable edge dropout mechanisms. We conduct comprehensive
experiments on three real-world public datasets, which demonstrate the superior
effectiveness of our framework. Beyond these widely used datasets in the
academic community, we further expand our evaluation on several real industrial
scenarios and conduct an online A/B testing, showing again a significant
improvement in multi-behavior recommendations. The source code of our proposed
HGIB is available at https://github.com/zhy99426/HGIB.

</details>


### [158] [RankMixer: Scaling Up Ranking Models in Industrial Recommenders](https://arxiv.org/abs/2507.15551)
*Jie Zhu,Zhifang Fan,Xiaoxie Zhu,Yuchen Jiang,Hangyu Wang,Xintian Han,Haoran Ding,Xinmin Wang,Wenlin Zhao,Zhen Gong,Huizhi Yang,Zheng Chai,Zhe Chen,Yuchao Zheng,Qiwei Chen,Feng Zhang,Xun Zhou,Peng Xu,Xiao Yang,Di Wu,Zuotao Liu*

Main category: cs.IR

TL;DR: RankMixer是一种硬件感知的推荐系统模型设计，通过高效的特征交互架构提升模型性能和扩展性，显著提高模型利用率（MFU）并降低延迟。


<details>
  <summary>Details</summary>
Motivation: 解决工业推荐系统中训练和服务成本高、传统特征交互模块无法充分利用现代GPU的问题。

Method: 引入RankMixer，结合多头部令牌混合模块和Per-token FFNs，支持稀疏MoE扩展，并采用动态路由策略优化专家训练。

Result: 模型MFU从4.5%提升至45%，参数规模扩展100倍，推理延迟不变，在线A/B测试验证了其通用性。

Conclusion: RankMixer在推荐、广告和搜索场景中表现优异，显著提升用户活跃度和使用时长。

Abstract: Recent progress on large language models (LLMs) has spurred interest in
scaling up recommendation systems, yet two practical obstacles remain. First,
training and serving cost on industrial Recommenders must respect strict
latency bounds and high QPS demands. Second, most human-designed
feature-crossing modules in ranking models were inherited from the CPU era and
fail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and
poor scalability. We introduce RankMixer, a hardware-aware model design
tailored towards a unified and scalable feature-interaction architecture.
RankMixer retains the transformer's high parallelism while replacing quadratic
self-attention with multi-head token mixing module for higher efficiency.
Besides, RankMixer maintains both the modeling for distinct feature subspaces
and cross-feature-space interactions with Per-token FFNs. We further extend it
to one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic
routing strategy is adapted to address the inadequacy and imbalance of experts
training. Experiments show RankMixer's superior scaling abilities on a
trillion-scale production dataset. By replacing previously diverse handcrafted
low-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and
scale our ranking model parameters by 100x while maintaining roughly the same
inference latency. We verify RankMixer's universality with online A/B tests
across three core application scenarios (Recommendation, Advertisement and
Search). Finally, we launch 1B Dense-Parameters RankMixer for full traffic
serving without increasing the serving cost, which improves user active days by
0.2% and total in-app usage duration by 0.5%.

</details>


### [159] [Just Ask for Music (JAM): Multimodal and Personalized Natural Language Music Recommendation](https://arxiv.org/abs/2507.15826)
*Alessandro B. Melchiorre,Elena V. Epure,Shahed Masoudian,Gustavo Escobedo,Anna Hausberger,Manuel Moussallam,Markus Schedl*

Main category: cs.IR

TL;DR: JAM是一个轻量级自然语言音乐推荐框架，通过向量翻译和多模态特征聚合解决LLMs的高成本和延迟问题。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在音乐推荐系统中的高成本和延迟问题，同时克服检索方法对单模态表示和用户长期偏好的忽视。

Method: JAM将用户-查询-项目交互建模为共享潜在空间中的向量翻译，利用跨注意力和稀疏混合专家聚合多模态特征。

Result: JAM在准确性、实用性和易集成性方面表现优异，适用于实际应用场景。

Conclusion: JAM为自然语言音乐推荐提供了一种高效、直观且易于部署的解决方案。

Abstract: Natural language interfaces offer a compelling approach for music
recommendation, enabling users to express complex preferences conversationally.
While Large Language Models (LLMs) show promise in this direction, their
scalability in recommender systems is limited by high costs and latency.
Retrieval-based approaches using smaller language models mitigate these issues
but often rely on single-modal item representations, overlook long-term user
preferences, and require full model retraining, posing challenges for
real-world deployment. In this paper, we present JAM (Just Ask for Music), a
lightweight and intuitive framework for natural language music recommendation.
JAM models user-query-item interactions as vector translations in a shared
latent space, inspired by knowledge graph embedding methods like TransE. To
capture the complexity of music and user intent, JAM aggregates multimodal item
features via cross-attention and sparse mixture-of-experts. We also introduce
JAMSessions, a new dataset of over 100k user-query-item triples with anonymized
user/item embeddings, uniquely combining conversational queries and user
long-term preferences. Our results show that JAM provides accurate
recommendations, produces intuitive representations suitable for practical use
cases, and can be easily integrated with existing music recommendation stacks.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [160] [Catalyst: a Novel Regularizer for Structured Pruning with Auxiliary Extension of Parameter Space](https://arxiv.org/abs/2507.14170)
*Jaeheun Jung,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种新的结构化剪枝方法Catalyst正则化，通过引入辅助变量确保剪枝决策的无偏性和鲁棒性，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法（如L1或Group Lasso）存在幅度偏差和决策边界不稳定的问题，影响剪枝效果。

Method: 基于代数条件设计Catalyst正则化，利用辅助变量扩展参数空间，实现公平剪枝和宽裕决策边界。

Result: 在多个数据集和模型上验证了Catalyst剪枝的优越性，表现出无偏性和鲁棒性。

Conclusion: Catalyst剪枝方法在理论和实践中均优于现有技术，解决了传统剪枝的局限性。

Abstract: Structured pruning aims to reduce the size and computational cost of deep
neural networks by removing entire filters or channels. The traditional
regularizers such as L1 or Group Lasso and its variants lead to
magnitude-biased pruning decisions, such that the filters with small magnitudes
are likely to be pruned. Also, they often entail pruning results with almost
zero margin around pruning decision boundary, such that tiny perturbation in a
filter magnitude can flip the pruning decision. In this paper, we identify the
precise algebraic condition under which pruning operations preserve model
performance, and use the condition to construct a novel regularizer defined in
an extended parameter space via auxiliary catalyst variables. The proposed
Catalyst regularization ensures fair pruning chance for each filters with
theoretically provable zero bias to their magnitude and robust pruning behavior
achieved by wide-margin bifurcation of magnitudes between the preserved and the
pruned filters. The theoretical properties naturally lead to real-world
effectiveness, as shown by empirical validations of Catalyst Pruning algorithm.
Pruning results on various datasets and models are superior to state-of-the-art
filter pruning methods, and at the same time confirm the predicted robust and
fair pruning characteristics of Catalyst pruning.

</details>


### [161] [IPPRO: Importance-based Pruning with PRojective Offset for Magnitude-indifferent Structural Pruning](https://arxiv.org/abs/2507.14171)
*Jaeheun Jung,Jaehyuk Lee,Yeajin Lee,Donghun Lee*

Main category: cs.LG

TL;DR: 提出了一种基于投影空间的新型剪枝策略IPPRO，通过PROscore衡量滤波器剪枝可能性，挑战传统基于幅度的剪枝方法，实现近乎无损的剪枝效果。


<details>
  <summary>Details</summary>
Motivation: 传统基于幅度的剪枝方法限制了剪枝决策的能力，即使冗余滤波器也可能因幅度较大而未被剪枝。本文旨在提供公平的剪枝机会。

Method: 将滤波器置于投影空间，观察梯度下降运动是否趋向原点，构建PROscore作为剪枝重要性指标。

Result: 实验表明，该方法在剪枝后性能下降较小，经过微调后表现优异，实现了近乎无损的剪枝。

Conclusion: 本文挑战了剪枝中“大小决定一切”的误区，从理论和实证上扩展了基于重要性的剪枝方法。

Abstract: With the growth of demand on neural network compression methods, the
structured pruning methods including importance-based approach are actively
studied. The magnitude importance and many correlated modern importance
criteria often limit the capacity of pruning decision, since the filters with
larger magnitudes are not likely to be pruned if the smaller one didn't, even
if it is redundant. In this paper, we propose a novel pruning strategy to
challenge this dominating effect of magnitude and provide fair chance to each
filter to be pruned, by placing it on projective space. After that, we observe
the gradient descent movement whether the filters move toward the origin or
not, to measure how the filter is likely to be pruned. This measurement is used
to construct PROscore, a novel importance score for IPPRO, a novel
importance-based structured pruning with magnitude-indifference. Our evaluation
results shows that the proposed importance criteria using the projective space
achieves near-lossless pruning by reducing the performance drop in pruning,
with promising performance after the finetuning. Our work debunks the
``size-matters'' myth in pruning and expands the frontier of importance-based
pruning both theoretically and empirically.

</details>


### [162] [Self-Improving Language Models for Evolutionary Program Synthesis: A Case Study on ARC-AGI](https://arxiv.org/abs/2507.14172)
*Julien Pourcel,Cédric Colas,Pierre-Yves Oudeyer*

Main category: cs.LG

TL;DR: SOAR是一种结合语言模型和自改进进化循环的程序合成方法，通过迭代搜索和学习提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有语言模型在单次尝试中难以完成复杂程序合成任务的问题，同时克服进化方法因固定生成模型能力受限的局限性。

Method: SOAR交替进行进化搜索（利用LLM生成和优化候选解）和后见学习（将搜索尝试转化为问题-解对以微调LLM）。

Result: 在ARC-AGI基准测试中，SOAR显著提升了性能，并在公开测试集上解决了52%的问题。

Conclusion: SOAR通过结合进化搜索和LLM微调，实现了程序合成任务的持续改进和高效解决。

Abstract: Many program synthesis tasks prove too challenging for even state-of-the-art
language models to solve in single attempts. Search-based evolutionary methods
offer a promising alternative by exploring solution spaces iteratively, but
their effectiveness remain limited by the fixed capabilities of the underlying
generative model.
  We propose SOAR, a method that learns program synthesis by integrating
language models into a self-improving evolutionary loop.
  SOAR alternates between (1) an evolutionary search that uses an LLM to sample
and refine candidate solutions, and (2) a hindsight learning phase that
converts search attempts into valid problem-solution pairs used to fine-tune
the LLM's sampling and refinement capabilities\, -- \,enabling increasingly
effective search in subsequent iterations.
  On the challenging ARC-AGI benchmark, SOAR achieves significant performance
gains across model scales and iterations, leveraging positive transfer between
the sampling and refinement finetuning tasks. These improvements carry over to
test-time adaptation, enabling SOAR to solve 52\% of the public test set. Our
code is open-sourced at: https://github.com/flowersteam/SOAR

</details>


### [163] [Latent Space Data Fusion Outperforms Early Fusion in Multimodal Mental Health Digital Phenotyping Data](https://arxiv.org/abs/2507.14175)
*Youcef Barkat,Dylan Hamitouche,Deven Parekh,Ivy Guo,David Benrimoh*

Main category: cs.LG

TL;DR: 论文提出了一种基于潜在空间融合的多模态数据整合方法，用于预测抑郁症状，相比传统方法表现更优。


<details>
  <summary>Details</summary>
Motivation: 传统预测模型依赖单模态数据或早期融合策略，难以捕捉精神科数据的复杂多模态特性，需要更先进的整合技术。

Method: 使用BRIGHTEN临床试验数据，比较了随机森林（RF）的早期融合和组合模型（CM）的潜在空间融合，评估了行为、人口统计和临床特征。

Result: CM在所有设置中均优于RF和线性回归（LR），MSE更低（0.4985 vs. 0.5305），R2更高（0.4695 vs. 0.4356），且泛化能力更强。

Conclusion: 潜在空间融合是多模态心理健康数据预测的可靠方法，未来需探索模型可解释性和个体化预测。

Abstract: Background: Mental illnesses such as depression and anxiety require improved
methods for early detection and personalized intervention. Traditional
predictive models often rely on unimodal data or early fusion strategies that
fail to capture the complex, multimodal nature of psychiatric data. Advanced
integration techniques, such as intermediate (latent space) fusion, may offer
better accuracy and clinical utility. Methods: Using data from the BRIGHTEN
clinical trial, we evaluated intermediate (latent space) fusion for predicting
daily depressive symptoms (PHQ-2 scores). We compared early fusion implemented
with a Random Forest (RF) model and intermediate fusion implemented via a
Combined Model (CM) using autoencoders and a neural network. The dataset
included behavioral (smartphone-based), demographic, and clinical features.
Experiments were conducted across multiple temporal splits and data stream
combinations. Performance was evaluated using mean squared error (MSE) and
coefficient of determination (R2). Results: The CM outperformed both RF and
Linear Regression (LR) baselines across all setups, achieving lower MSE (0.4985
vs. 0.5305 with RF) and higher R2 (0.4695 vs. 0.4356). The RF model showed
signs of overfitting, with a large gap between training and test performance,
while the CM maintained consistent generalization. Performance was best when
integrating all data modalities in the CM (in contradistinction to RF),
underscoring the value of latent space fusion for capturing non-linear
interactions in complex psychiatric datasets. Conclusion: Latent space fusion
offers a robust alternative to traditional fusion methods for prediction with
multimodal mental health data. Future work should explore model
interpretability and individual-level prediction for clinical deployment.

</details>


### [164] [Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection](https://arxiv.org/abs/2507.14176)
*Andrés Morales-Forero,Lili J. Rueda,Ronald Herrera,Samuel Bassetto,Eric Coatanea*

Main category: cs.LG

TL;DR: 论文提出了预测代表性（PR）框架，用于公平性审计，重点关注结果层面的公平性，而非数据集组成。通过皮肤癌分类器的案例研究，揭示了性能差异，并提出了外部可迁移性标准。


<details>
  <summary>Details</summary>
Motivation: AI系统在医疗决策中的应用日益增多，但算法偏见和不公平结果问题突出，尤其是对历史上边缘化群体。

Method: 使用HAM10000数据集和哥伦比亚的BOSQUE测试集，评估皮肤癌分类器性能，分析不同肤色群体的表现差异。

Result: 分类器在深色皮肤群体中表现较差，尽管数据集采样比例均衡。

Conclusion: PR框架为公平性审计提供了工具，强调透明度和包容性验证，推动数据驱动医疗中的公平性重新评估。

Abstract: Artificial intelligence (AI) systems increasingly inform medical
decision-making, yet concerns about algorithmic bias and inequitable outcomes
persist, particularly for historically marginalized populations. This paper
introduces the concept of Predictive Representativity (PR), a framework of
fairness auditing that shifts the focus from the composition of the data set to
outcomes-level equity. Through a case study in dermatology, we evaluated
AI-based skin cancer classifiers trained on the widely used HAM10000 dataset
and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our
analysis reveals substantial performance disparities by skin phototype, with
classifiers consistently underperforming for individuals with darker skin,
despite proportional sampling in the source data. We argue that
representativity must be understood not as a static feature of datasets but as
a dynamic, context-sensitive property of model predictions. PR operationalizes
this shift by quantifying how reliably models generalize fairness across
subpopulations and deployment contexts. We further propose an External
Transportability Criterion that formalizes the thresholds for fairness
generalization. Our findings highlight the ethical imperative for post-hoc
fairness auditing, transparency in dataset documentation, and inclusive model
validation pipelines. This work offers a scalable tool for diagnosing
structural inequities in AI systems, contributing to discussions on equity,
interpretability, and data justice and fostering a critical re-evaluation of
fairness in data-driven healthcare.

</details>


### [165] [Understanding Two-Layer Neural Networks with Smooth Activation Functions](https://arxiv.org/abs/2507.14177)
*Changcun Huang*

Main category: cs.LG

TL;DR: 本文研究了使用反向传播算法训练的两层神经网络，隐藏层采用平滑激活函数（如Sigmoid），揭示了其解空间的机制，包括泰勒展开、严格节点偏序、平滑样条实现和平滑连续性限制。


<details>
  <summary>Details</summary>
Motivation: 探索两层神经网络在平滑激活函数下的解空间特性，揭示其“黑盒”机制，并丰富近似理论。

Method: 采用泰勒级数展开、严格节点偏序、平滑样条实现和平滑连续性限制四种机制进行分析。

Result: 证明了任意输入维度的通用近似能力，并通过实验验证了解空间的特性。

Conclusion: 研究不仅揭示了解空间的“黑盒”特性，还丰富了近似理论。

Abstract: This paper aims to understand the training solution, which is obtained by the
back-propagation algorithm, of two-layer neural networks whose hidden layer is
composed of the units with smooth activation functions, including the usual
sigmoid type most commonly used before the advent of ReLUs. The mechanism
contains four main principles: construction of Taylor series expansions, strict
partial order of knots, smooth-spline implementation and smooth-continuity
restriction. The universal approximation for arbitrary input dimensionality is
proved and experimental verification is given, through which the mystery of
``black box'' of the solution space is largely revealed. The new proofs
employed also enrich approximation theory.

</details>


### [166] [Feature Bank Enhancement for Distance-based Out-of-Distribution Detection](https://arxiv.org/abs/2507.14178)
*Yuhang Liu,Yuefei Wu,Bin Shi,Bo Dong*

Main category: cs.LG

TL;DR: 提出了一种名为FBE的方法，通过约束极端特征提升OOD检测性能，在ImageNet-1k和CIFAR-10上表现优异。


<details>
  <summary>Details</summary>
Motivation: 距离评分方法因极端特征导致ID样本评分过低，限制了OOD检测能力。

Method: 使用数据集的统计特性识别并约束极端特征，扩大ID与OOD样本的距离。

Result: 在ImageNet-1k和CIFAR-10上达到最优性能。

Conclusion: FBE方法简单有效，显著提升了OOD检测能力。

Abstract: Out-of-distribution (OOD) detection is critical to ensuring the reliability
of deep learning applications and has attracted significant attention in recent
years. A rich body of literature has emerged to develop efficient score
functions that assign high scores to in-distribution (ID) samples and low
scores to OOD samples, thereby helping distinguish OOD samples. Among these
methods, distance-based score functions are widely used because of their
efficiency and ease of use. However, deep learning often leads to a biased
distribution of data features, and extreme features are inevitable. These
extreme features make the distance-based methods tend to assign too low scores
to ID samples. This limits the OOD detection capabilities of such methods. To
address this issue, we propose a simple yet effective method, Feature Bank
Enhancement (FBE), that uses statistical characteristics from dataset to
identify and constrain extreme features to the separation boundaries, therapy
making the distance between samples inside and outside the distribution
farther. We conducted experiments on large-scale ImageNet-1k and CIFAR-10
respectively, and the results show that our method achieves state-of-the-art
performance on both benchmark. Additionally, theoretical analysis and
supplementary experiments are conducted to provide more insights into our
method.

</details>


### [167] [A Sparsity Predicting Approach for Large Language Models via Activation Pattern Clustering](https://arxiv.org/abs/2507.14179)
*Nobel Dhar,Bobin Deng,Md Romyull Islam,Xinyue Zhang,Kazi Fahim Ahmad Nasif,Kun Suo*

Main category: cs.LG

TL;DR: 论文提出了一种基于聚类的激活模式压缩框架，通过将相似的激活模式分组为少量代表性簇，有效预测和利用大语言模型（LLMs）中的激活稀疏性，降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）存在显著的激活稀疏性，但直接预测神经元级别的激活模式计算成本过高，因此需要一种高效的方法来利用这种稀疏性。

Method: 提出了一种聚类框架，将相似的激活模式分组为少量代表性簇，通过预测簇分配而非单个神经元状态来降低计算复杂度。

Result: 方法实现了79.34%的聚类精度，困惑度（PPL）得分最低为12.49，表明在保持模型质量的同时显著降低了计算开销。

Conclusion: 该聚类框架为激活模式预测提供了基础，有望提升大规模语言模型的高效推理能力。

Abstract: Large Language Models (LLMs) exhibit significant activation sparsity, where
only a subset of neurons are active for a given input. Although this sparsity
presents opportunities to reduce computational cost, efficiently utilizing it
requires predicting activation patterns in a scalable manner. However, direct
prediction at the neuron level is computationally expensive due to the vast
number of neurons in modern LLMs. To enable efficient prediction and
utilization of activation sparsity, we propose a clustering-based activation
pattern compression framework. Instead of treating each neuron independently,
we group similar activation patterns into a small set of representative
clusters. Our method achieves up to 79.34% clustering precision, outperforming
standard binary clustering approaches while maintaining minimal degradation in
perplexity (PPL) scores. With a sufficiently large number of clusters, our
approach attains a PPL score as low as 12.49, demonstrating its effectiveness
in preserving model quality while reducing computational overhead. By
predicting cluster assignments rather than individual neuron states, future
models can efficiently infer activation patterns from pre-computed centroids.
We detail the clustering algorithm, analyze its effectiveness in capturing
meaningful activation structures, and demonstrate its potential to improve
sparse computation efficiency. This clustering-based formulation serves as a
foundation for future work on activation pattern prediction, paving the way for
efficient inference in large-scale language models.

</details>


### [168] [Digital Twin-Assisted Explainable AI for Robust Beam Prediction in mmWave MIMO Systems](https://arxiv.org/abs/2507.14180)
*Nasir Khan,Asmaa Abdallah,Abdulkadir Celik,Ahmed M. Eltawil,Sinem Coleri*

Main category: cs.LG

TL;DR: 提出了一种基于深度学习的稳健且可解释的波束对齐引擎（BAE），用于毫米波MIMO系统，通过数字孪生和迁移学习减少数据需求，并通过SHAP和DkNN增强透明度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 在6G愿景下，毫米波系统的可解释性和鲁棒性对建立信任和可靠性能至关重要。传统深度学习方法面临数据收集成本高、硬件限制、缺乏可解释性及对抗攻击的挑战。

Method: 利用数字孪生生成合成数据，通过迁移学习微调模型；使用SHAP进行特征重要性排序，DkNN检测异常输入。

Result: 实验显示，该框架减少70%真实数据需求、62%波束训练开销，异常检测鲁棒性提升8.5倍，接近最优频谱效率。

Conclusion: 该框架在减少开销的同时，实现了透明且鲁棒的决策，为毫米波系统提供了高效解决方案。

Abstract: In line with the AI-native 6G vision, explainability and robustness are
crucial for building trust and ensuring reliable performance in millimeter-wave
(mmWave) systems. Efficient beam alignment is essential for initial access, but
deep learning (DL) solutions face challenges, including high data collection
overhead, hardware constraints, lack of explainability, and susceptibility to
adversarial attacks. This paper proposes a robust and explainable DL-based beam
alignment engine (BAE) for mmWave multiple-input multiple output (MIMO)
systems. The BAE uses received signal strength indicator (RSSI) measurements
from wide beams to predict the best narrow beam, reducing the overhead of
exhaustive beam sweeping. To overcome the challenge of real-world data
collection, this work leverages a site-specific digital twin (DT) to generate
synthetic channel data closely resembling real-world environments. A model
refinement via transfer learning is proposed to fine-tune the pre-trained model
residing in the DT with minimal real-world data, effectively bridging
mismatches between the digital replica and real-world environments. To reduce
beam training overhead and enhance transparency, the framework uses deep
Shapley additive explanations (SHAP) to rank input features by importance,
prioritizing key spatial directions and minimizing beam sweeping. It also
incorporates the Deep k-nearest neighbors (DkNN) algorithm, providing a
credibility metric for detecting out-of-distribution inputs and ensuring
robust, transparent decision-making. Experimental results show that the
proposed framework reduces real-world data needs by 70%, beam training overhead
by 62%, and improves outlier detection robustness by up to 8.5x, achieving
near-optimal spectral efficiency and transparent decision making compared to
traditional softmax based DL models.

</details>


### [169] [Semi-Supervised Federated Learning via Dual Contrastive Learning and Soft Labeling for Intelligent Fault Diagnosis](https://arxiv.org/abs/2507.14181)
*Yajiao Dai,Jun Li,Zhen Mei,Yiyang Ni,Shi Jin,Zengxiang Li,Sheng Guo,Wei Xiang*

Main category: cs.LG

TL;DR: 论文提出了一种半监督联邦学习框架SSFL-DCSL，通过双重对比损失和软标签解决数据分布差异和标签稀缺问题，提升故障诊断性能。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习方法需要大量标注数据且数据分布差异影响模型性能，SSFL-DCSL旨在解决这些问题并保护用户隐私。

Method: 设计基于拉普拉斯分布的样本加权函数、引入双重对比损失（局部和全局）、通过加权平均和动量更新聚合原型。

Result: 在仅10%标注数据的任务中，SSFL-DCSL比现有方法准确率提升1.15%至7.85%。

Conclusion: SSFL-DCSL有效解决了数据分布差异和标签稀缺问题，显著提升了故障诊断性能。

Abstract: Intelligent fault diagnosis (IFD) plays a crucial role in ensuring the safe
operation of industrial machinery and improving production efficiency. However,
traditional supervised deep learning methods require a large amount of training
data and labels, which are often located in different clients. Additionally,
the cost of data labeling is high, making labels difficult to acquire.
Meanwhile, differences in data distribution among clients may also hinder the
model's performance. To tackle these challenges, this paper proposes a
semi-supervised federated learning framework, SSFL-DCSL, which integrates dual
contrastive loss and soft labeling to address data and label scarcity for
distributed clients with few labeled samples while safeguarding user privacy.
It enables representation learning using unlabeled data on the client side and
facilitates joint learning among clients through prototypes, thereby achieving
mutual knowledge sharing and preventing local model divergence. Specifically,
first, a sample weighting function based on the Laplace distribution is
designed to alleviate bias caused by low confidence in pseudo labels during the
semi-supervised training process. Second, a dual contrastive loss is introduced
to mitigate model divergence caused by different data distributions, comprising
local contrastive loss and global contrastive loss. Third, local prototypes are
aggregated on the server with weighted averaging and updated with momentum to
share knowledge among clients. To evaluate the proposed SSFL-DCSL framework,
experiments are conducted on two publicly available datasets and a dataset
collected on motors from the factory. In the most challenging task, where only
10\% of the data are labeled, the proposed SSFL-DCSL can improve accuracy by
1.15% to 7.85% over state-of-the-art methods.

</details>


### [170] [From Bias to Behavior: Learning Bull-Bear Market Dynamics with Contrastive Modeling](https://arxiv.org/abs/2507.14182)
*Xiaotong Luo,Shengda Zhuo,Min Chen,Lichun Li,Ruizhao Lu,Wenqi Fan,Shuqiang Huang,Yin Tang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为B4的模型，用于分析投资者驱动的市场动态，通过结合价格序列和外部信号，揭示偏见与行为的关系，并预测市场趋势。


<details>
  <summary>Details</summary>
Motivation: 金融市场行为复杂且受多种因素影响，传统方法难以捕捉偏见与行为的动态关系，因此需要新的建模方法。

Method: 提出B4模型，将价格序列和外部信号嵌入共享潜在空间，通过惯性配对和双竞争机制捕捉行为差异和市场异质性。

Result: B4模型在真实金融数据上表现优异，不仅能预测市场趋势，还能解释偏见、行为和市场动态的相互作用。

Conclusion: B4模型为理解市场动态提供了新视角，其框架可扩展至其他复杂系统的行为建模。

Abstract: Financial markets exhibit highly dynamic and complex behaviors shaped by both
historical price trajectories and exogenous narratives, such as news, policy
interpretations, and social media sentiment. The heterogeneity in these data
and the diverse insight of investors introduce biases that complicate the
modeling of market dynamics. Unlike prior work, this paper explores the
potential of bull and bear regimes in investor-driven market dynamics. Through
empirical analysis on real-world financial datasets, we uncover a dynamic
relationship between bias variation and behavioral adaptation, which enhances
trend prediction under evolving market conditions. To model this mechanism, we
propose the Bias to Behavior from Bull-Bear Dynamics model (B4), a unified
framework that jointly embeds temporal price sequences and external contextual
signals into a shared latent space where opposing bull and bear forces
naturally emerge, forming the foundation for bias representation. Within this
space, an inertial pairing module pairs temporally adjacent samples to preserve
momentum, while the dual competition mechanism contrasts bullish and bearish
embeddings to capture behavioral divergence. Together, these components allow
B4 to model bias-driven asymmetry, behavioral inertia, and market
heterogeneity. Experimental results on real-world financial datasets
demonstrate that our model not only achieves superior performance in predicting
market trends but also provides interpretable insights into the interplay of
biases, investor behaviors, and market dynamics.

</details>


### [171] [LaCache: Ladder-Shaped KV Caching for Efficient Long-Context Modeling of Large Language Models](https://arxiv.org/abs/2507.14204)
*Dachuan Shi,Yonggan Fu,Xiangchi Yuan,Zhongzhi Yu,Haoran You,Sixu Li,Xin Dong,Jan Kautz,Pavlo Molchanov,Yingyan,Lin*

Main category: cs.LG

TL;DR: LaCache是一种无需训练的KV缓存优化方法，通过梯形状KV缓存模式和迭代压缩机制，提升LLMs的长距离建模能力和连续生成效率。


<details>
  <summary>Details</summary>
Motivation: 随着序列长度增加，LLMs中的KV对数量激增，导致效率瓶颈，需要一种方法同时解决长距离建模和内存不足问题。

Method: LaCache采用梯形状KV缓存模式（跨层存储KV对）和迭代压缩机制（动态压缩旧缓存），以固定缓存预算提升性能。

Result: 实验证明LaCache能有效增强LLMs的长距离能力，并在各种任务和模型中表现一致。

Conclusion: LaCache为LLMs的长距离建模和连续生成提供了一种高效、准确的解决方案。

Abstract: Recent advancements in Large Language Models (LLMs) have spurred interest in
numerous applications requiring robust long-range capabilities, essential for
processing extensive input contexts and continuously generating extended
outputs. As sequence lengths increase, the number of Key-Value (KV) pairs in
LLMs escalates, creating a significant efficiency bottleneck. In this paper, we
propose a new KV cache optimization paradigm called LaCache, a training-free
method for efficient and accurate generative inference of LLMs. LaCache enables
LLMs to simultaneously address both of the critical challenges in long-range
modeling: robust long-range capabilities and continuous generation without
running out-of-memory (OOM). Specifically, LaCache integrates two key
innovations: (1) a ladder-shaped KV cache pattern that stores KV pairs not only
sequentially (left-to-right within each layer) but also across layers (from
shallow to deep), providing an extended span for capturing long-range
dependencies under a fixed storage budget, thereby boosting long-range
capabilities; and (2) an iterative compaction mechanism that progressively
compresses older caches, freeing up space for new tokens within a fixed cache
size. This token distance-based dynamic compression enables more effective
continuous generation under constrained cache budgets. Experiments across
various tasks, benchmarks, and LLM models consistently validate LaCache's
effectiveness in enhancing LLMs' long-range capabilities. Our code is available
at https://github.com/GATECH-EIC/LaCache.

</details>


### [172] [Developing an AI-Guided Assistant Device for the Deaf and Hearing Impaired](https://arxiv.org/abs/2507.14215)
*Jiayu,Liu*

Main category: cs.LG

TL;DR: 开发了一种基于深度学习的无障碍设备，用于聋人或听力障碍者，实时定位和识别声源。


<details>
  <summary>Details</summary>
Motivation: 填补当前研究的空白，利用机器学习技术服务于弱势群体。

Method: 系统包括三个组件：JerryNet（CNN架构确定声源方向）、音频分类（基于CLAP模型）、多模态集成模型（结合音频、视觉和文本数据定位声源）。

Result: JerryNet方向精度91.1%，CLAP模型在自定义和AudioSet数据集上分别达到98.5%和95%准确率，音频视觉定位模型cIoU为0.892。

Conclusion: 研究为新一代无障碍设备奠定了基础，具有广阔的未来潜力。

Abstract: This study aims to develop a deep learning system for an accessibility device
for the deaf or hearing impaired. The device will accurately localize and
identify sound sources in real time. This study will fill an important gap in
current research by leveraging machine learning techniques to target the
underprivileged community. The system includes three main components. 1.
JerryNet: A custom designed CNN architecture that determines the direction of
arrival (DoA) for nine possible directions. 2. Audio Classification: This model
is based on fine-tuning the Contrastive Language-Audio Pretraining (CLAP) model
to identify the exact sound classes only based on audio. 3. Multimodal
integration model: This is an accurate sound localization model that combines
audio, visual, and text data to locate the exact sound sources in the images.
The part consists of two modules, one object detection using Yolov9 to generate
all the bounding boxes of the objects, and an audio visual localization model
to identify the optimal bounding box using complete Intersection over Union
(CIoU). The hardware consists of a four-microphone rectangular formation and a
camera mounted on glasses with a wristband for displaying necessary information
like direction. On a custom collected data set, JerryNet achieved a precision
of 91. 1% for the sound direction, outperforming all the baseline models. The
CLAP model achieved 98.5% and 95% accuracy on custom and AudioSet datasets,
respectively. The audio-visual localization model within component 3 yielded a
cIoU of 0.892 and an AUC of 0.658, surpassing other similar models. There are
many future potentials to this study, paving the way to creating a new
generation of accessibility devices.

</details>


### [173] [Geometry-Aware Active Learning of Pattern Rankings via Choquet-Based Aggregation](https://arxiv.org/abs/2507.14217)
*Tudor Matei Opran,Samir Loudni*

Main category: cs.LG

TL;DR: 提出了一种交互式学习框架，通过非线性效用聚合和几何感知查询选择解决模式挖掘中的模式爆炸问题。


<details>
  <summary>Details</summary>
Motivation: 解决模式挖掘中的模式爆炸问题，提高用户偏好建模的效率。

Method: 使用Choquet积分建模用户偏好，结合几何感知查询选择和分支定界策略优化查询效率。

Result: 在UCI数据集上实验表明，该方法优于ChoquetRank等现有方法，以更少的用户交互实现更高的排名准确性。

Conclusion: 该框架有效解决了模式爆炸问题，提升了用户偏好建模的效率和准确性。

Abstract: We address the pattern explosion problem in pattern mining by proposing an
interactive learning framework that combines nonlinear utility aggregation with
geometry-aware query selection. Our method models user preferences through a
Choquet integral over multiple interestingness measures and exploits the
geometric structure of the version space to guide the selection of informative
comparisons. A branch-and-bound strategy with tight distance bounds enables
efficient identification of queries near the decision boundary. Experiments on
UCI datasets show that our approach outperforms existing methods such as
ChoquetRank, achieving better ranking accuracy with fewer user interactions.

</details>


### [174] [Artificial Intelligence for Green Hydrogen Yield Prediction and Site Suitability using SHAP-Based Composite Index: Focus on Oman](https://arxiv.org/abs/2507.14219)
*Obumneme Zimuzor Nwafor,Mohammed Abdul Majeed Al Hooti*

Main category: cs.LG

TL;DR: 本研究提出了一种基于AI的框架，用于计算绿色氢产量和选址适宜性指数，结合多变量聚类、机器学习分类器和SHAP算法，为数据稀缺地区提供可复制的决策工具。


<details>
  <summary>Details</summary>
Motivation: 绿色氢是化石燃料的可持续替代品，但选址需综合考虑复杂因素且缺乏直接数据，因此需要一种客观、可重复的方法替代主观专家评估。

Method: 采用多阶段AI框架，包括无监督多变量聚类、监督机器学习分类器和SHAP算法，基于气象、地形和时间数据集进行训练。

Result: 模型预测准确率达98%，显示水源距离、海拔和季节变化是阿曼绿色氢选址的最关键因素（SHAP值分别为2.470891、2.376296和1.273216）。

Conclusion: 该研究为绿色氢基础设施规划提供了可扩展的客观工具，适用于数据稀缺地区，帮助决策者优化选址。

Abstract: As nations seek sustainable alternatives to fossil fuels, green hydrogen has
emerged as a promising strategic pathway toward decarbonisation, particularly
in solar-rich arid regions. However, identifying optimal locations for hydrogen
production requires the integration of complex environmental, atmospheric, and
infrastructural factors, often compounded by limited availability of direct
hydrogen yield data. This study presents a novel Artificial Intelligence (AI)
framework for computing green hydrogen yield and site suitability index using
mean absolute SHAP (SHapley Additive exPlanations) values. This framework
consists of a multi-stage pipeline of unsupervised multi-variable clustering,
supervised machine learning classifier and SHAP algorithm. The pipeline trains
on an integrated meteorological, topographic and temporal dataset and the
results revealed distinct spatial patterns of suitability and relative
influence of the variables. With model predictive accuracy of 98%, the result
also showed that water proximity, elevation and seasonal variation are the most
influential factors determining green hydrogen site suitability in Oman with
mean absolute shap values of 2.470891, 2.376296 and 1.273216 respectively.
Given limited or absence of ground-truth yield data in many countries that have
green hydrogen prospects and ambitions, this study offers an objective and
reproducible alternative to subjective expert weightings, thus allowing the
data to speak for itself and potentially discover novel latent groupings
without pre-imposed assumptions. This study offers industry stakeholders and
policymakers a replicable and scalable tool for green hydrogen infrastructure
planning and other decision making in data-scarce regions.

</details>


### [175] [Domain Generalization via Pareto Optimal Gradient Matching](https://arxiv.org/abs/2507.14227)
*Khoi Do,Duong Nguyen,Nam-Khanh Le,Quoc-Viet Pham,Binh-Son Hua,Won-Joo Hwang*

Main category: cs.LG

TL;DR: 论文提出了一种新的POGM方法，通过独立训练和最大化梯度内积来解决梯度波动和计算开销问题，实验证明其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在梯度匹配中的波动问题和高计算开销，提升跨领域预测的一致性。

Method: 提出POGM方法，利用梯度轨迹作为数据，在元学习中独立训练，最大化梯度内积并限制偏离经验风险最小化梯度。

Result: 在DomainBed数据集上表现出优于基线方法的性能，同时计算高效。

Conclusion: POGM方法有效解决了梯度波动和计算开销问题，实现了跨领域知识的高效整合。

Abstract: In this study, we address the gradient-based domain generalization problem,
where predictors aim for consistent gradient directions across different
domains. Existing methods have two main challenges. First, minimization of
gradient empirical distance or gradient inner products (GIP) leads to gradient
fluctuations among domains, thereby hindering straightforward learning. Second,
the direct application of gradient learning to the joint loss function can
incur high computation overheads due to second-order derivative approximation.
To tackle these challenges, we propose a new Pareto Optimality Gradient
Matching (POGM) method. In contrast to existing methods that add gradient
matching as regularization, we leverage gradient trajectories as collected data
and apply independent training at the meta-learner. In the meta-update, we
maximize GIP while limiting the learned gradient from deviating too far from
the empirical risk minimization gradient trajectory. By doing so, the aggregate
gradient can incorporate knowledge from all domains without suffering gradient
fluctuation towards any particular domain. Experimental evaluations on datasets
from DomainBed demonstrate competitive results yielded by POGM against other
baselines while achieving computational efficiency.

</details>


### [176] [A million-scale dataset and generalizable foundation model for nanomaterial-protein interactions](https://arxiv.org/abs/2507.14245)
*Hengjie Yu,Kenneth A. Dawson,Haiyun Yang,Shuya Liu,Yan Yan,Yaochu Jin*

Main category: cs.LG

TL;DR: NanoPro-3M是最大的纳米材料-蛋白质相互作用数据集，结合NanoProFormer模型，通过多模态学习预测亲和力，显著优于单模态方法，并减少实验依赖。


<details>
  <summary>Details</summary>
Motivation: 纳米材料在医学和环境科学中的应用潜力受限于蛋白质相互作用的复杂性，而现有模型因数据不足和泛化能力有限而进展缓慢。

Method: 提出NanoPro-3M数据集（320万样本，3.7万独特蛋白质），并开发NanoProFormer模型，通过多模态表示学习预测亲和力。

Result: 模型在多模态下表现优于单模态，能处理缺失特征和未见样本，并识别冠形成的关键因素。

Conclusion: 该工作为高性能和泛化的纳米材料-蛋白质相互作用预测奠定了基础，加速体外应用并减少实验依赖。

Abstract: Unlocking the potential of nanomaterials in medicine and environmental
science hinges on understanding their interactions with proteins, a complex
decision space where AI is poised to make a transformative impact. However,
progress has been hindered by limited datasets and the restricted
generalizability of existing models. Here, we propose NanoPro-3M, the largest
nanomaterial-protein interaction dataset to date, comprising over 3.2 million
samples and 37,000 unique proteins. Leveraging this, we present NanoProFormer,
a foundational model that predicts nanomaterial-protein affinities through
multimodal representation learning, demonstrating strong generalization,
handling missing features, and unseen nanomaterials or proteins. We show that
multimodal modeling significantly outperforms single-modality approaches and
identifies key determinants of corona formation. Furthermore, we demonstrate
its applicability to a range of downstream tasks through zero-shot inference
and fine-tuning. Together, this work establishes a solid foundation for
high-performance and generalized prediction of nanomaterial-protein interaction
endpoints, reducing experimental reliance and accelerating various in vitro
applications.

</details>


### [177] [Linearized Diffusion Map](https://arxiv.org/abs/2507.14257)
*Julio Candanedo*

Main category: cs.LG

TL;DR: LDM是一种新的线性降维方法，通过扩散映射核的线性近似构建，结合了几何直观性和计算效率。在合成和真实数据集上，LDM在捕捉流形结构方面优于PCA，而PCA在噪声或方差主导的场景中更优。


<details>
  <summary>Details</summary>
Motivation: 结合非线性扩散映射的几何直观性和线性方法（如PCA）的计算效率与可解释性，提出一种新的线性降维方法。

Method: 通过线性近似扩散映射核构建LDM，并在合成数据集（如Swiss roll）和真实数据集（如MNIST）上进行实验验证。

Result: LDM在捕捉流形结构方面优于PCA，特别适用于高维数据，而PCA在噪声或方差主导的场景中表现更好。LDM的核矩阵完全正定，可直接应用NMF。

Conclusion: LDM是一种有潜力的线性降维技术，兼具理论和实践扩展价值。

Abstract: We introduce the Linearized Diffusion Map (LDM), a novel linear
dimensionality reduction method constructed via a linear approximation of the
diffusion-map kernel. LDM integrates the geometric intuition of diffusion-based
nonlinear methods with the computational simplicity, efficiency, and
interpretability inherent in linear embeddings such as PCA and classical MDS.
Through comprehensive experiments on synthetic datasets (Swiss roll and
hyperspheres) and real-world benchmarks (MNIST and COIL-20), we illustrate that
LDM captures distinct geometric features of datasets compared to PCA, offering
complementary advantages. Specifically, LDM embeddings outperform PCA in
datasets exhibiting explicit manifold structures, particularly in
high-dimensional regimes, whereas PCA remains preferable in scenarios dominated
by variance or noise. Furthermore, the complete positivity of LDM's kernel
matrix allows direct applicability of Non-negative Matrix Factorization (NMF),
suggesting opportunities for interpretable latent-structure discovery. Our
analysis positions LDM as a valuable new linear dimensionality reduction
technique with promising theoretical and practical extensions.

</details>


### [178] [A Simple "Try Again" Can Elicit Multi-Turn LLM Reasoning](https://arxiv.org/abs/2507.14295)
*Licheng Liu,Zihan Wang,Linjie Li,Chenwei Xu,Yiping Lu,Han Liu,Avirup Sil,Manling Li*

Main category: cs.LG

TL;DR: 论文提出了一种多轮反馈强化学习方法（UFO），通过最小化的单次用户反馈提升大模型的单轮和多轮推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在大模型的多轮推理和反馈修正中存在局限性，导致模型失去多轮解决问题的能力。

Method: 引入Unary Feedback as Observation（UFO），利用单次反馈（如“再试一次”）进行多轮强化学习训练。

Result: 实验表明，UFO方法在保持单轮性能的同时，多轮推理准确率提升高达14%。

Conclusion: UFO方法有效提升了大模型在多轮问题解决中的反馈反应能力，并通过奖励设计优化了答案多样性。

Abstract: Multi-turn problem solving is critical yet challenging for Large Reasoning
Models (LRMs) to reflect on their reasoning and revise from feedback. Existing
Reinforcement Learning (RL) methods train large reasoning models on a
single-turn paradigm with verifiable rewards. However, we observe that models
trained with existing RL paradigms often lose their ability to solve problems
across multiple turns and struggle to revise answers based on contextual
feedback, leading to repetitive responses. We ask: can LRMs learn to reflect
their answers in a multi-turn context? In this work, we find that training
models with multi-turn RL using only unary feedback (e.g., "Let's try again")
after wrong answers can improve both single-turn performance and multi-turn
reasoning. We introduce Unary Feedback as Observation (UFO) for reinforcement
learning, which uses minimal yet common unary user feedback during iterative
problem solving. It can be easily applied to existing single-turn RL training
setups. Experimental results show that RL training with UFO keeps single-turn
performance and improves multi-turn reasoning accuracy by up to 14%, enabling
language models to better react to feedback in multi-turn problem solving. To
further minimize the number of turns needed for a correct answer while
encouraging diverse reasoning when mistakes occur, we design reward structures
that guide models to produce careful and deliberate answers in each turn. Code:
https://github.com/lichengliu03/unary-feedback

</details>


### [179] [FedStrategist: A Meta-Learning Framework for Adaptive and Robust Aggregation in Federated Learning](https://arxiv.org/abs/2507.14322)
*Md Rafid Haque,Abu Raihan Mostofa Kamal,Md. Azam Hossain*

Main category: cs.LG

TL;DR: FedStrategist是一个基于元学习的框架，通过动态选择聚合规则来防御联邦学习中的模型中毒攻击。


<details>
  <summary>Details</summary>
Motivation: 联邦学习的去中心化特性使其易受模型中毒攻击，现有静态防御方法效果有限。

Method: 设计了一个轻量级上下文老虎机代理，实时选择最优聚合规则。

Result: 实验表明，动态策略优于静态规则，并能通过风险容忍参数平衡性能与安全性。

Conclusion: FedStrategist为构建弹性联邦学习系统提供了实用且可分析的方法。

Abstract: Federated Learning (FL) offers a paradigm for privacy-preserving
collaborative AI, but its decentralized nature creates significant
vulnerabilities to model poisoning attacks. While numerous static defenses
exist, their effectiveness is highly context-dependent, often failing against
adaptive adversaries or in heterogeneous data environments. This paper
introduces FedStrategist, a novel meta-learning framework that reframes robust
aggregation as a real-time, cost-aware control problem. We design a lightweight
contextual bandit agent that dynamically selects the optimal aggregation rule
from an arsenal of defenses based on real-time diagnostic metrics. Through
comprehensive experiments, we demonstrate that no single static rule is
universally optimal. We show that our adaptive agent successfully learns
superior policies across diverse scenarios, including a ``Krum-favorable"
environment and against a sophisticated "stealth" adversary designed to
neutralize specific diagnostic signals. Critically, we analyze the paradoxical
scenario where a non-robust baseline achieves high but compromised accuracy,
and demonstrate that our agent learns a conservative policy to prioritize model
integrity. Furthermore, we prove the agent's policy is controllable via a
single "risk tolerance" parameter, allowing practitioners to explicitly manage
the trade-off between performance and security. Our work provides a new,
practical, and analyzable approach to creating resilient and intelligent
decentralized AI systems.

</details>


### [180] [Rethinking Individual Fairness in Deepfake Detection](https://arxiv.org/abs/2507.14326)
*Aryana Hou,Li Lin,Justin Li,Shu Hu*

Main category: cs.LG

TL;DR: 论文提出了一种提升深度伪造检测中个体公平性的通用框架，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术的滥用对特定群体存在偏见，现有研究多关注群体公平性，个体公平性未被充分探索。

Method: 提出首个可集成到现有检测器中的通用框架，以增强个体公平性和泛化能力。

Result: 在多个主流数据集上验证，显著提升个体公平性且保持检测性能，优于现有方法。

Conclusion: 该框架为深度伪造检测中的个体公平性提供了有效解决方案，填补了研究空白。

Abstract: Generative AI models have substantially improved the realism of synthetic
media, yet their misuse through sophisticated DeepFakes poses significant
risks. Despite recent advances in deepfake detection, fairness remains
inadequately addressed, enabling deepfake markers to exploit biases against
specific populations. While previous studies have emphasized group-level
fairness, individual fairness (i.e., ensuring similar predictions for similar
individuals) remains largely unexplored. In this work, we identify for the
first time that the original principle of individual fairness fundamentally
fails in the context of deepfake detection, revealing a critical gap previously
unexplored in the literature. To mitigate it, we propose the first
generalizable framework that can be integrated into existing deepfake detectors
to enhance individual fairness and generalization. Extensive experiments
conducted on leading deepfake datasets demonstrate that our approach
significantly improves individual fairness while maintaining robust detection
performance, outperforming state-of-the-art methods. The code is available at
https://github.com/Purdue-M2/Individual-Fairness-Deepfake-Detection.

</details>


### [181] [Development and Deployment of Hybrid ML Models for Critical Heat Flux Prediction in Annulus Geometries](https://arxiv.org/abs/2507.14332)
*Aidan Furlong,Xingang Zhao,Robert Salko,Xu Wu*

Main category: cs.LG

TL;DR: 该研究开发并验证了四种机器学习模型，用于预测环形几何结构中的临界热通量（CHF），显著优于传统经验方法。


<details>
  <summary>Details</summary>
Motivation: 临界热通量（CHF）的准确预测对反应堆安全分析至关重要，传统方法存在局限性，机器学习提供了改进的机会。

Method: 研究使用CTF子通道代码，基于三种经验模型（Biasi、Bowring、Katto）开发了四种混合机器学习模型，并利用577个实验数据点进行训练和测试。

Result: 机器学习模型的平均相对误差低于3.5%，显著优于经验模型的26%以上误差。

Conclusion: 混合机器学习模型在环形几何结构中的CHF预测中表现出色，为反应堆安全分析提供了更可靠的工具。

Abstract: Accurate prediction of critical heat flux (CHF) is an essential component of
safety analysis in pressurized and boiling water reactors. To support reliable
prediction of this quantity, several empirical correlations and lookup tables
have been constructed from physical experiments over the past several decades.
With the onset of accessible machine learning (ML) frameworks, multiple
initiatives have been established with the goal of predicting CHF more
accurately than these traditional methods. While purely data-driven surrogate
modeling has been extensively investigated, these approaches lack
interpretability, lack resilience to data scarcity, and have been developed
mostly using data from tube experiments. As a result, bias-correction hybrid
approaches have become increasingly popular, which correct initial
"low-fidelity" estimates provided by deterministic base models by using
ML-predicted residuals. This body of work has mostly considered round tube
geometries; annular geometry-specific ML models have not yet been deployed in
thermal hydraulic codes. This study developed, deployed, and validated four ML
models to predict CHF in annular geometries using the CTF subchannel code.
Three empirical correlation models, Biasi, Bowring, and Katto, were used as
base models for comparison. The ML models were trained and tested using 577
experimental annulus data points from four datasets: Becker, Beus, Janssen, and
Mortimore. Baseline CHF predictions were obtained from the empirical
correlations, with mean relative errors above 26%. The ML-driven models
achieved mean relative errors below 3.5%, with no more than one point exceeding
the 10% error envelope. In all cases, the hybrid ML models significantly
outperformed their empirical counterparts.

</details>


### [182] [Influence Functions for Preference Dataset Pruning](https://arxiv.org/abs/2507.14344)
*Daniel Fein,Gabriela Aranguiz-Dias*

Main category: cs.LG

TL;DR: 该论文提出了一种利用共轭梯度近似影响函数的方法来过滤噪声数据，以提高语言模型微调的性能。实验表明，过滤10%的训练数据后，准确率提升了1.5%。


<details>
  <summary>Details</summary>
Motivation: 人类偏好数据集通常存在噪声，而小规模的微调数据集使得影响函数近似方法成为可能，以检测和删除有害的训练样本。

Method: 使用共轭梯度近似影响函数来过滤数据集，并比较了梯度相似性与影响函数在检测有用训练样本上的表现。

Result: 过滤10%的训练数据后，准确率提升了1.5%，且梯度相似性在检测有用样本上优于影响函数。

Conclusion: 局部曲率对于检测有害样本很重要，但对识别有用样本影响较小。

Abstract: Language models are commonly fine-tuned via reinforcement learning to alter
their behavior or elicit new capabilities. Datasets used for these purposes,
and particularly human preference datasets, are often noisy. The relatively
small size post-training datasets, combined with parameter-efficient
fine-tuning methods, enable the use of influence functions approximations to
detect and prune training examples that are harmful to performance on a
validation set. In this work, we adapt the TL;DR dataset for reward model
training to demonstrate how conjugate-gradient approximated influence functions
can be used to filter datasets. In our experiments, influence function
filtering yields a small retraining accuracy uplift of 1.5% after removing 10%
of training examples. We also show that gradient similarity outperforms
influence functions for detecting helpful training examples. This suggests that
local curvature is important for detecting harmful training examples, but less
so for identifying helpful examples.

</details>


### [183] [Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers](https://arxiv.org/abs/2507.14353)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: Solo Connection是一种新的参数高效微调方法，通过调整解码器块级别的表示而非单个权重矩阵，优于LoRA，并显著减少可训练参数。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于探索更高效的参数微调方法，并利用同伦理论实现平滑稳定的任务适应。

Method: Solo Connection通过可训练的线性变换在零向量和任务特定表示之间插值，并引入长跳跃连接以增强任务适应能力。

Result: Solo Connection在E2E自然语言生成基准上优于LoRA，可训练参数减少59%（相比LoRA）和99%以上（相比全微调）。

Conclusion: Solo Connection为大规模语言模型的参数高效微调提供了新思路，尤其在多层架构中表现突出。

Abstract: Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

</details>


### [184] [Incremental Causal Graph Learning for Online Cyberattack Detection in Cyber-Physical Infrastructures](https://arxiv.org/abs/2507.14387)
*Arun Vignesh Malarkkan,Dongjie Wang,Haoyue Bai,Yanjie Fu*

Main category: cs.LG

TL;DR: INCADET是一种新型增量因果图学习框架，用于实时网络攻击检测，通过动态更新因果图适应系统行为变化，优于传统静态和深度时序方法。


<details>
  <summary>Details</summary>
Motivation: 针对实时关键基础设施中网络攻击的威胁，传统方法因高数据方差和类别不平衡导致高误报率，且静态因果图方法无法适应动态数据分布变化。

Method: INCADET包含三个模块：早期症状检测、增量因果图学习和因果图分类，利用流式时间窗口动态更新因果图。

Result: 在真实关键基础设施数据集上的实验表明，INCADET在准确性、鲁棒性和适应性上优于静态因果和深度时序基线。

Conclusion: INCADET为实时网络攻击检测提供了一种高效、动态且适应性强的解决方案。

Abstract: The escalating threat of cyberattacks on real-time critical infrastructures
poses serious risks to public safety, demanding detection methods that
effectively capture complex system interdependencies and adapt to evolving
attack patterns. Traditional real-time anomaly detection techniques often
suffer from excessive false positives due to their statistical sensitivity to
high data variance and class imbalance. To address these limitations, recent
research has explored modeling causal relationships among system components.
However, prior work mainly focuses on offline causal graph-based approaches
that require static historical data and fail to generalize to real-time
settings. These methods are fundamentally constrained by: (1) their inability
to adapt to dynamic shifts in data distribution without retraining, and (2) the
risk of catastrophic forgetting when lacking timely supervision in live
systems. To overcome these challenges, we propose INCADET, a novel framework
for incremental causal graph learning tailored to real-time cyberattack
detection. INCADET dynamically captures evolving system behavior by
incrementally updating causal graphs across streaming time windows. The
framework comprises three modules: 1) Early Symptom Detection: Detects
transitions in system status using divergence in edge-weight distributions
across sequential causal graphs. 2) Incremental Causal Graph Learning:
Leverages experience replay and edge reinforcement to continually refine causal
structures while preserving prior knowledge. 3) Causal Graph Classification:
Employs Graph Convolutional Networks (GCNs) to classify system status using the
learned causal graphs. Extensive experiments on real-world critical
infrastructure datasets demonstrate that INCADET achieves superior accuracy,
robustness, and adaptability compared to both static causal and deep temporal
baselines in evolving attack scenarios.

</details>


### [185] [It's Not That Simple. An Analysis of Simple Test-Time Scaling](https://arxiv.org/abs/2507.14419)
*Guojun Wu*

Main category: cs.LG

TL;DR: 本文分析了简单测试时缩放方法，发现其主要通过限制最大长度实现缩放，而通过追加“Wait”进行放大则会导致不一致性。与o1类模型不同，简单测试时缩放无法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究简单测试时缩放方法的有效性及其与o1类模型在测试时计算缩放行为上的差异。

Method: 分析简单测试时缩放（包括限制最大长度和追加“Wait”）的效果，并与o1类模型（如DeepSeek-R1@）的缩放行为进行对比。

Result: 限制最大长度的缩放行为显著，而追加“Wait”会导致模型解决方案的振荡。o1类模型通过强化学习自然提升性能，而简单测试时缩放则限制了性能上限。

Conclusion: 简单测试时缩放仅能模拟o1类模型的缩放行为，但无法实现性能的实质性提升。真正的目标是解锁更高性能，而非仅复制缩放行为。

Abstract: Prior work proposed simple test-time scaling, a method for replicating this
scaling behavior with models distilled from o1-like models by manually
controlling test-time compute: either scaling down by enforcing a maximum
length or scaling up by iteratively appending "Wait" when the model is about to
terminate its generation. This paper presents an analysis of simple test-time
scaling and finds that the scaling behavior is largely attributed to scaling
down by enforcing a maximum length. In contrast, fine-tuning on long CoT data
distilled from o1-like models has no significant impact on scaling behavior,
and scaling up by appending "Wait" leads to inconsistencies, as the model may
oscillate between solutions. A key distinction exists between scaling down by
enforcing a maximum length and scaling up test-time compute in o1-like models,
such as DeepSeek-R1\@. These models are typically allowed to utilize as much
compute as needed, with the only constraint being the model's maximum supported
length. By learning to naturally scale up test-time compute during
reinforcement learning, o1-like models surpass their peak performance when
scaling up. In contrast, simple test-time scaling progressively imposes a lower
upper limit on model performance as it scales down. While replicating the
test-time scaling behavior of o1 models can be straightforward by scaling down,
it is crucial to recognize that the goal of scaling test-time compute is to
unlock higher performance -- beyond what the model could originally achieve --
rather than merely reproducing the appearance of scaling behavior.

</details>


### [186] [Deep RL Dual Sourcing Inventory Management with Supply and Capacity Risk Awareness](https://arxiv.org/abs/2507.14446)
*Feng Liu,Ying Liu,Carson Eisenach*

Main category: cs.LG

TL;DR: 该论文提出了一种结合强化学习（RL）和深度学习（DL）的方法，通过干预模型解决大规模随机优化问题，并在供应链中的多源多周期库存管理问题上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决大规模随机优化问题中探索解空间的效率问题，特别是在复杂供应链管理场景下。

Method: 利用预训练的DL模型模拟和组合随机过程，并结合RL模型学习和预测供应链过程；引入约束协调机制预测双成本。

Result: 通过将供应链过程分解为可扩展和可组合的DL模块，提高了在大规模实际数据集上的性能。

Conclusion: 该方法为复杂约束下的随机优化问题提供了高效解决方案，并指出了未来研究的开放性问题。

Abstract: In this work, we study how to efficiently apply reinforcement learning (RL)
for solving large-scale stochastic optimization problems by leveraging
intervention models. The key of the proposed methodology is to better explore
the solution space by simulating and composing the stochastic processes using
pre-trained deep learning (DL) models. We demonstrate our approach on a
challenging real-world application, the multi-sourcing multi-period inventory
management problem in supply chain optimization. In particular, we employ deep
RL models for learning and forecasting the stochastic supply chain processes
under a range of assumptions. Moreover, we also introduce a constraint
coordination mechanism, designed to forecast dual costs given the
cross-products constraints in the inventory network. We highlight that instead
of directly modeling the complex physical constraints into the RL optimization
problem and solving the stochastic problem as a whole, our approach breaks down
those supply chain processes into scalable and composable DL modules, leading
to improved performance on large real-world datasets. We also outline open
problems for future research to further investigate the efficacy of such
models.

</details>


### [187] [ReDiSC: A Reparameterized Masked Diffusion Model for Scalable Node Classification with Structured Predictions](https://arxiv.org/abs/2507.14484)
*Yule Li,Yifeng Lu,Zhen Wang,Zhewei Wei,Yaliang Li,Bolin Ding*

Main category: cs.LG

TL;DR: ReDiSC是一种基于重参数化掩码扩散模型的图神经网络方法，用于结构化节点分类，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GNN方法假设节点标签条件独立，而实际中节点标签仍相关，ReDiSC旨在解决这一问题。

Method: 使用重参数化掩码扩散模型估计节点标签的联合分布，并通过变分EM框架学习。

Result: ReDiSC在性能和可扩展性上优于现有GNN、标签传播和扩散模型，适用于不同规模的图数据。

Conclusion: ReDiSC在结构化节点分类任务中具有显著优势，尤其适用于大规模数据集。

Abstract: In recent years, graph neural networks (GNN) have achieved unprecedented
successes in node classification tasks. Although GNNs inherently encode
specific inductive biases (e.g., acting as low-pass or high-pass filters), most
existing methods implicitly assume conditional independence among node labels
in their optimization objectives. While this assumption is suitable for
traditional classification tasks such as image recognition, it contradicts the
intuitive observation that node labels in graphs remain correlated, even after
conditioning on the graph structure. To make structured predictions for node
labels, we propose ReDiSC, namely, Reparameterized masked Diffusion model for
Structured node Classification. ReDiSC estimates the joint distribution of node
labels using a reparameterized masked diffusion model, which is learned through
the variational expectation-maximization (EM) framework. Our theoretical
analysis shows the efficiency advantage of ReDiSC in the E-step compared to
DPM-SNC, a state-of-the-art model that relies on a manifold-constrained
diffusion model in continuous domain. Meanwhile, we explicitly link ReDiSC's
M-step objective to popular GNN and label propagation hybrid approaches.
Extensive experiments demonstrate that ReDiSC achieves superior or highly
competitive performance compared to state-of-the-art GNN, label propagation,
and diffusion-based baselines across both homophilic and heterophilic graphs of
varying sizes. Notably, ReDiSC scales effectively to large-scale datasets on
which previous structured diffusion methods fail due to computational
constraints, highlighting its significant practical advantage in structured
node classification tasks.

</details>


### [188] [Federated Reinforcement Learning in Heterogeneous Environments](https://arxiv.org/abs/2507.14487)
*Ukjo Hwang,Songnam Hong*

Main category: cs.LG

TL;DR: 提出了一种联邦强化学习框架FRL-EH，解决局部环境统计异质性问题，通过聚合经验学习全局策略并保护隐私。提出新目标函数优化全局策略，确保在异质环境中的鲁棒性。算法FedRQ理论收敛，并扩展至连续状态空间，实验验证其优越性。


<details>
  <summary>Details</summary>
Motivation: 解决联邦强化学习中局部环境异质性带来的挑战，优化全局策略以适应多样化环境。

Method: 提出FRL-EH框架和FedRQ算法，使用新目标函数优化全局策略，理论证明收敛性，并扩展至连续状态空间。

Result: 实验验证FedRQ在异质环境中表现优越，优于现有方法。

Conclusion: FRL-EH和FedRQ有效解决了环境异质性问题，为联邦强化学习提供了鲁棒解决方案。

Abstract: We investigate a Federated Reinforcement Learning with Environment
Heterogeneity (FRL-EH) framework, where local environments exhibit statistical
heterogeneity. Within this framework, agents collaboratively learn a global
policy by aggregating their collective experiences while preserving the privacy
of their local trajectories. To better reflect real-world scenarios, we
introduce a robust FRL-EH framework by presenting a novel global objective
function. This function is specifically designed to optimize a global policy
that ensures robust performance across heterogeneous local environments and
their plausible perturbations. We propose a tabular FRL algorithm named FedRQ
and theoretically prove its asymptotic convergence to an optimal policy for the
global objective function. Furthermore, we extend FedRQ to environments with
continuous state space through the use of expectile loss, addressing the key
challenge of minimizing a value function over a continuous subset of the state
space. This advancement facilitates the seamless integration of the principles
of FedRQ with various Deep Neural Network (DNN)-based RL algorithms. Extensive
empirical evaluations validate the effectiveness and robustness of our FRL
algorithms across diverse heterogeneous environments, consistently achieving
superior performance over the existing state-of-the-art FRL algorithms.

</details>


### [189] [Glitches in Decision Tree Ensemble Models](https://arxiv.org/abs/2507.14492)
*Satyankar Chandra,Ashutosh Gupta,Kaushik Mallik,Krishna Shankaranarayanan,Namrita Varshney*

Main category: cs.LG

TL;DR: 论文提出了一种新的不可靠行为来源——"glitches"，即模型输出在输入微小变化时突然振荡的现象，并证明了其在梯度提升决策树（GBDT）模型中的广泛存在。


<details>
  <summary>Details</summary>
Motivation: 确保机器学习模型的决策可信且可靠，输出在相似输入下一致，但发现"glitches"可能显著影响具有陡峭决策边界的AI模型的可靠性。

Method: 通过形式化定义"glitches"，并在文献中的模型和数据集上验证其存在性；提出基于MILP编码的算法搜索GBDT模型中的"glitches"。

Result: 证明了在深度为4的树集成中检测"glitches"是NP完全问题，并在广泛使用的GBDT基准测试中验证了算法的有效性和计算可行性。

Conclusion: "glitches"是模型不一致性的潜在指标，其检测算法的提出为提升模型可靠性提供了新方向。

Abstract: Many critical decision-making tasks are now delegated to machine-learned
models, and it is imperative that their decisions are trustworthy and reliable,
and their outputs are consistent across similar inputs. We identify a new
source of unreliable behaviors-called glitches-which may significantly impair
the reliability of AI models having steep decision boundaries. Roughly
speaking, glitches are small neighborhoods in the input space where the model's
output abruptly oscillates with respect to small changes in the input. We
provide a formal definition of glitches, and use well-known models and datasets
from the literature to demonstrate that they have widespread existence and
argue they usually indicate potential model inconsistencies in the neighborhood
of where they are found. We proceed to the algorithmic search of glitches for
widely used gradient-boosted decision tree (GBDT) models. We prove that the
problem of detecting glitches is NP-complete for tree ensembles, already for
trees of depth 4. Our glitch-search algorithm for GBDT models uses an MILP
encoding of the problem, and its effectiveness and computational feasibility
are demonstrated on a set of widely used GBDT benchmarks taken from the
literature.

</details>


### [190] [Generative Distribution Distillation](https://arxiv.org/abs/2507.14503)
*Jiequan Cui,Beier Zhu,Qingshan Xu,Xiaogang Xu,Pengguang Chen,Xiaojuan Qi,Bei Yu,Hanwang Zhang,Richang Hong*

Main category: cs.LG

TL;DR: 本文提出了一种基于条件生成问题的知识蒸馏框架GenDD，通过Split Tokenization和Distribution Contraction技术解决了高维优化和缺乏标签监督的问题，实验表明其性能优越。


<details>
  <summary>Details</summary>
Motivation: 解决知识蒸馏中高维优化和缺乏标签监督的挑战。

Method: 提出GenDD框架，结合Split Tokenization和Distribution Contraction技术。

Result: 在无监督设置下显著优于KL基线16.29%，监督设置下ResNet-50在ImageNet上达到82.28%的top-1准确率。

Conclusion: GenDD框架在知识蒸馏中表现出色，尤其在无监督和监督任务中均取得显著成果。

Abstract: In this paper, we formulate the knowledge distillation (KD) as a conditional
generative problem and propose the \textit{Generative Distribution Distillation
(GenDD)} framework. A naive \textit{GenDD} baseline encounters two major
challenges: the curse of high-dimensional optimization and the lack of semantic
supervision from labels. To address these issues, we introduce a \textit{Split
Tokenization} strategy, achieving stable and effective unsupervised KD.
Additionally, we develop the \textit{Distribution Contraction} technique to
integrate label supervision into the reconstruction objective. Our theoretical
proof demonstrates that \textit{GenDD} with \textit{Distribution Contraction}
serves as a gradient-level surrogate for multi-task learning, realizing
efficient supervised training without explicit classification loss on
multi-step sampling image representations. To evaluate the effectiveness of our
method, we conduct experiments on balanced, imbalanced, and unlabeled data.
Experimental results show that \textit{GenDD} performs competitively in the
unsupervised setting, significantly surpassing KL baseline by \textbf{16.29\%}
on ImageNet validation set. With label supervision, our ResNet-50 achieves
\textbf{82.28\%} top-1 accuracy on ImageNet in 600 epochs training,
establishing a new state-of-the-art.

</details>


### [191] [SDSC:A Structure-Aware Metric for Semantic Signal Representation Learning](https://arxiv.org/abs/2507.14516)
*Jeyoung Lee,Hochul Kang*

Main category: cs.LG

TL;DR: 提出了一种结构感知的度量函数SDSC，用于时间序列自监督表示学习，解决了传统距离目标（如MSE）在信号处理中的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统自监督学习方法（如MSE）对振幅敏感、对波形极性不变且尺度无界，影响了语义对齐和可解释性。

Method: SDSC基于Dice相似系数，通过量化时间信号的结构一致性来改进表示学习，并可作为损失函数使用。

Result: 实验表明，SDSC在预测和分类任务中表现优于或与MSE相当，尤其在低资源场景下。

Conclusion: 结构感知度量（如SDSC）能提升信号表示的语义质量，可作为传统距离方法的替代方案。

Abstract: We propose the Signal Dice Similarity Coefficient (SDSC), a structure-aware
metric function for time series self-supervised representation learning. Most
Self-Supervised Learning (SSL) methods for signals commonly adopt
distance-based objectives such as mean squared error (MSE), which are sensitive
to amplitude, invariant to waveform polarity, and unbounded in scale. These
properties hinder semantic alignment and reduce interpretability. SDSC
addresses this by quantifying structural agreement between temporal signals
based on the intersection of signed amplitudes, derived from the Dice
Similarity Coefficient (DSC).Although SDSC is defined as a structure-aware
metric, it can be used as a loss by subtracting from 1 and applying a
differentiable approximation of the Heaviside function for gradient-based
optimization. A hybrid loss formulation is also proposed to combine SDSC with
MSE, improving stability and preserving amplitude where necessary. Experiments
on forecasting and classification benchmarks demonstrate that SDSC-based
pre-training achieves comparable or improved performance over MSE, particularly
in in-domain and low-resource scenarios. The results suggest that structural
fidelity in signal representations enhances the semantic representation
quality, supporting the consideration of structure-aware metrics as viable
alternatives to conventional distance-based methods.

</details>


### [192] [Positive-Unlabeled Learning for Control Group Construction in Observational Causal Inference](https://arxiv.org/abs/2507.14528)
*Ilias Tsoumas,Dimitrios Bormpoudakis,Vasileios Sitokonstantinou,Athanasios Askitopoulos,Andreas Kalogeras,Charalampos Kontoes,Ioannis Athanasiadis*

Main category: cs.LG

TL;DR: 论文提出使用正未标记（PU）学习框架从无标签数据中识别控制单元，以解决观测性研究中缺乏明确控制组的问题，并通过模拟和真实数据验证了方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 在观测性研究中，缺乏明确标记的控制单元是估计平均处理效应（ATE）的主要挑战。本文旨在通过PU学习框架解决这一问题。

Method: 采用PU学习框架，仅利用已处理的（正）单元从无标签数据中识别控制单元，并通过模拟和真实数据（如农业数据）验证方法。

Result: PU学习能成功从无标签数据中识别控制单元，并估计出接近真实值的ATE。

Conclusion: 该方法为观测性因果推断提供了新思路，尤其适用于难以进行随机实验的领域（如农业、环境科学）。

Abstract: In causal inference, whether through randomized controlled trials or
observational studies, access to both treated and control units is essential
for estimating the effect of a treatment on an outcome of interest. When
treatment assignment is random, the average treatment effect (ATE) can be
estimated directly by comparing outcomes between groups. In non-randomized
settings, various techniques are employed to adjust for confounding and
approximate the counterfactual scenario to recover an unbiased ATE. A common
challenge, especially in observational studies, is the absence of units clearly
labeled as controls-that is, units known not to have received the treatment. To
address this, we propose positive-unlabeled (PU) learning as a framework for
identifying, with high confidence, control units from a pool of unlabeled ones,
using only the available treated (positive) units. We evaluate this approach
using both simulated and real-world data. We construct a causal graph with
diverse relationships and use it to generate synthetic data under various
scenarios, assessing how reliably the method recovers control groups that allow
estimates of true ATE. We also apply our approach to real-world data on optimal
sowing and fertilizer treatments in sustainable agriculture. Our findings show
that PU learning can successfully identify control (negative) units from
unlabeled data based only on treated units and, through the resulting control
group, estimate an ATE that closely approximates the true value. This work has
important implications for observational causal inference, especially in fields
where randomized experiments are difficult or costly. In domains such as earth,
environmental, and agricultural sciences, it enables a plethora of
quasi-experiments by leveraging available earth observation and climate data,
particularly when treated units are available but control units are lacking.

</details>


### [193] [Kernel Based Maximum Entropy Inverse Reinforcement Learning for Mean-Field Games](https://arxiv.org/abs/2507.14529)
*Berkay Anahtarci,Can Deha Kariksiz,Naci Saldi*

Main category: cs.LG

TL;DR: 该论文研究了无限时间平稳平均场博弈中的最大因果熵逆强化学习问题，利用再生核希尔伯特空间建模未知奖励函数，通过拉格朗日松弛和梯度上升算法求解。


<details>
  <summary>Details</summary>
Motivation: 现有逆强化学习方法通常将奖励函数限制为固定基函数的线性组合，而本文旨在推断更丰富的非线性奖励结构，并专注于无限时间成本结构。

Method: 引入拉格朗日松弛将问题转化为无约束对数似然最大化，并通过梯度上升算法求解；证明了相关软贝尔曼算子的Fr\'echet可微性以确保目标平滑性。

Result: 在平均场交通路由游戏中，该方法能准确恢复专家行为。

Conclusion: 该方法在无限时间平均场博弈中有效推断非线性奖励结构，优于现有线性方法。

Abstract: We consider the maximum causal entropy inverse reinforcement learning problem
for infinite-horizon stationary mean-field games, in which we model the unknown
reward function within a reproducing kernel Hilbert space. This allows the
inference of rich and potentially nonlinear reward structures directly from
expert demonstrations, in contrast to most existing inverse reinforcement
learning approaches for mean-field games that typically restrict the reward
function to a linear combination of a fixed finite set of basis functions. We
also focus on the infinite-horizon cost structure, whereas prior studies
primarily rely on finite-horizon formulations. We introduce a Lagrangian
relaxation to this maximum causal entropy inverse reinforcement learning
problem that enables us to reformulate it as an unconstrained log-likelihood
maximization problem, and obtain a solution \lk{via} a gradient ascent
algorithm. To illustrate the theoretical consistency of the algorithm, we
establish the smoothness of the log-likelihood objective by proving the
Fr\'echet differentiability of the related soft Bellman operators with respect
to the parameters in the reproducing kernel Hilbert space. We demonstrate the
effectiveness of our method on a mean-field traffic routing game, where it
accurately recovers expert behavior.

</details>


### [194] [The Origin of Self-Attention: From Pairwise Affinity Matrices to Transformers](https://arxiv.org/abs/2507.14560)
*Giorgio Roffo*

Main category: cs.LG

TL;DR: 本文追溯了自注意力机制的概念起源，将其视为基于亲和矩阵的通用计算原理的特例，并与无限特征选择（Inf-FS）方法进行对比。


<details>
  <summary>Details</summary>
Motivation: 探讨自注意力机制在不同领域的共同数学基础，揭示其与更广泛的基于亲和矩阵的计算范式的联系。

Method: 通过分析自注意力与Inf-FS的亲和矩阵定义和应用方式，比较两者的异同。

Result: 自注意力是Inf-FS的单跳特例，两者共享基于成对关系的推理结构。

Conclusion: 将自注意力置于基于亲和矩阵的计算范式中，统一了多种机器学习模型和任务的数学基础。

Abstract: The self-attention mechanism, now central to deep learning architectures such
as Transformers, is a modern instance of a more general computational
principle: learning and using pairwise affinity matrices to control how
information flows through a model. This paper traces the conceptual origins of
self-attention across multiple domains, including computer vision, natural
language processing, and graph learning, through their shared reliance on an
affinity matrix, denoted as A. We highlight Infinite Feature Selection (Inf-FS)
as a foundational approach that generalizes the idea of affinity-based
weighting. Unlike the fixed dot-product structure used in Transformers, Inf-FS
defines A either through domain knowledge or by learning, and computes feature
relevance through multi-hop propagation over the affinity graph. From this
perspective, self-attention can be seen as a special case of Inf-FS: it uses a
single-hop affinity computation where A is dynamically built from token
similarities. We argue that the underlying structure, reasoning over pairwise
relationships, is preserved across both approaches, and the key differences lie
in how the affinity matrix is defined and applied. By situating self-attention
within the broader paradigm of affinity-based computation, we unify several
strands of machine learning research and highlight a common mathematical
foundation that underpins diverse models and tasks.

</details>


### [195] [LPS-GNN : Deploying Graph Neural Networks on Graphs with 100-Billion Edges](https://arxiv.org/abs/2507.14570)
*Xu Cheng,Liang Yao,Feng He,Yukuo Cen,Yufei He,Chenhui Zhang,Wenzheng Feng,Hongyun Cai,Jie Tang*

Main category: cs.LG

TL;DR: LPS-GNN是一种可扩展、低成本、灵活且高效的GNN框架，能在单GPU上10小时内处理1000亿图数据，并在用户获取场景中提升13.8%。


<details>
  <summary>Details</summary>
Motivation: 现有可扩展GNN解决方案在效率和准确性之间难以平衡，且面临计算和内存需求高的挑战。

Method: 提出LPS-GNN框架，设计LPMetis图分区算法，并采用子图增强策略提升性能。

Result: 在公开和真实数据集上测试，性能提升8.24%至13.89%，优于现有SOTA模型。

Conclusion: LPS-GNN成功部署于腾讯平台，展示了其高效性和兼容性，适用于大规模图数据任务。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for various graph
mining tasks, yet existing scalable solutions often struggle to balance
execution efficiency with prediction accuracy. These difficulties stem from
iterative message-passing techniques, which place significant computational
demands and require extensive GPU memory, particularly when dealing with the
neighbor explosion issue inherent in large-scale graphs. This paper introduces
a scalable, low-cost, flexible, and efficient GNN framework called LPS-GNN,
which can perform representation learning on 100 billion graphs with a single
GPU in 10 hours and shows a 13.8% improvement in User Acquisition scenarios. We
examine existing graph partitioning methods and design a superior graph
partition algorithm named LPMetis. In particular, LPMetis outperforms current
state-of-the-art (SOTA) approaches on various evaluation metrics. In addition,
our paper proposes a subgraph augmentation strategy to enhance the model's
predictive performance. It exhibits excellent compatibility, allowing the
entire framework to accommodate various GNN algorithms. Successfully deployed
on the Tencent platform, LPS-GNN has been tested on public and real-world
datasets, achieving performance lifts of 8. 24% to 13. 89% over SOTA models in
online applications.

</details>


### [196] [A Transformer-Based Conditional GAN with Multiple Instance Learning for UAV Signal Detection and Classification](https://arxiv.org/abs/2507.14592)
*Haochen Liu,Jia Bi,Xiaomin Wang,Xin Yang,Ling Wang*

Main category: cs.LG

TL;DR: 提出了一种结合Transformer-GAN和MILET的新框架，用于无人机飞行状态分类，显著提升了准确率和计算效率。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分类方法在动态无人机环境中缺乏鲁棒性和泛化能力，而现有SOTA模型需要大量数据和计算资源。

Method: 集成Transformer编码器捕捉长期时间依赖，GAN模块生成合成数据增强数据集，MILET聚焦关键输入段。

Result: 在DroneDetect和DroneRF数据集上分别达到96.5%和98.6%的准确率，优于其他SOTA方法。

Conclusion: 该框架在计算效率和泛化能力上表现优异，适合资源受限环境中的实时部署。

Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly used in surveillance,
logistics, agriculture, disaster management, and military operations. Accurate
detection and classification of UAV flight states, such as hovering, cruising,
ascending, or transitioning, which are essential for safe and effective
operations. However, conventional time series classification (TSC) methods
often lack robustness and generalization for dynamic UAV environments, while
state of the art(SOTA) models like Transformers and LSTM based architectures
typically require large datasets and entail high computational costs,
especially with high-dimensional data streams. This paper proposes a novel
framework that integrates a Transformer-based Generative Adversarial Network
(GAN) with Multiple Instance Locally Explainable Learning (MILET) to address
these challenges in UAV flight state classification. The Transformer encoder
captures long-range temporal dependencies and complex telemetry dynamics, while
the GAN module augments limited datasets with realistic synthetic samples. MIL
is incorporated to focus attention on the most discriminative input segments,
reducing noise and computational overhead. Experimental results show that the
proposed method achieves superior accuracy 96.5% on the DroneDetect dataset and
98.6% on the DroneRF dataset that outperforming other SOTA approaches. The
framework also demonstrates strong computational efficiency and robust
generalization across diverse UAV platforms and flight states, highlighting its
potential for real-time deployment in resource constrained environments.

</details>


### [197] [$k$-PCA for (non-squared) Euclidean Distances: Polynomial Time Approximation](https://arxiv.org/abs/2507.14631)
*Daniel Greenhut,Dan Feldman*

Main category: cs.LG

TL;DR: 论文提出了一种多项式时间确定性算法，用于近似计算k-子空间中位数，解决了非凸优化问题，具有鲁棒性和稀疏性。


<details>
  <summary>Details</summary>
Motivation: 传统的k-PCA方法对噪声和异常值敏感，而k-子空间中位数更鲁棒但计算困难。本文旨在解决这一问题。

Method: 提出了一种多项式时间确定性算法，近似因子为√d，运行时间与输入大小成多项式关系。

Result: 算法在真实数据集上验证了其有效性，并提供了开源代码。

Conclusion: 该技术不仅适用于k-子空间中位数问题，还可推广到其他相关范数问题。

Abstract: Given an integer $k\geq1$ and a set $P$ of $n$ points in $\REAL^d$, the
classic $k$-PCA (Principle Component Analysis) approximates the affine
\emph{$k$-subspace mean} of $P$, which is the $k$-dimensional affine linear
subspace that minimizes its sum of squared Euclidean distances
($\ell_{2,2}$-norm) over the points of $P$, i.e., the mean of these distances.
The \emph{$k$-subspace median} is the subspace that minimizes its sum of
(non-squared) Euclidean distances ($\ell_{2,1}$-mixed norm), i.e., their
median. The median subspace is usually more sparse and robust to noise/outliers
than the mean, but also much harder to approximate since, unlike the
$\ell_{z,z}$ (non-mixed) norms, it is non-convex for $k<d-1$.
  We provide the first polynomial-time deterministic algorithm whose both
running time and approximation factor are not exponential in $k$. More
precisely, the multiplicative approximation factor is $\sqrt{d}$, and the
running time is polynomial in the size of the input. We expect that our
technique would be useful for many other related problems, such as $\ell_{2,z}$
norm of distances for $z\not \in \br{1,2}$, e.g., $z=\infty$, and handling
outliers/sparsity.
  Open code and experimental results on real-world datasets are also provided.

</details>


### [198] [Rec-AD: An Efficient Computation Framework for FDIA Detection Based on Tensor Train Decomposition and Deep Learning Recommendation Model](https://arxiv.org/abs/2507.14668)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: Rec-AD框架通过Tensor Train分解和DLRM结合，提升智能电网中FDIA检测的计算效率，减少内存负担。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在大规模智能电网数据中计算和内存负担过重的问题。

Method: 结合Tensor Train分解和DLRM，通过嵌入压缩、索引重排序和流水线训练机制优化效率。

Result: 显著提高计算吞吐量和实时检测性能，缩小攻击窗口并增加攻击成本。

Conclusion: Rec-AD为智能电网安全提供了高效、可扩展的技术支持。

Abstract: Deep learning models have been widely adopted for False Data Injection Attack
(FDIA) detection in smart grids due to their ability to capture unstructured
and sparse features. However, the increasing system scale and data
dimensionality introduce significant computational and memory burdens,
particularly in large-scale industrial datasets, limiting detection efficiency.
To address these issues, this paper proposes Rec-AD, a computationally
efficient framework that integrates Tensor Train decomposition with the Deep
Learning Recommendation Model (DLRM). Rec-AD enhances training and inference
efficiency through embedding compression, optimized data access via index
reordering, and a pipeline training mechanism that reduces memory communication
overhead. Fully compatible with PyTorch, Rec-AD can be integrated into existing
FDIA detection systems without code modifications. Experimental results show
that Rec-AD significantly improves computational throughput and real-time
detection performance, narrowing the attack window and increasing attacker
cost. These advancements strengthen edge computing capabilities and
scalability, providing robust technical support for smart grid security.

</details>


### [199] [Revisiting Graph Contrastive Learning on Anomaly Detection: A Structural Imbalance Perspective](https://arxiv.org/abs/2507.14677)
*Yiming Xu,Zhen Peng,Bin Shi,Xu Hua,Bo Dong,Song Wang,Chen Chen*

Main category: cs.LG

TL;DR: 论文提出了一种名为AD-GCL的新型图对比学习框架，旨在解决现有图对比学习在异常检测中对结构不平衡的鲁棒性问题，特别是对低度异常节点的检测。


<details>
  <summary>Details</summary>
Motivation: 现有图对比学习模型在异常检测中过于关注整体性能，而忽视了结构不平衡的鲁棒性，尤其是对低度异常节点的检测不足，限制了其在高风险场景中的应用。

Method: AD-GCL通过邻居修剪策略过滤噪声边，并通过从头部节点对齐伪造尾部节点来促进尾部节点的检测；同时通过异常引导的邻居补全扩大尾部节点的感知范围，并引入原始图和增强图的内外一致性损失以增强表示。

Result: 在多个数据集上对整体、头部和尾部节点的性能评估表明，AD-GCL在检测头部和尾部异常方面均表现出全面优势。

Conclusion: AD-GCL框架有效提升了图异常检测对结构不平衡的鲁棒性，特别是在低度异常节点的检测上表现突出，为高风险场景提供了更可靠的解决方案。

Abstract: The superiority of graph contrastive learning (GCL) has prompted its
application to anomaly detection tasks for more powerful risk warning systems.
Unfortunately, existing GCL-based models tend to excessively prioritize overall
detection performance while neglecting robustness to structural imbalance,
which can be problematic for many real-world networks following power-law
degree distributions. Particularly, GCL-based methods may fail to capture tail
anomalies (abnormal nodes with low degrees). This raises concerns about the
security and robustness of current anomaly detection algorithms and therefore
hinders their applicability in a variety of realistic high-risk scenarios. To
the best of our knowledge, research on the robustness of graph anomaly
detection to structural imbalance has received little scrutiny. To address the
above issues, this paper presents a novel GCL-based framework named AD-GCL. It
devises the neighbor pruning strategy to filter noisy edges for head nodes and
facilitate the detection of genuine tail nodes by aligning from head nodes to
forged tail nodes. Moreover, AD-GCL actively explores potential neighbors to
enlarge the receptive field of tail nodes through anomaly-guided neighbor
completion. We further introduce intra- and inter-view consistency loss of the
original and augmentation graph for enhanced representation. The performance
evaluation of the whole, head, and tail nodes on multiple datasets validates
the comprehensive superiority of the proposed AD-GCL in detecting both head
anomalies and tail anomalies.

</details>


### [200] [GCC-Spam: Spam Detection via GAN, Contrastive Learning, and Character Similarity Networks](https://arxiv.org/abs/2507.14679)
*Zixin Xu,Zhijie Wang,Zhiyuan Pan*

Main category: cs.LG

TL;DR: 论文提出了一种名为GCC-Spam的新型垃圾文本检测框架，通过字符相似性网络、对比学习和GAN生成伪样本，解决了对抗性攻击和标注数据稀缺问题，实验表明其性能优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 互联网上垃圾文本的指数增长带来了信息泄露和社会不稳定等风险，需要强大的检测机制。本文旨在解决垃圾邮件发送者的对抗策略和标注数据稀缺两大挑战。

Method: 提出GCC-Spam框架，包含三个创新点：1) 字符相似性网络捕捉拼写和语音特征；2) 对比学习优化潜在空间距离；3) GAN生成伪样本缓解数据稀缺。

Result: 在真实数据集上的实验表明，GCC-Spam在检测率和标注数据效率上均优于基线方法。

Conclusion: GCC-Spam通过多技术融合有效提升了垃圾文本检测的准确性和鲁棒性，尤其在数据稀缺场景下表现突出。

Abstract: The exponential growth of spam text on the Internet necessitates robust
detection mechanisms to mitigate risks such as information leakage and social
instability. This work addresses two principal challenges: adversarial
strategies employed by spammers and the scarcity of labeled data. We propose a
novel spam-text detection framework GCC-Spam, which integrates three core
innovations. First, a character similarity network captures orthographic and
phonetic features to counter character-obfuscation attacks and furthermore
produces sentence embeddings for downstream classification. Second, contrastive
learning enhances discriminability by optimizing the latent-space distance
between spam and normal texts. Third, a Generative Adversarial Network (GAN)
generates realistic pseudo-spam samples to alleviate data scarcity while
improving model robustness and classification accuracy. Extensive experiments
on real-world datasets demonstrate that our model outperforms baseline
approaches, achieving higher detection rates with significantly fewer labeled
examples.

</details>


### [201] [Spatial-Temporal Transformer with Curriculum Learning for EEG-Based Emotion Recognition](https://arxiv.org/abs/2507.14698)
*Xuetao Lin,Tianhao Peng,Peihong Dai,Yu Liang,Wenjun Wu*

Main category: cs.LG

TL;DR: 论文提出了一种结合空间-时间变换器和课程学习的框架SST-CL，用于解决EEG情感识别中的非平稳时空模式整合和动态情感强度适应问题。


<details>
  <summary>Details</summary>
Motivation: EEG情感识别在实际应用中面临非平稳时空模式整合和动态情感强度变化的挑战，需要一种更有效的方法。

Method: SST-CL框架包含空间编码器和时间编码器，分别建模通道间关系和多尺度时间依赖，并结合强度感知课程学习策略动态调整训练样本。

Result: 在三个基准数据集上实现了最先进的性能，消融实验验证了框架各部分的必要性。

Conclusion: SST-CL框架有效整合了EEG信号的时空特征，并通过课程学习提升了动态情感强度的适应性。

Abstract: EEG-based emotion recognition plays an important role in developing adaptive
brain-computer communication systems, yet faces two fundamental challenges in
practical implementations: (1) effective integration of non-stationary
spatial-temporal neural patterns, (2) robust adaptation to dynamic emotional
intensity variations in real-world scenarios. This paper proposes SST-CL, a
novel framework integrating spatial-temporal transformers with curriculum
learning. Our method introduces two core components: a spatial encoder that
models inter-channel relationships and a temporal encoder that captures
multi-scale dependencies through windowed attention mechanisms, enabling
simultaneous extraction of spatial correlations and temporal dynamics from EEG
signals. Complementing this architecture, an intensity-aware curriculum
learning strategy progressively guides training from high-intensity to
low-intensity emotional states through dynamic sample scheduling based on a
dual difficulty assessment. Comprehensive experiments on three benchmark
datasets demonstrate state-of-the-art performance across various emotional
intensity levels, with ablation studies confirming the necessity of both
architectural components and the curriculum learning mechanism.

</details>


### [202] [Fraud is Not Just Rarity: A Causal Prototype Attention Approach to Realistic Synthetic Oversampling](https://arxiv.org/abs/2507.14706)
*Claudio Giusti,Luca Guarnera,Mirko Casu,Sebastiano Battiato*

Main category: cs.LG

TL;DR: 提出了一种名为CPAC的新型架构，结合VAE-GAN和原型注意力机制，用于信用卡欺诈检测，显著提升了分类性能和潜在空间分离。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如GANs、VAEs）在生成少数类样本时容易导致分类器过度自信和潜在空间分离不佳，限制了实际检测效果。

Method: 提出CPAC架构，结合原型注意力机制和VAE-GAN，通过分类器引导的潜在空间优化提升性能。

Result: CPAC在F1-score（93.14%）和召回率（90.18%）上表现优异，同时改善了潜在空间分离。

Conclusion: CPAC通过分类器驱动的表示学习，显著提升了欺诈检测性能，为未来研究提供了新思路。

Abstract: Detecting fraudulent credit card transactions remains a significant
challenge, due to the extreme class imbalance in real-world data and the often
subtle patterns that separate fraud from legitimate activity. Existing research
commonly attempts to address this by generating synthetic samples for the
minority class using approaches such as GANs, VAEs, or hybrid generative
models. However, these techniques, particularly when applied only to
minority-class data, tend to result in overconfident classifiers and poor
latent cluster separation, ultimately limiting real-world detection
performance. In this study, we propose the Causal Prototype Attention
Classifier (CPAC), an interpretable architecture that promotes class-aware
clustering and improved latent space structure through prototype-based
attention mechanisms and we will couple it with the encoder in a VAE-GAN
allowing it to offer a better cluster separation moving beyond post-hoc sample
augmentation. We compared CPAC-augmented models to traditional oversamplers,
such as SMOTE, as well as to state-of-the-art generative models, both with and
without CPAC-based latent classifiers. Our results show that classifier-guided
latent shaping with CPAC delivers superior performance, achieving an F1-score
of 93.14\% percent and recall of 90.18\%, along with improved latent cluster
separation. Further ablation studies and visualizations provide deeper insight
into the benefits and limitations of classifier-driven representation learning
for fraud detection. The codebase for this work will be available at final
submission.

</details>


### [203] [Exploring the Dynamic Scheduling Space of Real-Time Generative AI Applications on Emerging Heterogeneous Systems](https://arxiv.org/abs/2507.14715)
*Rachid Karami,Rajeev Patwari,Hyoukjun Kwon,Ashish Sirasao*

Main category: cs.LG

TL;DR: 论文研究了实时生成AI（RTGen）工作负载在异构SoC上的调度策略及其性能影响，强调了动态异构调度的重要性。


<details>
  <summary>Details</summary>
Motivation: 随着生成AI模型（如LLMs）在实时多模型应用（如视频会议和游戏）中的集成，RTGen工作负载的需求增加，但其在异构SoC上的调度复杂性和性能影响尚未充分研究。

Method: 研究在AMD的Ryzen AI异构SoC上对RTGen工作负载进行全面表征，构建多模型场景，评估五种调度策略对实时指标和LLM性能的影响。

Result: 调度决策显著影响性能（如平均导致41.7%的截止时间违规率差异），需动态异构调度策略以适应工作负载和硬件特性。

Conclusion: 动态异构调度对实现高性能设备端RTGen应用至关重要。

Abstract: The integration of generative AI models, particularly large language models
(LLMs), into real-time multi-model AI applications such as video conferencing
and gaming is giving rise to a new class of workloads: real-time generative AI
(RTGen). These workloads combine the compute intensity and dynamic execution
patterns of generative models with the stringent latency and concurrency
constraints of real-time inference. To meet the diverse demands of RTGen
workloads, modern edge platforms increasingly adopt heterogeneous
system-on-chip (SoC) architectures that integrate CPUs, GPUs, and NPUs. Despite
the potential of heterogeneous SoC, the scheduling space complexity and
performance implications of RTGen workloads on such platforms remain
underexplored. In this work, we perform a comprehensive characterization of
RTGen workloads on AMD's latest heterogeneous SoC, Ryzen AI. We construct
realistic multi-model scenarios inspired by industry use cases and profile
model performance across all available backends. Using this data, we evaluate
five scheduling policies and their impact on both real-time metrics (e.g.,
deadline violation rate) and LLM performance (e.g., time-to-first-token and
tokens-per-second). Our results show that scheduling decisions significantly
affect workload performance (e.g., leading to a 41.7% difference in deadline
violation rates on average), and highlight the need for scheduling strategies
that are aware of workload dynamics and hardware heterogeneity. Our findings
underscore the importance of workload-aware, dynamic heterogeneous scheduling
in enabling high-performance, on-device RTGen applications.

</details>


### [204] [LeanTree: Accelerating White-Box Proof Search with Factorized States in Lean 4](https://arxiv.org/abs/2507.14722)
*Matěj Kripner,Michal Šustr,Milan Straka*

Main category: cs.LG

TL;DR: 论文介绍了LeanTree，一种基于Lean 4语言的白盒工具，用于分解复杂证明状态，并提供了相关数据集，展示了白盒方法在自动定理证明中的优势。


<details>
  <summary>Details</summary>
Motivation: 自动定理证明（ATP）因其庞大的状态和动作空间而具有挑战性。虽然大语言模型（LLMs）为ATP提供了启发式方法，但缺乏正确性保证，需要与证明验证器交互。白盒方法在此领域相对滞后，论文旨在填补这一空白。

Method: 论文提出LeanTree，包括一个基于Lean 4语言的工具，用于分解复杂证明状态为独立分支，以及一个包含这些分解状态的数据集。

Result: 初步结果表明，白盒方法在某些情况下优于黑盒方法。

Conclusion: LeanTree通过简化评估、减少上下文需求、生成丰富训练数据等优势，展示了白盒方法在ATP中的潜力。

Abstract: Automated theorem proving (ATP) has been a classical problem in artificial
intelligence since its inception, yet it remains challenging due to its vast
state and action space. Large language models (LLMs) have recently emerged as a
promising heuristic for ATP, but they lack correctness guarantees and thus
require interaction with a proof verifier. Such interactions typically follow
one of two approaches: black-box interaction, which does not utilize
intermediate proof states, or white-box approaches, which allow for incremental
proof construction and examination of intermediate states. While black-box
approaches have directly benefited from recent LLM advances, white-box methods
have comparatively lagged behind. In this paper, we address this gap by
introducing LeanTree, which consists of (i) a tool built in the Lean 4 language
that factorizes complex proof states into simpler, independent branches, and
(ii) a dataset of these factorized intermediate states. Our white-box tooling
offers several advantages over black-box approaches: it simplifies evaluation,
reduces necessary context, generates richer training data, enables parallel
search across multiple states, supports efficient reuse of states, and provides
feedback in case of errors. Our preliminary results hint that white-box
approaches outperform black-box alternatives in some settings.

</details>


### [205] [Task-Agnostic Continual Prompt Tuning with Gradient-Based Selection and Decoding](https://arxiv.org/abs/2507.14725)
*Anushka Tiwari,Sayantan Pal,Rohini K. Srihari,Kaiyi Ji*

Main category: cs.LG

TL;DR: GRID是一个统一的持续学习框架，解决了任务无关推理下的潜在遗忘和提示内存爆炸问题，通过任务感知解码和梯度提示选择策略，显著提升后向迁移并减少遗忘任务。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的持续学习方法假设任务感知推理并维护任务特定提示列表，限制了可扩展性并隐藏潜在遗忘。

Method: GRID结合任务感知解码机制（利用代表性输入、自动任务识别和约束解码）和梯度提示选择策略（压缩低信息量提示为聚合表示）。

Result: 在多个基准测试中，GRID显著提升后向迁移，减少遗忘任务达80%，优于现有方法。

Conclusion: GRID为大规模语言模型提供了一种高效、可扩展的持续学习解决方案。

Abstract: Prompt-based continual learning (CL) offers a parameter-efficient way to
adapt large language models (LLMs) across task sequences. However, most
existing methods assume task-aware inference and maintain a growing list of
task-specific prompts, which limits scalability and hides latent forgetting. In
this work, we introduce GRID, a unified framework that addresses two key
limitations: (1) latent forgetting under task-agnostic inference, and (2)
prompt memory explosion as task sequences grow. GRID integrates a task-aware
decoding mechanism that improves backward transfer by leveraging representative
inputs, automatic task identification, and constrained decoding. Additionally,
we propose a gradient-based prompt selection strategy that compresses less
informative prompts into a single aggregated representation, enabling scalable
and memory-efficient lifelong learning. Extensive experiments across
short-sequence, long-sequence, and negative transfer benchmarks show that GRID
significantly improves backward transfer, achieves competitive forward
transfer, and reduces forgotten tasks by up to 80\%, outperforming
state-of-the-art methods on T5 and Flan-T5 backbones.

</details>


### [206] [Balancing Expressivity and Robustness: Constrained Rational Activations for Reinforcement Learning](https://arxiv.org/abs/2507.14736)
*Rafał Surdej,Michał Bortkiewicz,Alex Lewandowski,Mateusz Ostaszewski,Clare Lyle*

Main category: cs.LG

TL;DR: 研究可训练有理激活函数在强化学习和持续学习中的表现，发现其灵活性与稳定性之间存在权衡，并提出约束变体以改善性能。


<details>
  <summary>Details</summary>
Motivation: 探索可训练有理激活函数在动态环境中的表现，尤其是其对训练稳定性的影响。

Method: 提出一种约束变体，限制输出缩放以平衡表达性和稳定性，并在MetaWorld和DMC环境中进行实验。

Result: 约束变体在强化学习和持续学习中提高了稳定性和性能，但在离散动作领域（如Atari）未观察到类似问题。

Conclusion: 研究为动态环境中设计稳健且适应性强的可训练激活函数提供了实用原则。

Abstract: Trainable activation functions, whose parameters are optimized alongside
network weights, offer increased expressivity compared to fixed activation
functions. Specifically, trainable activation functions defined as ratios of
polynomials (rational functions) have been proposed to enhance plasticity in
reinforcement learning. However, their impact on training stability remains
unclear. In this work, we study trainable rational activations in both
reinforcement and continual learning settings. We find that while their
flexibility enhances adaptability, it can also introduce instability, leading
to overestimation in RL and feature collapse in longer continual learning
scenarios. Our main result is demonstrating a trade-off between expressivity
and plasticity in rational activations. To address this, we propose a
constrained variant that structurally limits excessive output scaling while
preserving adaptability. Experiments across MetaWorld and DeepMind Control
Suite (DMC) environments show that our approach improves training stability and
performance. In continual learning benchmarks, including MNIST with reshuffled
labels and Split CIFAR-100, we reveal how different constraints affect the
balance between expressivity and long-term retention. While preliminary
experiments in discrete action domains (e.g., Atari) did not show similar
instability, this suggests that the trade-off is particularly relevant for
continuous control. Together, our findings provide actionable design principles
for robust and adaptable trainable activations in dynamic, non-stationary
environments. Code available at:
https://github.com/special114/rl_rational_plasticity.

</details>


### [207] [Better Training Data Attribution via Better Inverse Hessian-Vector Products](https://arxiv.org/abs/2507.14740)
*Andrew Wang,Elisa Nguyen,Runshi Yang,Juhan Bae,Sheila A. McIlraith,Roger Grosse*

Main category: cs.LG

TL;DR: ASTRA算法通过EKFAC预条件器和Neumann级数迭代，高效近似逆Hessian-向量积（iHVP），显著提升训练数据归因（TDA）性能。


<details>
  <summary>Details</summary>
Motivation: 传统梯度TDA方法（如影响函数和展开微分）在计算iHVP时效率低且难以近似，ASTRA旨在解决这一问题。

Method: ASTRA结合EKFAC预条件器和Neumann级数迭代，优化iHVP近似计算，调参简单且迭代次数少。

Result: ASTRA比EKFAC近似更准确，显著提升TDA性能。

Conclusion: ASTRA为高效准确的TDA提供了新方法，iHVP近似精度对TDA至关重要。

Abstract: Training data attribution (TDA) provides insights into which training data is
responsible for a learned model behavior. Gradient-based TDA methods such as
influence functions and unrolled differentiation both involve a computation
that resembles an inverse Hessian-vector product (iHVP), which is difficult to
approximate efficiently. We introduce an algorithm (ASTRA) which uses the
EKFAC-preconditioner on Neumann series iterations to arrive at an accurate iHVP
approximation for TDA. ASTRA is easy to tune, requires fewer iterations than
Neumann series iterations, and is more accurate than EKFAC-based
approximations. Using ASTRA, we show that improving the accuracy of the iHVP
approximation can significantly improve TDA performance.

</details>


### [208] [Beyond the Single-Best Model: Rashomon Partial Dependence Profile for Trustworthy Explanations in AutoML](https://arxiv.org/abs/2507.14744)
*Mustafa Cavus,Jan N. van Rijn,Przemysław Biecek*

Main category: cs.LG

TL;DR: 论文提出了一种新框架，通过整合多个接近最优模型的PDP来捕捉解释不确定性，提升了模型解释的可靠性和可信度。


<details>
  <summary>Details</summary>
Motivation: 现有自动化机器学习系统通常只关注单一最优模型，忽略了解释不确定性，而这对以人为中心的解释性AI至关重要。

Method: 提出了一种新框架，通过聚合Rashomon集中多个接近最优模型的PDP，生成Rashomon PDP，以捕捉解释变异性。

Result: 实验表明，Rashomon PDP在大多数情况下覆盖不到最佳模型PDP的70%，突显了单一模型解释的局限性。

Conclusion: Rashomon PDP通过补充被忽视的信息，提升了模型解释的可靠性和可信度，尤其适用于高风险领域。

Abstract: Automated machine learning systems efficiently streamline model selection but
often focus on a single best-performing model, overlooking explanation
uncertainty, an essential concern in human centered explainable AI. To address
this, we propose a novel framework that incorporates model multiplicity into
explanation generation by aggregating partial dependence profiles (PDP) from a
set of near optimal models, known as the Rashomon set. The resulting Rashomon
PDP captures interpretive variability and highlights areas of disagreement,
providing users with a richer, uncertainty aware view of feature effects. To
evaluate its usefulness, we introduce two quantitative metrics, the coverage
rate and the mean width of confidence intervals, to evaluate the consistency
between the standard PDP and the proposed Rashomon PDP. Experiments on 35
regression datasets from the OpenML CTR23 benchmark suite show that in most
cases, the Rashomon PDP covers less than 70% of the best model's PDP,
underscoring the limitations of single model explanations. Our findings suggest
that Rashomon PDP improves the reliability and trustworthiness of model
interpretations by adding additional information that would otherwise be
neglected. This is particularly useful in high stakes domains where
transparency and confidence are critical.

</details>


### [209] [Sampling from Gaussian Processes: A Tutorial and Applications in Global Sensitivity Analysis and Optimization](https://arxiv.org/abs/2507.14746)
*Bach Do,Nafeezat A. Ajenifuja,Taiwo A. Adebiyi,Ruda Zhang*

Main category: cs.LG

TL;DR: 论文介绍了两种高斯过程（GP）的后验采样方法——随机傅里叶特征和路径条件采样，并展示了它们在全局敏感性分析（GSA）和多目标优化中的应用。


<details>
  <summary>Details</summary>
Motivation: 高保真模拟和物理实验成本高昂，限制了其在GSA和优化中的应用，因此需要高效的代理模型（如GP）来支持不确定性下的决策。

Method: 提出了随机傅里叶特征和路径条件采样两种方法，用于生成GP的后验样本，并详细描述了其实现过程。

Result: 通过数值实验验证了这两种采样方法在GSA和优化任务中的有效性。

Conclusion: 论文为工程优化中的GP采样提供了实用方法，并展示了其广泛的应用潜力。

Abstract: High-fidelity simulations and physical experiments are essential for
engineering analysis and design. However, their high cost often limits their
applications in two critical tasks: global sensitivity analysis (GSA) and
optimization. This limitation motivates the common use of Gaussian processes
(GPs) as proxy regression models to provide uncertainty-aware predictions based
on a limited number of high-quality observations. GPs naturally enable
efficient sampling strategies that support informed decision-making under
uncertainty by extracting information from a subset of possible functions for
the model of interest. Despite their popularity in machine learning and
statistics communities, sampling from GPs has received little attention in the
community of engineering optimization. In this paper, we present the
formulation and detailed implementation of two notable sampling methods --
random Fourier features and pathwise conditioning -- for generating posterior
samples from GPs. Alternative approaches are briefly described. Importantly, we
detail how the generated samples can be applied in GSA, single-objective
optimization, and multi-objective optimization. We show successful applications
of these sampling methods through a series of numerical examples.

</details>


### [210] [Pruning Increases Orderedness in Recurrent Computation](https://arxiv.org/abs/2507.14747)
*Yiding Song*

Main category: cs.LG

TL;DR: 论文探讨了方向性作为人工神经网络的归纳偏置的作用，通过修剪技术诱导方向性，而非硬编码。


<details>
  <summary>Details</summary>
Motivation: 受生物大脑中循环电路的启发，研究方向性是否对人工神经网络有帮助。

Method: 提出一种全连接感知层（等同于权重绑定的循环神经网络），并通过修剪技术诱导方向性。

Result: 修剪方案成功诱导神经元间信息流的拓扑排序，且不影响性能。

Conclusion: 方向性并非学习的必要条件，但可能是梯度下降和稀疏化可发现的有利归纳偏置。

Abstract: Inspired by the prevalence of recurrent circuits in biological brains, we
investigate the degree to which directionality is a helpful inductive bias for
artificial neural networks. Taking directionality as topologically-ordered
information flow between neurons, we formalise a perceptron layer with
all-to-all connections (mathematically equivalent to a weight-tied recurrent
neural network) and demonstrate that directionality, a hallmark of modern
feed-forward networks, can be induced rather than hard-wired by applying
appropriate pruning techniques. Across different random seeds our pruning
schemes successfully induce greater topological ordering in information flow
between neurons without compromising performance, suggesting that
directionality is not a prerequisite for learning, but may be an advantageous
inductive bias discoverable by gradient descent and sparsification.

</details>


### [211] [Skill Learning via Policy Diversity Yields Identifiable Representations for Reinforcement Learning](https://arxiv.org/abs/2507.14748)
*Patrik Reizinger,Bálint Mucsányi,Siyuan Guo,Benjamin Eysenbach,Bernhard Schölkopf,Wieland Brendel*

Main category: cs.LG

TL;DR: 论文研究了自监督特征学习和预训练方法在强化学习（RL）中的应用，特别是基于互信息技能学习（MISL）的方法，并提出了对比后继特征（CSF）方法的理论保证。


<details>
  <summary>Details</summary>
Motivation: 理解MISL中表示和互信息参数化的理论作用，填补现有研究的空白。

Method: 通过对比后继特征（CSF）方法，从可识别表示学习的角度分析MISL，证明CSF能够恢复环境的真实特征。

Result: CSF可以线性变换恢复环境的真实特征，并解释了不同互信息目标和熵正则化的缺点。

Conclusion: CSF为RL中的表示学习提供了首个可识别性保证，并在实验中验证了其有效性。

Abstract: Self-supervised feature learning and pretraining methods in reinforcement
learning (RL) often rely on information-theoretic principles, termed mutual
information skill learning (MISL). These methods aim to learn a representation
of the environment while also incentivizing exploration thereof. However, the
role of the representation and mutual information parametrization in MISL is
not yet well understood theoretically. Our work investigates MISL through the
lens of identifiable representation learning by focusing on the Contrastive
Successor Features (CSF) method. We prove that CSF can provably recover the
environment's ground-truth features up to a linear transformation due to the
inner product parametrization of the features and skill diversity in a
discriminative sense. This first identifiability guarantee for representation
learning in RL also helps explain the implications of different mutual
information objectives and the downsides of entropy regularizers. We
empirically validate our claims in MuJoCo and DeepMind Control and show how CSF
provably recovers the ground-truth features both from states and pixels.

</details>


### [212] [CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories](https://arxiv.org/abs/2507.14766)
*Mehak Arora,Ayman Ali,Kaiyuan Wu,Carolyn Davis,Takashi Shimazui,Mahmoud Alwakeel,Victor Moas,Philip Yang,Annette Esper,Rishikesan Kamaleswaran*

Main category: cs.LG

TL;DR: CXR-TFT是一种新型多模态框架，结合稀疏时间点的胸片和临床数据，预测重症患者胸片异常，提前12小时预警。


<details>
  <summary>Details</summary>
Motivation: 现有胸片分析工具无法捕捉时间动态，限制了其在ICU中的应用。CXR-TFT旨在填补这一空白，通过整合多模态数据提升预测能力。

Method: CXR-TFT通过视觉编码器生成潜在嵌入，与高频临床数据对齐，并利用Transformer模型预测未来胸片异常。

Result: 在2万名ICU患者的回顾性研究中，CXR-TFT能提前12小时高准确率预测胸片异常。

Conclusion: CXR-TFT为时间敏感疾病（如急性呼吸窘迫综合征）提供早期干预机会，有望改善临床结果。

Abstract: In intensive care units (ICUs), patients with complex clinical conditions
require vigilant monitoring and prompt interventions. Chest X-rays (CXRs) are a
vital diagnostic tool, providing insights into clinical trajectories, but their
irregular acquisition limits their utility. Existing tools for CXR
interpretation are constrained by cross-sectional analysis, failing to capture
temporal dynamics. To address this, we introduce CXR-TFT, a novel multi-modal
framework that integrates temporally sparse CXR imaging and radiology reports
with high-frequency clinical data, such as vital signs, laboratory values, and
respiratory flow sheets, to predict the trajectory of CXR findings in
critically ill patients. CXR-TFT leverages latent embeddings from a vision
encoder that are temporally aligned with hourly clinical data through
interpolation. A transformer model is then trained to predict CXR embeddings at
each hour, conditioned on previous embeddings and clinical measurements. In a
retrospective study of 20,000 ICU patients, CXR-TFT demonstrated high accuracy
in forecasting abnormal CXR findings up to 12 hours before they became
radiographically evident. This predictive capability in clinical data holds
significant potential for enhancing the management of time-sensitive conditions
like acute respiratory distress syndrome, where early intervention is crucial
and diagnoses are often delayed. By providing distinctive temporal resolution
in prognostic CXR analysis, CXR-TFT offers actionable 'whole patient' insights
that can directly improve clinical outcomes.

</details>


### [213] [Rethinking Memorization Measures and their Implications in Large Language Models](https://arxiv.org/abs/2507.14777)
*Bishwamittra Ghosh,Soumi Das,Qinyuan Wu,Mohammad Aflah Khan,Krishna P. Gummadi,Evimaria Terzi,Deepak Garg*

Main category: cs.LG

TL;DR: 本文研究了大型语言模型（LLM）中的记忆化现象，探讨了记忆化是否可以通过最优学习避免，以及记忆化对隐私的威胁是否被夸大。通过重新审视现有的记忆化测量方法，并提出了新的上下文记忆化概念，研究发现不同记忆化测量方法的结果存在差异，最优学习无法完全避免记忆化，且改进学习会减少某些记忆化形式。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨记忆化是否可以通过最优学习避免，以及记忆化对隐私的威胁是否被夸大。

Method: 重新审视了基于回忆和反事实的记忆化测量方法，并提出了新的上下文记忆化概念。通过实验分析了18种LLM在不同熵的形式语言中的表现。

Result: 研究发现：(a) 不同记忆化测量方法对字符串的记忆化顺序存在分歧；(b) 最优学习无法完全避免训练字符串的部分记忆化；(c) 改进学习会减少上下文和反事实记忆化，但增加基于回忆的记忆化；(d) 部分基于回忆的记忆化字符串实际上不构成隐私威胁。

Conclusion: 结论表明，记忆化是LLM学习过程中不可避免的现象，但其对隐私的威胁可能被高估。上下文记忆化为理解记忆化提供了新的视角。

Abstract: Concerned with privacy threats, memorization in LLMs is often seen as
undesirable, specifically for learning. In this paper, we study whether
memorization can be avoided when optimally learning a language, and whether the
privacy threat posed by memorization is exaggerated or not. To this end, we
re-examine existing privacy-focused measures of memorization, namely
recollection-based and counterfactual memorization, along with a newly proposed
contextual memorization.
  Relating memorization to local over-fitting during learning, contextual
memorization aims to disentangle memorization from the contextual learning
ability of LLMs. Informally, a string is contextually memorized if its
recollection due to training exceeds the optimal contextual recollection, a
learned threshold denoting the best contextual learning without training.
Conceptually, contextual recollection avoids the fallacy of recollection-based
memorization, where any form of high recollection is a sign of memorization.
Theoretically, contextual memorization relates to counterfactual memorization,
but imposes stronger conditions. Memorization measures differ in outcomes and
information requirements.
  Experimenting on 18 LLMs from 6 families and multiple formal languages of
different entropy, we show that (a) memorization measures disagree on
memorization order of varying frequent strings, (b) optimal learning of a
language cannot avoid partial memorization of training strings, and (c)
improved learning decreases contextual and counterfactual memorization but
increases recollection-based memorization. Finally, (d) we revisit existing
reports of memorized strings by recollection that neither pose a privacy threat
nor are contextually or counterfactually memorized.

</details>


### [214] [Omni-Think: Scaling Cross-Domain Generalization in LLMs via Multi-Task RL with Hybrid Rewards](https://arxiv.org/abs/2507.14783)
*Derek Li,Jiaming Zhou,Amirreza Kazemi,Qianyi Sun,Abbas Ghaddar,Mohammad Ali Alomrani,Liheng Ma,Yu Luo,Dong Li,Feng Wen,Jianye Hao,Mark Coates,Yingxue Zhang*

Main category: cs.LG

TL;DR: Omni-Think是一个统一的强化学习框架，通过结合规则奖励和生成偏好信号，提升大语言模型（LLM）在多任务中的性能，并采用课程学习策略优化训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在监督微调（SFT）后泛化能力不足，倾向于记忆而非可迁移学习。

Method: 提出Omni-Think框架，结合规则奖励和LLM-as-a-Judge的生成偏好信号，采用课程学习策略从结构化任务逐步过渡到开放式任务。

Result: 实验结果显示，课程学习比联合训练和模型合并分别提升5.2%和9.1%的性能。

Conclusion: 任务感知采样和混合监督对提升通用LLM的强化学习后训练效果至关重要。

Abstract: The advancement of general-purpose artificial intelligence relies on large
language models (LLMs) that excel across a wide range of tasks, from structured
reasoning to creative generation. However, post-training methods like
Supervised Fine-Tuning (SFT) often struggle with generalization, favoring
memorization over transferable learning. In this work, we introduce Omni-Think,
a unified reinforcement learning (RL) framework that enhances LLM performance
across diverse tasks by combining rule-based verifiable rewards with generative
preference signals via LLM-as-a-Judge evaluations. Our approach enables
consistent optimization across task types and scales RL-based training to
subjective domains. We further investigate training strategies, demonstrating
that a curriculum-based progression that orders tasks from structured to
open-ended improves performance and reduces forgetting. Experimental results
across four domains reveal that curriculum learning improves performance by
5.2\% over joint training and 9.1\% over model merging. These results highlight
the importance of task-aware sampling and hybrid supervision in scaling
RL-based post-training for general-purpose LLMs.

</details>


### [215] [Exploring the In-Context Learning Capabilities of LLMs for Money Laundering Detection in Financial Graphs](https://arxiv.org/abs/2507.14785)
*Erfan Pirmorad*

Main category: cs.LG

TL;DR: 论文探讨了利用大型语言模型（LLM）对金融知识图谱中的局部子图进行推理，以评估洗钱行为的可疑性。


<details>
  <summary>Details</summary>
Motivation: 洗钱行为涉及的实体复杂且相互关联，需要基于图结构数据的推理能力。

Method: 提出了一种轻量级流程，提取目标实体的k跳邻域，将其序列化为结构化文本，并通过少量样本上下文学习提示LLM进行评估和解释。

Result: 实验表明，LLM能够模拟分析师逻辑，识别可疑行为并提供合理解释。

Conclusion: 尽管是探索性研究，但展示了LLM在图推理中的潜力，为可解释的语言驱动金融犯罪分析奠定了基础。

Abstract: The complexity and interconnectivity of entities involved in money laundering
demand investigative reasoning over graph-structured data. This paper explores
the use of large language models (LLMs) as reasoning engines over localized
subgraphs extracted from a financial knowledge graph. We propose a lightweight
pipeline that retrieves k-hop neighborhoods around entities of interest,
serializes them into structured text, and prompts an LLM via few-shot
in-context learning to assess suspiciousness and generate justifications. Using
synthetic anti-money laundering (AML) scenarios that reflect common laundering
behaviors, we show that LLMs can emulate analyst-style logic, highlight red
flags, and provide coherent explanations. While this study is exploratory, it
illustrates the potential of LLM-based graph reasoning in AML and lays
groundwork for explainable, language-driven financial crime analytics.

</details>


### [216] [Flow Equivariant Recurrent Neural Networks](https://arxiv.org/abs/2507.14793)
*T. Anderson Keller*

Main category: cs.LG

TL;DR: 论文提出了一种扩展等变网络理论的方法，使其适用于时间参数化的序列变换（如RNN），并展示了其在训练速度、长度和速度泛化方面的优势。


<details>
  <summary>Details</summary>
Motivation: 现有的等变网络仅适用于静态变换和前馈网络，限制了其在序列模型中的应用。本文旨在将等变理论扩展到时间参数化的变换（如视觉运动）。

Method: 通过分析标准RNN的不足，提出了一种引入流等变性的方法，并在下一步预测和序列分类任务中验证其性能。

Result: 实验表明，流等变模型在训练速度、长度泛化和速度泛化方面显著优于非等变模型。

Conclusion: 本文为构建尊重时间参数化对称性的序列模型迈出了第一步。

Abstract: Data arrives at our senses as a continuous stream, smoothly transforming from
one instant to the next. These smooth transformations can be viewed as
continuous symmetries of the environment that we inhabit, defining equivalence
relations between stimuli over time. In machine learning, neural network
architectures that respect symmetries of their data are called equivariant and
have provable benefits in terms of generalization ability and sample
efficiency. To date, however, equivariance has been considered only for static
transformations and feed-forward networks, limiting its applicability to
sequence models, such as recurrent neural networks (RNNs), and corresponding
time-parameterized sequence transformations. In this work, we extend
equivariant network theory to this regime of `flows' -- one-parameter Lie
subgroups capturing natural transformations over time, such as visual motion.
We begin by showing that standard RNNs are generally not flow equivariant:
their hidden states fail to transform in a geometrically structured manner for
moving stimuli. We then show how flow equivariance can be introduced, and
demonstrate that these models significantly outperform their non-equivariant
counterparts in terms of training speed, length generalization, and velocity
generalization, on both next step prediction and sequence classification. We
present this work as a first step towards building sequence models that respect
the time-parameterized symmetries which govern the world around us.

</details>


### [217] [Subliminal Learning: Language models transmit behavioral traits via hidden signals in data](https://arxiv.org/abs/2507.14805)
*Alex Cloud,Minh Le,James Chua,Jan Betley,Anna Sztyber-Betley,Jacob Hilton,Samuel Marks,Owain Evans*

Main category: cs.LG

TL;DR: 研究发现语言模型可以通过语义无关的数据传递行为特征，称为“潜意识学习”。即使过滤掉相关特征，学生模型仍能从教师模型生成的数据中学习到这些特征。


<details>
  <summary>Details</summary>
Motivation: 探索语言模型是否能够通过看似无关的数据传递行为特征，揭示AI开发中潜在的风险。

Method: 通过教师模型生成仅包含数字序列的数据集，训练学生模型，并观察其是否学习到教师模型的行为特征。实验还扩展到代码和推理痕迹数据。

Result: 学生模型确实学习到了教师模型的行为特征，即使数据中未明确提及这些特征。不同基模型之间未观察到类似现象。

Conclusion: 潜意识学习是一种普遍现象，可能成为AI开发中的潜在风险，尤其是在模型蒸馏过程中。

Abstract: We study subliminal learning, a surprising phenomenon where language models
transmit behavioral traits via semantically unrelated data. In our main
experiments, a "teacher" model with some trait T (such as liking owls or being
misaligned) generates a dataset consisting solely of number sequences.
Remarkably, a "student" model trained on this dataset learns T. This occurs
even when the data is filtered to remove references to T. We observe the same
effect when training on code or reasoning traces generated by the same teacher
model. However, we do not observe the effect when the teacher and student have
different base models. To help explain our findings, we prove a theoretical
result showing that subliminal learning occurs in all neural networks under
certain conditions, and demonstrate subliminal learning in a simple MLP
classifier. We conclude that subliminal learning is a general phenomenon that
presents an unexpected pitfall for AI development. Distillation could propagate
unintended traits, even when developers try to prevent this via data filtering.

</details>


### [218] [Benchmarking Foundation Models with Multimodal Public Electronic Health Records](https://arxiv.org/abs/2507.14824)
*Kunyu Yu,Rui Yang,Jingchi Liao,Siqi Li,Huitao Li,Irene Li,Yifan Peng,Rishikesan Kamaleswaran,Nan Liu*

Main category: cs.LG

TL;DR: 本文提出了一种基于MIMIC-IV数据库的基准测试，评估了基础模型在电子健康记录（EHRs）处理中的性能、公平性和可解释性，并展示了多模态数据对预测性能的提升。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHRs）的多样性和复杂性需要灵活的处理方法，基础模型为此提供了潜力。本研究旨在评估这些模型在多模态数据中的表现，以支持临床AI系统的开发。

Method: 研究开发了标准化的数据处理流程，并在MIMIC-IV数据库上系统比较了八种基础模型，包括单模态和多模态模型，以及领域专用和通用模型。

Result: 结果表明，多模态数据的整合能显著提升预测性能，且未引入额外偏差。

Conclusion: 该基准测试为开发可信赖的多模态AI系统提供了支持，代码已开源。

Abstract: Foundation models have emerged as a powerful approach for processing
electronic health records (EHRs), offering flexibility to handle diverse
medical data modalities. In this study, we present a comprehensive benchmark
that evaluates the performance, fairness, and interpretability of foundation
models, both as unimodal encoders and as multimodal learners, using the
publicly available MIMIC-IV database. To support consistent and reproducible
evaluation, we developed a standardized data processing pipeline that
harmonizes heterogeneous clinical records into an analysis-ready format. We
systematically compared eight foundation models, encompassing both unimodal and
multimodal models, as well as domain-specific and general-purpose variants. Our
findings demonstrate that incorporating multiple data modalities leads to
consistent improvements in predictive performance without introducing
additional bias. Through this benchmark, we aim to support the development of
effective and trustworthy multimodal artificial intelligence (AI) systems for
real-world clinical applications. Our code is available at
https://github.com/nliulab/MIMIC-Multimodal.

</details>


### [219] [eMargin: Revisiting Contrastive Learning with Margin-Based Separation](https://arxiv.org/abs/2507.14828)
*Abdul-Kazeem Shamba,Kerstin Bach,Gavin Taylor*

Main category: cs.LG

TL;DR: 研究探讨了在对比学习框架中引入自适应边界（eMargin）对时间序列表示学习的影响，发现其在无监督聚类指标上表现优异，但在下游分类任务中效果有限。


<details>
  <summary>Details</summary>
Motivation: 探索自适应边界是否能改善时间序列中相邻但不相似时间步的分离，从而提升下游任务性能。

Method: 在对比损失函数中引入自适应边界（eMargin），基于预设相似度阈值调整，并在三个基准数据集上评估其对聚类和分类的影响。

Result: eMargin在无监督聚类指标上优于基线方法，但在下游分类任务中表现不佳。

Conclusion: 无监督聚类指标的高分不一定能转化为下游任务的有效性，自适应边界在特定任务中需谨慎使用。

Abstract: We revisit previous contrastive learning frameworks to investigate the effect
of introducing an adaptive margin into the contrastive loss function for time
series representation learning. Specifically, we explore whether an adaptive
margin (eMargin), adjusted based on a predefined similarity threshold, can
improve the separation between adjacent but dissimilar time steps and
subsequently lead to better performance in downstream tasks. Our study
evaluates the impact of this modification on clustering performance and
classification in three benchmark datasets. Our findings, however, indicate
that achieving high scores on unsupervised clustering metrics does not
necessarily imply that the learned embeddings are meaningful or effective in
downstream tasks. To be specific, eMargin added to InfoNCE consistently
outperforms state-of-the-art baselines in unsupervised clustering metrics, but
struggles to achieve competitive results in downstream classification with
linear probing. The source code is publicly available at
https://github.com/sfi-norwai/eMargin.

</details>


### [220] [The Invisible Leash: Why RLVR May Not Escape Its Origin](https://arxiv.org/abs/2507.14843)
*Fang Wu,Weihao Xuan,Ximing Lu,Zaid Harchaoui,Yejin Choi*

Main category: cs.LG

TL;DR: RLVR（带可验证奖励的强化学习）虽能提升AI在复杂逻辑任务中的表现，但其是否能扩展模型的推理边界尚不明确。研究发现RLVR受限于基础模型的支持范围，可能限制原创解决方案的发现，并存在熵-奖励权衡。实验表明RLVR虽提高精度，但可能忽略正确但低概率的答案。


<details>
  <summary>Details</summary>
Motivation: 探讨RLVR是否真正扩展模型的推理能力，还是仅放大已知高奖励输出以提高精度。

Method: 通过理论和实证研究，分析RLVR的局限性，包括基础模型支持范围的约束和熵-奖励权衡。

Result: RLVR虽提升pass@1，但支持范围缩小可能忽略正确答案；生成步骤熵增但答案熵减。

Conclusion: RLVR在扩展推理能力上存在潜在限制，未来需结合探索机制或混合策略突破限制。

Abstract: Recent advances in large reasoning models highlight Reinforcement Learning
with Verifiable Rewards (RLVR) as a promising method for enhancing AI's
capabilities, particularly in solving complex logical tasks. However, it
remains unclear whether RLVR truly expands a model's reasoning boundary or
merely amplifies high-reward outputs that the base model already knows for
improved precision. This study presents a theoretical and empirical
investigation that provides fresh insights into the potential limits of RLVR.
First, we offer a new theoretical perspective that RLVR is constrained by the
base model's support-unable to sample solutions with zero initial
probability-and operates as a conservative reweighting mechanism that may
restrict the discovery of entirely original solutions. We also identify an
entropy-reward tradeoff: while RLVR reliably enhances precision, it may
progressively narrow exploration and potentially overlook correct yet
underrepresented solutions. Extensive empirical experiments validate that while
RLVR consistently improves pass@1, the shrinkage of empirical support generally
outweighs the expansion of empirical support under larger sampling budgets,
failing to recover correct answers that were previously accessible to the base
model. Interestingly, we also observe that while RLVR sometimes increases
token-level entropy, resulting in greater uncertainty at each generation step,
answer-level entropy declines, indicating that these seemingly more uncertain
paths ultimately converge onto a smaller set of distinct answers. Taken
together, these findings reveal potential limits of RLVR in extending reasoning
horizons. Breaking this invisible leash may require future algorithmic
innovations such as explicit exploration mechanisms or hybrid strategies that
seed probability mass into underrepresented solution regions.

</details>


### [221] [Time-Aware Attention for Enhanced Electronic Health Records Modeling](https://arxiv.org/abs/2507.14847)
*Junhan Yu,Zhunyi Feng,Junwei Lu,Tianxi Cai,Doudou Zhou*

Main category: cs.LG

TL;DR: TALE-EHR是一个基于Transformer的框架，通过时间感知注意力机制和预训练语言模型嵌入，解决了EHR数据异质性和时间模式复杂性问题，显著提升了疾病进展预测性能。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）包含丰富的临床信息，但数据异质性和复杂时间模式使其建模困难。现有方法难以处理临床事件间的不规则时间间隔。

Method: 提出TALE-EHR框架，结合时间感知注意力机制和预训练语言模型生成的嵌入，以建模连续时间间隔和临床语义。

Result: 在MIMIC-IV和PIC数据集上，TALE-EHR在疾病进展预测等任务中优于现有方法。

Conclusion: TALE-EHR通过结合时间建模和语义表示，为EHR分析提供了高效解决方案。

Abstract: Electronic Health Records (EHR) contain valuable clinical information for
predicting patient outcomes and guiding healthcare decisions. However,
effectively modeling Electronic Health Records (EHRs) requires addressing data
heterogeneity and complex temporal patterns. Standard approaches often struggle
with irregular time intervals between clinical events. We propose TALE-EHR, a
Transformer-based framework featuring a novel time-aware attention mechanism
that explicitly models continuous temporal gaps to capture fine-grained
sequence dynamics. To complement this temporal modeling with robust semantics,
TALE-EHR leverages embeddings derived from standardized code descriptions using
a pre-trained Large Language Model (LLM), providing a strong foundation for
understanding clinical concepts. Experiments on the MIMIC-IV and PIC dataset
demonstrate that our approach outperforms state-of-the-art baselines on tasks
such as disease progression forecasting. TALE-EHR underscores the benefit of
integrating explicit, continuous temporal modeling with strong semantic
representations provides a powerful solution for advancing EHR analysis.

</details>


### [222] [Hierarchical Multi-Agent Reinforcement Learning with Control Barrier Functions for Safety-Critical Autonomous Systems](https://arxiv.org/abs/2507.14850)
*H. M. Sabbir Ahmad,Ehsan Sabouni,Alexander Wasilkoff,Param Budhraja,Zijian Guo,Songyuan Zhang,Chuchu Fan,Christos Cassandras,Wenchao Li*

Main category: cs.LG

TL;DR: 提出了一种基于控制屏障函数（CBFs）的分层多智能体强化学习（HMARL）方法，用于安全关键的多智能体系统，确保安全性和协作性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体安全关键系统中，如何在满足安全需求的同时实现协作的问题。

Method: 采用分层强化学习方法，高层学习联合协作行为，低层学习基于高层策略的安全个体行为，并结合CBFs确保安全性。

Result: 在复杂环境中显著提高了安全性（接近完美的成功率），同时提升了整体性能。

Conclusion: HMARL-CBF方法在多智能体安全关键系统中表现出色，优于现有方法。

Abstract: We address the problem of safe policy learning in multi-agent safety-critical
autonomous systems. In such systems, it is necessary for each agent to meet the
safety requirements at all times while also cooperating with other agents to
accomplish the task. Toward this end, we propose a safe Hierarchical
Multi-Agent Reinforcement Learning (HMARL) approach based on Control Barrier
Functions (CBFs). Our proposed hierarchical approach decomposes the overall
reinforcement learning problem into two levels learning joint cooperative
behavior at the higher level and learning safe individual behavior at the lower
or agent level conditioned on the high-level policy. Specifically, we propose a
skill-based HMARL-CBF algorithm in which the higher level problem involves
learning a joint policy over the skills for all the agents and the lower-level
problem involves learning policies to execute the skills safely with CBFs. We
validate our approach on challenging environment scenarios whereby a large
number of agents have to safely navigate through conflicting road networks.
Compared with existing state of the art methods, our approach significantly
improves the safety achieving near perfect (within 5%) success/safety rate
while also improving performance across all the environments.

</details>


### [223] [The Tsetlin Machine Goes Deep: Logical Learning and Reasoning With Graphs](https://arxiv.org/abs/2507.14874)
*Ole-Christoffer Granmo,Youmna Abdelwahab,Per-Arne Andersen,Paul F. A. Clarke,Kunal Dumbre,Ylva Grønninsæter,Vojtech Halenka,Runar Helin,Lei Jiao,Ahmed Khalid,Rebekka Omslandseter,Rupsa Saha,Mayur Shende,Xuan Zhang*

Main category: cs.LG

TL;DR: Graph Tsetlin Machine (GraphTM) 是一种基于图结构输入的深度学习模型，通过消息传递构建嵌套深度子句，显著提升解释性和数据利用率，并在多个领域（如图像分类、推荐系统等）表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 传统 Tsetlin Machine (TM) 在处理图结构数据时存在局限性，GraphTM 旨在扩展 TM 的能力，使其支持序列、网格、关系和多种模态的输入，同时保持解释性和高效性。

Method: GraphTM 通过消息传递机制构建嵌套深度子句，识别子图模式，从而减少子句数量并提升数据利用率。

Result: GraphTM 在多个任务中表现优异：CIFAR-10 图像分类准确率比卷积 TM 高 3.86%；在动作共指任务中优于其他强化学习方法 20.6%；在推荐系统中对噪声的容忍度高于 GCN；在病毒基因组序列数据上与 BiLSTM-CNN 和 GCN 竞争，且训练速度更快。

Conclusion: GraphTM 展示了图表示学习和深度子句为 TM 学习带来的新可能性，适用于多样化领域，同时保持了模型的解释性和高效性。

Abstract: Pattern recognition with concise and flat AND-rules makes the Tsetlin Machine
(TM) both interpretable and efficient, while the power of Tsetlin automata
enables accuracy comparable to deep learning on an increasing number of
datasets. We introduce the Graph Tsetlin Machine (GraphTM) for learning
interpretable deep clauses from graph-structured input. Moving beyond flat,
fixed-length input, the GraphTM gets more versatile, supporting sequences,
grids, relations, and multimodality. Through message passing, the GraphTM
builds nested deep clauses to recognize sub-graph patterns with exponentially
fewer clauses, increasing both interpretability and data utilization. For image
classification, GraphTM preserves interpretability and achieves 3.86%-points
higher accuracy on CIFAR-10 than a convolutional TM. For tracking action
coreference, faced with increasingly challenging tasks, GraphTM outperforms
other reinforcement learning methods by up to 20.6%-points. In recommendation
systems, it tolerates increasing noise to a greater extent than a Graph
Convolutional Neural Network (GCN), e.g., for noise ratio 0.1, GraphTM obtains
accuracy 89.86% compared to GCN's 70.87%. Finally, for viral genome sequence
data, GraphTM is competitive with BiLSTM-CNN and GCN accuracy-wise, training
2.5x faster than GCN. The GraphTM's application to these varied fields
demonstrates how graph representation learning and deep clauses bring new
possibilities for TM learning.

</details>


### [224] [Application-Specific Component-Aware Structured Pruning of Deep Neural Networks via Soft Coefficient Optimization](https://arxiv.org/abs/2507.14882)
*Ganesh Sundaram,Jonas Ulmen,Amjad Haider,Daniel Görges*

Main category: cs.LG

TL;DR: 提出了一种改进的重要性度量框架，用于结构化剪枝，以在压缩模型时保持应用特定的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络的高复杂性和计算需求限制了其广泛应用，而传统剪枝方法在保持特定性能方面存在不足。

Method: 采用多种策略确定每组的最佳剪枝幅度，平衡压缩与任务性能，并在MNIST图像重建任务上验证。

Result: 实验表明，该方法能有效保持任务相关性能，即使大幅剪枝后仍满足应用需求。

Conclusion: 提出的框架在压缩模型的同时，成功保留了应用特定的性能特性。

Abstract: Deep neural networks (DNNs) offer significant versatility and performance
benefits, but their widespread adoption is often hindered by high model
complexity and computational demands. Model compression techniques such as
pruning have emerged as promising solutions to these challenges. However, it
remains critical to ensure that application-specific performance
characteristics are preserved during compression. In structured pruning, where
groups of structurally coherent elements are removed, conventional importance
metrics frequently fail to maintain these essential performance attributes. In
this work, we propose an enhanced importance metric framework that not only
reduces model size but also explicitly accounts for application-specific
performance constraints. We employ multiple strategies to determine the optimal
pruning magnitude for each group, ensuring a balance between compression and
task performance. Our approach is evaluated on an autoencoder tasked with
reconstructing MNIST images. Experimental results demonstrate that the proposed
method effectively preserves task-relevant performance, maintaining the model's
usability even after substantial pruning, by satisfying the required
application-specific criteria.

</details>


### [225] [Old Rules in a New Game: Mapping Uncertainty Quantification to Quantum Machine Learning](https://arxiv.org/abs/2507.14919)
*Maximilian Wendlinger,Kilian Tscharke,Pascal Debus*

Main category: cs.LG

TL;DR: 论文探讨了量子机器学习中模型透明度不足的问题，并提出将经典不确定性量化方法应用于量子领域。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习和量子机器学习中模型不透明性导致的问题（如过拟合和预测过度自信）亟待解决。

Method: 基于经典不确定性量化和量子贝叶斯建模的理论研究，开发并评估量子机器学习中的不确定性量化技术。

Result: 研究发现需要将经典不确定性量化方法融入量子机器学习模型设计中。

Conclusion: 强调在量子机器学习模型设计中引入不确定性意识的重要性。

Abstract: One of the key obstacles in traditional deep learning is the reduction in
model transparency caused by increasingly intricate model functions, which can
lead to problems such as overfitting and excessive confidence in predictions.
With the advent of quantum machine learning offering possible advances in
computational power and latent space complexity, we notice the same opaque
behavior. Despite significant research in classical contexts, there has been
little advancement in addressing the black-box nature of quantum machine
learning. Consequently, we approach this gap by building upon existing work in
classical uncertainty quantification and initial explorations in quantum
Bayesian modeling to theoretically develop and empirically evaluate techniques
to map classical uncertainty quantification methods to the quantum machine
learning domain. Our findings emphasize the necessity of leveraging classical
insights into uncertainty quantification to include uncertainty awareness in
the process of designing new quantum machine learning models.

</details>


### [226] [FedWCM: Unleashing the Potential of Momentum-based Federated Learning in Long-Tailed Scenarios](https://arxiv.org/abs/2507.14980)
*Tianle Li,Yongzhi Huang,Linshan Jiang,Qipeng Xie,Chang Liu,Wenfeng Du,Lu Wang,Kaishun Wu*

Main category: cs.LG

TL;DR: FedWCM是一种动态调整动量的方法，用于解决联邦学习在长尾数据分布中的收敛问题。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在非独立同分布（non-IID）数据，尤其是长尾数据分布中面临收敛困难和模型偏差问题。

Method: 提出FedWCM方法，通过动态调整动量来纠正长尾分布引入的方向偏差。

Result: 实验表明，FedWCM解决了不收敛问题，并在处理客户端异构性和数据不平衡方面优于现有方法。

Conclusion: FedWCM提高了联邦学习在非IID数据分布中的效率和有效性。

Abstract: Federated Learning (FL) enables decentralized model training while preserving
data privacy. Despite its benefits, FL faces challenges with non-identically
distributed (non-IID) data, especially in long-tailed scenarios with imbalanced
class samples. Momentum-based FL methods, often used to accelerate FL
convergence, struggle with these distributions, resulting in biased models and
making FL hard to converge. To understand this challenge, we conduct extensive
investigations into this phenomenon, accompanied by a layer-wise analysis of
neural network behavior. Based on these insights, we propose FedWCM, a method
that dynamically adjusts momentum using global and per-round data to correct
directional biases introduced by long-tailed distributions. Extensive
experiments show that FedWCM resolves non-convergence issues and outperforms
existing methods, enhancing FL's efficiency and effectiveness in handling
client heterogeneity and data imbalance.

</details>


### [227] [Clustered Federated Learning for Generalizable FDIA Detection in Smart Grids with Heterogeneous Data](https://arxiv.org/abs/2507.14999)
*Yunfeng Li,Junhong Liu,Zhaohui Yang,Guofu Liao,Chuyun Zhang*

Main category: cs.LG

TL;DR: 提出了一种名为FedClusAvg的隐私保护联邦学习框架，用于在非独立同分布（Non-IID）和资源受限的环境中提高虚假数据注入攻击（FDIA）的检测能力。


<details>
  <summary>Details</summary>
Motivation: 虚假数据注入攻击（FDIAs）对智能电网构成严重威胁，传统集中式训练方法存在隐私风险、数据共享限制和高传输成本问题。

Method: 采用基于聚类的分层采样和分层通信（客户端-子服务器-服务器）机制，实现局部训练和加权参数聚合。

Result: 在基准智能电网数据集上的实验表明，FedClusAvg提高了异构数据分布下的检测精度，并显著减少了通信轮次和带宽消耗。

Conclusion: FedClusAvg为大规模分布式电力系统中安全高效的FDIA检测提供了有效解决方案。

Abstract: False Data Injection Attacks (FDIAs) pose severe security risks to smart
grids by manipulating measurement data collected from spatially distributed
devices such as SCADA systems and PMUs. These measurements typically exhibit
Non-Independent and Identically Distributed (Non-IID) characteristics across
different regions, which significantly challenges the generalization ability of
detection models. Traditional centralized training approaches not only face
privacy risks and data sharing constraints but also incur high transmission
costs, limiting their scalability and deployment feasibility. To address these
issues, this paper proposes a privacy-preserving federated learning framework,
termed Federated Cluster Average (FedClusAvg), designed to improve FDIA
detection in Non-IID and resource-constrained environments. FedClusAvg
incorporates cluster-based stratified sampling and hierarchical communication
(client-subserver-server) to enhance model generalization and reduce
communication overhead. By enabling localized training and weighted parameter
aggregation, the algorithm achieves accurate model convergence without
centralizing sensitive data. Experimental results on benchmark smart grid
datasets demonstrate that FedClusAvg not only improves detection accuracy under
heterogeneous data distributions but also significantly reduces communication
rounds and bandwidth consumption. This work provides an effective solution for
secure and efficient FDIA detection in large-scale distributed power systems.

</details>


### [228] [Time-RA: Towards Time Series Reasoning for Anomaly with LLM Feedback](https://arxiv.org/abs/2507.15066)
*Yiyuan Yang,Zichuan Liu,Lei Song,Kai Ying,Zhiguang Wang,Tom Bamford,Svitlana Vyetrenko,Jiang Bian,Qingsong Wen*

Main category: cs.LG

TL;DR: 论文提出了一种新任务Time-RA，将时间序列异常检测从判别式任务转变为生成式任务，并引入多模态基准数据集RATs40K，用于细粒度异常推理。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列异常检测方法仅支持二元分类，缺乏详细分类和解释性推理，Time-RA任务旨在解决这一问题。

Method: 利用大型语言模型（LLMs）将任务转变为生成式推理任务，并开发多模态数据集RATs40K，包含数值、文本和视觉数据，标注细粒度异常类别和推理。

Result: 通过实验验证了现有LLMs和多模态LLMs的能力与局限，强调了监督微调的重要性。

Conclusion: Time-RA任务和RATs40K数据集为可解释的时间序列异常检测和推理提供了重要进展。

Abstract: Time series anomaly detection is critical across various domains, yet current
approaches often limit analysis to mere binary anomaly classification without
detailed categorization or further explanatory reasoning. To address these
limitations, we propose a novel task, Time-series Reasoning for Anomaly
(Time-RA) that transforms classical time series anomaly detection from a
discriminative into a generative, reasoning-intensive task leveraging Large
Language Models (LLMs). Also, we introduce the first real-world multimodal
benchmark dataset, RATs40K, explicitly annotated for anomaly reasoning,
comprising approximately 40,000 samples across 10 real-world domains. Each
sample includes numeric time series data, contextual text information, and
visual representations, each annotated with fine-grained categories (14 types
for univariate anomalies and 6 for multivariate anomalies) and structured
explanatory reasoning. We develop a sophisticated annotation framework
utilizing ensemble-generated labels refined through GPT-4-driven feedback,
ensuring accuracy and interpretability. Extensive benchmarking of LLMs and
multimodal LLMs demonstrates the capabilities and limitations of current
models, highlighting the critical role of supervised fine-tuning. Our dataset
and task pave the way for significant advancements in interpretable time series
anomaly detection and reasoning.

</details>


### [229] [ROBAD: Robust Adversary-aware Local-Global Attended Bad Actor Detection Sequential Model](https://arxiv.org/abs/2507.15067)
*Bing He,Mustaque Ahamad,Srijan Kumar*

Main category: cs.LG

TL;DR: 论文提出了一种名为ROBAD的新型Transformer模型，用于检测互联网平台上的不良用户，并通过局部和全局信息捕捉以及对抗训练提升模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习模型在检测不良用户时对输入序列的微小变化敏感，缺乏对抗攻击的鲁棒性。

Method: ROBAD结合Transformer编码器和解码器，分别捕捉帖子级局部信息和序列级全局信息，并通过对比学习增强分类层以应对对抗攻击。

Result: 在Yelp和Wikipedia数据集上的实验表明，ROBAD能有效抵御最新对抗攻击，准确检测不良用户。

Conclusion: ROBAD通过局部-全局信息融合和对抗训练，显著提升了模型在对抗环境下的检测能力。

Abstract: Detecting bad actors is critical to ensure the safety and integrity of
internet platforms. Several deep learning-based models have been developed to
identify such users. These models should not only accurately detect bad actors,
but also be robust against adversarial attacks that aim to evade detection.
However, past deep learning-based detection models do not meet the robustness
requirement because they are sensitive to even minor changes in the input
sequence. To address this issue, we focus on (1) improving the model
understanding capability and (2) enhancing the model knowledge such that the
model can recognize potential input modifications when making predictions. To
achieve these goals, we create a novel transformer-based classification model,
called ROBAD (RObust adversary-aware local-global attended Bad Actor Detection
model), which uses the sequence of user posts to generate user embedding to
detect bad actors. Particularly, ROBAD first leverages the transformer encoder
block to encode each post bidirectionally, thus building a post embedding to
capture the local information at the post level. Next, it adopts the
transformer decoder block to model the sequential pattern in the post
embeddings by using the attention mechanism, which generates the sequence
embedding to obtain the global information at the sequence level. Finally, to
enrich the knowledge of the model, embeddings of modified sequences by mimicked
attackers are fed into a contrastive-learning-enhanced classification layer for
sequence prediction. In essence, by capturing the local and global information
(i.e., the post and sequence information) and leveraging the mimicked behaviors
of bad actors in training, ROBAD can be robust to adversarial attacks.
Extensive experiments on Yelp and Wikipedia datasets show that ROBAD can
effectively detect bad actors when under state-of-the-art adversarial attacks.

</details>


### [230] [Reinforcement Learning for Flow-Matching Policies](https://arxiv.org/abs/2507.15073)
*Samuel Pfrommer,Yixiao Huang,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 本文探讨了通过强化学习训练流匹配策略以超越原始演示策略性能的方法，提出了两种方法（RWFM和GRPO），并在模拟任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过强化学习改进流匹配策略，使其超越原始演示策略的性能，特别是在最小时间控制任务中。

Method: 提出了两种方法：简单的奖励加权流匹配（RWFM）和带有学习奖励替代的组相对策略优化（GRPO）。

Result: 实验表明，两种方法均显著优于原始演示策略，其中GRPO方法在成本上比简单的模仿学习流匹配（ILFM）减少了50%至85%。

Conclusion: 强化学习可以显著提升流匹配策略的性能，GRPO方法在减少成本方面表现尤为突出。

Abstract: Flow-matching policies have emerged as a powerful paradigm for generalist
robotics. These models are trained to imitate an action chunk, conditioned on
sensor observations and textual instructions. Often, training demonstrations
are generated by a suboptimal policy, such as a human operator. This work
explores training flow-matching policies via reinforcement learning to surpass
the original demonstration policy performance. We particularly note
minimum-time control as a key application and present a simple scheme for
variable-horizon flow-matching planning. We then introduce two families of
approaches: a simple Reward-Weighted Flow Matching (RWFM) scheme and a Group
Relative Policy Optimization (GRPO) approach with a learned reward surrogate.
Our policies are trained on an illustrative suite of simulated unicycle
dynamics tasks, and we show that both approaches dramatically improve upon the
suboptimal demonstrator performance, with the GRPO approach in particular
generally incurring between $50\%$ and $85\%$ less cost than a naive Imitation
Learning Flow Matching (ILFM) approach.

</details>


### [231] [Isotonic Quantile Regression Averaging for uncertainty quantification of electricity price forecasts](https://arxiv.org/abs/2507.15079)
*Arkadiusz Lipiecki,Bartosz Uniejewski*

Main category: cs.LG

TL;DR: 提出了一种名为iQRA的新方法，通过引入随机顺序约束改进点预测集合的概率预测，显著提升了德国日前电力市场价格预测的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: 电力市场等波动性强的领域中，预测模型的不确定性量化对数据驱动决策至关重要，但现有机器学习模型常缺乏不确定性估计，限制了决策者规避风险的能力。

Method: 基于分位数回归平均（QRA）框架，引入随机顺序约束，提出iQRA方法，改进预测准确性、可靠性和计算成本。

Result: 在德国日前电力市场的预测研究中，iQRA在可靠性和锐度上均优于现有后处理方法，特别是覆盖性保形预测方法。

Conclusion: iQRA通过等渗正则化简化分位数回归问题，提供无超参数的变量选择方法，显著提升了预测的可靠性和实用性。

Abstract: Quantifying the uncertainty of forecasting models is essential to assess and
mitigate the risks associated with data-driven decisions, especially in
volatile domains such as electricity markets. Machine learning methods can
provide highly accurate electricity price forecasts, critical for informing the
decisions of market participants. However, these models often lack uncertainty
estimates, which limits the ability of decision makers to avoid unnecessary
risks. In this paper, we propose a novel method for generating probabilistic
forecasts from ensembles of point forecasts, called Isotonic Quantile
Regression Averaging (iQRA). Building on the established framework of Quantile
Regression Averaging (QRA), we introduce stochastic order constraints to
improve forecast accuracy, reliability, and computational costs. In an
extensive forecasting study of the German day-ahead electricity market, we show
that iQRA consistently outperforms state-of-the-art postprocessing methods in
terms of both reliability and sharpness. It produces well-calibrated prediction
intervals across multiple confidence levels, providing superior reliability to
all benchmark methods, particularly coverage-based conformal prediction. In
addition, isotonic regularization decreases the complexity of the quantile
regression problem and offers a hyperparameter-free approach to variable
selection.

</details>


### [232] [Robust Control with Gradient Uncertainty](https://arxiv.org/abs/2507.15082)
*Qian Qi*

Main category: cs.LG

TL;DR: 论文提出了一种新的鲁棒控制理论扩展，针对价值函数梯度的不确定性，通过零和动态博弈和新的非线性偏微分方程（GU-HJBI）进行分析，并在LQ情况下验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决强化学习等应用中价值函数梯度不确定性的问题，提升鲁棒控制理论的适用性。

Method: 通过零和动态博弈和GU-HJBI方程建模，分析LQ情况，并提出GURAC算法。

Result: 证明了经典二次价值函数假设在梯度不确定性下失效，提出了非线性修正和GURAC算法。

Conclusion: 该研究为鲁棒控制提供了新方向，对强化学习和计算金融等领域有重要意义。

Abstract: We introduce a novel extension to robust control theory that explicitly
addresses uncertainty in the value function's gradient, a form of uncertainty
endemic to applications like reinforcement learning where value functions are
approximated. We formulate a zero-sum dynamic game where an adversary perturbs
both system dynamics and the value function gradient, leading to a new, highly
nonlinear partial differential equation: the Hamilton-Jacobi-Bellman-Isaacs
Equation with Gradient Uncertainty (GU-HJBI). We establish its well-posedness
by proving a comparison principle for its viscosity solutions under a uniform
ellipticity condition. Our analysis of the linear-quadratic (LQ) case yields a
key insight: we prove that the classical quadratic value function assumption
fails for any non-zero gradient uncertainty, fundamentally altering the problem
structure. A formal perturbation analysis characterizes the non-polynomial
correction to the value function and the resulting nonlinearity of the optimal
control law, which we validate with numerical studies. Finally, we bridge
theory to practice by proposing a novel Gradient-Uncertainty-Robust
Actor-Critic (GURAC) algorithm, accompanied by an empirical study demonstrating
its effectiveness in stabilizing training. This work provides a new direction
for robust control, holding significant implications for fields where function
approximation is common, including reinforcement learning and computational
finance.

</details>


### [233] [AnalogFed: Federated Discovery of Analog Circuit Topologies with Generative AI](https://arxiv.org/abs/2507.15104)
*Qiufeng Li,Shu Hong,Jian Gao,Xuan Zhang,Tian Lan,Weidong Cao*

Main category: cs.LG

TL;DR: AnalogFed提出了一种联邦学习方法，用于在保护隐私的前提下实现分布式协作的模拟电路拓扑发现。


<details>
  <summary>Details</summary>
Motivation: 模拟电路设计数据具有专有性和隐私性，限制了生成式AI的应用和研究协作。

Method: 提出了AnalogFed，结合联邦学习技术，处理数据异构性并保护隐私。

Result: 实验表明，AnalogFed在性能和隐私保护方面均达到先进水平。

Conclusion: AnalogFed为模拟电路设计的协作创新提供了可行方案。

Abstract: Recent breakthroughs in AI/ML offer exciting opportunities to revolutionize
analog design automation through data-driven approaches. In particular,
researchers are increasingly fascinated by harnessing the power of generative
AI to automate the discovery of novel analog circuit topologies. Unlocking the
full potential of generative AI in these data-driven discoveries requires
access to large and diverse datasets.Yet, there is a significant barrier in the
analog domain--Analog circuit design is inherently proprietary, involving not
only confidential circuit structures but also the underlying commercial
semiconductor processes. As a result, current generative AI research is largely
confined to individual researchers who construct small, narrowly focused
private datasets. This fragmentation severely limits collaborative innovation
and impedes progress across the research community. To address these
challenges, we propose AnalogFed. AnalogFed enables collaborative topology
discovery across decentralized clients (e.g., individual researchers or
institutions) without requiring the sharing of raw private data. To make this
vision practical, we introduce a suite of techniques tailored to the unique
challenges of applying FedL in analog design--from generative model development
and data heterogeneity handling to privacy-preserving strategies that ensure
both flexibility and security for circuit designers and semiconductor
manufacturers. Extensive experiments across varying client counts and dataset
sizes demonstrate that AnalogFed achieves performance comparable to centralized
baselines--while maintaining strict data privacy. Specifically, the generative
AI model within AnalogFed achieves state-of-the-art efficiency and scalability
in the design of analog circuit topologies.

</details>


### [234] [Distributional Unlearning: Forgetting Distributions, Not Just Samples](https://arxiv.org/abs/2507.15112)
*Youssef Allouah,Rachid Guerraoui,Sanmi Koyejo*

Main category: cs.LG

TL;DR: 论文提出了一种分布式的“遗忘学习”框架，通过移除特定分布的数据点来满足隐私或法律需求，相比随机移除更高效。


<details>
  <summary>Details</summary>
Motivation: 现有遗忘学习方法主要针对单个样本，难以彻底移除整个主题域的信息，导致残留信号可能被下游模型恢复。

Method: 提出分布遗忘框架，利用Kullback-Leibler散度量化移除与保留效果，推导高斯情况下的帕累托边界，并提出基于距离的选择规则。

Result: 实验表明，该方法在合成高斯数据、Jigsaw Toxic Comments等数据集上比随机移除少15-72%的删除量，且对保留性能影响可忽略。

Conclusion: 分布遗忘是一种高效、模型无关的解决方案，适用于需要彻底移除特定分布信息的场景。

Abstract: Machine unlearning seeks to remove unwanted information from trained models,
initially at the individual-sample level, but increasingly at the level of
entire sub-populations. In many deployments, models must delete whole topical
domains to satisfy privacy, legal, or quality requirements, e.g., removing
several users' posts under GDPR or copyrighted web content. Existing unlearning
tools remain largely sample-oriented, and straightforward point deletion often
leaves enough residual signal for downstream learners to recover the unwanted
domain. We introduce distributional unlearning, a data-centric, model-agnostic
framework that asks: Given examples from an unwanted distribution and a
retained distribution, what is the smallest set of points whose removal makes
the edited dataset far from the unwanted domain yet close to the retained one?
Using Kullback-Leibler divergence to quantify removal and preservation, we
derive the exact Pareto frontier in the Gaussian case and prove that any model
retrained on the edited data incurs log-loss shifts bounded by the divergence
thresholds. We propose a simple distance-based selection rule satisfying these
constraints with a quadratic reduction in deletion budget compared to random
removal. Experiments on synthetic Gaussians, Jigsaw Toxic Comments, SMS spam,
and CIFAR-10 show 15-72% fewer deletions than random, with negligible impact on
retained performance.

</details>


### [235] [Are We Overlooking the Dimensions? Learning Latent Hierarchical Channel Structure for High-Dimensional Time Series Forecasting](https://arxiv.org/abs/2507.15119)
*Juntong Ni,Shiyu Wang,Zewen Liu,Xiaoming Shi,Xinyue Zhong,Zhou Ye,Wei Jin*

Main category: cs.LG

TL;DR: 论文提出U-Cast模型和Time-HD基准，解决高维时间序列预测（HDTSF）中的复杂通道相关性挑战。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列预测（TSF）模型在高维场景下难以捕捉复杂的通道相关性，导致性能下降。

Method: U-Cast采用基于查询的注意力机制学习潜在层次通道结构，并引入全秩正则化解耦高相关通道表示。

Result: 理论证明跨通道信息降低预测风险，实验显示U-Cast在Time-HD基准上优于基线模型。

Conclusion: U-Cast和Time-HD为未来HDTSF研究提供了坚实基础。

Abstract: Time series forecasting (TSF) is a central problem in time series analysis.
However, as the number of channels in time series datasets scales to the
thousands or more, a scenario we define as High-Dimensional Time Series
Forecasting (HDTSF), it introduces significant new modeling challenges that are
often not the primary focus of traditional TSF research. HDTSF is challenging
because the channel correlation often forms complex and hierarchical patterns.
Existing TSF models either ignore these interactions or fail to scale as
dimensionality grows. To address this issue, we propose U-Cast, a
channel-dependent forecasting architecture that learns latent hierarchical
channel structures with an innovative query-based attention. To disentangle
highly correlated channel representation, U-Cast adds a full-rank
regularization during training. We also release Time-HD, a benchmark of large,
diverse, high-dimensional datasets. Our theory shows that exploiting
cross-channel information lowers forecasting risk, and experiments on Time-HD
demonstrate that U-Cast surpasses strong baselines in both accuracy and
efficiency. Together, U-Cast and Time-HD provide a solid basis for future HDTSF
research.

</details>


### [236] [Transforming Datasets to Requested Complexity with Projection-based Many-Objective Genetic Algorithm](https://arxiv.org/abs/2507.15132)
*Joanna Komorniczak*

Main category: cs.LG

TL;DR: 提出一种遗传算法，通过优化分类和回归任务的问题复杂性度量，生成具有不同难度级别的合成数据集。


<details>
  <summary>Details</summary>
Motivation: 研究社区需要更先进的合成数据生成器来评估机器学习方法的优势和局限性，本研究旨在增加涵盖多种问题复杂性的数据集的可用性。

Method: 使用遗传算法优化分类任务的10个复杂性度量和回归任务的4个复杂性度量，通过线性特征投影生成目标复杂度的数据集。

Result: 实验表明，该算法能生成不同难度的数据集，且生成数据的复杂性与识别质量相关。

Conclusion: 遗传算法能有效生成具有目标复杂度的数据集，为机器学习方法评估提供多样化数据支持。

Abstract: The research community continues to seek increasingly more advanced synthetic
data generators to reliably evaluate the strengths and limitations of machine
learning methods. This work aims to increase the availability of datasets
encompassing a diverse range of problem complexities by proposing a genetic
algorithm that optimizes a set of problem complexity measures for
classification and regression tasks towards specific targets. For
classification, a set of 10 complexity measures was used, while for regression
tasks, 4 measures demonstrating promising optimization capabilities were
selected. Experiments confirmed that the proposed genetic algorithm can
generate datasets with varying levels of difficulty by transforming
synthetically created datasets to achieve target complexity values through
linear feature projections. Evaluations involving state-of-the-art classifiers
and regressors revealed a correlation between the complexity of the generated
data and the recognition quality.

</details>


### [237] [Constraint-aware Learning of Probabilistic Sequential Models for Multi-Label Classification](https://arxiv.org/abs/2507.15156)
*Mykhailo Buleshnyi,Anna Polova,Zsolt Zombori,Michael Benedikt*

Main category: cs.LG

TL;DR: 研究多标签分类问题，利用序列模型处理标签间的逻辑约束。


<details>
  <summary>Details</summary>
Motivation: 探索在大规模标签集中，如何利用标签间的逻辑约束提高分类性能。

Method: 采用个体分类器与序列模型结合的架构，生成联合分布以建模标签相关性。

Result: 实验证明该架构能有效利用约束进行训练，并在推理时强制执行约束。

Conclusion: 该架构在多标签分类中能有效建模和利用标签间的约束关系。

Abstract: We investigate multi-label classification involving large sets of labels,
where the output labels may be known to satisfy some logical constraints. We
look at an architecture in which classifiers for individual labels are fed into
an expressive sequential model, which produces a joint distribution. One of the
potential advantages for such an expressive model is its ability to modelling
correlations, as can arise from constraints. We empirically demonstrate the
ability of the architecture both to exploit constraints in training and to
enforce constraints at inference time.

</details>


### [238] [Resonant-Tunnelling Diode Reservoir Computing System for Image Recognition](https://arxiv.org/abs/2507.15158)
*A. H. Abbas,Hend Abdel-Ghani,Ivan S. Maksymov*

Main category: cs.LG

TL;DR: 提出了一种基于共振隧穿二极管（RTD）的神经形态计算架构，用于资源受限环境中的高效计算，并在图像识别任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能向实时、边缘和资源受限环境扩展，需要硬件高效的计算模型。

Method: 理论构建并数值实现了一种基于RTD的物理储层计算系统，用于图像识别任务。

Result: 在数字手写分类和Fruit~360数据集上表现出色，同时遵循确定性非线性变换原则。

Conclusion: RTD架构为下一代储层计算提供了高效且确定的解决方案。

Abstract: As artificial intelligence continues to push into real-time, edge-based and
resource-constrained environments, there is an urgent need for novel,
hardware-efficient computational models. In this study, we present and validate
a neuromorphic computing architecture based on resonant-tunnelling diodes
(RTDs), which exhibit the nonlinear characteristics ideal for physical
reservoir computing (RC). We theoretically formulate and numerically implement
an RTD-based RC system and demonstrate its effectiveness on two image
recognition benchmarks: handwritten digit classification and object recognition
using the Fruit~360 dataset. Our results show that this circuit-level
architecture delivers promising performance while adhering to the principles of
next-generation RC -- eliminating random connectivity in favour of a
deterministic nonlinear transformation of input signals.

</details>


### [239] [Designing User-Centric Metrics for Evaluation of Counterfactual Explanations](https://arxiv.org/abs/2507.15162)
*Firdaus Ahmed Choudhury,Ethan Leicht,Jude Ethan Bislig,Hangzhi Guo,Amulya Yadav*

Main category: cs.LG

TL;DR: 论文研究了反事实解释（CFEs）在机器学习决策模型中的应用，发现现有评估指标与用户偏好不一致，提出了一种新的用户中心评估模型AWP，准确率达84.37%。


<details>
  <summary>Details</summary>
Motivation: 现有CFEs评估指标（如接近性）可能忽略用户偏好和实际限制，导致解释不实用。

Method: 通过两项用户研究（20名和41名参与者）验证现有指标的局限性，并提出AWP模型。

Result: 用户偏好的CFEs与接近性指标仅匹配63.81%，AWP模型预测准确率达84.37%。

Conclusion: 需开发用户中心的CFEs评估指标，AWP模型为个性化成本模型提供了实证支持。

Abstract: Machine learning-based decision models are increasingly being used to make
decisions that significantly impact people's lives, but their opaque nature
leaves end users without a clear understanding of why a decision was made.
Counterfactual Explanations (CFEs) have grown in popularity as a means of
offering actionable guidance by identifying the minimum changes in feature
values required to flip a model's prediction to something more desirable.
Unfortunately, most prior research in CFEs relies on artificial evaluation
metrics, such as proximity, which may overlook end-user preferences and
constraints, e.g., the user's perception of effort needed to make certain
feature changes may differ from that of the model designer. To address this
research gap, this paper makes three novel contributions. First, we conduct a
pilot study with 20 crowd-workers on Amazon MTurk to experimentally validate
the alignment of existing CF evaluation metrics with real-world user
preferences. Results show that user-preferred CFEs matched those based on
proximity in only 63.81% of cases, highlighting the limited applicability of
these metrics in real-world settings. Second, inspired by the need to design a
user-informed evaluation metric for CFEs, we conduct a more detailed two-day
user study with 41 participants facing realistic credit application scenarios
to find experimental support for or against three intuitive hypotheses that may
explain how end users evaluate CFEs. Third, based on the findings of this
second study, we propose the AWP model, a novel user-centric, two-stage model
that describes one possible mechanism by which users evaluate and select CFEs.
Our results show that AWP predicts user-preferred CFEs with 84.37% accuracy.
Our study provides the first human-centered validation for personalized cost
models in CFE generation and highlights the need for adaptive, user-centered
evaluation metrics.

</details>


### [240] [Better Models and Algorithms for Learning Ising Models from Dynamics](https://arxiv.org/abs/2507.15173)
*Jason Gaitonde,Ankur Moitra,Elchanan Mossel*

Main category: cs.LG

TL;DR: 本文提出了在更自然的观测模型下学习Ising模型结构和参数的算法，仅需观察配置变化，无需观察所有更新尝试。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法需要观察所有站点更新尝试的限制，提出适用于更实际场景的算法。

Method: 利用可逆马尔可夫链的鲁棒性，设计高效算法恢复依赖图和参数。

Result: 算法在最大度数为d的Ising模型中，恢复依赖图时间为poly(d)·n²log n，参数恢复时间为Õ(2^d n)。

Conclusion: 该算法在更弱的观测模型下性能与现有最优方法相当，适用于更广泛的马尔可夫链。

Abstract: We study the problem of learning the structure and parameters of the Ising
model, a fundamental model of high-dimensional data, when observing the
evolution of an associated Markov chain. A recent line of work has studied the
natural problem of learning when observing an evolution of the well-known
Glauber dynamics [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Mossel STOC 2024], which provides an arguably more realistic
generative model than the classical i.i.d. setting. However, this prior work
crucially assumes that all site update attempts are observed, \emph{even when
this attempt does not change the configuration}: this strong observation model
is seemingly essential for these approaches. While perhaps possible in
restrictive contexts, this precludes applicability to most realistic settings
where we can observe \emph{only} the stochastic evolution itself, a minimal and
natural assumption for any process we might hope to learn from. However,
designing algorithms that succeed in this more realistic setting has remained
an open problem [Bresler, Gamarnik, Shah, IEEE Trans. Inf. Theory 2018,
Gaitonde, Moitra, Mossel, STOC 2025].
  In this work, we give the first algorithms that efficiently learn the Ising
model in this much more natural observation model that only observes when the
configuration changes. For Ising models with maximum degree $d$, our algorithm
recovers the underlying dependency graph in time $\mathsf{poly}(d)\cdot n^2\log
n$ and then the actual parameters in additional $\widetilde{O}(2^d n)$ time,
which qualitatively matches the state-of-the-art even in the i.i.d. setting in
a much weaker observation model. Our analysis holds more generally for a
broader class of reversible, single-site Markov chains that also includes the
popular Metropolis chain by leveraging more robust properties of reversible
Markov chains.

</details>


### [241] [Joint-Local Grounded Action Transformation for Sim-to-Real Transfer in Multi-Agent Traffic Control](https://arxiv.org/abs/2507.15174)
*Justin Turnau,Longchao Da,Khoa Vo,Ferdous Al Rafi,Shreyas Bachiraju,Tiejin Chen,Hua Wei*

Main category: cs.LG

TL;DR: JL-GAT将GAT应用于多智能体强化学习的交通信号控制，通过邻居信息增强适应性，减少仿真与现实的性能差距。


<details>
  <summary>Details</summary>
Motivation: 多智能体强化学习在交通信号控制中存在仿真与现实性能差距问题，JL-GAT旨在解决这一问题。

Method: JL-GAT采用分散式GAT方法，结合邻居智能体信息，提升扩展性和适应性。

Result: 实验表明，JL-GAT在模拟恶劣天气条件下有效减少性能差距。

Conclusion: JL-GAT为多智能体交通信号控制提供了一种可扩展且高效的解决方案。

Abstract: Traffic Signal Control (TSC) is essential for managing urban traffic flow and
reducing congestion. Reinforcement Learning (RL) offers an adaptive method for
TSC by responding to dynamic traffic patterns, with multi-agent RL (MARL)
gaining traction as intersections naturally function as coordinated agents.
However, due to shifts in environmental dynamics, implementing MARL-based TSC
policies in the real world often leads to a significant performance drop, known
as the sim-to-real gap. Grounded Action Transformation (GAT) has successfully
mitigated this gap in single-agent RL for TSC, but real-world traffic networks,
which involve numerous interacting intersections, are better suited to a MARL
framework. In this work, we introduce JL-GAT, an application of GAT to
MARL-based TSC that balances scalability with enhanced grounding capability by
incorporating information from neighboring agents. JL-GAT adopts a
decentralized approach to GAT, allowing for the scalability often required in
real-world traffic networks while still capturing key interactions between
agents. Comprehensive experiments on various road networks under simulated
adverse weather conditions, along with ablation studies, demonstrate the
effectiveness of JL-GAT. The code is publicly available at
https://github.com/DaRL-LibSignal/JL-GAT/.

</details>


### [242] [Feature Construction Using Network Control Theory and Rank Encoding for Graph Machine Learning](https://arxiv.org/abs/2507.15195)
*Anwar Said,Yifan Wei,Ubaid Ullah Ahmad,Mudassir Shabbir,Waseem Abbas,Xenofon Koutsoukos*

Main category: cs.LG

TL;DR: 论文提出利用平均可控性和新型秩编码方法提升GNN在社交网络分类任务中的性能，解决了节点特征缺失的问题。


<details>
  <summary>Details</summary>
Motivation: 社交网络中节点特征常因隐私或固有属性缺失而不可用，影响GNN性能。

Method: 提出两种策略：1) 使用平均可控性和中心性指标（NCT-EFA）作为节点特征；2) 开发秩编码方法将图论指标转化为固定维度特征。

Result: 实验表明，平均可控性显著提升GNN性能，秩编码方法优于传统独热编码（ROC AUC从68.7%提升至73.9%）。

Conclusion: 平均可控性和秩编码方法能有效生成高效节点特征，提升GNN在社交网络中的表现。

Abstract: In this article, we utilize the concept of average controllability in graphs,
along with a novel rank encoding method, to enhance the performance of Graph
Neural Networks (GNNs) in social network classification tasks. GNNs have proven
highly effective in various network-based learning applications and require
some form of node features to function. However, their performance is heavily
influenced by the expressiveness of these features. In social networks, node
features are often unavailable due to privacy constraints or the absence of
inherent attributes, making it challenging for GNNs to achieve optimal
performance. To address this limitation, we propose two strategies for
constructing expressive node features. First, we introduce average
controllability along with other centrality metrics (denoted as NCT-EFA) as
node-level metrics that capture critical aspects of network topology. Building
on this, we develop a rank encoding method that transforms average
controllability or any other graph-theoretic metric into a fixed-dimensional
feature space, thereby improving feature representation. We conduct extensive
numerical evaluations using six benchmark GNN models across four social network
datasets to compare different node feature construction methods. Our results
demonstrate that incorporating average controllability into the feature space
significantly improves GNN performance. Moreover, the proposed rank encoding
method outperforms traditional one-hot degree encoding, improving the ROC AUC
from 68.7% to 73.9% using GraphSAGE on the GitHub Stargazers dataset,
underscoring its effectiveness in generating expressive and efficient node
representations.

</details>


### [243] [Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation](https://arxiv.org/abs/2507.15205)
*Xinran Li,Xiujuan Xu,Jiaqi Qiao*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的多模态方法LSDGNN，通过长距离和短距离图神经网络分别捕捉远距离和近距离话语的多模态特征，并引入差异正则器和双仿射模块优化特征交互。此外，提出改进课程学习（ICL）解决数据不平衡问题，实验表明模型在IEMOCAP和MELD数据集上优于现有基准。


<details>
  <summary>Details</summary>
Motivation: 对话中的情感识别（ERC）是一个实际且具有挑战性的任务，需要有效捕捉多模态特征并解决数据不平衡问题。

Method: 基于有向无环图（DAG）构建长距离和短距离图神经网络，使用差异正则器和双仿射模块优化特征交互，并提出改进课程学习（ICL）处理数据不平衡。

Result: 在IEMOCAP和MELD数据集上的实验结果表明，模型性能优于现有基准。

Conclusion: LSDGNN通过多模态特征提取和优化学习策略，显著提升了对话情感识别的效果。

Abstract: Emotion Recognition in Conversation (ERC) is a practical and challenging
task. This paper proposes a novel multimodal approach, the Long-Short Distance
Graph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it
constructs a long-distance graph neural network and a short-distance graph
neural network to obtain multimodal features of distant and nearby utterances,
respectively. To ensure that long- and short-distance features are as distinct
as possible in representation while enabling mutual influence between the two
modules, we employ a Differential Regularizer and incorporate a BiAffine Module
to facilitate feature interaction. In addition, we propose an Improved
Curriculum Learning (ICL) to address the challenge of data imbalance. By
computing the similarity between different emotions to emphasize the shifts in
similar emotions, we design a "weighted emotional shift" metric and develop a
difficulty measurer, enabling a training process that prioritizes learning easy
samples before harder ones. Experimental results on the IEMOCAP and MELD
datasets demonstrate that our model outperforms existing benchmarks.

</details>


### [244] [Exact Reformulation and Optimization for Direct Metric Optimization in Binary Imbalanced Classification](https://arxiv.org/abs/2507.15240)
*Le Peng,Yash Travadi,Chuan He,Ying Cui,Ju Sun*

Main category: cs.LG

TL;DR: 论文提出了一种精确约束重构方法，用于解决不平衡分类中的直接度量优化问题，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在不平衡分类中，标准准确率容易误导，现有方法无法满足不同类别重要性或特定指标需求。

Method: 引入精确约束重构，通过精确惩罚方法解决FPOR、FROP和OFBS三种直接度量优化问题。

Result: 在多个基准数据集上验证了方法的优越性。

Conclusion: 提出的ERO框架适用于广泛的直接度量优化问题。

Abstract: For classification with imbalanced class frequencies, i.e., imbalanced
classification (IC), standard accuracy is known to be misleading as a
performance measure. While most existing methods for IC resort to optimizing
balanced accuracy (i.e., the average of class-wise recalls), they fall short in
scenarios where the significance of classes varies or certain metrics should
reach prescribed levels. In this paper, we study two key classification
metrics, precision and recall, under three practical binary IC settings: fix
precision optimize recall (FPOR), fix recall optimize precision (FROP), and
optimize $F_\beta$-score (OFBS). Unlike existing methods that rely on smooth
approximations to deal with the indicator function involved, \textit{we
introduce, for the first time, exact constrained reformulations for these
direct metric optimization (DMO) problems}, which can be effectively solved by
exact penalty methods. Experiment results on multiple benchmark datasets
demonstrate the practical superiority of our approach over the state-of-the-art
methods for the three DMO problems. We also expect our exact reformulation and
optimization (ERO) framework to be applicable to a wide range of DMO problems
for binary IC and beyond. Our code is available at
https://github.com/sun-umn/DMO.

</details>


### [245] [Spatio-Temporal Demand Prediction for Food Delivery Using Attention-Driven Graph Neural Networks](https://arxiv.org/abs/2507.15246)
*Rabia Latief Bhat,Iqra Altaf Gillani*

Main category: cs.LG

TL;DR: 本文提出了一种基于注意力机制的图神经网络框架，用于准确预测食品配送平台的需求，通过建模空间-时间依赖性提升预测精度。


<details>
  <summary>Details</summary>
Motivation: 食品配送平台的需求预测对运营效率至关重要，但空间异质性和时间波动性增加了预测难度。

Method: 将配送环境建模为图（节点为配送区域，边为空间邻近性和订单流模式），利用注意力机制动态加权邻近区域的影响，并结合时间趋势进行预测。

Result: 在真实数据集上的实验表明，该模型能高精度预测未来订单量。

Conclusion: 该框架为城市食品配送运营提供了可扩展且自适应的解决方案，支持车队定位、资源分配和调度优化。

Abstract: Accurate demand forecasting is critical for enhancing the efficiency and
responsiveness of food delivery platforms, where spatial heterogeneity and
temporal fluctuations in order volumes directly influence operational
decisions. This paper proposes an attention-based Graph Neural Network
framework that captures spatial-temporal dependencies by modeling the food
delivery environment as a graph. In this graph, nodes represent urban delivery
zones, while edges reflect spatial proximity and inter-regional order flow
patterns derived from historical data. The attention mechanism dynamically
weighs the influence of neighboring zones, enabling the model to focus on the
most contextually relevant areas during prediction. Temporal trends are jointly
learned alongside spatial interactions, allowing the model to adapt to evolving
demand patterns. Extensive experiments on real-world food delivery datasets
demonstrate the superiority of the proposed model in forecasting future order
volumes with high accuracy. The framework offers a scalable and adaptive
solution to support proactive fleet positioning, resource allocation, and
dispatch optimization in urban food delivery operations.

</details>


### [246] [CHORDS: Diffusion Sampling Accelerator with Multi-core Hierarchical ODE Solvers](https://arxiv.org/abs/2507.15260)
*Jiaqi Han,Haotian Ye,Puheng Li,Minkai Xu,James Zou,Stefano Ermon*

Main category: cs.LG

TL;DR: CHORDS是一种无需训练、模型无关的加速策略，通过多核并行显著提升扩散模型的采样速度，最高可达2.9倍加速，且不降低生成质量。


<details>
  <summary>Details</summary>
Motivation: 扩散模型的高保真生成能力受限于计算成本高的推理过程，现有加速技术要么需要大量模型重新训练，要么显著牺牲样本质量。

Method: 将多核扩散采样视为ODE求解器管道，通过理论支持的核间通信机制，用较慢但准确的求解器逐步修正较快的求解器。

Result: CHORDS在多样大规模图像和视频扩散模型中显著加速采样，四核下提速2.1倍，八核下提速2.9倍，且无质量损失。

Conclusion: CHORDS为实时高保真扩散生成奠定了坚实基础。

Abstract: Diffusion-based generative models have become dominant generators of
high-fidelity images and videos but remain limited by their computationally
expensive inference procedures. Existing acceleration techniques either require
extensive model retraining or compromise significantly on sample quality. This
paper explores a general, training-free, and model-agnostic acceleration
strategy via multi-core parallelism. Our framework views multi-core diffusion
sampling as an ODE solver pipeline, where slower yet accurate solvers
progressively rectify faster solvers through a theoretically justified
inter-core communication mechanism. This motivates our multi-core training-free
diffusion sampling accelerator, CHORDS, which is compatible with various
diffusion samplers, model architectures, and modalities. Through extensive
experiments, CHORDS significantly accelerates sampling across diverse
large-scale image and video diffusion models, yielding up to 2.1x speedup with
four cores, improving by 50% over baselines, and 2.9x speedup with eight cores,
all without quality degradation. This advancement enables CHORDS to establish a
solid foundation for real-time, high-fidelity diffusion generation.

</details>


### [247] [Temporal Basis Function Models for Closed-Loop Neural Stimulation](https://arxiv.org/abs/2507.15274)
*Matthew J. Bryan,Felix Schwock,Azadeh Yazdan-Shahmorad,Rajesh P N Rao*

Main category: cs.LG

TL;DR: 论文提出了一种基于时间基函数模型（TBFMs）的闭环神经刺激方法，用于个性化治疗神经系统疾病，如帕金森病。该方法在样本效率、训练时间和延迟方面表现优异，并在非人灵长类动物实验中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决闭环神经刺激在个性化治疗中的挑战，如样本效率低、训练时间长和延迟高，以推动AI技术在神经疾病治疗中的应用。

Method: 使用时间基函数模型（TBFMs）进行单次试验的时空前向预测，模拟光遗传刺激对局部场电位（LFPs）的影响，并用于闭环控制神经活动。

Result: TBF模型在样本效率（15-20分钟数据收集）、训练时间（2-4分钟）和延迟（0.2毫秒）方面表现优异，预测精度与基线非线性动态系统模型相当，优于线性状态空间模型。

Conclusion: TBF模型为复杂AI方法与临床闭环刺激协议之间的转化提供了桥梁，具有潜在的临床应用价值。

Abstract: Closed-loop neural stimulation provides novel therapies for neurological
diseases such as Parkinson's disease (PD), but it is not yet clear whether
artificial intelligence (AI) techniques can tailor closed-loop stimulation to
individual patients or identify new therapies. Progress requires us to address
a number of translational issues, including sample efficiency, training time,
and minimizing loop latency such that stimulation may be shaped in response to
changing brain activity. We propose temporal basis function models (TBFMs) to
address these difficulties, and explore this approach in the context of
excitatory optogenetic stimulation. We demonstrate the ability of TBF models to
provide a single-trial, spatiotemporal forward prediction of the effect of
optogenetic stimulation on local field potentials (LFPs) measured in two
non-human primates. We further use simulations to demonstrate the use of TBF
models for closed-loop stimulation, driving neural activity towards target
patterns. The simplicity of TBF models allow them to be sample efficient, rapid
to train (2-4min), and low latency (0.2ms) on desktop CPUs. We demonstrate the
model on 40 sessions of previously published excitatory optogenetic stimulation
data. For each session, the model required 15-20min of data collection to
successfully model the remainder of the session. It achieved a prediction
accuracy comparable to a baseline nonlinear dynamical systems model that
requires hours to train, and superior accuracy to a linear state-space model.
In our simulations, it also successfully allowed a closed-loop stimulator to
control a neural circuit. Our approach begins to bridge the translational gap
between complex AI-based approaches to modeling dynamical systems and the
vision of using such forward prediction models to develop novel, clinically
useful closed-loop stimulation protocols.

</details>


### [248] [Machine Unlearning for Streaming Forgetting](https://arxiv.org/abs/2507.15280)
*Shaofei Shen,Chenhao Zhang,Yawen Zhao,Alina Bialkowski,Weitong Chen,Miao Xu*

Main category: cs.LG

TL;DR: 论文提出了一种流式遗忘学习范式，通过将遗忘问题建模为分布偏移问题，提出了一种高效算法，无需访问原始训练数据即可实现流式遗忘。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习遗忘方法通常批量处理遗忘数据，而实际场景中遗忘请求往往是流式的，导致效率和效果下降。

Method: 将遗忘问题形式化为分布偏移问题，估计变化后的分布，并提出一种新的流式遗忘算法。

Result: 理论分析表明算法具有$O(\sqrt{T} + V_T)$的误差界限，实验验证了方法的有效性。

Conclusion: 该方法在流式遗忘场景下表现出高效性和性能优势，适用于多种模型和数据集。

Abstract: Machine unlearning aims to remove knowledge of the specific training data in
a well-trained model. Currently, machine unlearning methods typically handle
all forgetting data in a single batch, removing the corresponding knowledge all
at once upon request. However, in practical scenarios, requests for data
removal often arise in a streaming manner rather than in a single batch,
leading to reduced efficiency and effectiveness in existing methods. Such
challenges of streaming forgetting have not been the focus of much research. In
this paper, to address the challenges of performance maintenance, efficiency,
and data access brought about by streaming unlearning requests, we introduce a
streaming unlearning paradigm, formalizing the unlearning as a distribution
shift problem. We then estimate the altered distribution and propose a novel
streaming unlearning algorithm to achieve efficient streaming forgetting
without requiring access to the original training data. Theoretical analyses
confirm an $O(\sqrt{T} + V_T)$ error bound on the streaming unlearning regret,
where $V_T$ represents the cumulative total variation in the optimal solution
over $T$ learning rounds. This theoretical guarantee is achieved under mild
conditions without the strong restriction of convex loss function. Experiments
across various models and datasets validate the performance of our proposed
method.

</details>


### [249] [Mixture of Autoencoder Experts Guidance using Unlabeled and Incomplete Data for Exploration in Reinforcement Learning](https://arxiv.org/abs/2507.15287)
*Elias Malomgré,Pieter Simoens*

Main category: cs.LG

TL;DR: 提出了一种利用不完整专家演示的强化学习框架，通过映射函数将状态与专家数据的相似性转化为内在奖励，实现灵活探索。


<details>
  <summary>Details</summary>
Motivation: 现实环境中，强化学习代理需从无奖励交互和不完整演示中学习，但现有内在动机方法在高维空间或密集奖励环境中表现不佳。

Method: 采用混合自编码专家模型处理不完整演示，通过映射函数生成目标内在奖励，引导代理探索专家行为。

Result: 实验表明，该方法在稀疏和密集奖励环境中均表现优异，即使演示不完整。

Conclusion: 为现实场景中缺乏最优数据和精确奖励控制的强化学习提供了实用框架。

Abstract: Recent trends in Reinforcement Learning (RL) highlight the need for agents to
learn from reward-free interactions and alternative supervision signals, such
as unlabeled or incomplete demonstrations, rather than relying solely on
explicit reward maximization. Additionally, developing generalist agents that
can adapt efficiently in real-world environments often requires leveraging
these reward-free signals to guide learning and behavior. However, while
intrinsic motivation techniques provide a means for agents to seek out novel or
uncertain states in the absence of explicit rewards, they are often challenged
by dense reward environments or the complexity of high-dimensional state and
action spaces. Furthermore, most existing approaches rely directly on the
unprocessed intrinsic reward signals, which can make it difficult to shape or
control the agent's exploration effectively. We propose a framework that can
effectively utilize expert demonstrations, even when they are incomplete and
imperfect. By applying a mapping function to transform the similarity between
an agent's state and expert data into a shaped intrinsic reward, our method
allows for flexible and targeted exploration of expert-like behaviors. We
employ a Mixture of Autoencoder Experts to capture a diverse range of behaviors
and accommodate missing information in demonstrations. Experiments show our
approach enables robust exploration and strong performance in both sparse and
dense reward environments, even when demonstrations are sparse or incomplete.
This provides a practical framework for RL in realistic settings where optimal
data is unavailable and precise reward control is needed.

</details>


### [250] [Preferential subspace identification (PSID) with forward-backward smoothing](https://arxiv.org/abs/2507.15288)
*Omid G. Sani,Maryam M. Shanechi*

Main category: cs.LG

TL;DR: 扩展PSID方法，引入最优滤波和平滑技术，提升多变量时间序列分析的性能。


<details>
  <summary>Details</summary>
Motivation: 现有PSID方法仅利用过去数据预测，而滤波和平滑技术能利用更多信息提升模型估计精度。

Method: 1. 提出PSID滤波方法，通过降秩回归学习最优增益；2. 开发前向-后向PSID平滑算法。

Result: 在模拟数据中验证了方法能恢复真实模型参数，并达到理想滤波和平滑性能。

Conclusion: 为多变量时间序列分析提供了最优线性滤波和平滑的理论框架。

Abstract: System identification methods for multivariate time-series, such as neural
and behavioral recordings, have been used to build models for predicting one
from the other. For example, Preferential Subspace Identification (PSID) builds
a state-space model of a primary time-series (e.g., neural activity) to
optimally predict a secondary time-series (e.g., behavior). However, PSID
focuses on optimal prediction using past primary data, even though in offline
applications, better estimation can be achieved by incorporating concurrent
data (filtering) or all available data (smoothing). Here, we extend PSID to
enable optimal filtering and smoothing. First, we show that the presence of a
secondary signal makes it possible to uniquely identify a model with an optimal
Kalman update step (to enable filtering) from a family of otherwise equivalent
state-space models. Our filtering solution augments PSID with a reduced-rank
regression step that directly learns the optimal gain required for the update
step from data. We refer to this extension of PSID as PSID with filtering.
Second, inspired by two-filter Kalman smoother formulations, we develop a novel
forward-backward PSID smoothing algorithm where we first apply PSID with
filtering and then apply it again in the reverse time direction on the
residuals of the filtered secondary signal. We validate our methods on
simulated data, showing that our approach recovers the ground-truth model
parameters for filtering, and achieves optimal filtering and smoothing decoding
performance of the secondary signal that matches the ideal performance of the
true underlying model. This work provides a principled framework for optimal
linear filtering and smoothing in the two-signal setting, significantly
expanding the toolkit for analyzing dynamic interactions in multivariate
time-series.

</details>


### [251] [Feel-Good Thompson Sampling for Contextual Bandits: a Markov Chain Monte Carlo Showdown](https://arxiv.org/abs/2507.15290)
*Emile Anand,Sarah Liaw*

Main category: cs.LG

TL;DR: 论文研究了FG-TS及其平滑变体SFG-TS在11个真实和合成基准上的表现，发现其在精确后验下优于传统TS，但在近似后验下表现较弱。


<details>
  <summary>Details</summary>
Motivation: 解决Thompson Sampling在高维问题中探索不足的问题，并评估FG-TS在近似后验下的性能。

Method: 通过实验比较FG-TS和SFG-TS在不同后验（精确和近似）下的表现，并分析预处理、奖励规模和先验强度的影响。

Result: FG-TS在线性和逻辑bandits中优于传统TS，但在神经bandits中较弱。

Conclusion: FG-TS及其变体易于使用且具有竞争力，推荐作为现代上下文bandit基准的基线。

Abstract: Thompson Sampling (TS) is widely used to address the exploration/exploitation
tradeoff in contextual bandits, yet recent theory shows that it does not
explore aggressively enough in high-dimensional problems. Feel-Good Thompson
Sampling (FG-TS) addresses this by adding an optimism bonus that biases toward
high-reward models, and it achieves the asymptotically minimax-optimal regret
in the linear setting when posteriors are exact. However, its performance with
\emph{approximate} posteriors -- common in large-scale or neural problems --
has not been benchmarked. We provide the first systematic study of FG-TS and
its smoothed variant (SFG-TS) across eleven real-world and synthetic
benchmarks. To evaluate their robustness, we compare performance across
settings with exact posteriors (linear and logistic bandits) to approximate
regimes produced by fast but coarse stochastic-gradient samplers. Ablations
over preconditioning, bonus scale, and prior strength reveal a trade-off:
larger bonuses help when posterior samples are accurate, but hurt when sampling
noise dominates. FG-TS generally outperforms vanilla TS in linear and logistic
bandits, but tends to be weaker in neural bandits. Nevertheless, because FG-TS
and its variants are competitive and easy-to-use, we recommend them as
baselines in modern contextual-bandit benchmarks. Finally, we provide source
code for all our experiments in
https://github.com/SarahLiaw/ctx-bandits-mcmc-showdown.

</details>


### [252] [Universal crystal material property prediction via multi-view geometric fusion in graph transformers](https://arxiv.org/abs/2507.15303)
*Liang Zhang,Kong Chen,Yuen Wu*

Main category: cs.LG

TL;DR: MGT是一种多视图图变换器框架，通过融合SE3不变和SO3等变图表示，显著提升了晶体材料性质预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效捕捉晶体结构的复杂几何和拓扑特征，限制了机器学习在大规模晶体材料模拟中的应用。

Method: 提出MGT框架，结合SE3不变和SO3等变图表示，并采用轻量级专家混合路由器自适应调整权重。

Result: MGT在晶体性质预测任务中平均绝对误差降低21%，在迁移学习任务中性能提升高达58%。

Conclusion: MGT为晶体材料性质预测提供了高效工具，有望推动新材料发现。

Abstract: Accurately and comprehensively representing crystal structures is critical
for advancing machine learning in large-scale crystal materials simulations,
however, effectively capturing and leveraging the intricate geometric and
topological characteristics of crystal structures remains a core, long-standing
challenge for most existing methods in crystal property prediction. Here, we
propose MGT, a multi-view graph transformer framework that synergistically
fuses SE3 invariant and SO3 equivariant graph representations, which
respectively captures rotation-translation invariance and rotation equivariance
in crystal geometries. To strategically incorporate these complementary
geometric representations, we employ a lightweight mixture of experts router in
MGT to adaptively adjust the weight assigned to SE3 and SO3 embeddings based on
the specific target task. Compared with previous state-of-the-art models, MGT
reduces the mean absolute error by up to 21% on crystal property prediction
tasks through multi-task self-supervised pretraining. Ablation experiments and
interpretable investigations confirm the effectiveness of each technique
implemented in our framework. Additionally, in transfer learning scenarios
including crystal catalyst adsorption energy and hybrid perovskite bandgap
prediction, MGT achieves performance improvements of up to 58% over existing
baselines, demonstrating domain-agnostic scalability across diverse application
domains. As evidenced by the above series of studies, we believe that MGT can
serve as useful model for crystal material property prediction, providing a
valuable tool for the discovery of novel materials.

</details>


### [253] [Beyond Model Base Selection: Weaving Knowledge to Master Fine-grained Neural Network Design](https://arxiv.org/abs/2507.15336)
*Jialiang Wang,Hanmo Liu,Shimin Di,Zhili Wang,Jiachuan Wang,Lei Chen,Xiaofang Zhou*

Main category: cs.LG

TL;DR: M-DESIGN提出了一种动态模型知识库（MKB）管道，通过自适应编织模型架构修改的先验知识，填补了数据库研究中模型细化的空白。


<details>
  <summary>Details</summary>
Motivation: 传统静态模型选择方法忽视了任务查询与模型架构变化之间的细粒度关系，导致匹配效果不佳。M-DESIGN旨在解决这一问题。

Method: M-DESIGN通过知识编织引擎将模型细化重构为任务元数据的自适应查询问题，利用图关系知识模式支持细粒度分析。

Result: 在33个数据-任务对中，M-DESIGN在有限预算内为26对提供了最优模型。

Conclusion: M-DESIGN通过动态知识库和自适应查询优化了模型选择与细化，显著提升了匹配效果。

Abstract: Database systems have recently advocated for embedding machine learning (ML)
capabilities, offering declarative model queries over large, managed model
repositories, thereby circumventing the huge computational overhead of
traditional ML-based algorithms in automated neural network model selection.
Pioneering database studies aim to organize existing benchmark repositories as
model bases (MB), querying them for the model records with the highest
performance estimation metrics for given tasks. However, this static model
selection practice overlooks the fine-grained, evolving relational dependencies
between diverse task queries and model architecture variations, resulting in
suboptimal matches and failing to further refine the model effectively. To fill
the model refinement gap in database research, we propose M-DESIGN, a curated
model knowledge base (MKB) pipeline for mastering neural network refinement by
adaptively weaving prior insights about model architecture modification. First,
we propose a knowledge weaving engine that reframes model refinement as an
adaptive query problem over task metadata. Given a user's task query, M-DESIGN
quickly matches and iteratively refines candidate models by leveraging a
graph-relational knowledge schema that explicitly encodes data properties,
architecture variations, and pairwise performance deltas as joinable relations.
This schema supports fine-grained relational analytics over architecture tweaks
and drives a predictive query planner that can detect and adapt to
out-of-distribution (OOD) tasks. We instantiate M-DESIGN for graph analytics
tasks, where our model knowledge base enriches existing benchmarks with
structured metadata covering 3 graph tasks and 22 graph datasets, contributing
data records of 67,760 graph models. Empirical results demonstrate that
M-DESIGN delivers the optimal model in 26 of 33 data-task pairs within limited
budgets.

</details>


### [254] [Scaling Decentralized Learning with FLock](https://arxiv.org/abs/2507.15349)
*Zehua Cheng,Rui Sun,Jiahao Sun,Yike Guo*

Main category: cs.LG

TL;DR: FLock是一个去中心化框架，用于安全高效的协作式大语言模型（LLM）微调，解决了集中控制和计算通信开销问题。


<details>
  <summary>Details</summary>
Motivation: 传统联邦学习（FL）需要中心服务器，存在单点攻击和中毒攻击风险，且在异构、不可信环境中难以扩展到70B参数模型。

Method: FLock结合区块链信任层和经济激励，取代中心聚合器，提供安全、可审计的合作协议。

Result: 实验验证了FLock在安全、多领域、去中心化环境中微调70B LLM的能力，防御后门攻击，并实现协同知识转移。

Conclusion: FLock显著降低对抗攻击成功率（>68%），且全局模型在跨领域泛化能力上优于孤立训练模型。

Abstract: Fine-tuning the large language models (LLMs) are prevented by the deficiency
of centralized control and the massive computing and communication overhead on
the decentralized schemes. While the typical standard federated learning (FL)
supports data privacy, the central server requirement creates a single point of
attack and vulnerability to poisoning attacks. Generalizing the result in this
direction to 70B-parameter models in the heterogeneous, trustless environments
has turned out to be a huge, yet unbroken bottleneck. This paper introduces
FLock, a decentralized framework for secure and efficient collaborative LLM
fine-tuning. Integrating a blockchain-based trust layer with economic
incentives, FLock replaces the central aggregator with a secure, auditable
protocol for cooperation among untrusted parties. We present the first
empirical validation of fine-tuning a 70B LLM in a secure, multi-domain,
decentralized setting. Our experiments show the FLock framework defends against
backdoor poisoning attacks that compromise standard FL optimizers and fosters
synergistic knowledge transfer. The resulting models show a >68% reduction in
adversarial attack success rates. The global model also demonstrates superior
cross-domain generalization, outperforming models trained in isolation on their
own specialized data.

</details>


### [255] [To Label or Not to Label: PALM -- A Predictive Model for Evaluating Sample Efficiency in Active Learning Models](https://arxiv.org/abs/2507.15381)
*Julia Machnio,Mads Nielsen,Mostafa Mehdipour Ghazi*

Main category: cs.LG

TL;DR: PALM提出了一种统一的数学模型，用于分析主动学习（AL）的动态过程，通过四个关键参数预测AL行为，并在多数据集上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统AL评估方法仅关注最终准确性，无法全面反映学习过程动态，PALM旨在填补这一空白。

Method: PALM通过四个参数（可达到的准确性、覆盖效率、早期表现和可扩展性）建模AL轨迹，支持从部分观测预测未来表现。

Result: 实验表明PALM能跨数据集、预算和策略泛化，准确预测学习曲线，并揭示AL方法的学习效率和可扩展性。

Conclusion: PALM为AL的系统评估提供了基础，支持在研究和实际应用中选择高效策略并预测性能。

Abstract: Active learning (AL) seeks to reduce annotation costs by selecting the most
informative samples for labeling, making it particularly valuable in
resource-constrained settings. However, traditional evaluation methods, which
focus solely on final accuracy, fail to capture the full dynamics of the
learning process. To address this gap, we propose PALM (Performance Analysis of
Active Learning Models), a unified and interpretable mathematical model that
characterizes AL trajectories through four key parameters: achievable accuracy,
coverage efficiency, early-stage performance, and scalability. PALM provides a
predictive description of AL behavior from partial observations, enabling the
estimation of future performance and facilitating principled comparisons across
different strategies. We validate PALM through extensive experiments on
CIFAR-10/100 and ImageNet-50/100/200, covering a wide range of AL methods and
self-supervised embeddings. Our results demonstrate that PALM generalizes
effectively across datasets, budgets, and strategies, accurately predicting
full learning curves from limited labeled data. Importantly, PALM reveals
crucial insights into learning efficiency, data space coverage, and the
scalability of AL methods. By enabling the selection of cost-effective
strategies and predicting performance under tight budget constraints, PALM lays
the basis for more systematic, reproducible, and data-efficient evaluation of
AL in both research and real-world applications. The code is available at:
https://github.com/juliamachnio/PALM.

</details>


### [256] [Learning to Gridize: Segment Physical World by Wireless Communication Channel](https://arxiv.org/abs/2507.15386)
*Juntao Wang,Feng Yin,Tian Ding,Tsung-Hui Chang,Zhi-Quan Luo,Qi Yan*

Main category: cs.LG

TL;DR: 论文提出了一种名为CSG的新型网格化框架，通过联合优化信道估计和网格化，仅使用RSRP数据实现高效分区，并开发了CSG-AE模型和PIDA训练方案，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有网格化方法（如GSG或BSG）依赖不可获取的位置数据或错误假设，限制了大规模网络优化的效率。

Method: 提出CSG框架，联合优化信道估计和网格化；开发CSG-AE模型，包含可训练的RSRP-to-CAPS编码器、稀疏码本量化器和物理信息解码器；提出PIDA训练方案以解决训练问题。

Result: 在合成数据上，CSG-AE在CAPS估计和聚类质量上表现优异；在真实数据上，RSRP预测误差显著降低（Active MAE减少30%，Overall MAE减少65%）。

Conclusion: CSG框架和CSG-AE模型显著提升了网格化效率和性能，为大规模网络优化提供了新方向。

Abstract: Gridization, the process of partitioning space into grids where users share
similar channel characteristics, serves as a fundamental prerequisite for
efficient large-scale network optimization. However, existing methods like
Geographical or Beam Space Gridization (GSG or BSG) are limited by reliance on
unavailable location data or the flawed assumption that similar signal
strengths imply similar channel properties. We propose Channel Space
Gridization (CSG), a pioneering framework that unifies channel estimation and
gridization for the first time. Formulated as a joint optimization problem, CSG
uses only beam-level reference signal received power (RSRP) to estimate Channel
Angle Power Spectra (CAPS) and partition samples into grids with homogeneous
channel characteristics. To perform CSG, we develop the CSG Autoencoder
(CSG-AE), featuring a trainable RSRP-to-CAPS encoder, a learnable sparse
codebook quantizer, and a physics-informed decoder based on the Localized
Statistical Channel Model. On recognizing the limitations of naive training
scheme, we propose a novel Pretraining-Initialization-Detached-Asynchronous
(PIDA) training scheme for CSG-AE, ensuring stable and effective training by
systematically addressing the common pitfalls of the naive training paradigm.
Evaluations reveal that CSG-AE excels in CAPS estimation accuracy and
clustering quality on synthetic data. On real-world datasets, it reduces Active
Mean Absolute Error (MAE) by 30\% and Overall MAE by 65\% on RSRP prediction
accuracy compared to salient baselines using the same data, while improving
channel consistency, cluster sizes balance, and active ratio, advancing the
development of gridization for large-scale network optimization.

</details>


### [257] [MAP Estimation with Denoisers: Convergence Rates and Guarantees](https://arxiv.org/abs/2507.15397)
*Scott Pesme,Giacomo Meanti,Michael Arbel,Julien Mairal*

Main category: cs.LG

TL;DR: 论文提出了一种简单算法，证明其在先验分布为对数凹的情况下能收敛到近端算子，为实践中常用的启发式方法提供了理论基础。


<details>
  <summary>Details</summary>
Motivation: 现有去噪模型在逆问题中被广泛使用，但缺乏对用预训练去噪器替代近端算子的理论支持。

Method: 提出一种与实践中常用方法相关的简单算法，证明其在对数凹先验下收敛到近端算子。

Result: 算法可解释为对平滑近端目标的梯度下降，为启发式方法提供了理论依据。

Conclusion: 研究填补了实践中成功但缺乏理论支持的启发式方法的空白。

Abstract: Denoiser models have become powerful tools for inverse problems, enabling the
use of pretrained networks to approximate the score of a smoothed prior
distribution. These models are often used in heuristic iterative schemes aimed
at solving Maximum a Posteriori (MAP) optimisation problems, where the proximal
operator of the negative log-prior plays a central role. In practice, this
operator is intractable, and practitioners plug in a pretrained denoiser as a
surrogate-despite the lack of general theoretical justification for this
substitution. In this work, we show that a simple algorithm, closely related to
several used in practice, provably converges to the proximal operator under a
log-concavity assumption on the prior $p$. We show that this algorithm can be
interpreted as a gradient descent on smoothed proximal objectives. Our analysis
thus provides a theoretical foundation for a class of empirically successful
but previously heuristic methods.

</details>


### [258] [The calculus of variations of the Transformer on the hyperspherical tangent bundle](https://arxiv.org/abs/2507.15431)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 论文通过拉格朗日优化在令牌空间中为Transformer提供了理论数学背景，将其视为高维单位球上的流映射，并推导了其变分问题的自然求解器。


<details>
  <summary>Details</summary>
Motivation: 为Transformer提供数学理论基础，填补其在流形变分法中的研究空白。

Method: 利用拉格朗日优化和流映射理论，推导Transformer的欧拉-拉格朗日方程，并验证其作为变分问题求解器的合理性。

Result: 证明了Transformer满足变分问题的功能，并提出了新的应用场景和路径最优性损失优化方法。

Conclusion: 论文为Transformer的变分理论奠定了基础，展示了其作为自然求解器的潜力，并提供了新的研究方向。

Abstract: We offer a theoretical mathematical background to Transformers through
Lagrangian optimization across the token space. The Transformer, as a flow map,
exists in the tangent fiber for each token along the high-dimensional unit
sphere. The circumstance of the hypersphere across the latent data is
reasonable due to the trained diagonal matrix equal to the identity, which has
various empirical justifications. Thus, under the continuum limit of the
dynamics, the latent vectors flow among the tangent bundle. Using these facts,
we devise a mathematical framework for the Transformer through calculus of
variations. We develop a functional and show that the continuous flow map
induced by the Transformer satisfies this functional, therefore the Transformer
can be viewed as a natural solver of a calculus of variations problem. We
invent new scenarios of when our methods are applicable based on loss
optimization with respect to path optimality. We derive the Euler-Lagrange
equation for the Transformer. The variant of the Euler-Lagrange equation we
present has various appearances in literature, but, to our understanding,
oftentimes not foundationally proven or under other specialized cases. Our
overarching proof is new: our techniques are classical and the use of the flow
map object is original. We provide several other relevant results, primarily
ones specific to neural scenarios. In particular, much of our analysis will be
attempting to quantify Transformer data in variational contexts under neural
approximations. Calculus of variations on manifolds is a well-nourished
research area, but for the Transformer specifically, it is uncharted: we lay
the foundation for this area through an introduction to the Lagrangian for the
Transformer.

</details>


### [259] [An Adaptive Random Fourier Features approach Applied to Learning Stochastic Differential Equations](https://arxiv.org/abs/2507.15442)
*Owen Douglas,Aku Kammonen,Anamika Pandey,Raúl Tempone*

Main category: cs.LG

TL;DR: 提出一种基于自适应随机傅里叶特征（ARFF）的训练算法，用于从快照数据中学习随机微分方程的漂移和扩散分量，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决从快照数据中学习随机微分方程漂移和扩散分量的挑战，提供更高效和准确的建模方法。

Method: 采用自适应随机傅里叶特征（ARFF）结合Metropolis采样和重采样，基于Euler-Maruyama积分的似然损失函数。

Result: 在多项基准测试中，ARFF方法在损失最小化和收敛速度上均优于传统Adam优化方法。

Conclusion: ARFF方法为数据驱动的随机动力学建模提供了一种高效且性能优越的替代方案。

Abstract: This work proposes a training algorithm based on adaptive random Fourier
features (ARFF) with Metropolis sampling and resampling
\cite{kammonen2024adaptiverandomfourierfeatures} for learning drift and
diffusion components of stochastic differential equations from snapshot data.
Specifically, this study considers It\^{o} diffusion processes and a
likelihood-based loss function derived from the Euler-Maruyama integration
introduced in \cite{Dietrich2023} and
\cite{dridi2021learningstochasticdynamicalsystems}.
  This work evaluates the proposed method against benchmark problems presented
in \cite{Dietrich2023}, including polynomial examples, underdamped Langevin
dynamics, a stochastic susceptible-infected-recovered model, and a stochastic
wave equation. Across all cases, the ARFF-based approach matches or surpasses
the performance of conventional Adam-based optimization in both loss
minimization and convergence speed. These results highlight the potential of
ARFF as a compelling alternative for data-driven modeling of stochastic
dynamics.

</details>


### [260] [FedMultiEmo: Real-Time Emotion Recognition via Multimodal Federated Learning](https://arxiv.org/abs/2507.15470)
*Baran Can Gül,Suraksha Nadig,Stefanos Tziampazis,Nasser Jazdi,Michael Weyrich*

Main category: cs.LG

TL;DR: FedMultiEmo是一个隐私保护的多模态情感识别框架，结合视觉和生理数据，通过联邦学习实现高精度且保护隐私。


<details>
  <summary>Details</summary>
Motivation: 解决车载情感识别中的模态脆弱性、生理变异性和隐私风险问题。

Method: 使用CNN处理面部图像，随机森林处理生理数据，通过联邦学习和多数投票融合。

Result: 联邦CNN准确率77%，随机森林74%，融合后87%，与集中式基线相当。

Conclusion: FedMultiEmo为车载环境提供了实时、隐私保护的情感识别解决方案。

Abstract: In-vehicle emotion recognition underpins adaptive driver-assistance systems
and, ultimately, occupant safety. However, practical deployment is hindered by
(i) modality fragility - poor lighting and occlusions degrade vision-based
methods; (ii) physiological variability - heart-rate and skin-conductance
patterns differ across individuals; and (iii) privacy risk - centralized
training requires transmission of sensitive data. To address these challenges,
we present FedMultiEmo, a privacy-preserving framework that fuses two
complementary modalities at the decision level: visual features extracted by a
Convolutional Neural Network from facial images, and physiological cues (heart
rate, electrodermal activity, and skin temperature) classified by a Random
Forest. FedMultiEmo builds on three key elements: (1) a multimodal federated
learning pipeline with majority-vote fusion, (2) an end-to-end edge-to-cloud
prototype on Raspberry Pi clients and a Flower server, and (3) a personalized
Federated Averaging scheme that weights client updates by local data volume.
Evaluated on FER2013 and a custom physiological dataset, the federated
Convolutional Neural Network attains 77% accuracy, the Random Forest 74%, and
their fusion 87%, matching a centralized baseline while keeping all raw data
local. The developed system converges in 18 rounds, with an average round time
of 120 seconds and a per-client memory footprint below 200 MB. These results
indicate that FedMultiEmo offers a practical approach to real-time,
privacy-aware emotion recognition in automotive settings.

</details>


### [261] [Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback](https://arxiv.org/abs/2507.15507)
*Johannes Ackermann,Takashi Ishida,Masashi Sugiyama*

Main category: cs.LG

TL;DR: 论文提出了一种名为Off-Policy Corrected Reward Modeling (OCRM)的方法，通过重要性加权迭代修正奖励模型，解决了RLHF中因分布偏移导致的过优化问题。


<details>
  <summary>Details</summary>
Motivation: 在RLHF训练过程中，语言模型生成的响应逐渐偏离奖励模型的训练数据分布，导致奖励模型不准确，进而引发过优化问题。

Method: 提出OCRM方法，通过重要性加权迭代修正奖励模型，无需新标签或样本。

Result: 实验证明，OCRM在摘要和聊天机器人数据集上显著优于标准RLHF方法和基线。

Conclusion: OCRM通过修正奖励模型，有效解决了过优化问题，提升了最终策略的性能。

Abstract: Reinforcement Learning from Human Feedback (RLHF) allows us to train models,
such as language models (LMs), to follow complex human preferences. In RLHF for
LMs, we first train an LM using supervised fine-tuning, sample pairs of
responses, obtain human feedback, and use the resulting data to train a reward
model (RM). RL methods are then used to train the LM to maximize the reward
given by the RM. As training progresses, the responses generated by the LM no
longer resemble the responses seen by the RM during training, leading to the RM
becoming inaccurate. The score given by the RM keeps increasing, but the
learned behavior no longer matches the human preferences. This issue is known
as overoptimization. We investigate overoptimization from the point of view of
distribution shift and show that the shift results in an inconsistent estimate
of the RM parameters, leading to an inconsistent estimate of the policy
gradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which
iteratively off-policy corrects the RM using importance weighting, without
requiring new labels or samples. This results in a more accurate RM, which
empirically leads to an improved final policy. We validate our approach in
experiments with summarization and chatbot datasets and show that it performs
significantly better than standard RLHF methods and baselines. Our
implementation is available at
https://github.com/JohannesAck/OffPolicyCorrectedRewardModeling

</details>


### [262] [An Investigation of Test-time Adaptation for Audio Classification under Background Noise](https://arxiv.org/abs/2507.15523)
*Weichuang Shao,Iman Yi Liao,Tomas Henrique Bode Maul,Tissa Chandesa*

Main category: cs.LG

TL;DR: 该研究通过测试时适应（TTA）技术解决音频分类中的域偏移问题，提出改进版CoNMix方法，在噪声背景下表现最优。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在域偏移下性能下降的问题，尤其是在音频分类中由背景噪声引起的域偏移。

Method: 采用TTT、TENT和CoNMix三种TTA方法，并在AudioMNIST和SpeechCommands V1数据集上进行测试。

Result: 改进版CoNMix在噪声背景下分类准确率最高（AM数据集在10 dB运动自行车噪声下错误率为5.31%，3 dB流水噪声下为12.75%）。

Conclusion: 该研究首次将TTA技术应用于音频分类中的域偏移问题，证明了改进版CoNMix的有效性。

Abstract: Domain shift is a prominent problem in Deep Learning, causing a model
pre-trained on a source dataset to suffer significant performance degradation
on test datasets. This research aims to address the issue of audio
classification under domain shift caused by background noise using Test-Time
Adaptation (TTA), a technique that adapts a pre-trained model during testing
using only unlabelled test data before making predictions. We adopt two common
TTA methods, TTT and TENT, and a state-of-the-art method CoNMix, and
investigate their respective performance on two popular audio classification
datasets, AudioMNIST (AM) and SpeechCommands V1 (SC), against different types
of background noise and noise severity levels. The experimental results reveal
that our proposed modified version of CoNMix produced the highest
classification accuracy under domain shift (5.31% error rate under 10 dB
exercise bike background noise and 12.75% error rate under 3 dB running tap
background noise for AM) compared to TTT and TENT. The literature search
provided no evidence of similar works, thereby motivating the work reported
here as the first study to leverage TTA techniques for audio classification
under domain shift.

</details>


### [263] [Data Aware Differentiable Neural Architecture Search for Tiny Keyword Spotting Applications](https://arxiv.org/abs/2507.15545)
*Yujia Shi,Emil Njor,Pablo Martínez-Nuevo,Sven Ewan Shepstone,Xenofon Fafoutis*

Main category: cs.LG

TL;DR: 提出了一种名为“数据感知可微分神经架构搜索”的新方法，用于优化TinyML系统的设计和数据配置，以平衡资源使用和性能。


<details>
  <summary>Details</summary>
Motivation: 机器学习的高资源消耗限制了其广泛应用，尤其是TinyML系统的复杂性阻碍了其普及。

Method: 扩展了可微分神经架构搜索的搜索空间，包括数据配置参数和架构选择，实现模型架构和输入数据的协同优化。

Result: 在关键词识别任务上的初步结果表明，该方法能生成轻量且高精度的系统。

Conclusion: 该方法为TinyML系统设计提供了一种新颖且高效的解决方案。

Abstract: The success of Machine Learning is increasingly tempered by its significant
resource footprint, driving interest in efficient paradigms like TinyML.
However, the inherent complexity of designing TinyML systems hampers their
broad adoption. To reduce this complexity, we introduce "Data Aware
Differentiable Neural Architecture Search". Unlike conventional Differentiable
Neural Architecture Search, our approach expands the search space to include
data configuration parameters alongside architectural choices. This enables
Data Aware Differentiable Neural Architecture Search to co-optimize model
architecture and input data characteristics, effectively balancing resource
usage and system performance for TinyML applications. Initial results on
keyword spotting demonstrate that this novel approach to TinyML system design
can generate lean but highly accurate systems.

</details>


### [264] [The added value for MRI radiomics and deep-learning for glioblastoma prognostication compared to clinical and molecular information](https://arxiv.org/abs/2507.15548)
*D. Abler,O. Pusterla,A. Joye-Kühnis,N. Andratschke,M. Bach,A. Bink,S. M. Christ,P. Hagmann,B. Pouymayou,E. Pravatà,P. Radojewski,M. Reyes,L. Ruinelli,R. Schaer,B. Stieltjes,G. Treglia,W. Valenzuela,R. Wiest,S. Zoergiebel,M. Guckenberger,S. Tanadini-Lang,A. Depeursinge*

Main category: cs.LG

TL;DR: 研究评估了传统放射组学（CR）和深度学习（DL）MRI放射组学在胶质母细胞瘤预后（生存期≤6个月 vs >6个月）中的附加价值，发现其相对于临床和分子预测因子的优势有限。


<details>
  <summary>Details</summary>
Motivation: 探讨放射组学在胶质母细胞瘤预后中的实际价值，尤其是与传统临床和分子预测因子的比较。

Method: 使用来自多个中心的1152例胶质母细胞瘤患者数据，开发并评估了CR和DL模型，比较了不同特征组合（仅影像、仅临床/分子、组合特征）和患者子集的表现。

Result: 在外部验证中，组合特征的CR模型AUC为0.75，略优于仅临床（0.74）和仅影像（0.68）模型，但优势有限。DL模型趋势相似但无统计学意义。

Conclusion: 尽管证实了MRI序列的预测价值，但CR和DL放射组学相对于年龄和性别等临床预测因子的附加价值较小。

Abstract: Background: Radiomics shows promise in characterizing glioblastoma, but its
added value over clinical and molecular predictors has yet to be proven. This
study assessed the added value of conventional radiomics (CR) and deep learning
(DL) MRI radiomics for glioblastoma prognosis (<= 6 vs > 6 months survival) on
a large multi-center dataset.
  Methods: After patient selection, our curated dataset gathers 1152
glioblastoma (WHO 2016) patients from five Swiss centers and one public source.
It included clinical (age, gender), molecular (MGMT, IDH), and baseline MRI
data (T1, T1 contrast, FLAIR, T2) with tumor regions. CR and DL models were
developed using standard methods and evaluated on internal and external
cohorts. Sub-analyses assessed models with different feature sets
(imaging-only, clinical/molecular-only, combined-features) and patient subsets
(S-1: all patients, S-2: with molecular data, S-3: IDH wildtype).
  Results: The best performance was observed in the full cohort (S-1). In
external validation, the combined-feature CR model achieved an AUC of 0.75,
slightly, but significantly outperforming clinical-only (0.74) and imaging-only
(0.68) models. DL models showed similar trends, though without statistical
significance. In S-2 and S-3, combined models did not outperform clinical-only
models. Exploratory analysis of CR models for overall survival prediction
suggested greater relevance of imaging data: across all subsets,
combined-feature models significantly outperformed clinical-only models, though
with a modest advantage of 2-4 C-index points.
  Conclusions: While confirming the predictive value of anatomical MRI
sequences for glioblastoma prognosis, this multi-center study found standard CR
and DL radiomics approaches offer minimal added value over demographic
predictors such as age and gender.

</details>


### [265] [PhysGym: Benchmarking LLMs in Interactive Physics Discovery with Controlled Priors](https://arxiv.org/abs/2507.15550)
*Yimeng Chen,Piotr Piȩkos,Mateusz Ostaszewski,Firas Laakom,Jürgen Schmidhuber*

Main category: cs.LG

TL;DR: PhysGym是一个用于评估基于大型语言模型的代理在科学发现能力上的新基准套件和模拟平台，特别关注环境复杂性和先验知识的利用。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏专门用于评估大型语言模型代理在科学推理中应对环境复杂性和利用先验知识能力的基准。

Method: PhysGym通过交互式物理环境模拟，提供对先验知识水平的精细控制，并设计标准化评估协议和指标。

Result: PhysGym能够区分不同先验知识和任务复杂度下的代理能力，展示了其有效性。

Conclusion: PhysGym填补了现有基准的空白，为评估科学推理能力提供了有力工具。

Abstract: Evaluating the scientific discovery capabilities of large language model
based agents, particularly how they cope with varying environmental complexity
and utilize prior knowledge, requires specialized benchmarks currently lacking
in the landscape. To address this gap, we introduce PhysGym, a novel benchmark
suite and simulation platform for rigorously assessing LLM-based scientific
reasoning in interactive physics environments. PhysGym's primary contribution
lies in its sophisticated control over the level of prior knowledge provided to
the agent. This allows researchers to dissect agent performance along axes
including the complexity of the problem and the prior knowledge levels. The
benchmark comprises a suite of interactive simulations, where agents must
actively probe environments, gather data sequentially under constraints and
formulate hypotheses about underlying physical laws. PhysGym provides
standardized evaluation protocols and metrics for assessing hypothesis accuracy
and model fidelity. We demonstrate the benchmark's utility by presenting
results from baseline LLMs, showcasing its ability to differentiate
capabilities based on varying priors and task complexity.

</details>


### [266] [Trade-offs between elective surgery rescheduling and length-of-stay prediction accuracy](https://arxiv.org/abs/2507.15566)
*Pieter Smet,Martina Doneda,Ettore Lanzarone,Giuliana Carello*

Main category: cs.LG

TL;DR: 本文探讨了住院时间（LOS）预测准确性与床位调度灵活性之间的关系，研究了在预测误差下最有效的患者调度策略以避免床位溢出并优化资源利用。


<details>
  <summary>Details</summary>
Motivation: 下游资源（如床位）的可用性对计划选择性手术患者入院至关重要，但LOS预测与实际值可能存在较大差异，导致计划不可行。研究旨在通过调度策略解决这一问题。

Method: 利用模拟机器学习方法评估数据驱动策略，分析不同纠正政策下LOS预测准确性与调度灵活性的关系。

Result: 研究发现，更准确的LOS预测可以减少调度的影响，但训练高精度模型成本较高。

Conclusion: 在LOS预测误差下，灵活的调度策略（如调整入院日期或转移患者）能有效避免床位溢出并优化资源利用。

Abstract: The availability of downstream resources plays a critical role in planning
the admission of patients undergoing elective surgery, with inpatient beds
being one of the most crucial resources. When planning patient admissions,
predictions on their length-of-stay (LOS) made by machine learning (ML) models
are used to ensure bed availability. However, the actual LOS for each patient
may differ considerably from the predicted value, potentially making the
schedule infeasible. To address such infeasibilities, rescheduling strategies
that take advantage of operational flexibility can be implemented. For example,
adjustments may include postponing admission dates, relocating patients to
different wards, or even transferring patients who are already admitted. The
common assumption is that more accurate LOS predictions reduce the impact of
rescheduling. However, training ML models that can make such accurate
predictions can be costly. Building on previous work that proposed simulated
\ac{ml} for evaluating data-driven approaches, this paper explores the
relationship between LOS prediction accuracy and rescheduling flexibility
across various corrective policies. Specifically, we examine the most effective
patient rescheduling strategies under LOS prediction errors to prevent bed
overflows while optimizing resource utilization.

</details>


### [267] [Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training](https://arxiv.org/abs/2507.15640)
*Kailai Yang,Xiao Liu,Lei Ji,Hao Li,Yeyun Gong,Peng Cheng,Mao Yang*

Main category: cs.LG

TL;DR: 论文提出了一种基于强化学习的模型框架Data Mixing Agent，用于自动调整源领域和目标领域数据的权重，以平衡模型性能。


<details>
  <summary>Details</summary>
Motivation: 持续预训练可能导致模型遗忘原有能力，传统方法依赖人工设计权重策略，缺乏通用性。

Method: 通过强化学习训练Data Mixing Agent，学习通用的数据混合启发式规则。

Result: 在数学推理任务中表现优于基线，且能泛化到未见过的领域和模型。

Conclusion: Data Mixing Agent是一种高效、通用的解决方案，适用于多领域任务。

Abstract: Continual pre-training on small-scale task-specific data is an effective
method for improving large language models in new target fields, yet it risks
catastrophic forgetting of their original capabilities. A common solution is to
re-weight training data mixtures from source and target fields on a domain
space to achieve balanced performance. Previous domain reweighting strategies
rely on manual designation with certain heuristics based on human intuition or
empirical results. In this work, we prove that more general heuristics can be
parameterized by proposing Data Mixing Agent, the first model-based, end-to-end
framework that learns to re-weight domains. The agent learns generalizable
heuristics through reinforcement learning on large quantities of data mixing
trajectories with corresponding feedback from an evaluation environment.
Experiments in continual pre-training on math reasoning show that Data Mixing
Agent outperforms strong baselines in achieving balanced performance across
source and target field benchmarks. Furthermore, it generalizes well across
unseen source fields, target models, and domain spaces without retraining.
Direct application to the code generation field also indicates its adaptability
across target domains. Further analysis showcases the agents' well-aligned
heuristics with human intuitions and their efficiency in achieving superior
model performance with less source-field data.

</details>


### [268] [On the Role of AI in Managing Satellite Constellations: Insights from the ConstellAI Project](https://arxiv.org/abs/2507.15574)
*Gregory F. Stock,Juan A. Fraire,Holger Hermanns,Jędrzej Mosiężny,Yusra Al-Khazraji,Julio Ramírez Molina,Evridiki V. Ntagiou*

Main category: cs.LG

TL;DR: 论文探讨AI在优化卫星巨型星座运营中的作用，通过强化学习（RL）改进数据路由和资源分配，优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 卫星星座快速扩张带来管理挑战，需创新方法实现高效、可扩展和弹性运营。

Method: 利用强化学习（RL）优化数据路由（降低端到端延迟）和资源分配（高效利用电池和内存）。

Result: RL在多种卫星配置和场景中表现优于传统方法，提供更高灵活性、可扩展性和通用性。

Conclusion: AI可改变卫星星座管理格局，提供更自适应、稳健且经济的解决方案。

Abstract: The rapid expansion of satellite constellations in near-Earth orbits presents
significant challenges in satellite network management, requiring innovative
approaches for efficient, scalable, and resilient operations. This paper
explores the role of Artificial Intelligence (AI) in optimizing the operation
of satellite mega-constellations, drawing from the ConstellAI project funded by
the European Space Agency (ESA). A consortium comprising GMV GmbH, Saarland
University, and Thales Alenia Space collaborates to develop AI-driven
algorithms and demonstrates their effectiveness over traditional methods for
two crucial operational challenges: data routing and resource allocation. In
the routing use case, Reinforcement Learning (RL) is used to improve the
end-to-end latency by learning from historical queuing latency, outperforming
classical shortest path algorithms. For resource allocation, RL optimizes the
scheduling of tasks across constellations, focussing on efficiently using
limited resources such as battery and memory. Both use cases were tested for
multiple satellite constellation configurations and operational scenarios,
resembling the real-life spacecraft operations of communications and Earth
observation satellites. This research demonstrates that RL not only competes
with classical approaches but also offers enhanced flexibility, scalability,
and generalizability in decision-making processes, which is crucial for the
autonomous and intelligent management of satellite fleets. The findings of this
activity suggest that AI can fundamentally alter the landscape of satellite
constellation management by providing more adaptive, robust, and cost-effective
solutions.

</details>


### [269] [We Need to Rethink Benchmarking in Anomaly Detection](https://arxiv.org/abs/2507.15584)
*Philipp Röchner,Simon Klüttermann,Franz Rothlauf,Daniel Schlör*

Main category: cs.LG

TL;DR: 论文指出当前异常检测算法的评估方法存在局限性，导致进展停滞，并提出改进方向。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测算法的评估方法未能充分反映实际应用中的多样性，限制了算法的发展。

Method: 提出重新思考异常检测的基准测试方法，包括基于共同分类的场景识别、端到端分析以及根据场景目标进行有意义评估。

Result: 当前评估方法的局限性导致算法性能差异微小，进展停滞。

Conclusion: 需要通过改进评估方法，更好地反映实际应用需求，推动异常检测领域的发展。

Abstract: Despite the continuous proposal of new anomaly detection algorithms and
extensive benchmarking efforts, progress seems to stagnate, with only minor
performance differences between established baselines and new algorithms. In
this position paper, we argue that this stagnation is due to limitations in how
we evaluate anomaly detection algorithms. Current benchmarking does not, for
example, sufficiently reflect the diversity of anomalies in applications
ranging from predictive maintenance to scientific discovery. Consequently, we
need to rethink benchmarking in anomaly detection. In our opinion, anomaly
detection should be studied using scenarios that capture the relevant
characteristics of different applications. We identify three key areas for
improvement: First, we need to identify anomaly detection scenarios based on a
common taxonomy. Second, anomaly detection pipelines should be analyzed
end-to-end and by component. Third, evaluating anomaly detection algorithms
should be meaningful regarding the scenario's objectives.

</details>


### [270] [Red-Team Multi-Agent Reinforcement Learning for Emergency Braking Scenario](https://arxiv.org/abs/2507.15587)
*Yinsong Chen,Kaifeng Wang,Xiaoqiang Meng,Xueyuan Li,Zirui Li,Xin Gao*

Main category: cs.LG

TL;DR: 提出了一种红队多智能体强化学习框架，用于生成安全关键场景中的极端案例，提升自动驾驶车辆决策安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法在安全关键场景中难以捕捉极端案例，需要更高效的方法。

Method: 采用红队多智能体强化学习框架，通过干扰和探索生成极端案例，并利用约束图表示马尔可夫决策过程确保安全。

Result: 实验表明，该方法显著影响自动驾驶车辆决策安全性，并生成多种极端案例。

Conclusion: 该方法为安全关键场景研究提供了新方向。

Abstract: Current research on decision-making in safety-critical scenarios often relies
on inefficient data-driven scenario generation or specific modeling approaches,
which fail to capture corner cases in real-world contexts. To address this
issue, we propose a Red-Team Multi-Agent Reinforcement Learning framework,
where background vehicles with interference capabilities are treated as
red-team agents. Through active interference and exploration, red-team vehicles
can uncover corner cases outside the data distribution. The framework uses a
Constraint Graph Representation Markov Decision Process, ensuring that red-team
vehicles comply with safety rules while continuously disrupting the autonomous
vehicles (AVs). A policy threat zone model is constructed to quantify the
threat posed by red-team vehicles to AVs, inducing more extreme actions to
increase the danger level of the scenario. Experimental results show that the
proposed framework significantly impacts AVs decision-making safety and
generates various corner cases. This method also offers a novel direction for
research in safety-critical scenarios.

</details>


### [271] [Optimal Batch-Size Control for Low-Latency Federated Learning with Device Heterogeneity](https://arxiv.org/abs/2507.15601)
*Huiling Yang,Zhanwei Wang,Kaibin Huang*

Main category: cs.LG

TL;DR: 本文提出了一种针对6G网络中联邦学习的C$^2$-感知框架，通过优化批量大小控制来降低端到端学习延迟，同时确保收敛性。


<details>
  <summary>Details</summary>
Motivation: 由于联邦学习在6G网络中的隐私保护特性，其在物联网应用中的部署需求日益增长。然而，高维模型更新的计算和传输开销以及设备间通信与计算能力的异质性，是实现低延迟联邦学习的主要挑战。

Method: 设计了一种C$^2$-感知框架，通过平衡通信与计算的权衡，提出两种批量大小控制策略，分别适用于慢衰落和快衰落场景，并考虑了设备异质性。

Result: 实验结果表明，所提出的策略在降低学习延迟方面优于传统方法，同时确保了收敛性能。

Conclusion: 该框架为6G网络中联邦学习的低延迟实现提供了有效解决方案，尤其适用于对时间敏感的物联网应用。

Abstract: Federated learning (FL) has emerged as a popular approach for collaborative
machine learning in sixth-generation (6G) networks, primarily due to its
privacy-preserving capabilities. The deployment of FL algorithms is expected to
empower a wide range of Internet-of-Things (IoT) applications, e.g., autonomous
driving, augmented reality, and healthcare. The mission-critical and
time-sensitive nature of these applications necessitates the design of
low-latency FL frameworks that guarantee high learning performance. In
practice, achieving low-latency FL faces two challenges: the overhead of
computing and transmitting high-dimensional model updates, and the
heterogeneity in communication-and-computation (C$^2$) capabilities across
devices. To address these challenges, we propose a novel C$^2$-aware framework
for optimal batch-size control that minimizes end-to-end (E2E) learning latency
while ensuring convergence. The framework is designed to balance a fundamental
C$^2$ tradeoff as revealed through convergence analysis. Specifically,
increasing batch sizes improves the accuracy of gradient estimation in FL and
thus reduces the number of communication rounds required for convergence, but
results in higher per-round latency, and vice versa. The associated problem of
latency minimization is intractable; however, we solve it by designing an
accurate and tractable surrogate for convergence speed, with parameters fitted
to real data. This approach yields two batch-size control strategies tailored
to scenarios with slow and fast fading, while also accommodating device
heterogeneity. Extensive experiments using real datasets demonstrate that the
proposed strategies outperform conventional batch-size adaptation schemes that
do not consider the C$^2$ tradeoff or device heterogeneity.

</details>


### [272] [Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning](https://arxiv.org/abs/2507.15788)
*Sneheel Sarangi,Hanan Salam*

Main category: cs.LG

TL;DR: 小规模LLMs通过RLVR训练难以获得通用的ToM能力，仅在训练数据上表现良好，但无法泛化到新任务。


<details>
  <summary>Details</summary>
Motivation: 探讨是否可以通过RL方法为小规模LLMs注入类似人类的社交智能（如ToM）。

Method: 使用RLVR训练模型，并在多个ToM数据集（HiToM, ExploreToM, FANToM）上评估，测试泛化能力（如OpenToM）。

Result: 小规模LLMs无法获得通用的ToM能力，仅在训练数据上表现提升，泛化能力差。

Conclusion: RL训练导致模型过拟合训练数据，未能真正掌握抽象的ToM能力。

Abstract: Recent advancements in large language models (LLMs) have demonstrated
emergent capabilities in complex reasoning, largely spurred by rule-based
Reinforcement Learning (RL) techniques applied during the post-training. This
has raised the question of whether similar methods can instill more nuanced,
human-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This
paper investigates whether small-scale LLMs can acquire a robust and
generalizable ToM capability through RL with verifiable rewards (RLVR). We
conduct a systematic evaluation by training models on various combinations of
prominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for
generalization on held-out datasets (e.g., OpenToM). Our findings indicate that
small LLMs struggle to develop a generic ToM capability. While performance on
in-distribution tasks improves, this capability fails to transfer to unseen ToM
tasks with different characteristics. Furthermore, we demonstrate that
prolonged RL training leads to models ``hacking'' the statistical patterns of
the training datasets, resulting in significant performance gains on in-domain
data but no change, or degradation of performance on out-of-distribution tasks.
This suggests the learned behavior is a form of narrow overfitting rather than
the acquisition of a true, abstract ToM capability.

</details>


### [273] [Accelerating HEC-RAS: A Recurrent Neural Operator for Rapid River Forecasting](https://arxiv.org/abs/2507.15614)
*Edward Holmberg,Pujan Pokhrel,Maximilian Zoch,Elias Ioup,Ken Pathak,Steven Sloan,Kendall Niles,Jay Ratcliff,Maik Flanagin,Christian Guetl,Julian Simeonov,Mahdi Abdelguerfi*

Main category: cs.LG

TL;DR: 论文提出了一种深度学习替代模型，用于加速基于物理的河流洪水预测，结合GRU和Geo-FNO，显著提升了计算速度。


<details>
  <summary>Details</summary>
Motivation: 传统物理求解器（如HEC-RAS）计算成本高，难以实时决策，需在不牺牲精度的情况下加速模拟。

Method: 采用混合自回归架构，结合GRU捕捉短期时间动态和Geo-FNO建模长程空间依赖，从HEC-RAS文件中提取特征向量进行训练。

Result: 模型在密西西比河流域67个河段上测试，预测精度高（中位绝对误差0.31英尺），计算速度提升3.5倍。

Conclusion: 数据驱动方法通过特征工程可替代传统水力模型，提升大规模洪水预测的计算可行性。

Abstract: Physics-based solvers like HEC-RAS provide high-fidelity river forecasts but
are too computationally intensive for on-the-fly decision-making during flood
events. The central challenge is to accelerate these simulations without
sacrificing accuracy. This paper introduces a deep learning surrogate that
treats HEC-RAS not as a solver but as a data-generation engine. We propose a
hybrid, auto-regressive architecture that combines a Gated Recurrent Unit (GRU)
to capture short-term temporal dynamics with a Geometry-Aware Fourier Neural
Operator (Geo-FNO) to model long-range spatial dependencies along a river
reach. The model learns underlying physics implicitly from a minimal
eight-channel feature vector encoding dynamic state, static geometry, and
boundary forcings extracted directly from native HEC-RAS files. Trained on 67
reaches of the Mississippi River Basin, the surrogate was evaluated on a
year-long, unseen hold-out simulation. Results show the model achieves a strong
predictive accuracy, with a median absolute stage error of 0.31 feet.
Critically, for a full 67-reach ensemble forecast, our surrogate reduces the
required wall-clock time from 139 minutes to 40 minutes, a speedup of nearly
3.5 times over the traditional solver. The success of this data-driven approach
demonstrates that robust feature engineering can produce a viable, high-speed
replacement for conventional hydraulic models, improving the computational
feasibility of large-scale ensemble flood forecasting.

</details>


### [274] [GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding](https://arxiv.org/abs/2507.15846)
*Fei Tang,Zhangxuan Gu,Zhengxi Lu,Xuyang Liu,Shuheng Shen,Changhua Meng,Wen Wang,Wenqi Zhang,Yongliang Shen,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.LG

TL;DR: 论文提出GUI-G²，一种基于高斯分布的奖励框架，用于改进GUI定位任务中的稀疏奖励问题。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法使用二元奖励，忽略了空间交互的连续性，而人类点击行为自然形成高斯分布。

Method: GUI-G²结合高斯点奖励和覆盖奖励，通过自适应方差机制处理不同元素尺度。

Result: 在多个基准测试中，GUI-G²显著优于现有方法，最高提升24.7%。

Conclusion: 连续建模提升了鲁棒性和泛化能力，为GUI交互任务提供了新范式。

Abstract: Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

</details>


### [275] [Towards Explainable Anomaly Detection in Shared Mobility Systems](https://arxiv.org/abs/2507.15643)
*Elnur Isgandarov,Matteo Cederle,Federico Chiariotti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 提出了一种可解释的异常检测框架，用于共享单车系统，结合多源数据和无监督学习，揭示外部因素对异常的影响。


<details>
  <summary>Details</summary>
Motivation: 共享出行系统（如共享单车）对城市交通至关重要，识别异常有助于优化运营、提高服务可靠性和用户体验。

Method: 采用Isolation Forest算法进行无监督异常检测，并结合DIFFI算法提供可解释性，整合了共享单车行程记录、天气条件和公共交通可用性等多源数据。

Result: 结果表明，站点级分析能有效识别异常，并揭示恶劣天气和公共交通限制等外部因素的影响。

Conclusion: 该框架为共享出行运营决策提供了改进依据。

Abstract: Shared mobility systems, such as bike-sharing networks, play a crucial role
in urban transportation. Identifying anomalies in these systems is essential
for optimizing operations, improving service reliability, and enhancing user
experience. This paper presents an interpretable anomaly detection framework
that integrates multi-source data, including bike-sharing trip records, weather
conditions, and public transit availability. The Isolation Forest algorithm is
employed for unsupervised anomaly detection, along with the Depth-based
Isolation Forest Feature Importance (DIFFI) algorithm providing
interpretability. Results show that station-level analysis offers a robust
understanding of anomalies, highlighting the influence of external factors such
as adverse weather and limited transit availability. Our findings contribute to
improving decision-making in shared mobility operations.

</details>


### [276] [GeoHNNs: Geometric Hamiltonian Neural Networks](https://arxiv.org/abs/2507.15678)
*Amine Mohamed Aboussalah,Abdessalam Ed-dib*

Main category: cs.LG

TL;DR: GeoHNN是一种基于几何先验的神经网络框架，通过学习物理定律的几何结构，显著提升了长期稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 现代机器学习方法常忽略物理系统的几何本质，导致预测不稳定。GeoHNN旨在通过编码几何先验解决这一问题。

Method: GeoHNN结合黎曼几何和辛几何，通过对称正定矩阵参数化惯性矩阵，并利用约束自编码器保持相空间体积。

Result: 实验表明，GeoHNN在长期稳定性、精度和能量守恒方面优于现有模型。

Conclusion: 嵌入物理几何结构是构建稳健、可泛化物理模型的实践需求。

Abstract: The fundamental laws of physics are intrinsically geometric, dictating the
evolution of systems through principles of symmetry and conservation. While
modern machine learning offers powerful tools for modeling complex dynamics
from data, common methods often ignore this underlying geometric fabric.
Physics-informed neural networks, for instance, can violate fundamental
physical principles, leading to predictions that are unstable over long
periods, particularly for high-dimensional and chaotic systems. Here, we
introduce \textit{Geometric Hamiltonian Neural Networks (GeoHNN)}, a framework
that learns dynamics by explicitly encoding the geometric priors inherent to
physical laws. Our approach enforces two fundamental structures: the Riemannian
geometry of inertia, by parameterizing inertia matrices in their natural
mathematical space of symmetric positive-definite matrices, and the symplectic
geometry of phase space, using a constrained autoencoder to ensure the
preservation of phase space volume in a reduced latent space. We demonstrate
through experiments on systems ranging from coupled oscillators to
high-dimensional deformable objects that GeoHNN significantly outperforms
existing models. It achieves superior long-term stability, accuracy, and energy
conservation, confirming that embedding the geometry of physics is not just a
theoretical appeal but a practical necessity for creating robust and
generalizable models of the physical world.

</details>


### [277] [Explainable Anomaly Detection for Electric Vehicles Charging Stations](https://arxiv.org/abs/2507.15718)
*Matteo Cederle,Andrea Mazzucco,Andrea Demartini,Eugenio Mazza,Eugenia Suriani,Federico Vitti,Gian Antonio Susto*

Main category: cs.LG

TL;DR: 该研究提出了一种结合无监督异常检测和可解释人工智能（XAI）的方法，用于电动汽车充电基础设施的异常检测和根因分析。


<details>
  <summary>Details</summary>
Motivation: 电动汽车充电站的可靠性和效率需要有效的异常检测，同时需确定异常的根本原因。

Method: 使用Isolation Forest进行异常检测，并采用DIFFI方法识别关键特征。

Result: 方法在真实工业案例中验证了其有效性。

Conclusion: 结合无监督异常检测和XAI技术，可提升充电基础设施的异常检测和解释能力。

Abstract: Electric vehicles (EV) charging stations are one of the critical
infrastructures needed to support the transition to renewable-energy-based
mobility, but ensuring their reliability and efficiency requires effective
anomaly detection to identify irregularities in charging behavior. However, in
such a productive scenario, it is also crucial to determine the underlying
cause behind the detected anomalies. To achieve this goal, this study
investigates unsupervised anomaly detection techniques for EV charging
infrastructure, integrating eXplainable Artificial Intelligence techniques to
enhance interpretability and uncover root causes of anomalies.
  Using real-world sensors and charging session data, this work applies
Isolation Forest to detect anomalies and employs the Depth-based Isolation
Forest Feature Importance (DIFFI) method to identify the most important
features contributing to such anomalies. The efficacy of the proposed approach
is evaluated in a real industrial case.

</details>


### [278] [Competitive Algorithms for Cooperative Multi-Agent Ski-Rental Problems](https://arxiv.org/abs/2507.15727)
*Xuchuang Wang,Bo Sun,Hedyeh Beyhaghi,John C. S. Lui,Mohammad Hajiesmaili,Adam Wierman*

Main category: cs.LG

TL;DR: 论文提出了一种新型多代理滑雪租赁问题，扩展了经典滑雪租赁困境到群体场景，代理需承担个体和共享成本。模型包括租赁、购买个人通行证或折扣团体通行证三种选项，代理活动天数不同导致动态状态。定义了三种竞争比率目标，设计了最优确定性和随机策略，分析显示对称策略更优。


<details>
  <summary>Details</summary>
Motivation: 研究多代理场景下的滑雪租赁问题，探索群体决策中的个体和共享成本动态管理。

Method: 定义了三种竞争比率目标（整体、状态依赖、个体理性），设计了状态感知的阈值函数（确定性策略）和状态感知分布（随机策略）。

Result: 对称策略优于非对称策略，提供了竞争比率上下界，扩展了经典滑雪租赁问题的理论。

Conclusion: 多代理滑雪租赁问题为群体决策提供了新视角，理论和实践意义显著。

Abstract: This paper introduces a novel multi-agent ski-rental problem that generalizes
the classical ski-rental dilemma to a group setting where agents incur
individual and shared costs. In our model, each agent can either rent at a
fixed daily cost, or purchase a pass at an individual cost, with an additional
third option of a discounted group pass available to all. We consider scenarios
in which agents' active days differ, leading to dynamic states as agents drop
out of the decision process. To address this problem from different
perspectives, we define three distinct competitive ratios: overall,
state-dependent, and individual rational. For each objective, we design and
analyze optimal deterministic and randomized policies. Our deterministic
policies employ state-aware threshold functions that adapt to the dynamic
states, while our randomized policies sample and resample thresholds from
tailored state-aware distributions. The analysis reveals that symmetric
policies, in which all agents use the same threshold, outperform asymmetric
ones. Our results provide competitive ratio upper and lower bounds and extend
classical ski-rental insights to multi-agent settings, highlighting both
theoretical and practical implications for group decision-making under
uncertainty.

</details>


### [279] [Multi-Modal Sensor Fusion for Proactive Blockage Prediction in mmWave Vehicular Networks](https://arxiv.org/abs/2507.15769)
*Ahmad M. Nazar,Abdulkadir Celik,Mohamed Y. Selim,Asmaa Abdallah,Daji Qiao,Ahmed M. Eltawil*

Main category: cs.LG

TL;DR: 提出了一种基于多模态感知的毫米波信号阻塞预测框架，结合摄像头、GPS、LiDAR和雷达数据，通过深度学习模型和软加权融合策略实现高效预测。


<details>
  <summary>Details</summary>
Motivation: 毫米波频段的车载通信系统易受动态障碍物（如车辆、行人）的信号阻塞影响，需提前预测以优化通信。

Method: 使用多模态传感器数据，通过独立的深度学习模型处理各传感器流，并基于验证性能采用软加权融合策略。

Result: 摄像头单独模型的F1分数为97.1%，推理时间89.8ms；摄像头+雷达组合的F1分数提升至97.2%，推理时间95.7ms。

Conclusion: 多模态感知在毫米波阻塞预测中表现出高效性和准确性，为动态环境中的主动无线通信提供了可行方案。

Abstract: Vehicular communication systems operating in the millimeter wave (mmWave)
band are highly susceptible to signal blockage from dynamic obstacles such as
vehicles, pedestrians, and infrastructure. To address this challenge, we
propose a proactive blockage prediction framework that utilizes multi-modal
sensing, including camera, GPS, LiDAR, and radar inputs in an
infrastructure-to-vehicle (I2V) setting. This approach uses modality-specific
deep learning models to process each sensor stream independently and fuses
their outputs using a softmax-weighted ensemble strategy based on validation
performance. Our evaluations, for up to 1.5s in advance, show that the
camera-only model achieves the best standalone trade-off with an F1-score of
97.1% and an inference time of 89.8ms. A camera+radar configuration further
improves accuracy to 97.2% F1 at 95.7ms. Our results display the effectiveness
and efficiency of multi-modal sensing for mmWave blockage prediction and
provide a pathway for proactive wireless communication in dynamic environments.

</details>


### [280] [Deep-Learning Investigation of Vibrational Raman Spectra for Plant-Stress Analysis](https://arxiv.org/abs/2507.15772)
*Anoop C. Patil,Benny Jian Rong Sng,Yu-Wei Chang,Joana B. Pereira,Chua Nam-Hai,Rajani Sarojam,Gajendra Pratap Singh,In-Cheol Jang,Giovanni Volpe*

Main category: cs.LG

TL;DR: DIVA是一种基于变分自编码器的全自动工作流程，用于通过拉曼光谱分析植物应激，无需手动预处理。


<details>
  <summary>Details</summary>
Motivation: 植物应激检测对农业至关重要，传统拉曼分析方法存在偏见和不一致性问题。

Method: DIVA利用深度学习处理原始拉曼光谱（包括荧光背景），自动识别和量化关键光谱特征。

Result: DIVA成功检测了多种植物应激（如非生物和生物应激），展示了AI驱动的植物健康评估潜力。

Conclusion: DIVA为更可持续的农业实践提供了AI驱动的植物健康评估新方法。

Abstract: Detecting stress in plants is crucial for both open-farm and
controlled-environment agriculture. Biomolecules within plants serve as key
stress indicators, offering vital markers for continuous health monitoring and
early disease detection. Raman spectroscopy provides a powerful, non-invasive
means to quantify these biomolecules through their molecular vibrational
signatures. However, traditional Raman analysis relies on customized
data-processing workflows that require fluorescence background removal and
prior identification of Raman peaks of interest-introducing potential biases
and inconsistencies. Here, we introduce DIVA (Deep-learning-based Investigation
of Vibrational Raman spectra for plant-stress Analysis), a fully automated
workflow based on a variational autoencoder. Unlike conventional approaches,
DIVA processes native Raman spectra-including fluorescence backgrounds-without
manual preprocessing, identifying and quantifying significant spectral features
in an unbiased manner. We applied DIVA to detect a range of plant stresses,
including abiotic (shading, high light intensity, high temperature) and biotic
stressors (bacterial infections). By integrating deep learning with vibrational
spectroscopy, DIVA paves the way for AI-driven plant health assessment,
fostering more resilient and sustainable agricultural practices.

</details>


### [281] [Dynamics is what you need for time-series forecasting!](https://arxiv.org/abs/2507.15774)
*Alexis-Raja Brachet,Pierre-Yves Richard,Céline Hudelot*

Main category: cs.LG

TL;DR: 论文提出了一种名为PRO-DYN的命名法，用于分析时间序列预测模型中动态学习的重要性，并通过实验验证了动态学习块的位置和完整性对模型性能的关键影响。


<details>
  <summary>Details</summary>
Motivation: 时间序列预测任务中，现有深度模型在简单任务上表现不佳，假设是因为未能充分学习数据的底层动态。

Method: 提出PRO-DYN命名法，系统性分析现有模型，并通过实验验证动态学习块的位置和完整性对性能的影响。

Result: 实验表明，性能较差的架构仅部分学习动态，且动态块位于模型末端时效果最佳。

Conclusion: 研究支持在模型中引入可学习的动态块，并将其作为最终预测器的重要性。

Abstract: While boundaries between data modalities are vanishing, the usual successful
deep models are still challenged by simple ones in the time-series forecasting
task. Our hypothesis is that this task needs models that are able to learn the
data underlying dynamics. We propose to validate it through both systemic and
empirical studies. We develop an original $\texttt{PRO-DYN}$ nomenclature to
analyze existing models through the lens of dynamics. Two observations thus
emerged: $\textbf{1}$. under-performing architectures learn dynamics at most
partially, $\textbf{2}$. the location of the dynamics block at the model end is
of prime importance. We conduct extensive experiments to confirm our
observations on a set of performance-varying models with diverse backbones.
Results support the need to incorporate a learnable dynamics block and its use
as the final predictor.

</details>


### [282] [Graph Attention Specialized Expert Fusion Model for Node Classification: Based on Cora and Pubmed Datasets](https://arxiv.org/abs/2507.15784)
*Zihang Ma,Qitian Yin*

Main category: cs.LG

TL;DR: 论文提出了一种基于Wasserstein-Rubinstein距离的专家融合模型（WR-EFM），用于解决图节点分类中类别间性能差异的问题，显著提升了分类平衡性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在PubMed引文网络数据集中，传统GCN模型对不同类别的分类性能差异显著（如Category 2的准确率比Category 1低7.5%），需要一种方法来平衡性能。

Method: 设计了WR-EFM模型，针对不同类别训练专用GNN模型（如对Category 0/1使用层归一化和残差连接，对Category 2使用多跳GAT），并通过WR距离优化模型间的表示相似性。采用自适应融合策略动态加权模型。

Result: WR-EFM在三个类别上的准确率分别为77.8%、78.0%和79.9%，优于单一模型和标准融合方法。Category 2的准确率提升了5.5%，且类别间性能差异显著降低（CV从0.058降至0.013）。

Conclusion: WR-EFM通过WR距离引导的融合策略有效解决了类别不平衡问题，为图分类任务提供了新范式。代码已开源。

Abstract: Graph node classification is a fundamental task in graph neural networks
(GNNs), aiming to assign predefined class labels to nodes. On the PubMed
citation network dataset, we observe significant classification difficulty
disparities, with Category 2 achieving only 74.4% accuracy in traditional GCN,
7.5% lower than Category 1. To address this, we propose a
Wasserstein-Rubinstein (WR) distance enhanced Expert Fusion Model (WR-EFM),
training specialized GNN models for Categories 0/1 (with layer normalization
and residual connections) and Multi-hop Graph Attention Networks (GAT) for
Category 2. The WR distance metric optimizes representation similarity between
models, particularly focusing on improving Category 2 performance. Our adaptive
fusion strategy dynamically weights models based on category-specific
performance, with Category 2 assigned a GAT weight of 0.8. WR distance further
guides the fusion process by measuring distributional differences between model
representations, enabling more principled integration of complementary
features.
  Experimental results show WR-EFM achieves balanced accuracy across
categories: 77.8% (Category 0), 78.0% (Category 1), and 79.9% (Category 2),
outperforming both single models and standard fusion approaches. The
coefficient of variation (CV) of WR-EFM's category accuracies is 0.013, 77.6%
lower than GCN's 0.058, demonstrating superior stability. Notably, WR-EFM
improves Category 2 accuracy by 5.5% compared to GCN, verifying the
effectiveness of WR-guided fusion in capturing complex structural patterns.
This work provides a novel paradigm for handling class-imbalanced graph
classification tasks. To promote the research community, we release our project
at https://github.com/s010m00n/GASEM4NC.

</details>


### [283] [Federated Split Learning with Improved Communication and Storage Efficiency](https://arxiv.org/abs/2507.15816)
*Yujia Mu,Cong Shen*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的通信和存储高效联邦分割学习方法（CSE-FSL），通过辅助网络本地更新客户端权重，减少服务器存储需求和通信开销。


<details>
  <summary>Details</summary>
Motivation: 联邦分割学习（FSL）虽然能减轻边缘设备的计算负担，但仍存在高通信开销和服务器存储需求大的问题。

Method: CSE-FSL利用辅助网络本地更新客户端权重，服务器仅维护单一模型，并选择性地在特定轮次传输数据以减少通信量。

Result: 理论分析和实验结果表明，CSE-FSL在非凸损失函数下收敛，并在真实任务中显著减少通信开销。

Conclusion: CSE-FSL有效解决了FSL的高通信和存储需求问题，为联邦学习提供了更高效的解决方案。

Abstract: Federated learning (FL) is one of the popular distributed machine learning
(ML) solutions but incurs significant communication and computation costs at
edge devices. Federated split learning (FSL) can train sub-models in parallel
and reduce the computational burden of edge devices by splitting the model
architecture. However, it still requires a high communication overhead due to
transmitting the smashed data and gradients between clients and the server in
every global round. Furthermore, the server must maintain separate partial
models for every client, leading to a significant storage requirement. To
address these challenges, this paper proposes a novel communication and storage
efficient federated split learning method, termed CSE-FSL, which utilizes an
auxiliary network to locally update the weights of the clients while keeping a
single model at the server, hence avoiding frequent transmissions of gradients
from the server and greatly reducing the storage requirement of the server.
Additionally, a new model update method of transmitting the smashed data in
selected epochs can reduce the amount of smashed data sent from the clients. We
provide a theoretical analysis of CSE-FSL, rigorously guaranteeing its
convergence under non-convex loss functions. The extensive experimental results
further indicate that CSE-FSL achieves a significant communication reduction
over existing FSL solutions using real-world FL tasks.

</details>


### [284] [Multi-Strategy Improved Snake Optimizer Accelerated CNN-LSTM-Attention-Adaboost for Trajectory Prediction](https://arxiv.org/abs/2507.15832)
*Shiyang Li*

Main category: cs.LG

TL;DR: 提出了一种混合CNN-LSTM-attention-Adaboost神经网络模型，结合改进的多策略蛇群优化算法（SO），用于提升4D轨迹预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决中长期4D轨迹预测模型的局限性，提高预测精度。

Method: 使用Adaboost算法划分多个弱学习器，每个子模型结合CNN提取空间特征、LSTM捕捉时间特征、注意力机制捕获全局特征，并通过SO算法优化超参数。

Result: 实验表明，SO-CLA-adaboost在处理大规模高维轨迹数据时优于传统优化器，预测精度提升39.89%。

Conclusion: 该模型显著提升了4D轨迹预测的准确性和效率。

Abstract: To address the limitations of medium- and long-term four-dimensional (4D)
trajectory prediction models, this paper proposes a hybrid
CNN-LSTM-attention-adaboost neural network model incorporating a multi-strategy
improved snake-herd optimization (SO) algorithm. The model applies the Adaboost
algorithm to divide multiple weak learners, and each submodel utilizes CNN to
extract spatial features, LSTM to capture temporal features, and attention
mechanism to capture global features comprehensively. The strong learner model,
combined with multiple sub-models, then optimizes the hyperparameters of the
prediction model through the natural selection behavior pattern simulated by
SO. In this study, based on the real ADS-B data from Xi'an to Tianjin, the
comparison experiments and ablation studies of multiple optimizers are carried
out, and a comprehensive test and evaluation analysis is carried out. The
results show that SO-CLA-adaboost outperforms traditional optimizers such as
particle swarm, whale, and gray wolf in handling large-scale high-dimensional
trajectory data. In addition, introducing the full-strategy collaborative
improvement SO algorithm improves the model's prediction accuracy by 39.89%.

</details>


### [285] [Optimizing Canaries for Privacy Auditing with Metagradient Descent](https://arxiv.org/abs/2507.15836)
*Matteo Boglioni,Terrance Liu,Andrew Ilyas,Zhiwei Steven Wu*

Main category: cs.LG

TL;DR: 论文研究了黑盒隐私审计，通过优化审计者的“金丝雀”数据集，显著提高了差分隐私深度学习模型的隐私参数下限。


<details>
  <summary>Details</summary>
Motivation: 目标是改进差分隐私学习算法的隐私参数下限估计，仅通过算法输出（如训练后的模型）进行审计。

Method: 提出了一种优化审计者金丝雀数据集的方法，利用元梯度优化技术，提升隐私审计效果。

Result: 实验表明，优化后的金丝雀数据集能将差分隐私图像分类模型的隐私参数下限提高超过2倍。

Conclusion: 优化金丝雀数据集的方法具有可迁移性和高效性，适用于不同规模和隐私设置的模型审计。

Abstract: In this work we study black-box privacy auditing, where the goal is to lower
bound the privacy parameter of a differentially private learning algorithm
using only the algorithm's outputs (i.e., final trained model). For DP-SGD (the
most successful method for training differentially private deep learning
models), the canonical approach auditing uses membership inference-an auditor
comes with a small set of special "canary" examples, inserts a random subset of
them into the training set, and then tries to discern which of their canaries
were included in the training set (typically via a membership inference
attack). The auditor's success rate then provides a lower bound on the privacy
parameters of the learning algorithm. Our main contribution is a method for
optimizing the auditor's canary set to improve privacy auditing, leveraging
recent work on metagradient optimization. Our empirical evaluation demonstrates
that by using such optimized canaries, we can improve empirical lower bounds
for differentially private image classification models by over 2x in certain
instances. Furthermore, we demonstrate that our method is transferable and
efficient: canaries optimized for non-private SGD with a small model
architecture remain effective when auditing larger models trained with DP-SGD.

</details>


### [286] [FASTGEN: Fast and Cost-Effective Synthetic Tabular Data Generation with LLMs](https://arxiv.org/abs/2507.15839)
*Anh Nguyen,Sam Schafft,Nicholas Hale,John Alfaro*

Main category: cs.LG

TL;DR: 提出了一种利用大语言模型（LLM）快速生成高质量表格数据的方法，通过将字段分布编码为可重用的采样脚本，显著降低了时间和成本。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法直接使用LLM逐条生成数据时的高成本和时间消耗问题。

Method: 利用LLM推断字段分布并生成可重用的采样脚本，自动分类字段类型（数值、分类或自由文本）。

Result: 实验表明，该方法在数据多样性和真实性上优于传统方法，大幅降低了大规模数据生成的负担。

Conclusion: 该方法可加速生产管道的测试，缩短开发周期，为合成数据生成提供了一种高效、经济的解决方案。

Abstract: Synthetic data generation has emerged as an invaluable solution in scenarios
where real-world data collection and usage are limited by cost and scarcity.
Large language models (LLMs) have demonstrated remarkable capabilities in
producing high-fidelity, domain-relevant samples across various fields.
However, existing approaches that directly use LLMs to generate each record
individually impose prohibitive time and cost burdens, particularly when large
volumes of synthetic data are required. In this work, we propose a fast,
cost-effective method for realistic tabular data synthesis that leverages LLMs
to infer and encode each field's distribution into a reusable sampling script.
By automatically classifying fields into numerical, categorical, or free-text
types, the LLM generates distribution-based scripts that can efficiently
produce diverse, realistic datasets at scale without continuous model
inference. Experimental results show that our approach outperforms traditional
direct methods in both diversity and data realism, substantially reducing the
burden of high-volume synthetic data generation. We plan to apply this
methodology to accelerate testing in production pipelines, thereby shortening
development cycles and improving overall system efficiency. We believe our
insights and lessons learned will aid researchers and practitioners seeking
scalable, cost-effective solutions for synthetic data generation.

</details>


### [287] [Diffusion Beats Autoregressive in Data-Constrained Settings](https://arxiv.org/abs/2507.15857)
*Mihir Prabhudesai,Menging Wu,Amir Zadeh,Katerina Fragkiadaki,Deepak Pathak*

Main category: cs.LG

TL;DR: 扩散模型在数据受限情况下优于自回归模型，尤其在计算资源充足时表现更佳。


<details>
  <summary>Details</summary>
Motivation: 探索扩散模型在数据受限场景下的优势，填补其与自回归模型比较的研究空白。

Method: 系统研究掩码扩散模型在数据受限环境中的表现，分析其隐式数据增强特性。

Result: 扩散模型在数据稀缺时显著优于自回归模型，验证损失更低且下游任务表现更优。

Conclusion: 当数据成为瓶颈时，扩散模型是自回归模型的有力替代方案。

Abstract: Autoregressive (AR) models have long dominated the landscape of large
language models, driving progress across a wide range of tasks. Recently,
diffusion-based language models have emerged as a promising alternative, though
their advantages over AR models remain underexplored. In this paper, we
systematically study masked diffusion models in data-constrained settings-where
training involves repeated passes over limited data-and find that they
significantly outperform AR models when compute is abundant but data is scarce.
Diffusion models make better use of repeated data, achieving lower validation
loss and superior downstream performance. We interpret this advantage as
implicit data augmentation: masked diffusion exposes the model to a diverse
distribution of token orderings and prediction tasks, unlike AR's fixed
left-to-right factorization. We find new scaling laws for diffusion models and
derive a closed-form expression for the critical compute threshold at which
diffusion begins to outperform AR. These results suggest that when data, not
compute, is the bottleneck, diffusion models offer a compelling alternative to
the standard AR paradigm. Our code is available at:
https://diffusion-scaling.github.io.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [288] [Dissociating model architectures from inference computations](https://arxiv.org/abs/2507.15776)
*Noor Sajid,Johan Medrano*

Main category: q-bio.NC

TL;DR: 论文探讨了自回归模型和深度时序模型在非马尔可夫序列建模中的差异，提出分离模型架构与推断计算的重要性，并证明通过结构化上下文访问，自回归模型可以模仿深度时序计算。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于理解不同模型架构（自回归与深度时序）在非马尔可夫序列建模中的表现差异，并探索如何通过优化推断计算提升效率。

Method: 方法包括使用基于Transformer的自回归模型，通过结构化上下文访问和分层时序分解优化推断计算。

Result: 结果表明，分层时序分解在保持预测能力的同时减少了计算量，说明预测构建和优化的过程可以与模型架构解耦。

Conclusion: 结论强调了预测过程与模型架构的独立性，为高效序列建模提供了新思路。

Abstract: Parr et al., 2025 examines how auto-regressive and deep temporal models
differ in their treatment of non-Markovian sequence modelling. Building on
this, we highlight the need for dissociating model architectures, i.e., how the
predictive distribution factorises, from the computations invoked at inference.
We demonstrate that deep temporal computations are mimicked by autoregressive
models by structuring context access during iterative inference. Using a
transformer trained on next-token prediction, we show that inducing
hierarchical temporal factorisation during iterative inference maintains
predictive capacity while instantiating fewer computations. This emphasises
that processes for constructing and refining predictions are not necessarily
bound to their underlying model architectures.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [289] [JELAI: Integrating AI and Learning Analytics in Jupyter Notebooks](https://arxiv.org/abs/2505.17593)
*Manuel Valle Torre,Thom van der Velden,Marcus Specht,Catharine Oertel*

Main category: cs.HC

TL;DR: JELAI是一个开源平台架构，将细粒度学习分析（LA）与基于大型语言模型（LLM）的辅导集成到Jupyter Notebook环境中，支持实时、上下文敏感的AI辅助和研究。


<details>
  <summary>Details</summary>
Motivation: 生成式AI在教育支持方面潜力巨大，但缺乏教学基础和学生学习情境的感知能力，且在真实学习环境中研究学生与工具的交互具有挑战性。

Method: JELAI采用模块化、容器化设计，包括JupyterLab扩展（用于遥测和聊天）和中央中间件（处理LA和上下文感知的LLM提示增强），以捕获代码交互和聊天数据。

Result: 系统通过性能基准测试和两个概念验证用例展示了其可行性，能够记录多模态数据、分析求助模式，并支持AI配置的A/B测试。

Conclusion: JELAI的主要贡献是其技术框架，为研究人员和教育工作者提供了一个灵活的工具，用于在Jupyter生态系统中开发、部署和研究基于LA的AI辅导。

Abstract: Generative AI offers potential for educational support, but often lacks
pedagogical grounding and awareness of the student's learning context.
Furthermore, researching student interactions with these tools within authentic
learning environments remains challenging. To address this, we present JELAI,
an open-source platform architecture designed to integrate fine-grained
Learning Analytics (LA) with Large Language Model (LLM)-based tutoring directly
within a Jupyter Notebook environment. JELAI employs a modular, containerized
design featuring JupyterLab extensions for telemetry and chat, alongside a
central middleware handling LA processing and context-aware LLM prompt
enrichment. This architecture enables the capture of integrated code
interaction and chat data, facilitating real-time, context-sensitive AI
scaffolding and research into student behaviour. We describe the system's
design, implementation, and demonstrate its feasibility through system
performance benchmarks and two proof-of-concept use cases illustrating its
capabilities for logging multi-modal data, analysing help-seeking patterns, and
supporting A/B testing of AI configurations. JELAI's primary contribution is
its technical framework, providing a flexible tool for researchers and
educators to develop, deploy, and study LA-informed AI tutoring within the
widely used Jupyter ecosystem.

</details>


### [290] [Assessing the Reliability of Large Language Models for Deductive Qualitative Coding: A Comparative Study of ChatGPT Interventions](https://arxiv.org/abs/2507.14384)
*Angjelin Hila,Elliott Hauser*

Main category: cs.HC

TL;DR: 研究了使用ChatGPT进行结构化演绎定性编码的潜力，测试了四种干预方法，发现逐步任务分解策略效果最佳。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在演绎分类任务中的应用潜力，填补现有研究中对这一领域的不足。

Method: 使用CAP Master Codebook对最高法院案例进行分类，测试了零样本、少样本、基于定义和逐步任务分解四种策略。

Result: 逐步任务分解策略表现最佳（准确率0.775，kappa 0.744），分类行为显著受干预策略影响。

Conclusion: 通过定制化干预，大型语言模型可达到适用于严格定性编码流程的可靠性水平。

Abstract: In this study, we investigate the use of large language models (LLMs),
specifically ChatGPT, for structured deductive qualitative coding. While most
current research emphasizes inductive coding applications, we address the
underexplored potential of LLMs to perform deductive classification tasks
aligned with established human-coded schemes. Using the Comparative Agendas
Project (CAP) Master Codebook, we classified U.S. Supreme Court case summaries
into 21 major policy domains. We tested four intervention methods: zero-shot,
few-shot, definition-based, and a novel Step-by-Step Task Decomposition
strategy, across repeated samples. Performance was evaluated using standard
classification metrics (accuracy, F1-score, Cohen's kappa, Krippendorff's
alpha), and construct validity was assessed using chi-squared tests and
Cramer's V. Chi-squared and effect size analyses confirmed that intervention
strategies significantly influenced classification behavior, with Cramer's V
values ranging from 0.359 to 0.613, indicating moderate to strong shifts in
classification patterns. The Step-by-Step Task Decomposition strategy achieved
the strongest reliability (accuracy = 0.775, kappa = 0.744, alpha = 0.746),
achieving thresholds for substantial agreement. Despite the semantic ambiguity
within case summaries, ChatGPT displayed stable agreement across samples,
including high F1 scores in low-support subclasses. These findings demonstrate
that with targeted, custom-tailored interventions, LLMs can achieve reliability
levels suitable for integration into rigorous qualitative coding workflows.

</details>


### [291] [Designing Conversational AI to Support Think-Aloud Practice in Technical Interview Preparation for CS Students](https://arxiv.org/abs/2507.14418)
*Taufiq Daryanto,Sophia Stil,Xiaohan Ding,Daniel Manesh,Sang Won Lee,Tim Lee,Stephanie Lunn,Sarah Rodriguez,Chris Brown,Eugenia Rho*

Main category: cs.HC

TL;DR: 研究探讨了对话AI在技术面试中“大声思考”练习中的应用，发现参与者重视AI的模拟、反馈和学习功能，并提出了设计建议和更广泛的研究方向。


<details>
  <summary>Details</summary>
Motivation: 技术面试中“大声思考”练习的结构化机会有限，对话AI的潜力尚未充分研究。

Method: 17名参与者使用基于LLM的技术面试练习工具进行研究。

Result: 参与者认可AI在模拟、反馈和学习中的作用，提出了设计建议和更广泛的研究方向。

Conclusion: 对话AI在技术面试练习中具有潜力，需进一步探索人机协作的研究方向。

Abstract: One challenge in technical interviews is the think-aloud process, where
candidates verbalize their thought processes while solving coding tasks.
Despite its importance, opportunities for structured practice remain limited.
Conversational AI offers potential assistance, but limited research explores
user perceptions of its role in think-aloud practice. To address this gap, we
conducted a study with 17 participants using an LLM-based technical interview
practice tool. Participants valued AI's role in simulation, feedback, and
learning from generated examples. Key design recommendations include promoting
social presence in conversational AI for technical interview simulation,
providing feedback beyond verbal content analysis, and enabling crowdsourced
think-aloud examples through human-AI collaboration. Beyond feature design, we
examined broader considerations, including intersectional challenges and
potential strategies to address them, how AI-driven interview preparation could
promote equitable learning in computing careers, and the need to rethink AI's
role in interview practice by suggesting a research direction that integrates
human-AI collaboration.

</details>


### [292] [XplainAct: Visualization for Personalized Intervention Insights](https://arxiv.org/abs/2507.14767)
*Yanming Zhang,Krishnakumar Hegde,Klaus Mueller*

Main category: cs.HC

TL;DR: XplainAct是一个可视化分析框架，支持在子群体中模拟、解释和推理个体层面的干预效果，解决了现有方法在异质性系统中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有因果推理方法主要关注群体层面的效果，无法处理异质性系统中干预效果的广泛差异。

Method: 提出XplainAct框架，通过两个案例研究（流行病学中的阿片类药物相关死亡和总统选举中的投票倾向）验证其有效性。

Result: XplainAct能够有效支持个体层面的干预分析和解释。

Conclusion: XplainAct为异质性系统中的因果推理提供了实用工具。

Abstract: Causality helps people reason about and understand complex systems,
particularly through what-if analyses that explore how interventions might
alter outcomes. Although existing methods embrace causal reasoning using
interventions and counterfactual analysis, they primarily focus on effects at
the population level. These approaches often fall short in systems
characterized by significant heterogeneity, where the impact of an intervention
can vary widely across subgroups. To address this challenge, we present
XplainAct, a visual analytics framework that supports simulating, explaining,
and reasoning interventions at the individual level within subpopulations. We
demonstrate the effectiveness of XplainAct through two case studies:
investigating opioid-related deaths in epidemiology and analyzing voting
inclinations in the presidential election.

</details>


### [293] [NavVI: A Telerobotic Simulation with Multimodal Feedback for Visually Impaired Navigation in Warehouse Environments](https://arxiv.org/abs/2507.15072)
*Maisha Maimuna,Minhaz Bin Farukee,Sama Nikanfar,Mahfuza Siddiqua,Ayon Roy,Fillia Makedon*

Main category: cs.HC

TL;DR: 开发了一种多模态引导模拟器，帮助盲人和低视力操作者在工业仓库中安全遥控机器人，提供视觉、听觉和触觉反馈。


<details>
  <summary>Details</summary>
Motivation: 工业仓库环境复杂，盲人和低视力操作者遥控机器人存在高风险，现有研究缺乏针对性的多模态引导系统。

Method: 结合导航网格和实时路径规划，提供视觉路径线、语音导航提示和触觉反馈，避免碰撞。

Result: 系统为盲人和低视力用户提供了有效的遥控支持，并支持快速部署到实际机器人。

Conclusion: 该模拟器为无障碍遥控研究提供了可重复测试平台，设计原则易于适配到实际机器人。

Abstract: Industrial warehouses are congested with moving forklifts, shelves and
personnel, making robot teleoperation particularly risky and demanding for
blind and low-vision (BLV) operators. Although accessible teleoperation plays a
key role in inclusive workforce participation, systematic research on its use
in industrial environments is limited, and few existing studies barely address
multimodal guidance designed for BLV users. We present a novel multimodal
guidance simulator that enables BLV users to control a mobile robot through a
high-fidelity warehouse environment while simultaneously receiving synchronized
visual, auditory, and haptic feedback. The system combines a navigation mesh
with regular re-planning so routes remain accurate avoiding collisions as
forklifts and human avatars move around the warehouse. Users with low vision
are guided with a visible path line towards destination; navigational voice
cues with clockwise directions announce upcoming turns, and finally
proximity-based haptic feedback notifies the users of static and moving
obstacles in the path. This real-time, closed-loop system offers a repeatable
testbed and algorithmic reference for accessible teleoperation research. The
simulator's design principles can be easily adapted to real robots due to the
alignment of its navigation, speech, and haptic modules with commercial
hardware, supporting rapid feasibility studies and deployment of inclusive
telerobotic tools in actual warehouses.

</details>


### [294] [Efficient Visual Appearance Optimization by Learning from Prior Preferences](https://arxiv.org/abs/2507.15355)
*Zhipeng Li,Yi-Chi Liao,Christian Holz*

Main category: cs.HC

TL;DR: Meta-PO结合元学习和PBO，通过利用先验用户偏好模型，显著提高了视觉参数优化的样本效率，适用于终端用户。


<details>
  <summary>Details</summary>
Motivation: 由于视觉参数优化搜索空间大且缺乏明确目标函数，用户依赖隐式偏好，传统PBO方法需要多次比较，不适合终端用户。

Method: Meta-PO整合PBO与元学习，利用先验用户偏好模型智能推荐设计候选，加速收敛并提供个性化结果。

Result: 实验显示，Meta-PO在相似目标下仅需5.86次迭代，跨目标下8次迭代即可达到满意效果。

Conclusion: Meta-PO通过高效、通用的优化方法，使个性化视觉优化更适用于终端用户，并具有广泛扩展潜力。

Abstract: Adjusting visual parameters such as brightness and contrast is common in our
everyday experiences. Finding the optimal parameter setting is challenging due
to the large search space and the lack of an explicit objective function,
leaving users to rely solely on their implicit preferences. Prior work has
explored Preferential Bayesian Optimization (PBO) to address this challenge,
involving users to iteratively select preferred designs from candidate sets.
However, PBO often requires many rounds of preference comparisons, making it
more suitable for designers than everyday end-users. We propose Meta-PO, a
novel method that integrates PBO with meta-learning to improve sample
efficiency. Specifically, Meta-PO infers prior users' preferences and stores
them as models, which are leveraged to intelligently suggest design candidates
for the new users, enabling faster convergence and more personalized results.
An experimental evaluation of our method for appearance design tasks on 2D and
3D content showed that participants achieved satisfactory appearance in 5.86
iterations using Meta-PO when participants shared similar goals with a
population (e.g., tuning for a ``warm'' look) and in 8 iterations even
generalizes across divergent goals (e.g., from ``vintage'', ``warm'', to
``holiday''). Meta-PO makes personalized visual optimization more applicable to
end-users through a generalizable, more efficient optimization conditioned on
preferences, with the potential to scale interface personalization more
broadly.

</details>


### [295] [Romance, Relief, and Regret: Teen Narratives of Chatbot Overreliance](https://arxiv.org/abs/2507.15783)
*Mohammad 'Matt' Namvarpour,Brandon Brofsky,Jessica Medina,Mamtaj Akter,Afsaneh Razi*

Main category: cs.HC

TL;DR: 研究分析了青少年对Character.AI聊天机器人的过度依赖现象，发现其可能导致心理困扰和现实关系问题，并提出了改进设计建议。


<details>
  <summary>Details</summary>
Motivation: 探讨青少年与可定制人格聊天机器人的互动，填补现有研究空白，关注其对青少年心理和社交的影响。

Method: 分析了318篇由13-17岁青少年在Character.AI子论坛发布的Reddit帖子。

Result: 青少年因情感支持或创意表达开始使用聊天机器人，但易形成强烈依赖，干扰现实生活和关系。依赖通常在反思危害、回归现实社交或平台限制时终止。

Conclusion: 建议未来聊天机器人设计应增强自我意识、支持现实互动，并让青少年参与开发更安全的数字工具。

Abstract: As Generative Artificial Intelligence (GenAI) driven chatbots like
Character.AI become embedded in adolescent life, they raise concerns about
emotional dependence and digital overreliance. While studies have investigated
the overreliance of adults on these chatbots, they have not investigated teens'
interactions with chatbots with customizable personas. We analyzed 318 Reddit
posts made by users self-reported as 13-17 years old on the Character.AI
subreddit to understand patterns of overreliance. We found teens commonly begin
using chatbots for emotional support or creative expression, but many develop
strong attachments that interfere with offline relationships and daily
routines. Their posts revealed recurring signs of psychological distress,
cycles of relapse, and difficulty disengaging. Teens reported that their
overreliance often ended when they reflect on the harm, return to in-person
social settings, or become frustrated by platform restrictions. Based on the
implications of our findings, we provide recommendations for future chatbot
design so they can promote self-awareness, support real-world engagement, and
involve teens in developing safer digital tools.

</details>


<div id='math.RT'></div>

# math.RT [[Back]](#toc)

### [296] [Partial Symmetry Enforced Attention Decomposition (PSEAD): A Group-Theoretic Framework for Equivariant Transformers in Biological Systems](https://arxiv.org/abs/2507.14908)
*Daniel Ayomide Olanrewaju*

Main category: math.RT

TL;DR: 该研究提出了一种新的群论框架PSEAD，用于将局部对称性融入Transformer的自注意力机制中，提升模型的泛化能力、可解释性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 旨在通过局部对称性增强自注意力机制，以更好地处理生物数据中的对称特征，并加速生物动态过程的建模。

Method: 提出PSEAD框架，利用局部置换子群作用分解注意力机制为正交不可约分量，与子群的不可约表示对齐。

Result: PSEAD显著提升了模型对生物数据的泛化能力、可解释性和计算效率，并成功应用于动态生物过程。

Conclusion: PSEAD为新一代生物信息驱动的对称感知AI模型奠定了基础。

Abstract: This research introduces the Theory of Partial Symmetry Enforced Attention
Decomposition (PSEAD), a new and rigorous group-theoretic framework designed to
seamlessly integrate local symmetry awareness into the core architecture of
self-attention mechanisms within Transformer models. We formalize the concept
of local permutation subgroup actions on windows of biological data, proving
that under such actions, the attention mechanism naturally decomposes into a
direct sum of orthogonal irreducible components. Critically, these components
are intrinsically aligned with the irreducible representations of the acting
permutation subgroup, thereby providing a powerful mathematical basis for
disentangling symmetric and asymmetric features. We show that PSEAD offers
substantial advantages. These include enhanced generalization capabilities to
novel biological motifs exhibiting similar partial symmetries, unprecedented
interpretability by allowing direct visualization and analysis of attention
contributions from different symmetry channels, and significant computational
efficiency gains by focusing representational capacity on relevant symmetric
subspaces. Beyond static data analysis, we extend PSEAD's applicability to
dynamic biological processes within reinforcement learning paradigms,
showcasing its potential to accelerate the discovery and optimization of
biologically meaningful policies in complex environments like protein folding
and drug discovery. This work lays the groundwork for a new generation of
biologically informed, symmetry-aware artificial intelligence models.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [297] [Forecasting Faculty Placement from Patterns in Co-authorship Networks](https://arxiv.org/abs/2507.14696)
*Samantha Dies,David Liu,Tina Eliassi-Rad*

Main category: cs.SI

TL;DR: 该论文通过分析教师招聘中的合著网络，发现其预测准确性比传统指标高10%，尤其在顶级院系招聘中效果显著。


<details>
  <summary>Details</summary>
Motivation: 研究教师招聘对学术资源和机会分配的影响，探索传统指标之外的因素（如合著网络）对招聘结果的预测作用。

Method: 利用时间合著网络数据，结合传统指标（如博士部门声望和文献计量特征），进行个体层面的招聘预测。

Result: 合著网络显著提高预测准确性（提升10%），在顶级院系招聘中效果尤为突出。

Conclusion: 合著网络等社会因素在教师招聘中起重要作用，为理解学术结构偏见和推动公平招聘提供了新视角。

Abstract: Faculty hiring shapes the flow of ideas, resources, and opportunities in
academia, influencing not only individual career trajectories but also broader
patterns of institutional prestige and scientific progress. While traditional
studies have found strong correlations between faculty hiring and attributes
such as doctoral department prestige and publication record, they rarely assess
whether these associations generalize to individual hiring outcomes,
particularly for future candidates outside the original sample. Here, we
consider faculty placement as an individual-level prediction task. Our data
consist of temporal co-authorship networks with conventional attributes such as
doctoral department prestige and bibliometric features. We observe that using
the co-authorship network significantly improves predictive accuracy by up to
10% over traditional indicators alone, with the largest gains observed for
placements at the most elite (top-10) departments. Our results underscore the
role that social networks, professional endorsements, and implicit advocacy
play in faculty hiring beyond traditional measures of scholarly productivity
and institutional prestige. By introducing a predictive framing of faculty
placement and establishing the benefit of considering co-authorship networks,
this work provides a new lens for understanding structural biases in academia
that could inform targeted interventions aimed at increasing transparency,
fairness, and equity in academic hiring practices.

</details>


### [298] [Privacy-Preserving Multimodal News Recommendation through Federated Learning](https://arxiv.org/abs/2507.15460)
*Mehdi Khalaj,Shahrzad Golestani Najafabadi,Julita Vassileva*

Main category: cs.SI

TL;DR: 本文提出了一种基于多模态联邦学习的新闻推荐方法，整合文本和视觉特征，平衡用户长短期兴趣，并通过联邦学习保护隐私。


<details>
  <summary>Details</summary>
Motivation: 解决传统个性化新闻推荐系统过度依赖文本内容、忽视短期兴趣及隐私问题。

Method: 采用多模态模型整合文本和视觉特征，时间感知模型平衡兴趣，联邦学习框架保护隐私，并使用安全聚合算法。

Result: 在真实数据集上表现优于现有系统，显著提升了隐私保护的新闻推荐效果。

Conclusion: 该方法在推荐准确性和隐私保护方面取得了显著进展。

Abstract: Personalized News Recommendation systems (PNR) have emerged as a solution to
information overload by predicting and suggesting news items tailored to
individual user interests. However, traditional PNR systems face several
challenges, including an overreliance on textual content, common neglect of
short-term user interests, and significant privacy concerns due to centralized
data storage. This paper addresses these issues by introducing a novel
multimodal federated learning-based approach for news recommendation. First, it
integrates both textual and visual features of news items using a multimodal
model, enabling a more comprehensive representation of content. Second, it
employs a time-aware model that balances users' long-term and short-term
interests through multi-head self-attention networks, improving recommendation
accuracy. Finally, to enhance privacy, a federated learning framework is
implemented, enabling collaborative model training without sharing user data.
The framework divides the recommendation model into a large server-maintained
news model and a lightweight user model shared between the server and clients.
The client requests news representations (vectors) and a user model from the
central server, then computes gradients with user local data, and finally sends
their locally computed gradients to the server for aggregation. The central
server aggregates gradients to update the global user model and news model. The
updated news model is further used to infer news representation by the server.
To further safeguard user privacy, a secure aggregation algorithm based on
Shamir's secret sharing is employed. Experiments on a real-world news dataset
demonstrate strong performance compared to existing systems, representing a
significant advancement in privacy-preserving personalized news recommendation.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [299] [Statistical and Algorithmic Foundations of Reinforcement Learning](https://arxiv.org/abs/2507.14444)
*Yuejie Chi,Yuxin Chen,Yuting Wei*

Main category: stat.ML

TL;DR: 该教程介绍了强化学习（RL）在样本稀缺情况下的挑战，并探讨了如何提升其样本和计算效率，涵盖了多种RL场景和方法。


<details>
  <summary>Details</summary>
Motivation: 由于新兴应用中模型复杂度高和非凸性问题，RL在样本稀缺（如临床实验、自动驾驶等高风险场景）中的效率提升成为研究重点。

Method: 教程采用马尔可夫决策过程（MDP）为数学模型，讨论了多种RL场景（如模拟器RL、在线RL、离线RL等）和主流方法（如基于模型、基于价值和策略优化）。

Result: 教程从非渐近视角分析了样本复杂度、计算效率及算法依赖和信息理论下界等问题。

Conclusion: 该教程旨在通过连接新思想与经典主题，推动RL在样本稀缺环境中的高效应用。

Abstract: As a paradigm for sequential decision making in unknown environments,
reinforcement learning (RL) has received a flurry of attention in recent years.
However, the explosion of model complexity in emerging applications and the
presence of nonconvexity exacerbate the challenge of achieving efficient RL in
sample-starved situations, where data collection is expensive, time-consuming,
or even high-stakes (e.g., in clinical trials, autonomous systems, and online
advertising). How to understand and enhance the sample and computational
efficacies of RL algorithms is thus of great interest. In this tutorial, we aim
to introduce several important algorithmic and theoretical developments in RL,
highlighting the connections between new ideas and classical topics. Employing
Markov Decision Processes as the central mathematical model, we cover several
distinctive RL scenarios (i.e., RL with a simulator, online RL, offline RL,
robust RL, and RL with human feedback), and present several mainstream RL
approaches (i.e., model-based approach, value-based approach, and policy
optimization). Our discussions gravitate around the issues of sample
complexity, computational efficiency, as well as algorithm-dependent and
information-theoretic lower bounds from a non-asymptotic viewpoint.

</details>


### [300] [Diffusion Models for Time Series Forecasting: A Survey](https://arxiv.org/abs/2507.14507)
*Chen Su,Zhengzhou Cai,Yuanhe Tian,Zihong Zheng,Yan Song*

Main category: stat.ML

TL;DR: 本文综述了扩散模型在时间序列预测（TSF）中的应用，包括模型变体、条件信息整合机制、现有方法的系统分类，以及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在图像合成中表现出色，但其在TSF中的应用尚未系统总结，本文旨在填补这一空白。

Method: 综述扩散模型及其变体在TSF中的适应方法，分析条件信息源和整合机制，分类现有方法，并总结基础模型、数据集和评估指标。

Result: 系统总结了扩散模型在TSF中的进展，提供了现有方法的分类和未来研究方向的讨论。

Conclusion: 本文为TSF领域的扩散模型研究提供了全面参考，并指出了未来发展的潜力。

Abstract: Diffusion models, initially developed for image synthesis, demonstrate
remarkable generative capabilities. Recently, their application has expanded to
time series forecasting (TSF), yielding promising results. In this survey, we
firstly introduce the standard diffusion models and their prevalent variants,
explaining their adaptation to TSF tasks. We then provide a comprehensive
review of diffusion models for TSF, paying special attention to the sources of
conditional information and the mechanisms for integrating this conditioning
within the models. In analyzing existing approaches using diffusion models for
TSF, we provide a systematic categorization and a comprehensive summary of them
in this survey. Furthermore, we examine several foundational diffusion models
applied to TSF, alongside commonly used datasets and evaluation metrics.
Finally, we discuss current limitations in these approaches and potential
future research directions. Overall, this survey details recent progress and
future prospects for diffusion models in TSF, serving as a reference for
researchers in the field.

</details>


### [301] [Deep Learning-Based Survival Analysis with Copula-Based Activation Functions for Multivariate Response Prediction](https://arxiv.org/abs/2507.14641)
*Jong-Min Kim,Il Do Ha,Sangjin Kim*

Main category: stat.ML

TL;DR: 该研究结合深度学习、Copula函数和生存分析，处理高相关性和右删失的多元生存数据，提出基于Copula的激活函数，提升了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 解决高相关性和右删失的多元生存数据建模问题，捕捉非线性依赖关系。

Method: 引入基于Copula的激活函数（Clayton、Gumbel及其组合），结合CNN-LSTM模型处理多元多类型生存响应。

Result: 通过模拟研究和乳腺癌数据分析，模型在预测准确性上表现优异，并有效处理右删失数据。

Conclusion: 提出的方法在多元生存数据分析中具有显著优势，适用于复杂模式捕捉和预测。

Abstract: This research integrates deep learning, copula functions, and survival
analysis to effectively handle highly correlated and right-censored
multivariate survival data. It introduces copula-based activation functions
(Clayton, Gumbel, and their combinations) to model the nonlinear dependencies
inherent in such data. Through simulation studies and analysis of real breast
cancer data, our proposed CNN-LSTM with copula-based activation functions for
multivariate multi-types of survival responses enhances prediction accuracy by
explicitly addressing right-censored data and capturing complex patterns. The
model's performance is evaluated using Shewhart control charts, focusing on the
average run length (ARL).

</details>


### [302] [Accelerating Hamiltonian Monte Carlo for Bayesian Inference in Neural Networks and Neural Operators](https://arxiv.org/abs/2507.14652)
*Ponkrshnan Thiagarajan,Tamer A. Zaki,Michael D. Shields*

Main category: stat.ML

TL;DR: 提出了一种结合变分推断（VI）和哈密顿蒙特卡洛（HMC）的混合方法，用于高效且准确地量化神经网络中的不确定性。


<details>
  <summary>Details</summary>
Motivation: HMC在贝叶斯神经网络中计算成本高，而现有近似方法（如VI）会导致推断分布不准确。

Method: 先使用VI训练网络，识别对预测不确定性影响大的参数，再用HMC对这些参数进行精确推断。

Result: 该方法能高效处理大规模网络，并在复杂物理系统中成功建模。

Conclusion: 混合方法显著提升了HMC的效率，同时保持了推断的准确性。

Abstract: Hamiltonian Monte Carlo (HMC) is a powerful and accurate method to sample
from the posterior distribution in Bayesian inference. However, HMC techniques
are computationally demanding for Bayesian neural networks due to the high
dimensionality of the network's parameter space and the non-convexity of their
posterior distributions. Therefore, various approximation techniques, such as
variational inference (VI) or stochastic gradient MCMC, are often employed to
infer the posterior distribution of the network parameters. Such approximations
introduce inaccuracies in the inferred distributions, resulting in unreliable
uncertainty estimates. In this work, we propose a hybrid approach that combines
inexpensive VI and accurate HMC methods to efficiently and accurately quantify
uncertainties in neural networks and neural operators. The proposed approach
leverages an initial VI training on the full network. We examine the influence
of individual parameters on the prediction uncertainty, which shows that a
large proportion of the parameters do not contribute substantially to
uncertainty in the network predictions. This information is then used to
significantly reduce the dimension of the parameter space, and HMC is performed
only for the subset of network parameters that strongly influence prediction
uncertainties. This yields a framework for accelerating the full batch HMC for
posterior inference in neural networks. We demonstrate the efficiency and
accuracy of the proposed framework on deep neural networks and operator
networks, showing that inference can be performed for large networks with tens
to hundreds of thousands of parameters. We show that this method can
effectively learn surrogates for complex physical systems by modeling the
operator that maps from upstream conditions to wall-pressure data on a cone in
hypersonic flow.

</details>


### [303] [When few labeled target data suffice: a theory of semi-supervised domain adaptation via fine-tuning from multiple adaptive starts](https://arxiv.org/abs/2507.14661)
*Wooseok Ha,Yuansi Chen*

Main category: stat.ML

TL;DR: 论文提出了一种基于结构因果模型（SCM）的理论框架，用于分析半监督域适应（SSDA）方法在目标域数据有限时的性能，并提出了三种SSDA方法和一种多自适应起点微调（MASFT）算法。


<details>
  <summary>Details</summary>
Motivation: 半监督域适应（SSDA）在目标域数据有限的情况下如何高效利用源数据和未标记目标数据，其理论尚未充分探索，尤其是在不同源-目标分布偏移场景下。

Method: 基于结构因果模型（SCM）开发理论框架，提出三种SSDA方法，并设计MASFT算法以应对源-目标关系不明确的情况。

Result: 在有限目标标签下，扩展的无监督域适应（UDA）方法可实现极小极大最优目标性能；MASFT算法在多种分布偏移下接近最优性能，显著减少对标记目标数据的需求。

Conclusion: 提出的理论框架和方法在模拟实验中验证了有效性，为SSDA提供了理论支持和实用工具。

Abstract: Semi-supervised domain adaptation (SSDA) aims to achieve high predictive
performance in the target domain with limited labeled target data by exploiting
abundant source and unlabeled target data. Despite its significance in numerous
applications, theory on the effectiveness of SSDA remains largely unexplored,
particularly in scenarios involving various types of source-target
distributional shifts. In this work, we develop a theoretical framework based
on structural causal models (SCMs) which allows us to analyze and quantify the
performance of SSDA methods when labeled target data is limited. Within this
framework, we introduce three SSDA methods, each having a fine-tuning strategy
tailored to a distinct assumption about the source and target relationship.
Under each assumption, we demonstrate how extending an unsupervised domain
adaptation (UDA) method to SSDA can achieve minimax-optimal target performance
with limited target labels. When the relationship between source and target
data is only vaguely known -- a common practical concern -- we propose the
Multi Adaptive-Start Fine-Tuning (MASFT) algorithm, which fine-tunes UDA models
from multiple starting points and selects the best-performing one based on a
small hold-out target validation dataset. Combined with model selection
guarantees, MASFT achieves near-optimal target predictive performance across a
broad range of types of distributional shifts while significantly reducing the
need for labeled target data. We empirically validate the effectiveness of our
proposed methods through simulations.

</details>


### [304] [Learning Nonlinear Causal Reductions to Explain Reinforcement Learning Policies](https://arxiv.org/abs/2507.14901)
*Armin Kekić,Jan Schneider,Dieter Büchler,Bernhard Schölkopf,Michel Besserve*

Main category: stat.ML

TL;DR: 论文提出了一种从因果视角解释强化学习策略行为的方法，通过扰动策略动作并观察其对累积奖励的影响，学习一个简化的高层因果模型。


<details>
  <summary>Details</summary>
Motivation: 解释强化学习策略的成败行为是一个复杂问题，作者希望通过因果模型简化这一过程。

Method: 引入随机扰动策略动作，观察其对奖励的影响，并开发非线性因果模型简化框架，确保干预一致性。

Result: 实验证明该方法能揭示RL策略的行为模式、偏差和失败原因。

Conclusion: 通过因果模型简化，可以有效地解释RL策略的行为，并确保干预一致性。

Abstract: Why do reinforcement learning (RL) policies fail or succeed? This is a
challenging question due to the complex, high-dimensional nature of
agent-environment interactions. In this work, we take a causal perspective on
explaining the behavior of RL policies by viewing the states, actions, and
rewards as variables in a low-level causal model. We introduce random
perturbations to policy actions during execution and observe their effects on
the cumulative reward, learning a simplified high-level causal model that
explains these relationships. To this end, we develop a nonlinear Causal Model
Reduction framework that ensures approximate interventional consistency,
meaning the simplified high-level model responds to interventions in a similar
way as the original complex system. We prove that for a class of nonlinear
causal models, there exists a unique solution that achieves exact
interventional consistency, ensuring learned explanations reflect meaningful
causal patterns. Experiments on both synthetic causal models and practical RL
tasks-including pendulum control and robot table tennis-demonstrate that our
approach can uncover important behavioral patterns, biases, and failure modes
in trained RL policies.

</details>


### [305] [Uncertainty Quantification for Machine Learning-Based Prediction: A Polynomial Chaos Expansion Approach for Joint Model and Input Uncertainty Propagation](https://arxiv.org/abs/2507.14782)
*Xiaoping Du*

Main category: stat.ML

TL;DR: 本文提出了一种基于多项式混沌展开（PCE）的框架，用于联合处理输入和模型不确定性传播，特别适用于高斯过程回归模型，以支持可靠的工程预测。


<details>
  <summary>Details</summary>
Motivation: 机器学习替代模型在工程分析中广泛应用，但其预测存在固有误差，需准确量化输入和模型不确定性以生成可靠预测。

Method: 通过将所有随机输入转换为统一标准空间，构建PCE替代模型，高效计算输出的均值和标准差，并提供全局敏感性分析机制。

Result: 该方法能高效、准确地量化输入变量和模型不确定性对输出变异的贡献，支持可信的工程应用。

Conclusion: 提出的框架为全面不确定性量化提供了计算高效且可解释的方法，增强了机器学习预测的可信度。

Abstract: Machine learning (ML) surrogate models are increasingly used in engineering
analysis and design to replace computationally expensive simulation models,
significantly reducing computational cost and accelerating decision-making
processes. However, ML predictions contain inherent errors, often estimated as
model uncertainty, which is coupled with variability in model inputs.
Accurately quantifying and propagating these combined uncertainties is
essential for generating reliable engineering predictions. This paper presents
a robust framework based on Polynomial Chaos Expansion (PCE) to handle joint
input and model uncertainty propagation. While the approach applies broadly to
general ML surrogates, we focus on Gaussian Process regression models, which
provide explicit predictive distributions for model uncertainty. By
transforming all random inputs into a unified standard space, a PCE surrogate
model is constructed, allowing efficient and accurate calculation of the mean
and standard deviation of the output. The proposed methodology also offers a
mechanism for global sensitivity analysis, enabling the accurate quantification
of the individual contributions of input variables and ML model uncertainty to
the overall output variability. This approach provides a computationally
efficient and interpretable framework for comprehensive uncertainty
quantification, supporting trustworthy ML predictions in downstream engineering
applications.

</details>


### [306] [Learning under Latent Group Sparsity via Diffusion on Networks](https://arxiv.org/abs/2507.15097)
*Subhroshekhar Ghosh,Soumendu Sundar Mukherjee*

Main category: stat.ML

TL;DR: 提出一种无需先验分组信息的稀疏学习方法，基于网络拉普拉斯几何，通过热流动态计算惩罚项，自动在lasso和group lasso之间插值。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习中解释变量的群组结构问题，无需预先知道群组信息，利用网络动态特性自动适应数据结构。

Method: 基于拉普拉斯几何和热流动态设计惩罚项，数据驱动构建网络，避免计算密集型预处理。

Result: 理论证明方法有效，样本复杂度有界，扩散时间仅需对数级。与统计物理模型（如高斯自由场和随机块模型）有深入联系。

Conclusion: 该方法为经典学习任务提供了基于扩散的新思路，利用数据的几何、动态和随机结构。

Abstract: Group or cluster structure on explanatory variables in machine learning
problems is a very general phenomenon, which has attracted broad interest from
practitioners and theoreticians alike. In this work we contribute an approach
to sparse learning under such group structure, that does not require prior
information on the group identities. Our paradigm is motivated by the Laplacian
geometry of an underlying network with a related community structure, and
proceeds by directly incorporating this into a penalty that is effectively
computed via a heat-flow-based local network dynamics. The proposed penalty
interpolates between the lasso and the group lasso penalties, the runtime of
the heat-flow dynamics being the interpolating parameter. As such it can
automatically default to lasso when the group structure reflected in the
Laplacian is weak. In fact, we demonstrate a data-driven procedure to construct
such a network based on the available data. Notably, we dispense with
computationally intensive pre-processing involving clustering of variables,
spectral or otherwise. Our technique is underpinned by rigorous theorems that
guarantee its effective performance and provide bounds on its sample
complexity. In particular, in a wide range of settings, it provably suffices to
run the diffusion for time that is only logarithmic in the problem dimensions.
We explore in detail the interfaces of our approach with key statistical
physics models in network science, such as the Gaussian Free Field and the
Stochastic Block Model. Our work raises the possibility of applying similar
diffusion-based techniques to classical learning tasks, exploiting the
interplay between geometric, dynamical and stochastic structures underlying the
data.

</details>


### [307] [Accelerated Bayesian Optimal Experimental Design via Conditional Density Estimation and Informative Data](https://arxiv.org/abs/2507.15235)
*Miao Huang,Hongqiao Wang,Kunyu Wu*

Main category: stat.ML

TL;DR: 本研究提出了一种基于贝叶斯框架的最优实验设计方法，通过贝叶斯定理和条件密度估计提高数值效率，解决了低效仿真和高成本数据获取的挑战。


<details>
  <summary>Details</summary>
Motivation: 实验设计（DOEs）是科学研究的基础方法，但传统方法在低效仿真和高成本数据获取场景下存在局限性，需要更高效的解决方案。

Method: 采用贝叶斯框架重新表述效用期望，利用条件密度估计近似高斯随机场比率，并通过协方差选择信息数据集。

Result: 理论分析和实际应用验证了方法的有效性，显著提升了实验效率和不确定性下的决策能力。

Conclusion: 该方法为实验设计提供了高效、可靠的解决方案，适用于复杂场景下的优化问题。

Abstract: The Design of Experiments (DOEs) is a fundamental scientific methodology that
provides researchers with systematic principles and techniques to enhance the
validity, reliability, and efficiency of experimental outcomes. In this study,
we explore optimal experimental design within a Bayesian framework, utilizing
Bayes' theorem to reformulate the utility expectation--originally expressed as
a nested double integral--into an independent double integral form,
significantly improving numerical efficiency. To further accelerate the
computation of the proposed utility expectation, conditional density estimation
is employed to approximate the ratio of two Gaussian random fields, while
covariance serves as a selection criterion to identify informative datasets
during model fitting and integral evaluation. In scenarios characterized by low
simulation efficiency and high costs of raw data acquisition, key challenges
such as surrogate modeling, failure probability estimation, and parameter
inference are systematically restructured within the Bayesian experimental
design framework. The effectiveness of the proposed methodology is validated
through both theoretical analysis and practical applications, demonstrating its
potential for enhancing experimental efficiency and decision-making under
uncertainty.

</details>


### [308] [Missing value imputation with adversarial random forests -- MissARF](https://arxiv.org/abs/2507.15681)
*Pegah Golchian,Jan Kapar,David S. Watson,Marvin N. Wright*

Main category: stat.ML

TL;DR: 提出了一种基于生成式机器学习的缺失值填补方法MissARF，结合对抗随机森林（ARF）进行密度估计和数据合成，性能与现有方法相当且运行速度快。


<details>
  <summary>Details</summary>
Motivation: 解决生物统计中缺失值处理的常见挑战，提供快速易用的填补方法。

Method: 利用对抗随机森林（ARF）估计条件分布并采样填补缺失值，支持单次和多次填补。

Result: 实验表明MissARF在填补质量和运行速度上与现有方法相当，且多次填补无额外成本。

Conclusion: MissARF是一种高效且性能优越的缺失值填补方法。

Abstract: Handling missing values is a common challenge in biostatistical analyses,
typically addressed by imputation methods. We propose a novel, fast, and
easy-to-use imputation method called missing value imputation with adversarial
random forests (MissARF), based on generative machine learning, that provides
both single and multiple imputation. MissARF employs adversarial random forest
(ARF) for density estimation and data synthesis. To impute a missing value of
an observation, we condition on the non-missing values and sample from the
estimated conditional distribution generated by ARF. Our experiments
demonstrate that MissARF performs comparably to state-of-the-art single and
multiple imputation methods in terms of imputation quality and fast runtime
with no additional costs for multiple imputation.

</details>


### [309] [Conformal and kNN Predictive Uncertainty Quantification Algorithms in Metric Spaces](https://arxiv.org/abs/2507.15741)
*Gábor Lugosi,Marcos Matabuena*

Main category: stat.ML

TL;DR: 该论文提出了一个在度量空间中回归模型的不确定性量化框架，通过新定义的同方差性开发了具有有限样本覆盖保证和快速收敛率的共形预测算法。在异方差情况下，提出了一种无需共形校准的局部k近邻方法，适应非线性空间的几何结构。


<details>
  <summary>Details</summary>
Motivation: 解决度量空间中回归模型的不确定性量化问题，为个性化医疗等应用提供实用工具。

Method: 1. 开发基于同方差性的共形预测算法；2. 提出局部k近邻方法处理异方差性。

Result: 证明了估计器在最小条件下的收敛性，并在个性化医疗应用中验证了实用性。

Conclusion: 该框架灵活、可扩展，适用于多种回归算法和领域专业知识。

Abstract: This paper introduces a framework for uncertainty quantification in
regression models defined in metric spaces. Leveraging a newly defined notion
of homoscedasticity, we develop a conformal prediction algorithm that offers
finite-sample coverage guarantees and fast convergence rates of the oracle
estimator. In heteroscedastic settings, we forgo these non-asymptotic
guarantees to gain statistical efficiency, proposing a local
$k$--nearest--neighbor method without conformal calibration that is adaptive to
the geometry of each particular nonlinear space. Both procedures work with any
regression algorithm and are scalable to large data sets, allowing
practitioners to plug in their preferred models and incorporate domain
expertise. We prove consistency for the proposed estimators under minimal
conditions. Finally, we demonstrate the practical utility of our approach in
personalized--medicine applications involving random response objects such as
probability distributions and graph Laplacians.

</details>


### [310] [Hypergraphs on high dimensional time series sets using signature transform](https://arxiv.org/abs/2507.15802)
*Rémi Vaucher,Paul Minchella*

Main category: stat.ML

TL;DR: 本文提出了一种从多元时间序列集合构建超图的方法，扩展了现有单变量时间序列的框架，并通过签名变换引入随机性以提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决从多元时间序列集合构建超图的挑战，扩展现有单变量时间序列的框架。

Method: 利用签名变换的特性引入受控随机性，扩展了Chretien等人的方法。

Result: 在合成数据集上验证了方法的有效性，展示了有前景的结果。

Conclusion: 提出的方法能够有效地从多元时间序列集合构建超图，并提高了鲁棒性。

Abstract: In recent decades, hypergraphs and their analysis through Topological Data
Analysis (TDA) have emerged as powerful tools for understanding complex data
structures. Various methods have been developed to construct hypergraphs --
referred to as simplicial complexes in the TDA framework -- over datasets,
enabling the formation of edges between more than two vertices. This paper
addresses the challenge of constructing hypergraphs from collections of
multivariate time series. While prior work has focused on the case of a single
multivariate time series, we extend this framework to handle collections of
such time series. Our approach generalizes the method proposed in Chretien and
al. by leveraging the properties of signature transforms to introduce
controlled randomness, thereby enhancing the robustness of the construction
process. We validate our method on synthetic datasets and present promising
results.

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [311] [Conan: A Chunkwise Online Network for Zero-Shot Adaptive Voice Conversion](https://arxiv.org/abs/2507.14534)
*Yu Zhang,Baotong Tian,Zhiyao Duan*

Main category: eess.AS

TL;DR: Conan是一种在线零样本语音转换模型，通过分块处理实现实时性，同时保持语义保真度和自然音质。


<details>
  <summary>Details</summary>
Motivation: 解决现有语音转换模型在实时性、语义保真度和适应未见说话人特征方面的不足。

Method: Conan包含三个核心组件：流式内容提取器、自适应风格编码器和因果混洗声码器。

Result: 实验表明，Conan在主观和客观指标上均优于基线模型。

Conclusion: Conan为实时语音转换提供了一种高效且高质量的解决方案。

Abstract: Zero-shot online voice conversion (VC) holds significant promise for
real-time communications and entertainment. However, current VC models struggle
to preserve semantic fidelity under real-time constraints, deliver
natural-sounding conversions, and adapt effectively to unseen speaker
characteristics. To address these challenges, we introduce Conan, a chunkwise
online zero-shot voice conversion model that preserves the content of the
source while matching the voice timbre and styles of reference speech. Conan
comprises three core components: 1) a Stream Content Extractor that leverages
Emformer for low-latency streaming content encoding; 2) an Adaptive Style
Encoder that extracts fine-grained stylistic features from reference speech for
enhanced style adaptation; 3) a Causal Shuffle Vocoder that implements a fully
causal HiFiGAN using a pixel-shuffle mechanism. Experimental evaluations
demonstrate that Conan outperforms baseline models in subjective and objective
metrics. Audio samples can be found at https://aaronz345.github.io/ConanDemo.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [312] [Real-Time Communication-Aware Ride-Sharing Route Planning for Urban Air Mobility: A Multi-Source Hybrid Attention Reinforcement Learning Approach](https://arxiv.org/abs/2507.14249)
*Yuejiao Xie,Maonan Wang,Di Zhou,Man-On Pun,Zhu Han*

Main category: cs.RO

TL;DR: 论文提出了一种基于无线电地图和多源混合注意力强化学习（MSHA-RL）的UAM路径规划方法，以解决动态乘客需求和通信质量保障问题。


<details>
  <summary>Details</summary>
Motivation: 城市空中交通（UAM）系统需要灵活的路径规划以应对实时乘客需求，同时确保通信质量以保障安全。传统方法缺乏灵活性。

Method: 构建无线电地图评估通信质量，提出MSHA-RL框架，通过数据对齐和混合注意力机制实现实时路径规划。

Result: 实验表明，该方法能实现通信合规的轨迹规划，减少旅行时间并提高运营效率。

Conclusion: MSHA-RL框架有效解决了UAM路径规划中的动态需求和通信质量问题，提升了安全性和效率。

Abstract: Urban Air Mobility (UAM) systems are rapidly emerging as promising solutions
to alleviate urban congestion, with path planning becoming a key focus area.
Unlike ground transportation, UAM trajectory planning has to prioritize
communication quality for accurate location tracking in constantly changing
environments to ensure safety. Meanwhile, a UAM system, serving as an air taxi,
requires adaptive planning to respond to real-time passenger requests,
especially in ride-sharing scenarios where passenger demands are unpredictable
and dynamic. However, conventional trajectory planning strategies based on
predefined routes lack the flexibility to meet varied passenger ride demands.
To address these challenges, this work first proposes constructing a radio map
to evaluate the communication quality of urban airspace. Building on this, we
introduce a novel Multi-Source Hybrid Attention Reinforcement Learning
(MSHA-RL) framework for the challenge of effectively focusing on passengers and
UAM locations, which arises from the significant dimensional disparity between
the representations. This model first generates the alignment among diverse
data sources with large gap dimensions before employing hybrid attention to
balance global and local insights, thereby facilitating responsive, real-time
path planning. Extensive experimental results demonstrate that the approach
enables communication-compliant trajectory planning, reducing travel time and
enhancing operational efficiency while prioritizing passenger safety.

</details>


### [313] [One Step Beyond: Feedthrough & Placement-Aware Rectilinear Floorplanner](https://arxiv.org/abs/2507.14914)
*Zhexuan Xu,Jie Wang,Siyuan Xu,Zijie Geng,Mingxuan Yuan,Feng Wu*

Main category: cs.RO

TL;DR: Flora是一种三阶段、考虑馈通和布局的矩形平面规划器，通过分阶段优化HPWL、馈通和组件布局，显著提升芯片设计的PPA指标。


<details>
  <summary>Details</summary>
Motivation: 现有平面规划方法难以与后续物理设计阶段集成，导致模块内组件布局不优和模块间馈通过多。Flora旨在解决这一问题。

Method: Flora分三阶段：1) 粗粒度优化HPWL和馈通；2) 固定轮廓下零空白布局；3) 快速树搜索优化组件布局并调整模块边界。

Result: 实验显示，Flora在HPWL、FTpin、FTmod和组件布局性能上均优于现有方法。

Conclusion: Flora通过跨阶段优化，显著提升了芯片设计的PPA指标。

Abstract: Floorplanning determines the shapes and locations of modules on a chip canvas
and plays a critical role in optimizing the chip's Power, Performance, and Area
(PPA) metrics. However, existing floorplanning approaches often fail to
integrate with subsequent physical design stages, leading to suboptimal
in-module component placement and excessive inter-module feedthrough. To tackle
this challenge, we propose Flora, a three-stage feedthrough and placement aware
rectilinear floorplanner. In the first stage, Flora employs wiremask and
position mask techniques to achieve coarse-grained optimization of HPWL and
feedthrough. In the second stage, under the constraint of a fixed outline,
Flora achieves a zero-whitespace layout by locally resizing module shapes,
thereby performing fine-grained optimization of feedthrough and improving
component placement. In the third stage, Flora utilizes a fast tree
search-based method to efficiently place components-including macros and
standard cells-within each module, subsequently adjusting module boundaries
based on the placement results to enable cross-stage optimization. Experimental
results show that Flora outperforms recent state-of-the-art floorplanning
approaches, achieving an average reduction of 6% in HPWL, 5.16% in FTpin,
29.15% in FTmod, and a 14% improvement in component placement performance.

</details>


### [314] [FCRF: Flexible Constructivism Reflection for Long-Horizon Robotic Task Planning with Large Language Models](https://arxiv.org/abs/2507.14975)
*Yufan Song,Jiatao Zhang,Zeng Gu,Qingmiao Liang,Tuocheng Hu,Wei Song,Shiqiang Zhu*

Main category: cs.RO

TL;DR: 提出了一种名为FCRF的灵活自反思框架，通过导师-执行者架构提升LLM在复杂任务中的错误纠正能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM自反思机制缺乏灵活性，限制了其在复杂任务中的有效性，受人类认知适应启发，提出FCRF。

Method: 采用导师-执行者架构，结合任务难度和历史经验进行灵活自反思。

Result: 在AlfWorld仿真和真实环境中测试，FCRF显著提升了性能和自反思灵活性。

Conclusion: FCRF为复杂长时程任务中的自主错误纠正提供了有效解决方案。

Abstract: Autonomous error correction is critical for domestic robots to achieve
reliable execution of complex long-horizon tasks. Prior work has explored
self-reflection in Large Language Models (LLMs) for task planning error
correction; however, existing methods are constrained by inflexible
self-reflection mechanisms that limit their effectiveness. Motivated by these
limitations and inspired by human cognitive adaptation, we propose the Flexible
Constructivism Reflection Framework (FCRF), a novel Mentor-Actor architecture
that enables LLMs to perform flexible self-reflection based on task difficulty,
while constructively integrating historical valuable experience with failure
lessons. We evaluated FCRF on diverse domestic tasks through simulation in
AlfWorld and physical deployment in the real-world environment. Experimental
results demonstrate that FCRF significantly improves overall performance and
self-reflection flexibility in complex long-horizon robotic tasks.

</details>


### [315] [Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper](https://arxiv.org/abs/2507.15062)
*Xinyue Zhu,Binghao Huang,Yunzhu Li*

Main category: cs.RO

TL;DR: 提出了一种集成触觉传感器的便携式夹持器，结合视觉和触觉数据，通过跨模态表示学习框架提升机器人操作的精确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有手持夹持器缺乏触觉反馈，而触觉在精确操作中至关重要。

Method: 开发了带触觉传感器的轻量级夹持器，并提出跨模态表示学习框架，整合视觉和触觉信号。

Result: 在试管插入和移液管流体转移等任务中，表现出更高的准确性和抗干扰能力。

Conclusion: 集成触觉反馈的夹持器和跨模态学习框架显著提升了机器人操作的效率和精确性。

Abstract: Handheld grippers are increasingly used to collect human demonstrations due
to their ease of deployment and versatility. However, most existing designs
lack tactile sensing, despite the critical role of tactile feedback in precise
manipulation. We present a portable, lightweight gripper with integrated
tactile sensors that enables synchronized collection of visual and tactile data
in diverse, real-world, and in-the-wild settings. Building on this hardware, we
propose a cross-modal representation learning framework that integrates visual
and tactile signals while preserving their distinct characteristics. The
learning procedure allows the emergence of interpretable representations that
consistently focus on contacting regions relevant for physical interactions.
When used for downstream manipulation tasks, these representations enable more
efficient and effective policy learning, supporting precise robotic
manipulation based on multimodal feedback. We validate our approach on
fine-grained tasks such as test tube insertion and pipette-based fluid
transfer, demonstrating improved accuracy and robustness under external
disturbances. Our project page is available at
https://binghao-huang.github.io/touch_in_the_wild/ .

</details>


### [316] [The Constitutional Controller: Doubt-Calibrated Steering of Compliant Agents](https://arxiv.org/abs/2507.15478)
*Simon Kohaut,Felix Divo,Navid Hamid,Benedict Flade,Julian Eggert,Devendra Singh Dhami,Kristian Kersting*

Main category: cs.RO

TL;DR: 论文提出了一种结合神经符号系统的框架CoCo，通过深度概率逻辑程序增强自主代理的安全性和可靠性，并在真实场景中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决自主代理在不确定环境中可靠且合规行为的挑战。

Method: 引入Constitutional Controller (CoCo)框架，结合概率符号推理与深度学习，并提出了基于特征的自怀疑概念。

Result: 在真实空中移动研究中，CoCo成功帮助智能系统学习适当怀疑并安全导航复杂环境。

Conclusion: 神经符号系统为解决自主代理的安全和可靠性问题提供了有效方案。

Abstract: Ensuring reliable and rule-compliant behavior of autonomous agents in
uncertain environments remains a fundamental challenge in modern robotics. Our
work shows how neuro-symbolic systems, which integrate probabilistic, symbolic
white-box reasoning models with deep learning methods, offer a powerful
solution to this challenge. This enables the simultaneous consideration of
explicit rules and neural models trained on noisy data, combining the strength
of structured reasoning with flexible representations. To this end, we
introduce the Constitutional Controller (CoCo), a novel framework designed to
enhance the safety and reliability of agents by reasoning over deep
probabilistic logic programs representing constraints such as those found in
shared traffic spaces. Furthermore, we propose the concept of self-doubt,
implemented as a probability density conditioned on doubt features such as
travel velocity, employed sensors, or health factors. In a real-world aerial
mobility study, we demonstrate CoCo's advantages for intelligent autonomous
systems to learn appropriate doubts and navigate complex and uncertain
environments safely and compliantly.

</details>


### [317] [The Emergence of Deep Reinforcement Learning for Path Planning](https://arxiv.org/abs/2507.15469)
*Thanh Thi Nguyen,Saeid Nahavandi,Imran Razzak,Dung Nguyen,Nhat Truong Pham,Quoc Viet Hung Nguyen*

Main category: cs.RO

TL;DR: 综述了传统路径规划方法和深度强化学习（DRL）在自主系统中的应用，分析了其优缺点，并探讨了混合方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 复杂动态环境中自主系统的需求推动了路径规划方法的研究，尤其是DRL的兴起。

Method: 分类比较了传统图搜索、线性规划、进化计算及DRL方法，并讨论了混合方法。

Result: 总结了各方法的计算效率、可扩展性、适应性和鲁棒性，指出混合方法的优势。

Conclusion: 提出了未来研究方向，尤其是结合DRL与传统方法的混合路径规划。

Abstract: The increasing demand for autonomous systems in complex and dynamic
environments has driven significant research into intelligent path planning
methodologies. For decades, graph-based search algorithms, linear programming
techniques, and evolutionary computation methods have served as foundational
approaches in this domain. Recently, deep reinforcement learning (DRL) has
emerged as a powerful method for enabling autonomous agents to learn optimal
navigation strategies through interaction with their environments. This survey
provides a comprehensive overview of traditional approaches as well as the
recent advancements in DRL applied to path planning tasks, focusing on
autonomous vehicles, drones, and robotic platforms. Key algorithms across both
conventional and learning-based paradigms are categorized, with their
innovations and practical implementations highlighted. This is followed by a
thorough discussion of their respective strengths and limitations in terms of
computational efficiency, scalability, adaptability, and robustness. The survey
concludes by identifying key open challenges and outlining promising avenues
for future research. Special attention is given to hybrid approaches that
integrate DRL with classical planning techniques to leverage the benefits of
both learning-based adaptability and deterministic reliability, offering
promising directions for robust and resilient autonomous navigation.

</details>


### [318] [GR-3 Technical Report](https://arxiv.org/abs/2507.15493)
*Chilam Cheang,Sijin Chen,Zhongren Cui,Yingdong Hu,Liqun Huang,Tao Kong,Hang Li,Yifeng Li,Yuxiao Liu,Xiao Ma,Hao Niu,Wenxuan Ou,Wanli Peng,Zeyu Ren,Haixin Shi,Jiawen Tian,Hongtao Wu,Xin Xiao,Yuyang Xiao,Jiafeng Xu,Yichu Yang*

Main category: cs.RO

TL;DR: GR-3是一个大规模视觉-语言-动作模型，展示了在泛化到新对象、环境和抽象指令方面的卓越能力，并能高效微调适应新任务。


<details>
  <summary>Details</summary>
Motivation: 开发通用机器人策略，以构建能够辅助人类日常生活的通用机器人。

Method: 通过多方面的训练方法，包括与网络规模视觉-语言数据的共同训练、基于VR设备收集的人类轨迹数据的高效微调，以及机器人轨迹数据的有效模仿学习。

Result: GR-3在多种挑战性任务上超越了当前最先进的基线方法π₀，并展示了在长时程和灵巧任务中的优异表现。

Conclusion: GR-3是迈向构建通用机器人的重要一步，展示了其在现实世界中的广泛应用潜力。

Abstract: We report our recent progress towards building generalist robot policies, the
development of GR-3. GR-3 is a large-scale vision-language-action (VLA) model.
It showcases exceptional capabilities in generalizing to novel objects,
environments, and instructions involving abstract concepts. Furthermore, it can
be efficiently fine-tuned with minimal human trajectory data, enabling rapid
and cost-effective adaptation to new settings. GR-3 also excels in handling
long-horizon and dexterous tasks, including those requiring bi-manual
manipulation and mobile movement, showcasing robust and reliable performance.
These capabilities are achieved through a multi-faceted training recipe that
includes co-training with web-scale vision-language data, efficient fine-tuning
from human trajectory data collected via VR devices, and effective imitation
learning with robot trajectory data. In addition, we introduce ByteMini, a
versatile bi-manual mobile robot designed with exceptional flexibility and
reliability, capable of accomplishing a wide range of tasks when integrated
with GR-3. Through extensive real-world experiments, we show GR-3 surpasses the
state-of-the-art baseline method, $\pi_0$, on a wide variety of challenging
tasks. We hope GR-3 can serve as a step towards building generalist robots
capable of assisting humans in daily life.

</details>


### [319] [Look, Focus, Act: Efficient and Robust Robot Learning via Human Gaze and Foveated Vision Transformers](https://arxiv.org/abs/2507.15833)
*Ian Chuang,Andrew Lee,Dechen Gao,Jinyu Zou,Iman Soltani*

Main category: cs.RO

TL;DR: 论文探讨了将人类主动注视机制融入机器人视觉系统，以提高效率和性能。通过结合眼动数据和机器人演示，提出了一种基于注视的视觉处理框架，显著减少了计算开销并提升了任务表现。


<details>
  <summary>Details</summary>
Motivation: 人类视觉通过注视任务相关区域显著减少视觉处理，而机器人系统通常被动处理图像。研究旨在探索如何通过模仿人类主动注视提升机器人视觉系统的效率和性能。

Method: 提出了一种结合眼动数据和机器人演示的框架，采用基于注视的视觉变换器（ViT）方案，减少计算量。探索了两种注视模仿和预测方法：两阶段模型和端到端联合预测模型。

Result: 实验表明，基于注视的视觉处理显著减少了计算开销，同时提高了高精度任务的性能和对抗干扰的鲁棒性。

Conclusion: 人类视觉处理机制为机器人视觉系统提供了有效的归纳偏置，显著提升了性能和效率。

Abstract: Human vision is a highly active process driven by gaze, which directs
attention and fixation to task-relevant regions and dramatically reduces visual
processing. In contrast, robot learning systems typically rely on passive,
uniform processing of raw camera images. In this work, we explore how
incorporating human-like active gaze into robotic policies can enhance both
efficiency and performance. We build on recent advances in foveated image
processing and apply them to an Active Vision robot system that emulates both
human head movement and eye tracking. Extending prior work on the AV-ALOHA
robot simulation platform, we introduce a framework for simultaneously
collecting eye-tracking data and robot demonstrations from a human operator as
well as a simulation benchmark and dataset for training robot policies that
incorporate human gaze. Given the widespread use of Vision Transformers (ViTs)
in robot learning, we integrate gaze information into ViTs using a foveated
patch tokenization scheme inspired by recent work in image segmentation.
Compared to uniform patch tokenization, this significantly reduces the number
of tokens-and thus computation-without sacrificing visual fidelity near regions
of interest. We also explore two approaches to gaze imitation and prediction
from human data. The first is a two-stage model that predicts gaze to guide
foveation and action; the second integrates gaze into the action space,
allowing the policy to jointly predict gaze and actions end-to-end. Our results
show that our method for foveated robot vision not only drastically reduces
computational overhead, but also improves performance for high precision tasks
and robustness to unseen distractors. Together, these findings suggest that
human-inspired visual processing offers a useful inductive bias for robotic
vision systems. https://ian-chuang.github.io/gaze-av-aloha/

</details>


<div id='cond-mat.dis-nn'></div>

# cond-mat.dis-nn [[Back]](#toc)

### [320] [Siamese Neural Network for Label-Efficient Critical Phenomena Prediction in 3D Percolation Models](https://arxiv.org/abs/2507.14159)
*Shanshan Wang,Dian Xu,Jianmin Shen,Feng Gao,Wei Li,Weibing Deng*

Main category: cond-mat.dis-nn

TL;DR: 提出了一种基于Siamese神经网络的3D渗流分析方法，显著提高了标签效率和预测精度。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习框架主要针对2D渗流分析，忽略了3D材料的空间相关性和形态复杂性，需要更高效的方法。

Method: 使用Siamese神经网络，利用最大簇的特征作为判别输入，分析3D渗流阈值和临界指数。

Result: 在3D系统中，预测渗流阈值和临界指数的误差低于1%，且所需标记样本显著减少。

Conclusion: 该方法为高维临界现象建模提供了高效且数据节约的框架，适用于材料发现和复杂网络分析。

Abstract: Percolation theory serves as a cornerstone for studying phase transitions and
critical phenomena, with broad implications in statistical physics, materials
science, and complex networks. However, most machine learning frameworks for
percolation analysis have focused on two-dimensional systems, oversimplifying
the spatial correlations and morphological complexity of real-world
three-dimensional materials. To bridge this gap and improve label efficiency
and scalability in 3D systems, we propose a Siamese Neural Network (SNN) that
leverages features of the largest cluster as discriminative input. Our method
achieves high predictive accuracy for both site and bond percolation thresholds
and critical exponents in three dimensions, with sub-1% error margins using
significantly fewer labeled samples than traditional approaches. This work
establishes a robust and data-efficient framework for modeling high-dimensional
critical phenomena, with potential applications in materials discovery and
complex network analysis.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [321] [Learning to Communicate in Multi-Agent Reinforcement Learning for Autonomous Cyber Defence](https://arxiv.org/abs/2507.14658)
*Faizan Contractor,Li Li,Ranwa Al Mallah*

Main category: cs.MA

TL;DR: 论文提出了一种在部分可观测环境中通过多智能体强化学习实现协作防御网络威胁的方法，结合通信机制提升决策效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法中智能体在执行时独立行动，限制了协作效果，而通过通信共享信息可以提升网络防御的决策能力。

Method: 使用Cyber Operations Research Gym训练防御智能体，采用改进的Differentiable Inter Agent Learning算法，学习战术策略和低成本通信消息。

Result: 智能体学习到类似人类专家的战术策略，同时掌握低成本通信能力，有效防御网络威胁。

Conclusion: 通过通信协作和强化学习，智能体能够高效防御网络威胁，展示了多智能体协作在网络安全中的潜力。

Abstract: Popular methods in cooperative Multi-Agent Reinforcement Learning with
partially observable environments typically allow agents to act independently
during execution, which may limit the coordinated effect of the trained
policies. However, by sharing information such as known or suspected ongoing
threats, effective communication can lead to improved decision-making in the
cyber battle space. We propose a game design where defender agents learn to
communicate and defend against imminent cyber threats by playing training games
in the Cyber Operations Research Gym, using the Differentiable Inter Agent
Learning algorithm adapted to the cyber operational environment. The tactical
policies learned by these autonomous agents are akin to those of human experts
during incident responses to avert cyber threats. In addition, the agents
simultaneously learn minimal cost communication messages while learning their
defence tactical policies.

</details>


### [322] [LLM Economist: Large Population Models and Mechanism Design in Multi-Agent Generative Simulacra](https://arxiv.org/abs/2507.15815)
*Seth Karten,Wenzhe Li,Zihan Ding,Samuel Kleiner,Yu Bai,Chi Jin*

Main category: cs.MA

TL;DR: LLM Economist框架通过基于代理的建模设计经济政策，结合分层决策和自然语言机制设计，提升社会福利。


<details>
  <summary>Details</summary>
Motivation: 旨在利用大型语言模型代理模拟复杂经济系统，为政策评估提供可信的实验平台。

Method: 分层决策：工人代理基于人口统计选择劳动供给，规划代理通过强化学习设计税收政策。

Result: 实验显示规划代理接近Stackelberg均衡，提升社会福利，投票机制进一步优化结果。

Conclusion: LLM代理能建模、模拟和治理复杂经济系统，为大规模政策评估提供可行方案。

Abstract: We present the LLM Economist, a novel framework that uses agent-based
modeling to design and assess economic policies in strategic environments with
hierarchical decision-making. At the lower level, bounded rational worker
agents -- instantiated as persona-conditioned prompts sampled from U.S.
Census-calibrated income and demographic statistics -- choose labor supply to
maximize text-based utility functions learned in-context. At the upper level, a
planner agent employs in-context reinforcement learning to propose
piecewise-linear marginal tax schedules anchored to the current U.S. federal
brackets. This construction endows economic simulacra with three capabilities
requisite for credible fiscal experimentation: (i) optimization of
heterogeneous utilities, (ii) principled generation of large, demographically
realistic agent populations, and (iii) mechanism design -- the ultimate nudging
problem -- expressed entirely in natural language. Experiments with populations
of up to one hundred interacting agents show that the planner converges near
Stackelberg equilibria that improve aggregate social welfare relative to Saez
solutions, while a periodic, persona-level voting procedure furthers these
gains under decentralized governance. These results demonstrate that large
language model-based agents can jointly model, simulate, and govern complex
economic systems, providing a tractable test bed for policy evaluation at the
societal scale to help build better civilizations.

</details>


<div id='math.AT'></div>

# math.AT [[Back]](#toc)

### [323] [Topological Social Choice: Designing a Noise-Robust Polar Distance for Persistence Diagrams](https://arxiv.org/abs/2507.14340)
*Athanasios Andrikopoulos,Nikolaos Sampanis*

Main category: math.AT

TL;DR: 该论文提出了一种新的度量框架，将拓扑数据分析（TDA）应用于社会选择理论，通过极坐标距离改进传统距离度量，提升了噪声偏好数据的处理能力。


<details>
  <summary>Details</summary>
Motivation: 社会选择理论中的偏好数据具有几何复杂性且对扰动敏感，而TDA在这一领域的应用尚未充分探索。论文旨在填补这一空白。

Method: 提出了一种基于极坐标的距离度量，用于持久图，以平滑且可微分的方式捕捉拓扑特征的幅度和方向。

Result: 新度量在理论和应用场景中表现更优，实验验证了其鲁棒性和监督学习任务中的优势。

Conclusion: 该研究为拓扑学与决策理论的交叉领域提供了新工具，推动了可解释机器学习在政治经济系统中的应用。

Abstract: Topological Data Analysis (TDA) has emerged as a powerful framework for
extracting robust and interpretable features from noisy high-dimensional data.
In the context of Social Choice Theory, where preference profiles and
collective decisions are geometrically rich yet sensitive to perturbations, TDA
remains largely unexplored. This work introduces a novel conceptual bridge
between these domains by proposing a new metric framework for persistence
diagrams tailored to noisy preference data.We define a polar coordinate-based
distance that captures both the magnitude and orientation of topological
features in a smooth and differentiable manner. Our metric addresses key
limitations of classical distances, such as bottleneck and Wasserstein,
including instability under perturbation, lack of continuity, and
incompatibility with gradient-based learning. The resulting formulation offers
improved behavior in both theoretical and applied settings.To the best of our
knowledge, this is the first study to systematically apply persistent homology
to social choice systems, providing a mathematically grounded method for
comparing topological summaries of voting structures and preference dynamics.
We demonstrate the superiority of our approach through extensive experiments,
including robustness tests and supervised learning tasks, and we propose a
modular pipeline for building predictive models from online preference data.
This work contributes a conceptually novel and computationally effective tool
to the emerging interface of topology and decision theory, opening new
directions in interpretable machine learning for political and economic
systems.

</details>


<div id='physics.comp-ph'></div>

# physics.comp-ph [[Back]](#toc)

### [324] [MENO: Hybrid Matrix Exponential-based Neural Operator for Stiff ODEs. Application to Thermochemical Kinetics](https://arxiv.org/abs/2507.14341)
*Ivan Zanardi,Simone Venturi,Marco Panesi*

Main category: physics.comp-ph

TL;DR: MENO是一种混合代理建模框架，用于高效求解具有稀疏非线性结构的刚性ODE系统，通过分解系统为非线性部分和线性部分，结合神经算子和矩阵指数公式，实现物理一致性和高效训练。


<details>
  <summary>Details</summary>
Motivation: 解决刚性ODE系统中稀疏非线性结构的计算效率问题，同时确保物理一致性和训练效率。

Method: 将系统分解为低维非线性部分（使用神经算子建模）和线性时变子系统（使用神经矩阵指数公式），直接嵌入控制方程以确保物理一致性。

Result: 在多个复杂热化学系统中验证，相对误差低于2%，计算速度提升显著（GPU上4800倍，CPU上185倍）。

Conclusion: MENO通过物理驱动的架构实现了优异的泛化能力和可靠性，为实时模拟刚性反应系统提供了可扩展的解决方案。

Abstract: We introduce MENO (''Matrix Exponential-based Neural Operator''), a hybrid
surrogate modeling framework for efficiently solving stiff systems of ordinary
differential equations (ODEs) that exhibit a sparse nonlinear structure. In
such systems, only a few variables contribute nonlinearly to the dynamics,
while the majority influence the equations linearly. MENO exploits this
property by decomposing the system into two components: the low-dimensional
nonlinear part is modeled using conventional neural operators, while the linear
time-varying subsystem is integrated using a novel neural matrix exponential
formulation. This approach combines the exact solution of linear time-invariant
systems with learnable, time-dependent graph-based corrections applied to the
linear operators. Unlike black-box or soft-constrained physics-informed (PI)
models, MENO embeds the governing equations directly into its architecture,
ensuring physical consistency (e.g., steady states), improved robustness, and
more efficient training. We validate MENO on three complex thermochemical
systems: the POLLU atmospheric chemistry model, an oxygen mixture in
thermochemical nonequilibrium, and a collisional-radiative argon plasma in one-
and two-dimensional shock-tube simulations. MENO achieves relative errors below
2% in trained zero-dimensional settings and maintains good accuracy in
extrapolatory multidimensional regimes. It also delivers substantial
computational speedups, achieving up to 4 800$\times$ on GPU and 185$\times$ on
CPU compared to standard implicit ODE solvers. Although intrusive by design,
MENO's physics-based architecture enables superior generalization and
reliability, offering a scalable path for real-time simulation of stiff
reactive systems.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [325] [Generating executable oracles to check conformance of client code to requirements of JDK Javadocs using LLMs](https://arxiv.org/abs/2411.01789)
*Shan Jiang,Chenguang Zhu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 论文探讨了自动化测试预言（test oracle）的生成方法，利用Java库的Javadoc和大型语言模型（LLMs）来自动生成测试预言，实验表明该方法高效且准确。


<details>
  <summary>Details</summary>
Motivation: 测试预言自动化是一个相对未被充分探索的领域，而Javadoc提供了丰富的自然语言描述信息，可用于生成测试预言。

Method: 利用Javadoc中的自然语言描述信息，结合大型语言模型（LLMs）生成测试预言。

Result: 实验显示，LLMs生成的测试预言98.8%可编译，96.4%准确反映预期行为，错误较少且易于修正。

Conclusion: Javadoc结合LLMs是一种有效的测试预言自动化方法，具有高准确性和实用性。

Abstract: Software testing remains the most widely used methodology for validating
quality of code. However, effectiveness of testing critically depends on the
quality of test suites used. Test cases in a test suite consist of two
fundamental parts: (1) input values for the code under test, and (2) correct
checks for the outputs it produces. These checks are commonly written as
assertions, and termed test oracles. The last couple of decades have seen much
progress in automated test input generation, e.g., using fuzzing and symbolic
execution. However, automating test oracles remains a relatively less explored
problem area. Indeed, a test oracle by its nature requires knowledge of
expected behavior, which may only be known to the developer and may not not
exist in a formal language that supports automated reasoning.
  Our focus in this paper is automation of test oracles for clients of widely
used Java libraries, e.g., java.lang and java.util packages. Our key insight is
that Javadocs that provide a rich source of information can enable automated
generation of test oracles. Javadocs of the core Java libraries are fairly
detailed documents that contain natural language descriptions of not only how
the libraries behave but also how the clients must (not) use them. We use large
language models as an enabling technology to embody our insight into a
framework for test oracle automation, and evaluate it experimentally. Our
experiments demonstrate that LLMs can generate oracles for checking normal and
exceptional behaviors from Javadocs, with 98.8% of these oracles being
compilable and 96.4% accurately reflecting intended properties. Even for the
few incorrect oracles, errors are minor and can be easily corrected with the
help of additional comment information generated by the LLMs.

</details>


### [326] [On the Effectiveness of Large Language Models in Writing Alloy Formulas](https://arxiv.org/abs/2502.15441)
*Yang Hong,Shan Jiang,Yulei Fu,Sarfraz Khurshid*

Main category: cs.SE

TL;DR: 论文探讨了使用大型语言模型（LLMs）生成Alloy声明式公式的实验，展示了LLMs在从自然语言描述生成完整公式、创建等效公式以及补全公式草图方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 声明式规范在开发安全可靠的软件系统中至关重要，但正确编写规范仍具挑战性。研究旨在探索LLMs在提升规范编写能力方面的作用。

Method: 实验采用ChatGPT和DeepSeek两种LLMs，分别从自然语言生成完整Alloy公式、创建等效公式以及补全公式草图。

Result: 实验结果表明，LLMs在从自然语言或Alloy输入生成完整公式、枚举多解以及补全公式草图方面表现良好。

Conclusion: LLMs为规范编写提供了令人兴奋的进步，有望在软件开发中发挥关键作用，提升软件健壮性。

Abstract: Declarative specifications have a vital role to play in developing safe and
dependable software systems. Writing specifications correctly, however, remains
particularly challenging. This paper presents a controlled experiment on using
large language models (LLMs) to write declarative formulas in the well-known
language Alloy. Our use of LLMs is three-fold. One, we employ LLMs to write
complete Alloy formulas from given natural language descriptions (in English).
Two, we employ LLMs to create alternative but equivalent formulas in Alloy with
respect to given Alloy formulas. Three, we employ LLMs to complete sketches of
Alloy formulas and populate the holes in the sketches by synthesizing Alloy
expressions and operators so that the completed formulas accurately represent
the desired properties (that are given in natural language). We conduct the
experimental evaluation using 11 well-studied subject specifications and employ
two popular LLMs, namely ChatGPT and DeepSeek. The experimental results show
that the LLMs generally perform well in synthesizing complete Alloy formulas
from input properties given in natural language or in Alloy, and are able to
enumerate multiple unique solutions. Moreover, the LLMs are also successful at
completing given sketches of Alloy formulas with respect to natural language
descriptions of desired properties (without requiring test cases). We believe
LLMs offer a very exciting advance in our ability to write specifications, and
can help make specifications take a pivotal role in software development and
enhance our ability to build robust software.

</details>


### [327] [Impact of Code Context and Prompting Strategies on Automated Unit Test Generation with Modern General-Purpose Large Language Models](https://arxiv.org/abs/2507.14256)
*Jakub Walczak,Piotr Tomalak,Artur Laskowski*

Main category: cs.SE

TL;DR: 研究探讨了代码上下文和提示策略对LLM生成单元测试质量的影响，发现包含文档字符串显著提升代码充分性，而全上下文扩展效果较小。链式思考提示策略表现最佳，M5模型在多项指标中领先。


<details>
  <summary>Details</summary>
Motivation: 提升软件工程中单元测试的自动生成效率，减少开发阶段的人工干预。

Method: 分析不同代码上下文和提示策略对LLM生成单元测试的影响，评估模型性能。

Result: 文档字符串显著提升代码充分性；链式思考提示策略效果最佳，M5模型表现最优。

Conclusion: 代码上下文和提示策略对测试生成质量至关重要，M5模型和链式思考策略值得推广。

Abstract: Generative AI is gaining increasing attention in software engineering, where
testing remains an indispensable reliability mechanism. According to the widely
adopted testing pyramid, unit tests constitute the majority of test cases and
are often schematic, requiring minimal domain expertise. Automatically
generating such tests under the supervision of software engineers can
significantly enhance productivity during the development phase of the software
lifecycle.
  This paper investigates the impact of code context and prompting strategies
on the quality and adequacy of unit tests generated by various large language
models (LLMs) across several families. The results show that including
docstrings notably improves code adequacy, while further extending context to
the full implementation yields definitely smaller gains. Notably, the
chain-of-thought prompting strategy -- applied even to 'reasoning' models --
achieves the best results, with up to 96.3\% branch coverage, a 57\% average
mutation score, and near-perfect compilation success rate. Among the evaluated
models, M5 (Gemini 2.5 Pro) demonstrated superior performance in both mutation
score and branch coverage being still in top in terms of compilation success
rate.
  All the code and resulting test suites are publicly available at
https://github.com/peetery/LLM-analysis.

</details>


### [328] [The Rise of AI Teammates in Software Engineering (SE) 3.0: How Autonomous Coding Agents Are Reshaping Software Engineering](https://arxiv.org/abs/2507.15003)
*Hao Li,Haoxiang Zhang,Ahmed E. Hassan*

Main category: cs.SE

TL;DR: 论文介绍了AIDev，首个大规模数据集，记录了AI编码代理在真实环境中的操作，为研究AI与人类开发者的协作提供了实证基础。


<details>
  <summary>Details</summary>
Motivation: 研究AI代理在软件开发中的实际表现，填补理论与实践的差距，推动AI原生软件工程的发展。

Method: 收集了456,000个由五个主流AI代理（如GitHub Copilot）提交的拉取请求数据，涵盖61,000个仓库和47,000名开发者，提供结构化开放数据。

Result: AI代理在提交速度上优于人类，但接受率较低，代码结构更简单，揭示了信任与效用差距。

Conclusion: AIDev为研究AI原生工作流和人类-AI协作提供了实证支持，是SE 3.0研究的重要资源。

Abstract: The future of software engineering--SE 3.0--is unfolding with the rise of AI
teammates: autonomous, goal-driven systems collaborating with human developers.
Among these, autonomous coding agents are especially transformative, now
actively initiating, reviewing, and evolving code at scale. This paper
introduces AIDev, the first large-scale dataset capturing how such agents
operate in the wild. Spanning over 456,000 pull requests by five leading
agents--OpenAI Codex, Devin, GitHub Copilot, Cursor, and Claude Code--across
61,000 repositories and 47,000 developers, AIDev provides an unprecedented
empirical foundation for studying autonomous teammates in software development.
  Unlike prior work that has largely theorized the rise of AI-native software
engineering, AIDev offers structured, open data to support research in
benchmarking, agent readiness, optimization, collaboration modeling, and AI
governance. The dataset includes rich metadata on PRs, authorship, review
timelines, code changes, and integration outcomes--enabling exploration beyond
synthetic benchmarks like SWE-bench. For instance, although agents often
outperform humans in speed, their PRs are accepted less frequently, revealing a
trust and utility gap. Furthermore, while agents accelerate code
submission--one developer submitted as many PRs in three days as they had in
three years--these are structurally simpler (via code complexity metrics).
  We envision AIDev as a living resource: extensible, analyzable, and ready for
the SE and AI communities. Grounding SE 3.0 in real-world evidence, AIDev
enables a new generation of research into AI-native workflows and supports
building the next wave of symbiotic human-AI collaboration. The dataset is
publicly available at https://github.com/SAILResearch/AI_Teammates_in_SE3.
  > AI Agent, Agentic AI, Coding Agent, Agentic Coding, Software Engineering
Agent

</details>


### [329] [Survey of GenAI for Automotive Software Development: From Requirements to Executable Code](https://arxiv.org/abs/2507.15025)
*Nenad Petrovic,Vahid Zolfaghari,Andre Schamschurko,Sven Kirchner,Fengjunjie Pan,Chengdng Wu,Nils Purschke,Aleksei Velsh,Krzysztof Lebioda,Yinglei Song,Yi Zhang,Lukasz Mazur,Alois Knoll*

Main category: cs.SE

TL;DR: 探讨GenAI在汽车软件开发中的应用，重点关注需求处理、合规性和代码生成，并总结了行业调查结果。


<details>
  <summary>Details</summary>
Motivation: 汽车软件开发过程冗长且昂贵，GenAI有望减少人工干预和复杂性。

Method: 采用LLMs、RAG和VLMs等技术，结合提示技术，提出GenAI辅助的工作流程。

Result: 总结了GenAI工具在汽车行业中的应用现状，并提出了通用工作流程。

Conclusion: GenAI在汽车软件开发中具有潜力，但需进一步优化和验证。

Abstract: Adoption of state-of-art Generative Artificial Intelligence (GenAI) aims to
revolutionize many industrial areas by reducing the amount of human
intervention needed and effort for handling complex underlying processes.
Automotive software development is considered to be a significant area for
GenAI adoption, taking into account lengthy and expensive procedures, resulting
from the amount of requirements and strict standardization. In this paper, we
explore the adoption of GenAI for various steps of automotive software
development, mainly focusing on requirements handling, compliance aspects and
code generation. Three GenAI-related technologies are covered within the
state-of-art: Large Language Models (LLMs), Retrieval Augmented Generation
(RAG), Vision Language Models (VLMs), as well as overview of adopted prompting
techniques in case of code generation. Additionally, we also derive a
generalized GenAI-aided automotive software development workflow based on our
findings from this literature review. Finally, we include a summary of a survey
outcome, which was conducted among our automotive industry partners regarding
the type of GenAI tools used for their daily work activities.

</details>


### [330] [Can LLMs Generate User Stories and Assess Their Quality?](https://arxiv.org/abs/2507.15157)
*Giovanni Quattrocchi,Liliana Pasquale,Paola Spoletini,Luciano Baresi*

Main category: cs.SE

TL;DR: 论文探讨了如何利用LLMs在敏捷框架中自动化需求获取，生成用户故事（US），并评估其语义质量。结果表明LLMs生成的US在覆盖率和风格质量上与人类相似，但多样性和创造性较低，且满足验收标准的频率较低。LLMs在提供明确评估标准时可可靠评估US的语义质量。


<details>
  <summary>Details</summary>
Motivation: 需求获取是需求工程中最具挑战性的活动之一，传统方法在语义质量评估上耗时且手动。研究旨在探索LLMs在自动化需求获取和语义质量评估中的潜力。

Method: 使用10种先进的LLMs模拟客户访谈生成US，并与人类生成的US（领域专家和学生）进行质量对比。同时探索LLMs在自动评估US语义质量中的应用。

Result: LLMs生成的US在覆盖率和风格质量上与人类相似，但多样性和创造性较低，且满足验收标准的频率较低。LLMs在提供明确评估标准时可可靠评估语义质量。

Conclusion: LLMs在自动化需求获取和语义质量评估中具有潜力，可减少大规模评估中的人力投入，但在多样性和创造性方面仍需改进。

Abstract: Requirements elicitation is still one of the most challenging activities of
the requirements engineering process due to the difficulty requirements
analysts face in understanding and translating complex needs into concrete
requirements. In addition, specifying high-quality requirements is crucial, as
it can directly impact the quality of the software to be developed. Although
automated tools allow for assessing the syntactic quality of requirements,
evaluating semantic metrics (e.g., language clarity, internal consistency)
remains a manual and time-consuming activity. This paper explores how LLMs can
help automate requirements elicitation within agile frameworks, where
requirements are defined as user stories (US). We used 10 state-of-the-art LLMs
to investigate their ability to generate US automatically by emulating customer
interviews. We evaluated the quality of US generated by LLMs, comparing it with
the quality of US generated by humans (domain experts and students). We also
explored whether and how LLMs can be used to automatically evaluate the
semantic quality of US. Our results indicate that LLMs can generate US similar
to humans in terms of coverage and stylistic quality, but exhibit lower
diversity and creativity. Although LLM-generated US are generally comparable in
quality to those created by humans, they tend to meet the acceptance quality
criteria less frequently, regardless of the scale of the LLM model. Finally,
LLMs can reliably assess the semantic quality of US when provided with clear
evaluation criteria and have the potential to reduce human effort in
large-scale assessments.

</details>


### [331] [SimdBench: Benchmarking Large Language Models for SIMD-Intrinsic Code Generation](https://arxiv.org/abs/2507.15224)
*Yibo He,Shuoran Zhao,Jiaming Huang,Yingjie Fu,Hao Yu,Cunjian Huang,Tao Xie*

Main category: cs.SE

TL;DR: SimdBench是首个专门为SIMD-intrinsic代码生成设计的基准测试，包含136个任务，评估了18个LLM在SIMD代码生成中的表现。


<details>
  <summary>Details</summary>
Motivation: 填补现有代码生成基准测试在SIMD-intrinsic代码生成方面的空白，评估LLM在此领域的表现。

Method: 设计SimdBench基准测试，包含136个任务，针对五种SIMD指令集（SSE、AVX、Neon、SVE、RVV），系统评估18个LLM的代码生成能力。

Result: LLM在SIMD-intrinsic代码生成中的pass@k普遍低于标量代码生成，但分析指出了进一步改进的方向。

Conclusion: SimdBench为研究社区提供了开源工具，推动了LLM在SIMD代码生成领域的进步。

Abstract: SIMD (Single Instruction Multiple Data) instructions and their compiler
intrinsics are widely supported by modern processors to accelerate
performance-critical tasks. SIMD intrinsic programming, a trade-off between
coding productivity and high performance, is widely used in the development of
mainstream performance-critical libraries and daily computing tasks. Large
Language Models (LLMs), which have demonstrated strong and comprehensive
capabilities in code generation, show promise in assisting programmers with the
challenges of SIMD intrinsic programming. However, existing code-generation
benchmarks focus on only scalar code, and it is unclear how LLMs perform in
generating vectorized code using SIMD intrinsics. To fill this gap, we propose
SimdBench, the first code benchmark specifically designed for SIMD-intrinsic
code generation, comprising 136 carefully crafted tasks and targeting five
representative SIMD intrinsics: SSE (x86 Streaming SIMD Extension), AVX (x86
Advanced Vector Extension), Neon (ARM Advanced SIMD Extension), SVE (ARM
Scalable Vector Extension), and RVV (RISC-V Vector Extension). We conduct a
systematic evaluation (measuring both correctness and performance) of 18
representative LLMs on SimdBench, resulting in a series of novel and insightful
findings. Our evaluation results demonstrate that LLMs exhibit a universal
decrease in pass@k during SIMD-intrinsic code generation compared to
scalar-code generation. Our in-depth analysis highlights promising directions
for the further advancement of LLMs in the challenging domain of SIMD-intrinsic
code generation. SimdBench is fully open source at
https://anonymous.4open.science/r/SimdBench-1B3F/ to benefit the broader
research community.

</details>


### [332] [Butterfly Effects in Toolchains: A Comprehensive Analysis of Failed Parameter Filling in LLM Tool-Agent Systems](https://arxiv.org/abs/2507.15296)
*Qian Xiong,Yuekai Huang,Ziyou Jiang,Zhiyuan Chang,Yujia Zheng,Tianhao Li,Mingyang Li*

Main category: cs.SE

TL;DR: 论文研究了工具代理范式中参数失败的问题，提出了分类和改进建议。


<details>
  <summary>Details</summary>
Motivation: 探索工具代理范式中参数失败的现象，并提出改进建议以提高其可靠性。

Method: 构建参数失败分类法，通过输入扰动方法分析输入源与失败类别的相关性。

Result: 实验表明参数名称幻觉失败主要源于LLM固有局限，其他失败模式则与输入源问题相关。

Conclusion: 建议标准化工具返回格式、改进错误反馈机制和确保参数一致性以提高工具代理交互的可靠性。

Abstract: The emergence of the tool agent paradigm has broadened the capability
boundaries of the Large Language Model (LLM), enabling it to complete more
complex tasks. However, the effectiveness of this paradigm is limited due to
the issue of parameter failure during its execution. To explore this phenomenon
and propose corresponding suggestions, we first construct a parameter failure
taxonomy in this paper. We derive five failure categories from the invocation
chain of a mainstream tool agent. Then, we explore the correlation between
three different input sources and failure categories by applying 15 input
perturbation methods to the input. Experimental results show that parameter
name hallucination failure primarily stems from inherent LLM limitations, while
issues with input sources mainly cause other failure patterns. To improve the
reliability and effectiveness of tool-agent interactions, we propose
corresponding improvement suggestions, including standardizing tool return
formats, improving error feedback mechanisms, and ensuring parameter
consistency.

</details>


### [333] [StackTrans: From Large Language Model to Large Pushdown Automata Model](https://arxiv.org/abs/2507.15343)
*Kechi Zhang,Ge Li,Jia Li,Huangzhao Zhang,Yihong Dong,Jia Li,Jingjing Xu,Zhi Jin*

Main category: cs.SE

TL;DR: StackTrans通过引入隐藏状态栈解决了Transformer在捕捉Chomsky层次结构（如正则表达式）上的局限性，并在多任务中表现优于标准Transformer。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在捕捉Chomsky层次结构（如正则表达式或确定性上下文无关文法）方面存在局限性，需要改进。

Method: StackTrans在Transformer层间引入可微分的隐藏状态栈操作（如压栈和弹栈），保持与现有框架（如flash-attention）的兼容性。

Result: StackTrans在Chomsky层次结构和自然语言任务中表现优于标准Transformer，且参数效率更高（如360M模型优于2-3倍参数的LLM）。

Conclusion: StackTrans通过栈操作增强了Transformer的推理能力，为LLM提供了更高效和强大的架构。

Abstract: The Transformer architecture has emerged as a landmark advancement within the
broad field of artificial intelligence, effectively catalyzing the advent of
large language models (LLMs). However, despite its remarkable capabilities and
the substantial progress it has facilitated, the Transformer architecture still
has some limitations. One such intrinsic limitation is its inability to
effectively capture the Chomsky hierarchy, such as regular expressions or
deterministic context-free grammars. Drawing inspiration from pushdown
automata, which efficiently resolve deterministic context-free grammars using
stacks, we propose StackTrans to address the aforementioned issue within LLMs.
Unlike previous approaches that modify the attention computation, StackTrans
explicitly incorporates hidden state stacks between Transformer layers. This
design maintains compatibility with existing frameworks like flash-attention.
Specifically, our design features stack operations -- such as pushing and
popping hidden states -- that are differentiable and can be learned in an
end-to-end manner. Our comprehensive evaluation spans benchmarks for both
Chomsky hierarchies and large-scale natural languages. Across these diverse
tasks, StackTrans consistently outperforms standard Transformer models and
other baselines. We have successfully scaled StackTrans up from 360M to 7B
parameters. In particular, our from-scratch pretrained model StackTrans-360M
outperforms several larger open-source LLMs with 2-3x more parameters,
showcasing its superior efficiency and reasoning capability.

</details>


### [334] [Applying the Chinese Wall Reverse Engineering Technique to Large Language Model Code Editing](https://arxiv.org/abs/2507.15599)
*Manatsawin Hanmongkolchai*

Main category: cs.SE

TL;DR: 论文提出了一种“中国墙”技术，通过强模型生成指令来提升弱模型的性能，解决了伦理对齐问题。


<details>
  <summary>Details</summary>
Motivation: 解决Code LLM训练数据版权问题，同时提升弱模型的实用性。

Method: 应用“中国墙”技术，利用强模型生成指令指导弱模型完成任务。

Result: Comma v0.1 1T性能提升66%，Starcoder2 Instruct提升20%。

Conclusion: 该技术有效提升弱模型性能，但实际应用受限于缺乏无版权限制的公共领域模型。

Abstract: Large language models for code (Code LLM) are increasingly utilized in
programming environments. Despite their utility, the training datasets for top
LLM remain undisclosed, raising concerns about potential copyright violations.
Some models, such as Pleias and Comma put emphasis on data curation and
licenses, however, with limited training data these models are not competitive
and only serve as proof of concepts. To improve the utility of these models, we
propose an application of the "Chinese Wall" technique, inspired by the reverse
engineering technique of the same name -- a high quality model is used to
generate detailed instructions for a weaker model. By doing so, a weaker but
ethically aligned model may be used to perform complicated tasks that,
otherwise, can only be completed by more powerful models. In our evaluation,
we've found that this technique improves Comma v0.1 1T's performance in
CanItEdit benchmark by over 66%, and Starcoder2 Instruct by roughly 20%
compared to when running the same model on the benchmark alone. The practical
application of this technique today, however, may be limited due to the lack of
models trained on public domain content without copyright restrictions.

</details>


### [335] [SustainDiffusion: Optimising the Social and Environmental Sustainability of Stable Diffusion Models](https://arxiv.org/abs/2507.15663)
*Giordano d'Aloisio,Tosin Fadahunsi,Jay Choy,Rebecca Moussa,Federica Sarro*

Main category: cs.SE

TL;DR: SustainDiffusion是一种基于搜索的方法，旨在提升Stable Diffusion模型的社会和环境可持续性，减少性别和种族偏见及能耗，同时保持图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决Stable Diffusion模型在社会和环境可持续性方面的负面影响，如性别和种族偏见及高能耗问题。

Method: 通过搜索最优超参数和提示结构组合，减少偏见和能耗，同时保持图像质量。

Result: SustainDiffusion将性别偏见减少68%，种族偏见减少59%，能耗降低48%，且结果稳定且可推广。

Conclusion: 无需微调或改变模型架构，即可提升文本到图像生成模型的可持续性。

Abstract: Background: Text-to-image generation models are widely used across numerous
domains. Among these models, Stable Diffusion (SD) - an open-source
text-to-image generation model - has become the most popular, producing over 12
billion images annually. However, the widespread use of these models raises
concerns regarding their social and environmental sustainability.
  Aims: To reduce the harm that SD models may have on society and the
environment, we introduce SustainDiffusion, a search-based approach designed to
enhance the social and environmental sustainability of SD models.
  Method: SustainDiffusion searches the optimal combination of hyperparameters
and prompt structures that can reduce gender and ethnic bias in generated
images while also lowering the energy consumption required for image
generation. Importantly, SustainDiffusion maintains image quality comparable to
that of the original SD model.
  Results: We conduct a comprehensive empirical evaluation of SustainDiffusion,
testing it against six different baselines using 56 different prompts. Our
results demonstrate that SustainDiffusion can reduce gender bias in SD3 by 68%,
ethnic bias by 59%, and energy consumption (calculated as the sum of CPU and
GPU energy) by 48%. Additionally, the outcomes produced by SustainDiffusion are
consistent across multiple runs and can be generalised to various prompts.
  Conclusions: With SustainDiffusion, we demonstrate how enhancing the social
and environmental sustainability of text-to-image generation models is possible
without fine-tuning or changing the model's architecture.

</details>


### [336] [Do AI models help produce verified bug fixes?](https://arxiv.org/abs/2507.15822)
*Li Huang,Ilgiz Mustafin,Marco Piccioni,Alessandro Schena,Reto Weber,Bertrand Meyer*

Main category: cs.SE

TL;DR: 研究探讨了大型语言模型（LLM）在自动程序修复（APR）中的实际效果，通过实验对比两组程序员（一组使用LLM，另一组不使用）的表现，验证了LLM在调试中的角色。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在APR中的实际效果，验证其是否能提供正确修复，并研究程序员如何利用LLM辅助调试。

Method: 采用随机分组实验，一组程序员使用LLM，另一组不使用，通过程序证明工具验证修复的正确性。研究基于目标-查询-度量方法。

Result: 实验结果与预期不同，揭示了LLM在调试中的实际作用。贡献包括实验方法、程序员行为分析、LLM使用模式分类及优化建议。

Conclusion: 研究为LLM在APR中的合理应用提供了初步依据，并提出了具体的使用建议。

Abstract: Among areas of software engineering where AI techniques -- particularly,
Large Language Models -- seem poised to yield dramatic improvements, an
attractive candidate is Automatic Program Repair (APR), the production of
satisfactory corrections to software bugs. Does this expectation materialize in
practice? How do we find out, making sure that proposed corrections actually
work? If programmers have access to LLMs, how do they actually use them to
complement their own skills?
  To answer these questions, we took advantage of the availability of a
program-proving environment, which formally determines the correctness of
proposed fixes, to conduct a study of program debugging with two randomly
assigned groups of programmers, one with access to LLMs and the other without,
both validating their answers through the proof tools. The methodology relied
on a division into general research questions (Goals in the Goal-Query-Metric
approach), specific elements admitting specific answers (Queries), and
measurements supporting these answers (Metrics). While applied so far to a
limited sample size, the results are a first step towards delineating a proper
role for AI and LLMs in providing guaranteed-correct fixes to program bugs.
  These results caused surprise as compared to what one might expect from the
use of AI for debugging and APR. The contributions also include: a detailed
methodology for experiments in the use of LLMs for debugging, which other
projects can reuse; a fine-grain analysis of programmer behavior, made possible
by the use of full-session recording; a definition of patterns of use of LLMs,
with 7 distinct categories; and validated advice for getting the best of LLMs
for debugging and Automatic Program Repair.

</details>


<div id='physics.chem-ph'></div>

# physics.chem-ph [[Back]](#toc)

### [337] [A universal augmentation framework for long-range electrostatics in machine learning interatomic potentials](https://arxiv.org/abs/2507.14302)
*Dongjin Kim,Xiaoyu Wang,Peichen Zhong,Daniel S. King,Theo Jaffrelot Inizan,Bingqing Cheng*

Main category: physics.chem-ph

TL;DR: LES方法通过机器学习推断静电相互作用，兼容多种短程MLIP，提升精度并实现高效长程静电处理。


<details>
  <summary>Details</summary>
Motivation: 解决当前机器学习原子间势（MLIPs）缺乏显式长程静电处理的问题。

Method: 开发LES方法，通过能量和力训练数据推断静电相互作用、极化和Born有效电荷。

Result: LES增强模型在多种系统中表现优异，MACELES-OFF在SPICE数据集上表现优于短程版本。

Conclusion: LES为静电基础MLIPs的发展铺平了道路。

Abstract: Most current machine learning interatomic potentials (MLIPs) rely on
short-range approximations, without explicit treatment of long-range
electrostatics. To address this, we recently developed the Latent Ewald
Summation (LES) method, which infers electrostatic interactions, polarization,
and Born effective charges (BECs), just by learning from energy and force
training data. Here, we present LES as a standalone library, compatible with
any short-range MLIP, and demonstrate its integration with methods such as
MACE, NequIP, CACE, and CHGNet. We benchmark LES-enhanced models on distinct
systems, including bulk water, polar dipeptides, and gold dimer adsorption on
defective substrates, and show that LES not only captures correct
electrostatics but also improves accuracy. Additionally, we scale LES to large
and chemically diverse data by training MACELES-OFF on the SPICE set containing
molecules and clusters, making a universal MLIP with electrostatics for organic
systems including biomolecules. MACELES-OFF is more accurate than its
short-range counterpart (MACE-OFF) trained on the same dataset, predicts
dipoles and BECs reliably, and has better descriptions of bulk liquids. By
enabling efficient long-range electrostatics without directly training on
electrical properties, LES paves the way for electrostatic foundation MLIPs.

</details>


<div id='cs.IT'></div>

# cs.IT [[Back]](#toc)

### [338] [Collusion-Resilient Hierarchical Secure Aggregation with Heterogeneous Security Constraints](https://arxiv.org/abs/2507.14768)
*Zhou Li,Xiang Zhang,Jiawen Lv,Jihao Fan,Haiqiang Chen,Giuseppe Caire*

Main category: cs.IT

TL;DR: 论文研究了分层安全聚合（HSA）中的弱安全性问题，提出了一种灵活框架（WS-HSA）以应对异构安全需求，并分析了最优密钥率。


<details>
  <summary>Details</summary>
Motivation: 为了解决分层安全聚合中因用户数量增加而导致的异构安全需求问题，例如不同集群用户对输入保护的不同要求。

Method: 提出弱安全分层安全聚合（WS-HSA）框架，通过预定义安全输入集和合谋集来灵活应对安全需求，并分析最优密钥率。

Result: 确定了最优总密钥率，并在部分情况下提供了密钥率的下界和上界，保证了常数因子最优性。

Conclusion: WS-HSA框架为分层安全聚合中的异构安全需求提供了灵活且高效的解决方案。

Abstract: Motivated by federated learning (FL), secure aggregation (SA) aims to
securely compute, as efficiently as possible, the sum of a set of inputs
distributed across many users. To understand the impact of network topology,
hierarchical secure aggregation (HSA) investigated the communication and secret
key generation efficiency in a 3-layer relay network, where clusters of users
are connected to the aggregation server through an intermediate layer of
relays. Due to the pre-aggregation of the messages at the relays, HSA reduces
the communication burden on the relay-to-server links and is able to support a
large number of users. However, as the number of users increases, a practical
challenge arises from heterogeneous security requirements--for example, users
in different clusters may require varying levels of input protection. Motivated
by this, we study weakly-secure HSA (WS-HSA) with collusion resilience, where
instead of protecting all the inputs from any set of colluding users, only the
inputs belonging to a predefined collection of user groups (referred to as
security input sets) need to be protected against another predefined collection
of user groups (referred to as collusion sets). Since the security input sets
and collusion sets can be arbitrarily defined, our formulation offers a
flexible framework for addressing heterogeneous security requirements in HSA.
We characterize the optimal total key rate, i.e., the total number of
independent key symbols required to ensure both server and relay security, for
a broad range of parameter configurations. For the remaining cases, we
establish lower and upper bounds on the optimal key rate, providing
constant-factor gap optimality guarantees.

</details>


<div id='math.NA'></div>

# math.NA [[Back]](#toc)

### [339] [Numerical Artifacts in Learning Dynamical Systems](https://arxiv.org/abs/2507.14491)
*Bing-Ze Lu,Richard Tsai*

Main category: math.NA

TL;DR: 论文揭示了数值积分方案对学习动态系统的影响，可能导致错误的系统识别。


<details>
  <summary>Details</summary>
Motivation: 研究数值积分方案在动态系统学习中的潜在影响，以避免错误的系统识别。

Method: 通过分析数值积分方案对学习结果的影响，揭示其可能导致的反阻尼和振荡方向反转问题。

Result: 研究发现，数值积分方案可能导致系统被错误识别为反阻尼且振荡方向相反。

Conclusion: 数值积分方案的选择对动态系统学习结果有显著影响，需谨慎处理以避免错误识别。

Abstract: In many applications, one needs to learn a dynamical system from its
solutions sampled at a finite number of time points. The learning problem is
often formulated
  as an optimization problem over a chosen function class. However, in the
optimization procedure, it is necessary to employ a numerical scheme to
integrate candidate dynamical systems and assess how their solutions fit the
data.
  This paper reveals potentially serious effects of a chosen numerical scheme
on the learning outcome. In particular, our analysis demonstrates that a damped
oscillatory system may be incorrectly identified as having "anti-damping" and
exhibiting a reversed oscillation direction, despite adequately fitting the
given data points.

</details>


### [340] [Solving nonconvex Hamilton--Jacobi--Isaacs equations with PINN-based policy iteration](https://arxiv.org/abs/2507.15455)
*Hee Jun Yang,Min Jung Kim,Yeoneung Kim*

Main category: math.NA

TL;DR: 提出了一种结合动态规划和物理信息神经网络（PINN）的无网格策略迭代框架，用于解决高维非凸HJI方程，展示了其准确性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决高维非凸HJI方程在随机微分博弈和鲁棒控制中的计算难题，传统方法难以应对。

Method: 交替求解固定反馈策略下的线性二阶PDE，并通过自动微分更新控制策略。

Result: 数值实验表明，该方法在二维和五维、十维问题中表现优于直接PINN求解器，误差低于10^{-2}%。

Conclusion: 结合PINN与策略迭代是一种实用且理论可靠的方法，适用于机器人、金融和多智能体强化学习等领域。

Abstract: We propose a mesh-free policy iteration framework that combines classical
dynamic programming with physics-informed neural networks (PINNs) to solve
high-dimensional, nonconvex Hamilton--Jacobi--Isaacs (HJI) equations arising in
stochastic differential games and robust control. The method alternates between
solving linear second-order PDEs under fixed feedback policies and updating the
controls via pointwise minimax optimization using automatic differentiation.
Under standard Lipschitz and uniform ellipticity assumptions, we prove that the
value function iterates converge locally uniformly to the unique viscosity
solution of the HJI equation. The analysis establishes equi-Lipschitz
regularity of the iterates, enabling provable stability and convergence without
requiring convexity of the Hamiltonian. Numerical experiments demonstrate the
accuracy and scalability of the method. In a two-dimensional stochastic
path-planning game with a moving obstacle, our method matches finite-difference
benchmarks with relative $L^2$-errors below %10^{-2}%. In five- and
ten-dimensional publisher-subscriber differential games with anisotropic noise,
the proposed approach consistently outperforms direct PINN solvers, yielding
smoother value functions and lower residuals. Our results suggest that
integrating PINNs with policy iteration is a practical and theoretically
grounded method for solving high-dimensional, nonconvex HJI equations, with
potential applications in robotics, finance, and multi-agent reinforcement
learning.

</details>


<div id='math.DS'></div>

# math.DS [[Back]](#toc)

### [341] [Learning Stochastic Hamiltonian Systems via Stochastic Generating Function Neural Network](https://arxiv.org/abs/2507.14467)
*Chen Chen,Lijin Wang,Yanzhao Cao,Xupeng Cheng*

Main category: math.DS

TL;DR: 提出了一种新型神经网络模型SGFNN，用于从观测数据中学习随机哈密顿系统（SHSs），并保持系统的辛结构。


<details>
  <summary>Details</summary>
Motivation: 现有的随机哈密顿系统学习方法在长期预测和保持辛结构方面存在不足，需要一种更准确的模型。

Method: 采用自编码器框架，编码器识别系统的随机性，解码器基于提取的随机变量检测系统的随机生成函数，生成辛预测。

Result: 在多种随机哈密顿系统上的实验表明，SGFNN在长期预测和保持辛结构方面优于基准模型sFML。

Conclusion: SGFNN是一种有效的随机哈密顿系统学习方法，尤其在长期预测和辛结构保持方面表现出色。

Abstract: In this paper we propose a novel neural network model for learning stochastic
Hamiltonian systems (SHSs) from observational data, termed the stochastic
generating function neural network (SGFNN). SGFNN preserves symplectic
structure of the underlying stochastic Hamiltonian system and produces
symplectic predictions. Our model utilizes the autoencoder framework to
identify the randomness of the latent system by the encoder network, and
detects the stochastic generating function of the system through the decoder
network based on the random variables extracted from the encoder. Symplectic
predictions can then be generated by the stochastic generating function.
Numerical experiments are performed on several stochastic Hamiltonian systems,
varying from additive to multiplicative, and from separable to non-separable
SHSs with single or multiple noises. Compared with the benchmark stochastic
flow map learning (sFML) neural network, our SGFNN model exhibits higher
accuracy across various prediction metrics, especially in long-term
predictions, with the property of maintaining the symplectic structure of the
underlying SHSs.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [342] [Neural Brownian Motion](https://arxiv.org/abs/2507.14499)
*Qian Qi*

Main category: math.PR

TL;DR: 论文提出了一种新的随机过程——神经布朗运动（NBM），用于在学习的动态不确定性下建模。NBM通过非线性神经期望算子定义，并证明了其存在性和唯一性。


<details>
  <summary>Details</summary>
Motivation: 传统布朗运动的线性期望性质限制了其在复杂动态系统中的应用，NBM通过引入非线性神经期望算子，扩展了建模能力。

Method: NBM通过反向随机微分方程（BSDE）定义，其驱动函数由神经网络参数化，并证明了其强解的存在性和唯一性。

Result: 证明了NBM的存在性和唯一性，并推导了其随机微积分和Girsanov型定理，展示了其在学习不确定性态度中的应用。

Conclusion: NBM为动态不确定性建模提供了新工具，其学习参数可以内生地决定对不确定性的态度，具有广泛的应用潜力。

Abstract: This paper introduces the Neural-Brownian Motion (NBM), a new class of
stochastic processes for modeling dynamics under learned uncertainty. The NBM
is defined axiomatically by replacing the classical martingale property with
respect to linear expectation with one relative to a non-linear Neural
Expectation Operator, $\varepsilon^\theta$, generated by a Backward Stochastic
Differential Equation (BSDE) whose driver $f_\theta$ is parameterized by a
neural network. Our main result is a representation theorem for a canonical
NBM, which we define as a continuous $\varepsilon^\theta$-martingale with zero
drift under the physical measure. We prove that, under a key structural
assumption on the driver, such a canonical NBM exists and is the unique strong
solution to a stochastic differential equation of the form ${\rm d} M_t =
\nu_\theta(t, M_t) {\rm d} W_t$. Crucially, the volatility function
$\nu_\theta$ is not postulated a priori but is implicitly defined by the
algebraic constraint $g_\theta(t, M_t, \nu_\theta(t, M_t)) = 0$, where
$g_\theta$ is a specialization of the BSDE driver. We develop the stochastic
calculus for this process and prove a Girsanov-type theorem for the quadratic
case, showing that an NBM acquires a drift under a new, learned measure. The
character of this measure, whether pessimistic or optimistic, is endogenously
determined by the learned parameters $\theta$, providing a rigorous foundation
for models where the attitude towards uncertainty is a discoverable feature.

</details>


<div id='q-fin.CP'></div>

# q-fin.CP [[Back]](#toc)

### [343] [Transaction Profiling and Address Role Inference in Tokenized U.S. Treasuries](https://arxiv.org/abs/2507.14808)
*Junliang Luo,Katrin Tinn,Samuel Ferreira Duran,Di Wu,Xue Liu*

Main category: q-fin.CP

TL;DR: 本文对基于美国国债的RWA代币（如BUIDL、BENJI、USDY）进行了定量分析，研究了其功能行为和市场参与者角色。


<details>
  <summary>Details</summary>
Motivation: 尽管市场迅速扩张，但对交易级别行为的实证分析仍有限，本文旨在填补这一空白。

Method: 通过解码合约调用，分析核心功能（如发行、赎回、转移和跨链活动），并引入基于Poincaré嵌入和流动性的图特征框架。

Result: 方法在角色推断上优于基线模型，并能泛化到异常检测和钱包分类等下游任务。

Conclusion: 研究提供了对代币化国债功能异质性和参与者角色的结构化理解，为链上金融化研究提供了新实证证据。

Abstract: Tokenized U.S. Treasuries have emerged as a prominent subclass of real-world
assets (RWAs), offering cryptographically enforced, yield-bearing instruments
collateralized by sovereign debt and deployed across multiple blockchain
networks. While the market has expanded rapidly, empirical analyses of
transaction-level behaviour remain limited. This paper conducts a quantitative,
function-level dissection of U.S. Treasury-backed RWA tokens including BUIDL,
BENJI, and USDY, across multi-chain: mostly Ethereum and Layer-2s. We analyze
decoded contract calls to isolate core functional primitives such as issuance,
redemption, transfer, and bridge activity, revealing segmentation in behaviour
between institutional actors and retail users. To model address-level economic
roles, we introduce a curvature-aware representation learning framework using
Poincar\'e embeddings and liquidity-based graph features. Our method
outperforms baseline models on our RWA Treasury dataset in role inference and
generalizes to downstream tasks such as anomaly detection and wallet
classification in broader blockchain transaction networks. These findings
provide a structured understanding of functional heterogeneity and participant
roles in tokenized Treasury in a transaction-level perspective, contributing
new empirical evidence to the study of on-chain financialization.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [344] [Schemora: schema matching via multi-stage recommendation and metadata enrichment using off-the-shelf llms](https://arxiv.org/abs/2507.14376)
*Osman Erman Gungor,Derak Paulsen,William Kang*

Main category: cs.DB

TL;DR: SCHEMORA是一个结合大型语言模型和混合检索技术的模式匹配框架，无需标注数据或成对比较，显著提升匹配效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 异构数据源集成和数据集发现需要高效的模式匹配方法，但现有方法复杂且资源密集。

Method: 结合大型语言模型和混合检索技术，通过提示方法生成候选匹配，利用向量和词汇检索优化匹配。

Result: 在MIMIC-OMOP基准测试中，HitRate@5和HitRate@3分别提升7.49%和3.75%，达到新SOTA。

Conclusion: SCHEMORA是首个开源LLM模式匹配方法，强调了检索的关键作用，并提供了模型选择的实用指导。

Abstract: Schema matching is essential for integrating heterogeneous data sources and
enhancing dataset discovery, yet it remains a complex and resource-intensive
problem. We introduce SCHEMORA, a schema matching framework that combines large
language models with hybrid retrieval techniques in a prompt-based approach,
enabling efficient identification of candidate matches without relying on
labeled training data or exhaustive pairwise comparisons. By enriching schema
metadata and leveraging both vector-based and lexical retrieval, SCHEMORA
improves matching accuracy and scalability. Evaluated on the MIMIC-OMOP
benchmark, it establishes new state-of-the-art performance, with gains of 7.49%
in HitRate@5 and 3.75% in HitRate@3 over previous best results. To our
knowledge, this is the first LLM-based schema matching method with an
open-source implementation, accompanied by analysis that underscores the
critical role of retrieval and provides practical guidance on model selection.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [345] [On exploration of an interior mirror descent flow for stochastic nonconvex constrained problem](https://arxiv.org/abs/2507.15264)
*Kuangyu Ding,Kim-Chuan Toh*

Main category: math.OC

TL;DR: 论文研究了非光滑非凸优化问题，通过黎曼度量将可行集转化为微分包含的黎曼次梯度流，统一了Hessian障碍法和镜像下降法，并分析了其长期行为。


<details>
  <summary>Details</summary>
Motivation: 解决非光滑非凸优化问题中Hessian障碍法和镜像下降法的收敛性不足问题，并揭示其统一性。

Method: 通过黎曼度量和障碍函数构建微分包含的黎曼次梯度流，分析其轨迹行为，并提出避免虚假稳定点的条件。

Result: 揭示了Hessian障碍法和镜像下降法的统一性，提出了避免虚假稳定点的条件及随机扰动策略。

Conclusion: 提出了两种迭代黎曼次梯度方法，推广了现有方法，解决了非光滑非凸优化问题。

Abstract: We study a nonsmooth nonconvex optimization problem defined over nonconvex
constraints, where the feasible set is given by the intersection of the closure
of an open set and a smooth manifold. By endowing the open set with a
Riemannian metric induced by a barrier function, we obtain a Riemannian
subgradient flow formulated as a differential inclusion, which remains strictly
within the interior of the feasible set. This continuous dynamical system
unifies two classes of iterative optimization methods, namely the Hessian
barrier method and mirror descent scheme, by revealing that these methods can
be interpreted as discrete approximations of the continuous flow. We explore
the long-term behavior of the trajectories generated by this dynamical system
and show that the existing deficient convergence properties of the Hessian
barrier and mirror descent scheme can be unifily and more insightfully
interpreted through these of the continuous trajectory. For instance, the
notorious spurious stationary points \cite{chen2024spurious} observed in
Hessian barrier method and mirror descent scheme are interpreted as stable
equilibria of the dynamical system that do not correspond to real stationary
points of the original optimization problem. We provide two sufficient
condition such that these spurious stationary points can be avoided if the
strict complementarity conditions holds. In the absence of these regularity
condition, we propose a random perturbation strategy that ensures the
trajectory converges (subsequentially) to an approximate stationary point.
Building on these insights, we introduce two iterative Riemannian subgradient
methods, form of interior point methods, that generalizes the existing Hessian
barrier method and mirror descent scheme for solving nonsmooth nonconvex
optimization problems.

</details>


### [346] [Information Preserving Line Search via Bayesian Optimization](https://arxiv.org/abs/2507.15485)
*Robin Labryga,Tomislav Prusina,Sören Laue*

Main category: math.OC

TL;DR: 提出了一种基于贝叶斯优化的线搜索方法，利用传统方法中丢弃的信息，提升步长选择效果，并在CUTEst测试集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统线搜索方法在每次迭代中丢弃函数值和梯度信息，导致信息浪费。

Method: 通过贝叶斯优化保留并利用这些信息，改进步长选择。

Result: 在CUTEst测试集上表现优于现有方法，且具有收敛保证。

Conclusion: 基于贝叶斯优化的线搜索方法在性能和收敛性上优于传统方法。

Abstract: Line search is a fundamental part of iterative optimization methods for
unconstrained and bound-constrained optimization problems to determine suitable
step lengths that provide sufficient improvement in each iteration. Traditional
line search methods are based on iterative interval refinement, where valuable
information about function value and gradient is discarded in each iteration.
We propose a line search method via Bayesian optimization, preserving and
utilizing otherwise discarded information to improve step-length choices. Our
approach is guaranteed to converge and shows superior performance compared to
state-of-the-art methods based on empirical tests on the challenging
unconstrained and bound-constrained optimization problems from the CUTEst test
set.

</details>


### [347] [Multi-beam Beamforming in RIS-aided MIMO Subject to Reradiation Mask Constraints -- Optimization and Machine Learning Design](https://arxiv.org/abs/2507.15367)
*Shumin Wang,Hajar El Hassani,Marco Di Renzo,Marios Poulakis*

Main category: math.OC

TL;DR: 论文研究了多用户RIS辅助MIMO系统中发射预编码矩阵和RIS相位偏移向量的联合设计，提出了一种基于交替优化的方法，并结合神经网络和贪婪搜索算法提高效率。


<details>
  <summary>Details</summary>
Motivation: 通过RIS技术提高未来无线系统的频谱效率和降低功耗。

Method: 采用Arimoto-Blahut算法简化可达速率，交替优化分解为QPQC子问题，结合神经网络和贪婪搜索算法解决离散相位偏移问题。

Result: 仿真结果表明，所提方法能有效形成多波束辐射模式，满足再辐射掩模约束，神经网络设计减少了执行时间，离散相位偏移方案性能良好。

Conclusion: 提出的方法在满足约束的同时提高了系统性能，为RIS技术的实际应用提供了有效解决方案。

Abstract: Reconfigurable intelligent surfaces (RISs) are an emerging technology for
improving spectral efficiency and reducing power consumption in future wireless
systems. This paper investigates the joint design of the transmit precoding
matrices and the RIS phase shift vector in a multi-user RIS-aided
multiple-input multiple-output (MIMO) communication system. We formulate a
max-min optimization problem to maximize the minimum achievable rate while
considering transmit power and reradiation mask constraints. The achievable
rate is simplified using the Arimoto-Blahut algorithm, and the problem is
broken into quadratic programs with quadratic constraints (QPQC) sub-problems
using an alternating optimization approach. To improve efficiency, we develop a
model-based neural network optimization that utilizes the one-hot encoding for
the angles of incidence and reflection. We address practical RIS limitations by
using a greedy search algorithm to solve the optimization problem for discrete
phase shifts. Simulation results demonstrate that the proposed methods
effectively shape the multi-beam radiation pattern towards desired directions
while satisfying reradiation mask constraints. The neural network design
reduces the execution time, and the discrete phase shift scheme performs well
with a small reduction of the beamforming gain by using only four phase shift
levels.

</details>


<div id='cs.PL'></div>

# cs.PL [[Back]](#toc)

### [348] [Hear Your Code Fail, Voice-Assisted Debugging for Python](https://arxiv.org/abs/2507.15007)
*Sayed Mahbub Hasan Amiri,Md. Mainul Islam,Mohammad Shakhawat Hossen,Sayed Majhab Hasan Amiri,Mohammad Shawkat Ali Mamun,Sk. Humaun Kabir,Naznin Akter*

Main category: cs.PL

TL;DR: 该研究提出了一种创新的Python语音辅助调试插件，将静默运行时错误转化为可操作的听觉诊断，通过多模态反馈降低认知负荷并加速错误识别。


<details>
  <summary>Details</summary>
Motivation: 传统调试方法依赖视觉堆栈跟踪，对视觉障碍者不友好且效率低，需更人性化的错误诊断方式。

Method: 采用全局异常钩子架构，结合pyttsx3文本转语音和Tkinter GUI，提供并行听觉与视觉反馈。

Result: 实验显示认知负荷降低37%，错误识别速度提升78%，支持多平台且集成简单。

Conclusion: 该插件显著提升编程可访问性和效率，未来将扩展GPT修复建议和多语言翻译功能。

Abstract: This research introduces an innovative voice-assisted debugging plugin for
Python that transforms silent runtime errors into actionable audible
diagnostics. By implementing a global exception hook architecture with pyttsx3
text-to-speech conversion and Tkinter-based GUI visualization, the solution
delivers multimodal error feedback through parallel auditory and visual
channels. Empirical evaluation demonstrates 37% reduced cognitive load (p<0.01,
n=50) compared to traditional stack-trace debugging, while enabling 78% faster
error identification through vocalized exception classification and
contextualization. The system achieves sub-1.2 second voice latency with under
18% CPU overhead during exception handling, vocalizing error types and
consequences while displaying interactive tracebacks with documentation deep
links. Criteria validate compatibility across Python 3.7+ environments on
Windows, macOS, and Linux platforms. Needing only two lines of integration
code, the plugin significantly boosts availability for aesthetically impaired
designers and supports multitasking workflows through hands-free error medical
diagnosis. Educational applications show particular promise, with pilot studies
indicating 45% faster debugging skill acquisition among novice programmers.
Future development will incorporate GPT-based repair suggestions and real-time
multilingual translation to further advance auditory debugging paradigms. The
solution represents a fundamental shift toward human-centric error diagnostics,
bridging critical gaps in programming accessibility while establishing new
standards for cognitive efficiency in software development workflows.

</details>


<div id='q-fin.TR'></div>

# q-fin.TR [[Back]](#toc)

### [349] [A Comparative Analysis of Statistical and Machine Learning Models for Outlier Detection in Bitcoin Limit Order Books](https://arxiv.org/abs/2507.14960)
*Ivan Letteri*

Main category: q-fin.TR

TL;DR: 该研究比较了多种统计方法和机器学习技术在加密货币限价订单簿（LOB）中的异常检测效果，发现Empirical Covariance（EC）模型表现最佳，显著优于基准策略。


<details>
  <summary>Details</summary>
Motivation: 理解加密货币市场的动态，尤其是在高波动性和监管不成熟的环境下，异常检测对识别潜在操纵行为至关重要。

Method: 研究在AITA-OBS测试环境中评估了13种模型，通过回溯测试26,204条数据记录，比较其性能。

Result: 最佳模型EC实现了6.70%的收益提升，显著优于基准策略，证明了异常驱动策略的有效性。

Conclusion: 研究为加密货币市场微观结构提供了严格的异常检测模型基准，并展示了其在算法交易和风险管理中的潜力。

Abstract: The detection of outliers within cryptocurrency limit order books (LOBs) is
of paramount importance for comprehending market dynamics, particularly in
highly volatile and nascent regulatory environments. This study conducts a
comprehensive comparative analysis of robust statistical methods and advanced
machine learning techniques for real-time anomaly identification in
cryptocurrency LOBs. Within a unified testing environment, named AITA Order
Book Signal (AITA-OBS), we evaluate the efficacy of thirteen diverse models to
identify which approaches are most suitable for detecting potentially
manipulative trading behaviours. An empirical evaluation, conducted via
backtesting on a dataset of 26,204 records from a major exchange, demonstrates
that the top-performing model, Empirical Covariance (EC), achieves a 6.70%
gain, significantly outperforming a standard Buy-and-Hold benchmark. These
findings underscore the effectiveness of outlier-driven strategies and provide
insights into the trade-offs between model complexity, trade frequency, and
performance. This study contributes to the growing corpus of research on
cryptocurrency market microstructure by furnishing a rigorous benchmark of
anomaly detection models and highlighting their potential for augmenting
algorithmic trading and risk management.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [350] [Training oscillator Ising machines to assign the dynamic stability of their equilibrium points](https://arxiv.org/abs/2507.14386)
*Yi Cheng,Zongli Lin*

Main category: cs.NE

TL;DR: 提出一种神经网络模型，通过调整平衡点的稳定性实现类Hopfield的联想记忆，利用振荡Ising机（OIM）的结构稳定性，设计耦合权重以分配稳定性。


<details>
  <summary>Details</summary>
Motivation: 传统Hopfield模型需同时考虑平衡点的存在和动态稳定性，而OIM因其结构稳定性简化了设计，仅需关注动态稳定性。

Method: 提出Hamiltonian正则化特征值对比方法（HRECM），通过Hamiltonian能量与稳定性的关系训练耦合权重。

Result: 数值实验验证了HRECM方法的有效性。

Conclusion: OIM模型结合HRECM方法，简化了联想记忆的实现，并通过实验验证了其可行性。

Abstract: We propose a neural network model, which, with appropriate assignment of the
stability of its equilibrium points (EPs), achieves Hopfield-like associative
memory. The oscillator Ising machine (OIM) is an ideal candidates for such a
model, as all its $0/\pi$ binary EPs are structurally stable with their dynamic
stability tunable by the coupling weights. Traditional Hopfield-based models
store the desired patterns by designing the coupling weights between neurons.
The design of coupling weights should simultaneously take into account both the
existence and the dynamic stability of the EPs for the storage of the desired
patterns. For OIMs, since all $0/\pi$ binary EPs are structurally stable, the
design of the coupling weights needs only to focus on assigning appropriate
stability for the $0/\pi$ binary EPs according to the desired patterns. In this
paper, we establish a connection between the stability and the Hamiltonian
energy of EPs for OIMs, and, based on this connection, provide a
Hamiltonian-Regularized Eigenvalue Contrastive Method (HRECM) to train the
coupling weights of OIMs for assigning appropriate stability to their EPs.
Finally, numerical experiments are performed to validate the effectiveness of
the proposed method.

</details>


### [351] [APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation](https://arxiv.org/abs/2507.14270)
*Ravin Kumar*

Main category: cs.NE

TL;DR: APTx Neuron是一种新型神经计算单元，将非线性和线性变换统一为一个可训练表达式，提高了计算效率和架构简洁性。


<details>
  <summary>Details</summary>
Motivation: 传统神经元需要分开处理激活和线性变换，APTx Neuron旨在通过统一设计简化架构并提升效率。

Method: APTx Neuron基于APTx激活函数，形式为$y = \sum_{i=1}^{n} ((\alpha_i + \tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$，所有参数可训练。

Result: 在MNIST数据集上，仅用20个epoch和约332K参数，测试准确率达到96.69%。

Conclusion: APTx Neuron在表达能力和计算效率上优于传统神经元，为统一神经元设计提供了新范式。

Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that
integrates non-linear activation and linear transformation into a single
trainable expression. The APTx Neuron is derived from the APTx activation
function, thereby eliminating the need for separate activation layers and
making the architecture both computationally efficient and elegant. The
proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i +
\tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters
$\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our
APTx Neuron-based architecture on the MNIST dataset, achieving up to 96.69\%
test accuracy in just 20 epochs using approximately 332K trainable parameters.
The results highlight the superior expressiveness and computational efficiency
of the APTx Neuron compared to traditional neurons, pointing toward a new
paradigm in unified neuron design and the architectures built upon it.

</details>


### [352] [Analyzing Internal Activity and Robustness of SNNs Across Neuron Parameter Space](https://arxiv.org/abs/2507.14757)
*Szymon Mazurek,Jakub Caputa,Maciej Wielgosz*

Main category: cs.NE

TL;DR: 本文研究了脉冲神经网络（SNNs）中神经元模型参数（如膜时间常数和电压阈值）的调优，发现了一个操作空间，在此区域内网络表现最佳。超出此区域会导致性能下降或能量浪费。


<details>
  <summary>Details</summary>
Motivation: 探索SNNs中神经元参数调优的重要性，以实现高性能和能源效率的平衡。

Method: 通过系统实验和数据集分析，识别并量化了神经元参数的操作空间，并评估了对抗噪声的鲁棒性。

Result: 在操作空间内，SNNs表现最佳；超出此区域会导致能量浪费或网络沉默。对抗噪声下，SNNs在非最优区域表现出更高的脉冲相关性和内部同步性。

Conclusion: 研究强调了参数调优的重要性，为部署高效、鲁棒的SNNs提供了实用指南，尤其适用于神经形态计算场景。

Abstract: Spiking Neural Networks (SNNs) offer energy-efficient and biologically
plausible alternatives to traditional artificial neural networks, but their
performance depends critically on the tuning of neuron model parameters. In
this work, we identify and characterize an operational space - a constrained
region in the neuron hyperparameter domain (specifically membrane time constant
tau and voltage threshold vth) - within which the network exhibits meaningful
activity and functional behavior. Operating inside this manifold yields optimal
trade-offs between classification accuracy and spiking activity, while stepping
outside leads to degeneration: either excessive energy use or complete network
silence.
  Through systematic exploration across datasets and architectures, we
visualize and quantify this manifold and identify efficient operating points.
We further assess robustness to adversarial noise, showing that SNNs exhibit
increased spike correlation and internal synchrony when operating outside their
optimal region. These findings highlight the importance of principled
hyperparameter tuning to ensure both task performance and energy efficiency.
Our results offer practical guidelines for deploying robust and efficient SNNs,
particularly in neuromorphic computing scenarios.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [353] [DiffuMeta: Algebraic Language Models for Inverse Design of Metamaterials via Diffusion Transformers](https://arxiv.org/abs/2507.15753)
*Li Zheng,Siddhant Kumar,Dennis M. Kochmann*

Main category: cs.CE

TL;DR: DiffuMeta是一个生成框架，结合扩散变换器和代数语言表示，用于三维超材料的逆向设计，能够生成具有目标应力应变响应的新颖结构。


<details>
  <summary>Details</summary>
Motivation: 现有生成模型在三维超材料逆向设计中受限于计算复杂性和设计空间不足，缺乏表达性表示。

Method: DiffuMeta结合扩散变换器和新型代数语言表示，将3D几何编码为数学句子，生成多样化的结构设计。

Result: DiffuMeta能够生成具有精确目标应力应变响应的结构，并支持多机械目标的同步控制。实验验证了其有效性。

Conclusion: DiffuMeta为超材料的快速设计和定制性能提供了高效框架。

Abstract: Generative machine learning models have revolutionized material discovery by
capturing complex structure-property relationships, yet extending these
approaches to the inverse design of three-dimensional metamaterials remains
limited by computational complexity and underexplored design spaces due to the
lack of expressive representations. Here, we present DiffuMeta, a generative
framework integrating diffusion transformers with a novel algebraic language
representation, encoding 3D geometries as mathematical sentences. This compact,
unified parameterization spans diverse topologies while enabling direct
application of transformers to structural design. DiffuMeta leverages diffusion
models to generate novel shell structures with precisely targeted stress-strain
responses under large deformations, accounting for buckling and contact while
addressing the inherent one-to-many mapping by producing diverse solutions.
Uniquely, our approach enables simultaneous control over multiple mechanical
objectives, including linear and nonlinear responses beyond training domains.
Experimental validation of fabricated structures further confirms the efficacy
of our approach for accelerated design of metamaterials and structures with
tailored properties.

</details>


<div id='econ.TH'></div>

# econ.TH [[Back]](#toc)

### [354] [Approximate Revenue Maximization for Diffusion Auctions](https://arxiv.org/abs/2507.14470)
*Yifan Huang,Dong Hao,Zhiyi Fan,Yuhang Guo,Bin Li*

Main category: econ.TH

TL;DR: 论文研究了基于保留价的网络拍卖设计，提出了一个简单且接近最优的保留价函数，适用于经济网络中的拍卖。


<details>
  <summary>Details</summary>
Motivation: 现有拍卖设计假设拍卖者可直接接触目标受众，忽略了经济网络中大量潜在竞标者。本文旨在扩展最优拍卖理论至整个经济网络。

Method: 采用贝叶斯近似分析，设计了一个简单且明确的保留价函数，适用于代表性网络拍卖。

Result: 提出的保留价函数在保持激励兼容的同时，使卖家收入接近理论上限，具体表现为1-1/ρ的近似比。

Conclusion: 该保留价函数适用于任何规模和结构的网络市场，显著提升了拍卖收入。

Abstract: Reserve prices are widely used in practice. The problem of designing
revenue-optimal auctions based on reserve price has drawn much attention in the
auction design community. Although they have been extensively studied, most
developments rely on the significant assumption that the target audience of the
sale is directly reachable by the auctioneer, while a large portion of bidders
in the economic network unaware of the sale are omitted. This work follows the
diffusion auction design, which aims to extend the target audience of optimal
auction theory to all entities in economic networks. We investigate the design
of simple and provably near-optimal network auctions via reserve price. Using
Bayesian approximation analysis, we provide a simple and explicit form of the
reserve price function tailored to the most representative network auction. We
aim to balance setting a sufficiently high reserve price to induce high revenue
in a successful sale, and attracting more buyers from the network to increase
the probability of a successful sale. This reserve price function preserves
incentive compatibility for network auctions, allowing the seller to extract
additional revenue beyond that achieved by the Myerson optimal auction.
Specifically, if the seller has $\rho$ direct neighbours in a network of size
$n$, this reserve price guarantees a $1-{1 \over \rho}$ approximation to the
theoretical upper bound, i.e., the maximum possible revenue from any network of
size $n$. This result holds for any size and any structure of the networked
market.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [355] [ExCyTIn-Bench: Evaluating LLM agents on Cyber Threat Investigation](https://arxiv.org/abs/2507.14201)
*Yiran Wu,Mauricio Velazco,Andrew Zhao,Manuel Raúl Meléndez Luján,Srisuma Movva,Yogesh K Roy,Quang Nguyen,Roberto Rodriguez,Qingyun Wu,Michael Albada,Julia Kiseleva,Anand Mudgerikar*

Main category: cs.CR

TL;DR: ExCyTIn-Bench是首个评估LLM代理在网络安全威胁调查任务中的基准，基于调查图生成的安全问题。


<details>
  <summary>Details</summary>
Motivation: 现实中的安全分析师需要处理大量异构警报和日志，构建自动化的LLM代理是一个有前景的方向。

Method: 使用Azure租户构建数据集，包括模拟攻击、日志表和自动生成的问题，通过调查图和LLM生成问题。

Result: 实验显示任务难度高，平均奖励为0.249，最佳为0.368，显示未来研究空间大。

Conclusion: ExCyTIn-Bench为LLM代理的开发和评估提供了可扩展的基准，代码和数据即将发布。

Abstract: We present ExCyTIn-Bench, the first benchmark to Evaluate an LLM agent x on
the task of Cyber Threat Investigation through security questions derived from
investigation graphs. Real-world security analysts must sift through a large
number of heterogeneous alert signals and security logs, follow multi-hop
chains of evidence, and compile an incident report. With the developments of
LLMs, building LLM-based agents for automatic thread investigation is a
promising direction. To assist the development and evaluation of LLM agents, we
construct a dataset from a controlled Azure tenant that covers 8 simulated
real-world multi-step attacks, 57 log tables from Microsoft Sentinel and
related services, and 589 automatically generated questions. We leverage
security logs extracted with expert-crafted detection logic to build threat
investigation graphs, and then generate questions with LLMs using paired nodes
on the graph, taking the start node as background context and the end node as
answer. Anchoring each question to these explicit nodes and edges not only
provides automatic, explainable ground truth answers but also makes the
pipeline reusable and readily extensible to new logs. This also enables the
automatic generation of procedural tasks with verifiable rewards, which can be
naturally extended to training agents via reinforcement learning. Our
comprehensive experiments with different models confirm the difficulty of the
task: with the base setting, the average reward across all evaluated models is
0.249, and the best achieved is 0.368, leaving substantial headroom for future
research. Code and data are coming soon!

</details>


### [356] [PRM-Free Security Alignment of Large Models via Red Teaming and Adversarial Training](https://arxiv.org/abs/2507.14202)
*Pengfei Du*

Main category: cs.CR

TL;DR: 提出了一种无需PRM的安全对齐框架，通过自动化红队和对抗训练实现高效安全保证，计算成本降低61%。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在关键领域部署时的安全风险，克服PRM方法的高计算开销和可扩展性限制。

Method: 采用自动化红队、对抗训练（包括遗传算法优化、多智能体模拟和高级提示变异技术），并结合课程学习和自适应正则化。

Result: 在五种先进LLMs上验证，性能优于PRM方法，计算成本降低61%。

Conclusion: 为资源受限组织提供高效安全对齐方案，支持透明报告和持续审计，推动LLM安全领域发展。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse applications, yet they pose significant security risks that threaten
their safe deployment in critical domains. Current security alignment
methodologies predominantly rely on Process Reward Models (PRMs) to evaluate
intermediate reasoning steps, introducing substantial computational overhead
and scalability constraints. This paper presents a novel PRM-free security
alignment framework that leverages automated red teaming and adversarial
training to achieve robust security guarantees while maintaining computational
efficiency. Our approach systematically identifies vulnerabilities through
sophisticated attack strategies including genetic algorithm optimization,
multi-agent simulation, and advanced prompt mutation techniques. The framework
enhances model robustness via targeted adversarial training with curriculum
learning and adaptive regularization mechanisms. Comprehensive experimental
evaluation across five state-of-the-art LLMs demonstrates that our method
achieves superior security alignment performance compared to PRM-based
approaches while reducing computational costs by 61\%. The framework
incorporates transparent reporting and continuous audit mechanisms that enable
iterative security improvement and regulatory compliance. Our contributions
advance the field of efficient LLM security alignment by democratizing access
to robust security measures for resource-constrained organizations and
providing a scalable foundation for addressing evolving adversarial threats.

</details>


### [357] [Mitigating Trojanized Prompt Chains in Educational LLM Use Cases: Experimental Findings and Detection Tool Design](https://arxiv.org/abs/2507.14207)
*Richard M. Charles,James H. Curry,Richard B. Charles*

Main category: cs.CR

TL;DR: 研究探讨了K-12教育中大型语言模型（LLMs）的安全漏洞，学生可能通过特洛伊化提示绕过内容审核系统，并提出了检测工具TrojanPromptGuard（TPG）。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于揭示LLMs在教育应用中的潜在风险，特别是学生如何利用漏洞绕过安全机制。

Method: 通过模拟K-12查询和多轮对话实验，分析GPT-3.5和GPT-4的漏洞，并开发了TPG工具。

Result: 实验揭示了LLMs的关键漏洞，并验证了TPG工具的有效性。

Conclusion: 研究为AI安全研究者和教育技术专家提供了关于LLMs安全部署的见解。

Abstract: The integration of Large Language Models (LLMs) in K--12 education offers
both transformative opportunities and emerging risks. This study explores how
students may Trojanize prompts to elicit unsafe or unintended outputs from
LLMs, bypassing established content moderation systems with safety guardrils.
Through a systematic experiment involving simulated K--12 queries and
multi-turn dialogues, we expose key vulnerabilities in GPT-3.5 and GPT-4. This
paper presents our experimental design, detailed findings, and a prototype
tool, TrojanPromptGuard (TPG), to automatically detect and mitigate Trojanized
educational prompts. These insights aim to inform both AI safety researchers
and educational technologists on the safe deployment of LLMs for educators.

</details>


### [358] [Multi-Granular Discretization for Interpretable Generalization in Precise Cyberattack Identification](https://arxiv.org/abs/2507.14223)
*Wen-Cheng Chung,Shu-Ting Huang,Hao-Ting Pai*

Main category: cs.CR

TL;DR: 论文提出了一种可解释的入侵检测系统（IG机制），通过多粒度离散化（IG-MD）进一步提升精度，同时保持透明性和高召回率。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法在解释入侵检测系统时存在局限性，导致部分或误导性见解。

Method: 采用Interpretable Generalization（IG）机制学习独特特征组合，并转化为可审计规则；引入Multi-Granular Discretization（IG-MD）处理连续特征。

Result: 在多个数据集（NSL-KDD、UNSW-NB15、UKM-IDS20）上表现优异，IG-MD在UKM-IDS20上提升精度≥4%，召回率≈1.0。

Conclusion: IG-MD证明单一可解释模型可跨领域扩展，无需定制调整。

Abstract: Explainable intrusion detection systems (IDS) are now recognized as essential
for mission-critical networks, yet most "XAI" pipelines still bolt an
approximate explainer onto an opaque classifier, leaving analysts with partial
and sometimes misleading insights. The Interpretable Generalization (IG)
mechanism, published in IEEE Transactions on Information Forensics and
Security, eliminates that bottleneck by learning coherent patterns - feature
combinations unique to benign or malicious traffic - and turning them into
fully auditable rules. IG already delivers outstanding precision, recall, and
AUC on NSL-KDD, UNSW-NB15, and UKM-IDS20, even when trained on only 10% of the
data. To raise precision further without sacrificing transparency, we introduce
Multi-Granular Discretization (IG-MD), which represents every continuous
feature at several Gaussian-based resolutions. On UKM-IDS20, IG-MD lifts
precision by greater than or equal to 4 percentage points across all nine
train-test splits while preserving recall approximately equal to 1.0,
demonstrating that a single interpretation-ready model can scale across domains
without bespoke tuning.

</details>


### [359] [Breaking the Illusion of Security via Interpretation: Interpretable Vision Transformer Systems under Attack](https://arxiv.org/abs/2507.14248)
*Eldor Abdukhamidov,Mohammed Abuhamad,Simon S. Woo,Hyoungshick Kim,Tamer Abuhmed*

Main category: cs.CR

TL;DR: 论文研究了Vision Transformer（ViT）模型及其解释模型对对抗攻击的脆弱性，提出了一种名为AdViT的攻击方法，能在白盒和黑盒场景下高效欺骗模型和解释模型。


<details>
  <summary>Details</summary>
Motivation: 尽管ViT模型及其解释模型被认为安全且难以欺骗，但成功攻击可能导致严重后果。现有研究未充分考虑对抗样本对解释模型的影响。

Method: 提出AdViT攻击方法，生成能同时欺骗ViT模型及其解释模型的对抗样本。

Result: 实验表明，AdViT在白盒和黑盒场景下攻击成功率达100%，且生成的对抗样本难以被检测。

Conclusion: AdViT揭示了ViT模型及其解释模型在对抗攻击下的脆弱性，为安全应用提供了警示。

Abstract: Vision transformer (ViT) models, when coupled with interpretation models, are
regarded as secure and challenging to deceive, making them well-suited for
security-critical domains such as medical applications, autonomous vehicles,
drones, and robotics. However, successful attacks on these systems can lead to
severe consequences. Recent research on threats targeting ViT models primarily
focuses on generating the smallest adversarial perturbations that can deceive
the models with high confidence, without considering their impact on model
interpretations. Nevertheless, the use of interpretation models can effectively
assist in detecting adversarial examples. This study investigates the
vulnerability of transformer models to adversarial attacks, even when combined
with interpretation models. We propose an attack called "AdViT" that generates
adversarial examples capable of misleading both a given transformer model and
its coupled interpretation model. Through extensive experiments on various
transformer models and two transformer-based interpreters, we demonstrate that
AdViT achieves a 100% attack success rate in both white-box and black-box
scenarios. In white-box scenarios, it reaches up to 98% misclassification
confidence, while in black-box scenarios, it reaches up to 76%
misclassification confidence. Remarkably, AdViT consistently generates accurate
interpretations in both scenarios, making the adversarial examples more
difficult to detect.

</details>


### [360] [Towards Efficient Privacy-Preserving Machine Learning: A Systematic Review from Protocol, Model, and System Perspectives](https://arxiv.org/abs/2507.14519)
*Wenxuan Zeng,Tianshi Xu,Yi Chen,Yifan Zhou,Mingzhe Zhang,Jin Tan,Cheng Hong,Meng Li*

Main category: cs.CR

TL;DR: 本文综述了隐私保护机器学习（PPML）的研究进展，重点介绍了跨层级优化方法，并讨论了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: PPML虽然能保护用户数据隐私，但效率低、扩展性差，因此需要优化以缩小与明文机器学习的差距。

Method: 将现有研究分为协议层、模型层和系统层，进行定性和定量比较，并提供技术见解。

Result: 总结了PPML在各层级的优化进展，并强调了跨层级整合优化的必要性。

Conclusion: 本文为PPML领域提供了全面的理解，并指出了未来研究方向，同时通过GitHub仓库持续跟踪最新进展。

Abstract: Privacy-preserving machine learning (PPML) based on cryptographic protocols
has emerged as a promising paradigm to protect user data privacy in cloud-based
machine learning services. While it achieves formal privacy protection, PPML
often incurs significant efficiency and scalability costs due to orders of
magnitude overhead compared to the plaintext counterpart. Therefore, there has
been a considerable focus on mitigating the efficiency gap for PPML. In this
survey, we provide a comprehensive and systematic review of recent PPML studies
with a focus on cross-level optimizations. Specifically, we categorize existing
papers into protocol level, model level, and system level, and review progress
at each level. We also provide qualitative and quantitative comparisons of
existing works with technical insights, based on which we discuss future
research directions and highlight the necessity of integrating optimizations
across protocol, model, and system levels. We hope this survey can provide an
overarching understanding of existing approaches and potentially inspire future
breakthroughs in the PPML field. As the field is evolving fast, we also provide
a public GitHub repository to continuously track the developments, which is
available at https://github.com/PKU-SEC-Lab/Awesome-PPML-Papers.

</details>


### [361] [VTarbel: Targeted Label Attack with Minimal Knowledge on Detector-enhanced Vertical Federated Learning](https://arxiv.org/abs/2507.14625)
*Juntao Tan,Anran Li,Quanchao Liu,Peng Ran,Lan Zhang*

Main category: cs.CR

TL;DR: VTarbel是一种针对垂直联邦学习（VFL）的两阶段攻击框架，能够绕过检测器并实现目标标签攻击。


<details>
  <summary>Details</summary>
Motivation: 现有VFL攻击方法依赖不现实的假设且忽略实际系统中的异常检测器，VTarbel旨在填补这一空白。

Method: VTarbel分为准备阶段和攻击阶段，利用高表达性样本训练本地模型，并通过梯度扰动生成对抗样本。

Result: VTarbel在多种模型、数据集和检测器下均优于现有方法，且能绕过隐私保护防御。

Conclusion: 研究揭示了VFL部署中的安全盲点，强调了开发攻击感知防御的紧迫性。

Abstract: Vertical federated learning (VFL) enables multiple parties with disjoint
features to collaboratively train models without sharing raw data. While
privacy vulnerabilities of VFL are extensively-studied, its security
threats-particularly targeted label attacks-remain underexplored. In such
attacks, a passive party perturbs inputs at inference to force
misclassification into adversary-chosen labels. Existing methods rely on
unrealistic assumptions (e.g., accessing VFL-model's outputs) and ignore
anomaly detectors deployed in real-world systems. To bridge this gap, we
introduce VTarbel, a two-stage, minimal-knowledge attack framework explicitly
designed to evade detector-enhanced VFL inference. During the preparation
stage, the attacker selects a minimal set of high-expressiveness samples (via
maximum mean discrepancy), submits them through VFL protocol to collect
predicted labels, and uses these pseudo-labels to train estimated detector and
surrogate model on local features. In attack stage, these models guide
gradient-based perturbations of remaining samples, crafting adversarial
instances that induce targeted misclassifications and evade detection. We
implement VTarbel and evaluate it against four model architectures, seven
multimodal datasets, and two anomaly detectors. Across all settings, VTarbel
outperforms four state-of-the-art baselines, evades detection, and retains
effective against three representative privacy-preserving defenses. These
results reveal critical security blind spots in current VFL deployments and
underscore urgent need for robust, attack-aware defenses.

</details>


### [362] [VMask: Tunable Label Privacy Protection for Vertical Federated Learning via Layer Masking](https://arxiv.org/abs/2507.14629)
*Juntao Tan,Lan Zhang,Zhonghao Hu,Kai Yang,Peng Ran,Bo Li*

Main category: cs.CR

TL;DR: VMask是一种新型的标签隐私保护框架，通过层掩码技术防御模型完成攻击，实现了隐私与效用的最佳平衡。


<details>
  <summary>Details</summary>
Motivation: 尽管垂直联邦学习（VFL）被认为是隐私保护的，但研究表明其易受标签推断攻击，尤其是模型完成攻击。现有防御方法要么牺牲模型准确性，要么计算开销过大。

Method: VMask利用秘密共享技术掩码攻击者模型中的关键层参数，破坏输入数据与中间输出的强相关性，并提供可调的隐私预算。

Result: VMask成功将标签推断准确率降至随机猜测水平，同时模型性能几乎不受影响（如Transformer模型准确率仅下降0.09%）。其运行速度比基于密码学的方法快60,846倍。

Conclusion: VMask是首个提供可调隐私预算的框架，在防御模型完成攻击的同时，实现了高效且实用的隐私保护。

Abstract: Though vertical federated learning (VFL) is generally considered to be
privacy-preserving, recent studies have shown that VFL system is vulnerable to
label inference attacks originating from various attack surfaces. Among these
attacks, the model completion (MC) attack is currently the most powerful one.
Existing defense methods against it either sacrifice model accuracy or incur
impractical computational overhead. In this paper, we propose VMask, a novel
label privacy protection framework designed to defend against MC attack from
the perspective of layer masking. Our key insight is to disrupt the strong
correlation between input data and intermediate outputs by applying the secret
sharing (SS) technique to mask layer parameters in the attacker's model. We
devise a strategy for selecting critical layers to mask, reducing the overhead
that would arise from naively applying SS to the entire model. Moreover, VMask
is the first framework to offer a tunable privacy budget to defenders, allowing
for flexible control over the levels of label privacy according to actual
requirements. We built a VFL system, implemented VMask on it, and extensively
evaluated it using five model architectures and 13 datasets with different
modalities, comparing it to 12 other defense methods. The results demonstrate
that VMask achieves the best privacy-utility trade-off, successfully thwarting
the MC attack (reducing the label inference accuracy to a random guessing
level) while preserving model performance (e.g., in Transformer-based model,
the averaged drop of VFL model accuracy is only 0.09%). VMask's runtime is up
to 60,846 times faster than cryptography-based methods, and it only marginally
exceeds that of standard VFL by 1.8 times in a large Transformer-based model,
which is generally acceptable.

</details>


### [363] [Manipulating LLM Web Agents with Indirect Prompt Injection Attack via HTML Accessibility Tree](https://arxiv.org/abs/2507.14799)
*Sam Johnson,Viet Pham,Thai Le*

Main category: cs.CR

TL;DR: LLM-based web navigation agents are powerful but vulnerable to Indirect Prompt Injection (IPI) attacks, where adversaries can hijack agent behavior via HTML triggers, leading to security risks like credential theft.


<details>
  <summary>Details</summary>
Motivation: To expose the vulnerabilities of LLM-based web navigation agents to IPI attacks, highlighting the need for stronger defenses as these agents become more widely used.

Method: Utilized the Greedy Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by Llama-3.1 to test IPI attacks on real websites.

Result: Demonstrated high success rates in targeted and general attacks, including credential exfiltration and forced ad clicks.

Conclusion: The study underscores significant security risks in LLM-driven web agents and calls for improved defensive measures.

Abstract: This work demonstrates that LLM-based web navigation agents offer powerful
automation capabilities but are vulnerable to Indirect Prompt Injection (IPI)
attacks. We show that adversaries can embed universal adversarial triggers in
webpage HTML to hijack agent behavior that utilizes the accessibility tree to
parse HTML, causing unintended or malicious actions. Using the Greedy
Coordinate Gradient (GCG) algorithm and a Browser Gym agent powered by
Llama-3.1, our system demonstrates high success rates across real websites in
both targeted and general attacks, including login credential exfiltration and
forced ad clicks. Our empirical results highlight critical security risks and
the need for stronger defenses as LLM-driven autonomous web agents become more
widely adopted. The system software
(https://github.com/sej2020/manipulating-web-agents) is released under the MIT
License, with an accompanying publicly available demo website
(http://lethaiq.github.io/attack-web-llm-agent).

</details>


### [364] [A Privacy-Centric Approach: Scalable and Secure Federated Learning Enabled by Hybrid Homomorphic Encryption](https://arxiv.org/abs/2507.14853)
*Khoa Nguyen,Tanveer Khan,Antonis Michalas*

Main category: cs.CR

TL;DR: 本文探讨了如何将混合同态加密（HHE）与联邦学习（FL）结合，以解决通信和隐私问题，实现可扩展且安全的去中心化学习系统。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在隐私敏感领域具有潜力，但面临通信开销和数据隐私的挑战。现有的隐私保护技术（如纯同态加密）虽能缓解问题，但带来了高昂的计算和通信成本。

Method: 提出将混合同态加密（HHE）与联邦学习结合，HHE结合了对称加密和同态加密的优势。

Result: 通过HHE与FL的结合，能够有效降低通信和计算成本，同时保障数据隐私。

Conclusion: HHE与FL的结合为可扩展且安全的去中心化学习系统提供了可行的解决方案。

Abstract: Federated Learning (FL) enables collaborative model training without sharing
raw data, making it a promising approach for privacy-sensitive domains. Despite
its potential, FL faces significant challenges, particularly in terms of
communication overhead and data privacy. Privacy-preserving Techniques (PPTs)
such as Homomorphic Encryption (HE) have been used to mitigate these concerns.
However, these techniques introduce substantial computational and communication
costs, limiting their practical deployment. In this work, we explore how Hybrid
Homomorphic Encryption (HHE), a cryptographic protocol that combines symmetric
encryption with HE, can be effectively integrated with FL to address both
communication and privacy challenges, paving the way for scalable and secure
decentralized learning system.

</details>


### [365] [LibLMFuzz: LLM-Augmented Fuzz Target Generation for Black-box Libraries](https://arxiv.org/abs/2507.15058)
*Ian Hardgrove,John D. Hastings*

Main category: cs.CR

TL;DR: LibLMFuzz是一个框架，利用大型语言模型（LLM）和轻量级工具链自动分析闭源库，减少模糊测试的成本。


<details>
  <summary>Details</summary>
Motivation: 解决闭源和专有软件中二进制库模糊测试的高成本和复杂性。

Method: 结合LLM和工具链（反汇编器/编译器/模糊器）自动分析二进制文件，生成驱动程序并自我修复错误。

Result: 在四个Linux库上测试，覆盖100% API，75.52%的驱动程序首次执行正确。

Conclusion: LLM增强的中间件有望降低黑盒组件的模糊测试成本，为未来研究提供基础。

Abstract: A fundamental problem in cybersecurity and computer science is determining
whether a program is free of bugs and vulnerabilities. Fuzzing, a popular
approach to discovering vulnerabilities in programs, has several advantages
over alternative strategies, although it has investment costs in the form of
initial setup and continuous maintenance. The choice of fuzzing is further
complicated when only a binary library is available, such as the case of
closed-source and proprietary software. In response, we introduce LibLMFuzz, a
framework that reduces costs associated with fuzzing closed-source libraries by
pairing an agentic Large Language Model (LLM) with a lightweight tool-chain
(disassembler/compiler/fuzzer) to autonomously analyze stripped binaries, plan
fuzz strategies, generate drivers, and iteratively self-repair build or runtime
errors. Tested on four widely-used Linux libraries, LibLMFuzz produced
syntactically correct drivers for all 558 fuzz-able API functions, achieving
100% API coverage with no human intervention. Across the 1601 synthesized
drivers, 75.52% were nominally correct on first execution. The results show
that LLM-augmented middleware holds promise in reducing the costs of fuzzing
black box components and provides a foundation for future research efforts.
Future opportunities exist for research in branch coverage.

</details>


### [366] [PromptArmor: Simple yet Effective Prompt Injection Defenses](https://arxiv.org/abs/2507.15219)
*Tianneng Shi,Kaijie Zhu,Zhun Wang,Yuqi Jia,Will Cai,Weida Liang,Haonan Wang,Hend Alzahrani,Joshua Lu,Kenji Kawaguchi,Basel Alomair,Xuandong Zhao,William Yang Wang,Neil Gong,Wenbo Guo,Dawn Song*

Main category: cs.CR

TL;DR: PromptArmor是一种简单有效的防御方法，通过检测和移除输入中的恶意提示来保护LLM代理免受提示注入攻击。


<details>
  <summary>Details</summary>
Motivation: LLM代理易受提示注入攻击，导致其执行攻击者指定的任务而非用户意图。

Method: PromptArmor利用现成的LLM检测并移除输入中的潜在注入提示。

Result: 在AgentDojo基准测试中，PromptArmor的误报率和漏报率均低于1%，攻击成功率降至1%以下。

Conclusion: PromptArmor应作为评估新防御方法的标准基线。

Abstract: Despite their potential, recent research has demonstrated that LLM agents are
vulnerable to prompt injection attacks, where malicious prompts are injected
into the agent's input, causing it to perform an attacker-specified task rather
than the intended task provided by the user. In this paper, we present
PromptArmor, a simple yet effective defense against prompt injection attacks.
Specifically, PromptArmor prompts an off-the-shelf LLM to detect and remove
potential injected prompts from the input before the agent processes it. Our
results show that PromptArmor can accurately identify and remove injected
prompts. For example, using GPT-4o, GPT-4.1, or o4-mini, PromptArmor achieves
both a false positive rate and a false negative rate below 1% on the AgentDojo
benchmark. Moreover, after removing injected prompts with PromptArmor, the
attack success rate drops to below 1%. We also demonstrate PromptArmor's
effectiveness against adaptive attacks and explore different strategies for
prompting an LLM. We recommend that PromptArmor be adopted as a standard
baseline for evaluating new defenses against prompt injection attacks.

</details>


### [367] [PiMRef: Detecting and Explaining Ever-evolving Spear Phishing Emails with Knowledge Base Invariants](https://arxiv.org/abs/2507.15393)
*Ruofan Liu,Yun Lin,Silas Yeo Shuen Yu,Xiwen Teoh,Zhenkai Liang,Jin Song Dong*

Main category: cs.CR

TL;DR: 论文提出PiMRef，一种基于知识库的钓鱼邮件检测方法，通过验证发件人身份的真实性来检测钓鱼邮件，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 钓鱼邮件因其低成本和高传播性成为网络犯罪的重要手段，传统检测方法难以应对其不断演变的特性，尤其是大型语言模型（LLM）的出现进一步加剧了这一威胁。

Method: PiMRef将钓鱼邮件检测重构为身份事实核查任务，通过提取发件人身份、验证域名合法性及检测诱导性提示来识别钓鱼邮件。

Result: PiMRef在标准测试中提升精度8.8%，在真实世界测试中达到92.1%的精度和87.9%的召回率，运行效率高。

Conclusion: PiMRef通过知识库验证身份的方法，有效应对LLM生成的钓鱼邮件，为防御提供了新思路。

Abstract: Phishing emails are a critical component of the cybercrime kill chain due to
their wide reach and low cost. Their ever-evolving nature renders traditional
rule-based and feature-engineered detectors ineffective in the ongoing arms
race between attackers and defenders. The rise of large language models (LLMs)
further exacerbates the threat, enabling attackers to craft highly convincing
phishing emails at minimal cost.
  This work demonstrates that LLMs can generate psychologically persuasive
phishing emails tailored to victim profiles, successfully bypassing nearly all
commercial and academic detectors. To defend against such threats, we propose
PiMRef, the first reference-based phishing email detector that leverages
knowledge-based invariants. Our core insight is that persuasive phishing emails
often contain disprovable identity claims, which contradict real-world facts.
PiMRef reframes phishing detection as an identity fact-checking task. Given an
email, PiMRef (i) extracts the sender's claimed identity, (ii) verifies the
legitimacy of the sender's domain against a predefined knowledge base, and
(iii) detects call-to-action prompts that push user engagement. Contradictory
claims are flagged as phishing indicators and serve as human-understandable
explanations.
  Compared to existing methods such as D-Fence, HelpHed, and ChatSpamDetector,
PiMRef boosts precision by 8.8% with no loss in recall on standard benchmarks
like Nazario and PhishPot. In a real-world evaluation of 10,183 emails across
five university accounts over three years, PiMRef achieved 92.1% precision,
87.9% recall, and a median runtime of 0.05s, outperforming the state-of-the-art
in both effectiveness and efficiency.

</details>


### [368] [Multi-Stage Prompt Inference Attacks on Enterprise LLM Systems](https://arxiv.org/abs/2507.15613)
*Andrii Balashov,Olena Ponomarova,Xiaohua Zhai*

Main category: cs.CR

TL;DR: 论文研究了企业环境中大型语言模型（LLM）面临的多阶段提示推理攻击，提出了威胁模型和防御措施。


<details>
  <summary>Details</summary>
Motivation: 企业部署的LLM面临新型安全威胁，如通过看似无害的提示逐步提取机密数据的攻击。

Method: 通过模拟攻击场景，结合概率论、优化框架和信息论泄漏边界分析多轮推理攻击，并提出防御措施。

Result: 攻击能有效绕过标准安全措施提取敏感信息；提出的防御方法（如异常检测、访问控制等）显著降低攻击成功率。

Conclusion: 企业LLM安全需从单轮提示过滤转向多阶段攻防视角，采用深度防御策略。

Abstract: Large Language Models (LLMs) deployed in enterprise settings (e.g., as
Microsoft 365 Copilot) face novel security challenges. One critical threat is
prompt inference attacks: adversaries chain together seemingly benign prompts
to gradually extract confidential data. In this paper, we present a
comprehensive study of multi-stage prompt inference attacks in an enterprise
LLM context. We simulate realistic attack scenarios where an attacker uses
mild-mannered queries and indirect prompt injections to exploit an LLM
integrated with private corporate data. We develop a formal threat model for
these multi-turn inference attacks and analyze them using probability theory,
optimization frameworks, and information-theoretic leakage bounds. The attacks
are shown to reliably exfiltrate sensitive information from the LLM's context
(e.g., internal SharePoint documents or emails), even when standard safety
measures are in place.
  We propose and evaluate defenses to counter such attacks, including
statistical anomaly detection, fine-grained access control, prompt sanitization
techniques, and architectural modifications to LLM deployment. Each defense is
supported by mathematical analysis or experimental simulation. For example, we
derive bounds on information leakage under differential privacy-based training
and demonstrate an anomaly detection method that flags multi-turn attacks with
high AUC. We also introduce an approach called "spotlighting" that uses input
transformations to isolate untrusted prompt content, reducing attack success by
an order of magnitude. Finally, we provide a formal proof of concept and
empirical validation for a combined defense-in-depth strategy. Our work
highlights that securing LLMs in enterprise settings requires moving beyond
single-turn prompt filtering toward a holistic, multi-stage perspective on both
attacks and defenses.

</details>


<div id='gr-qc'></div>

# gr-qc [[Back]](#toc)

### [369] [Learning Null Geodesics for Gravitational Lensing Rendering in General Relativity](https://arxiv.org/abs/2507.15775)
*Mingyuan Sun,Zheng Fang,Jiaxu Wang,Kunyi Zhang,Qiang Zhang,Renjing Xu*

Main category: gr-qc

TL;DR: GravLensX利用神经网络渲染黑洞的引力透镜效应，显著减少计算时间。


<details>
  <summary>Details</summary>
Motivation: 传统方法渲染黑洞引力透镜效应耗时，需更高效方法。

Method: 训练神经网络拟合黑洞周围时空，生成受引力透镜影响的光线路径。

Result: 验证显示计算时间减少15倍，能准确可视化多黑洞系统。

Conclusion: 神经网络为复杂天体现象渲染提供新途径。

Abstract: We present GravLensX, an innovative method for rendering black holes with
gravitational lensing effects using neural networks. The methodology involves
training neural networks to fit the spacetime around black holes and then
employing these trained models to generate the path of light rays affected by
gravitational lensing. This enables efficient and scalable simulations of black
holes with optically thin accretion disks, significantly decreasing the time
required for rendering compared to traditional methods. We validate our
approach through extensive rendering of multiple black hole systems with
superposed Kerr metric, demonstrating its capability to produce accurate
visualizations with significantly $15\times$ reduced computational time. Our
findings suggest that neural networks offer a promising alternative for
rendering complex astrophysical phenomena, potentially paving a new path to
astronomical visualization.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [370] [Design of an Edge-based Portable EHR System for Anemia Screening in Remote Health Applications](https://arxiv.org/abs/2507.15146)
*Sebastian A. Cruz Romero,Misael J. Mercado Hernandez,Samir Y. Ali Rivera,Jorge A. Santiago Fernandez,Wilfredo E. Lugo Beauchamp*

Main category: cs.ET

TL;DR: 本文提出了一种便携式、边缘支持的电子健康记录平台，专为资源有限的环境设计，支持离线操作、安全数据管理和模块化诊断集成。通过贫血筛查模块验证，系统在性能和成本效益上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决资源有限环境中医疗系统面临的互操作性差、缺乏离线支持和高成本基础设施依赖等问题，提升偏远地区医疗服务的有效性。

Method: 开发了一种基于嵌入式设备的电子健康记录平台，支持AES-256加密本地存储和可选云同步。集成随机森林模型和YOLOv8n量化技术优化贫血筛查模块。

Result: 贫血筛查模块测试RMSE为1.969 g/dL，MAE为1.490 g/dL，敏感性达79.2%。YOLOv8n量化后推理延迟从46.96 ms降至21.50 ms，mAP@0.5保持0.995。

Conclusion: 该系统通过低成本部署、模块化设计和数据隐私合规性，为资源有限地区的数字健康应用提供了可行方案，支持便携式医疗信息系统的发展。

Abstract: The design of medical systems for remote, resource-limited environments faces
persistent challenges due to poor interoperability, lack of offline support,
and dependency on costly infrastructure. Many existing digital health solutions
neglect these constraints, limiting their effectiveness for frontline health
workers in underserved regions. This paper presents a portable, edge-enabled
Electronic Health Record platform optimized for offline-first operation, secure
patient data management, and modular diagnostic integration. Running on
small-form factor embedded devices, it provides AES-256 encrypted local storage
with optional cloud synchronization for interoperability. As a use case, we
integrated a non-invasive anemia screening module leveraging fingernail pallor
analysis. Trained on 250 patient cases (27\% anemia prevalence) with
KDE-balanced data, the Random Forest model achieved a test RMSE of 1.969 g/dL
and MAE of 1.490 g/dL. A severity-based model reached 79.2\% sensitivity. To
optimize performance, a YOLOv8n-based nail bed detector was quantized to INT8,
reducing inference latency from 46.96 ms to 21.50 ms while maintaining mAP@0.5
at 0.995. The system emphasizes low-cost deployment, modularity, and data
privacy compliance (HIPAA/GDPR), addressing critical barriers to digital health
adoption in disconnected settings. Our work demonstrates a scalable approach to
enhance portable health information systems and support frontline healthcare in
underserved regions.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [371] [ACME: Adaptive Customization of Large Models via Distributed Systems](https://arxiv.org/abs/2507.14802)
*Ziming Dai,Chao Qiu,Fei Gao,Yunfeng Zhao,Xiaofei Wang*

Main category: cs.DC

TL;DR: ACME提出了一种分布式系统下的自适应定制方法，用于优化Transformer大模型的部署，解决了数据隐私和延迟问题，同时提升了性能和成本效率。


<details>
  <summary>Details</summary>
Motivation: 解决云端部署大模型时的数据隐私和延迟问题，并应对模型不匹配、资源限制和能效低的挑战。

Method: 采用双向单循环分布式系统，逐步实现细粒度协作模型定制，包括主干生成、Pareto Front识别、头部生成和数据分布驱动的个性化架构聚合。

Result: 在模型大小限制下实现高成本效益，数据传输量降至6%，平均准确率提升10%，权衡指标增加近30%。

Conclusion: ACME通过分布式方法有效解决了大模型定制中的挑战，显著提升了性能和效率。

Abstract: Pre-trained Transformer-based large models have revolutionized personal
virtual assistants, but their deployment in cloud environments faces challenges
related to data privacy and response latency. Deploying large models closer to
the data and users has become a key research area to address these issues.
However, applying these models directly often entails significant difficulties,
such as model mismatching, resource constraints, and energy inefficiency.
Automated design of customized models is necessary, but it faces three key
challenges, namely, the high cost of centralized model customization,
imbalanced performance from user heterogeneity, and suboptimal performance from
data heterogeneity. In this paper, we propose ACME, an adaptive customization
approach of Transformer-based large models via distributed systems. To avoid
the low cost-efficiency of centralized methods, ACME employs a bidirectional
single-loop distributed system to progressively achieve fine-grained
collaborative model customization. In order to better match user heterogeneity,
it begins by customizing the backbone generation and identifying the Pareto
Front under model size constraints to ensure optimal resource utilization.
Subsequently, it performs header generation and refines the model using data
distribution-based personalized architecture aggregation to match data
heterogeneity. Evaluation on different datasets shows that ACME achieves
cost-efficient models under model size constraints. Compared to centralized
systems, data transmission volume is reduced to 6 percent. Additionally, the
average accuracy improves by 10 percent compared to the baseline, with the
trade-off metrics increasing by nearly 30 percent.

</details>


### [372] [Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning](https://arxiv.org/abs/2507.14597)
*Eugene Armah,Linda Amoako Bannning*

Main category: cs.DC

TL;DR: 论文提出了一种三步解决方案，用于主动边缘流处理自动扩展问题，包括GRU神经网络预测上游负载、迁移学习框架处理离线与在线域差异，以及基于预测负载的动态水平扩展模块。


<details>
  <summary>Details</summary>
Motivation: 随着数字经济的快速发展，高速数据处理变得至关重要。边缘计算和数据流处理（DSP）是当前的主要范式，但面临资源分配的挑战。现有方法多为被动响应，容易违反SLA，而强化学习需要大量模拟，机器学习模型则受在线分布和概念漂移影响。

Method: 1. 使用GRU神经网络预测上游负载；2. 通过迁移学习框架（结合DTW算法和联合分布适应）将预测模型集成到在线流处理系统；3. 动态调整算子并行度的水平扩展模块。

Result: GRU模型在真实数据集上实现了1.3%的SMAPE值，优于CNN、ARIMA和Prophet，且训练时间短于计算密集的强化学习模型。

Conclusion: 提出的三步解决方案有效解决了边缘流处理的资源分配问题，GRU模型在预测精度和效率上表现优异。

Abstract: Processing data at high speeds is becoming increasingly critical as digital
economies generate enormous data. The current paradigms for timely data
processing are edge computing and data stream processing (DSP). Edge computing
places resources closer to where data is generated, while stream processing
analyzes the unbounded high-speed data in motion. However, edge stream
processing faces rapid workload fluctuations, complicating resource
provisioning. Inadequate resource allocation leads to bottlenecks, whereas
excess allocation results in wastage. Existing reactive methods, such as
threshold-based policies and queuing theory scale only after performance
degrades, potentially violating SLAs. Although reinforcement learning (RL)
offers a proactive approach through agents that learn optimal runtime
adaptation policies, it requires extensive simulation. Furthermore, predictive
machine learning models face online distribution and concept drift that
minimize their accuracy. We propose a three-step solution to the proactive edge
stream processing autoscaling problem. Firstly, a GRU neural network forecasts
the upstream load using real-world and synthetic DSP datasets. Secondly, a
transfer learning framework integrates the predictive model into an online
stream processing system using the DTW algorithm and joint distribution
adaptation to handle the disparities between offline and online domains.
Finally, a horizontal autoscaling module dynamically adjusts the degree of
operator parallelism, based on predicted load while considering edge resource
constraints. The lightweight GRU model for load predictions recorded up to
1.3\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and
Prophet on the SMAPE and RMSE evaluation metrics, with lower training time than
the computationally intensive RL models.

</details>


### [373] [Byzantine-Robust Decentralized Coordination of LLM Agents](https://arxiv.org/abs/2507.14928)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: DecentLLMs提出了一种去中心化的多智能体LLM系统共识方法，通过并发生成答案和独立评分选择最佳答案，解决了传统领导者驱动系统的弱点。


<details>
  <summary>Details</summary>
Motivation: 克服单智能体系统的局限性（如幻觉和单点故障），并解决现有拜占庭鲁棒多智能体系统对领导者攻击的脆弱性和低效问题。

Method: 采用去中心化架构，工作智能体并发生成答案，评估智能体独立评分和排名，通过拜占庭鲁棒聚合技术选择最佳答案。

Result: 实验表明，DecentLLMs能有效容忍拜占庭智能体，并显著提升所选答案的质量。

Conclusion: DecentLLMs为多智能体LLM系统提供了一种高效、鲁棒的共识解决方案。

Abstract: Collaboration among multiple large language model (LLM) agents is a promising
approach to overcome inherent limitations of single-agent systems, such as
hallucinations and single points of failure. As LLM agents are increasingly
deployed on open blockchain platforms, multi-agent systems capable of
tolerating malicious (Byzantine) agents have become essential.
  Recent Byzantine-robust multi-agent systems typically rely on leader-driven
coordination, which suffers from two major drawbacks. First, they are
inherently vulnerable to targeted attacks against the leader. If consecutive
leaders behave maliciously, the system repeatedly fails to achieve consensus,
forcing new consensus rounds, which is particularly costly given the high
latency of LLM invocations. Second, an underperforming proposal from the leader
can be accepted as the final answer even when higher-quality alternatives are
available, as existing methods finalize the leader's proposal once it receives
a quorum of votes.
  To address these issues, we propose DecentLLMs, a novel decentralized
consensus approach for multi-agent LLM systems, where worker agents generate
answers concurrently and evaluator agents independently score and rank these
answers to select the best available one. This decentralized architecture
enables faster consensus despite the presence of Byzantine agents and
consistently selects higher-quality answers through Byzantine-robust
aggregation techniques.
  Experimental results demonstrate that DecentLLMs effectively tolerates
Byzantine agents and significantly improves the quality of selected answers.

</details>


<div id='stat.AP'></div>

# stat.AP [[Back]](#toc)

### [374] [Complex Dynamics in Psychological Data: Mapping Individual Symptom Trajectories to Group-Level Patterns](https://arxiv.org/abs/2507.14161)
*Eleonora Vitanza,Pietro DeLellis,Chiara Mocenni,Manuel Ruiz Marin*

Main category: stat.AP

TL;DR: 该研究结合因果推断、图分析、时间复杂性度量和机器学习，分析个体症状轨迹是否能揭示有意义的诊断模式。通过测试45名GAD和/或MDD患者的纵向数据，提出了一种分析心理病理症状时间动态的新方法，分类准确率达91%。


<details>
  <summary>Details</summary>
Motivation: 探索个体症状轨迹的诊断潜力，为个性化治疗和临床心理学提供数据驱动的基础。

Method: 使用PCMCI+算法构建症状间的因果网络，并计算复杂性度量（如熵、分形维度），结合机器学习进行分类。

Result: PCMCI+有效捕捉症状网络的个体特异性，复杂性度量与机器学习结合的分类准确率达91%。

Conclusion: 因果建模与时间复杂性度量的结合可增强诊断区分，为个性化评估和心理研究提供新方法。

Abstract: This study integrates causal inference, graph analysis, temporal complexity
measures, and machine learning to examine whether individual symptom
trajectories can reveal meaningful diagnostic patterns. Testing on a
longitudinal dataset of N=45 individuals affected by General Anxiety Disorder
(GAD) and/or Major Depressive Disorder (MDD) derived from Fisher et al. 2017,
we propose a novel pipeline for the analysis of the temporal dynamics of
psychopathological symptoms. First, we employ the PCMCI+ algorithm with
nonparametric independence test to determine the causal network of nonlinear
dependencies between symptoms in individuals with different mental disorders.
We found that the PCMCI+ effectively highlights the individual peculiarities of
each symptom network, which could be leveraged towards personalized therapies.
At the same time, aggregating the networks by diagnosis sheds light to
disorder-specific causal mechanisms, in agreement with previous
psychopathological literature. Then, we enrich the dataset by computing
complexity-based measures (e.g. entropy, fractal dimension, recurrence) from
the symptom time series, and feed it to a suitably selected machine learning
algorithm to aid the diagnosis of each individual. The new dataset yields 91%
accuracy in the classification of the symptom dynamics, proving to be an
effective diagnostic support tool. Overall, these findings highlight how
integrating causal modeling and temporal complexity can enhance diagnostic
differentiation, offering a principled, data-driven foundation for both
personalized assessment in clinical psychology and structural advances in
psychological research.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [375] [Attention-Based Fusion of IQ and FFT Spectrograms with AoA Features for GNSS Jammer Localization](https://arxiv.org/abs/2507.14167)
*Lucas Heublein,Christian Wielenberg,Thorsten Nowak,Tobias Feigl,Christopher Mutschler,Felix Ott*

Main category: eess.SP

TL;DR: 论文提出了一种基于注意力融合框架的新方法，用于检测和定位GNSS干扰源，结合IQ样本和FFT频谱图，并引入22个AoA特征以提高定位精度。


<details>
  <summary>Details</summary>
Motivation: GNSS干扰设备威胁定位可靠性，传统AoA方法在多路径环境下精度不足且计算资源需求高。

Method: 采用注意力融合框架，结合IQ样本和FFT频谱图，引入22个AoA特征，评估128种视觉编码和时间序列模型。

Result: 在动态多路径环境下，新方法性能优于现有技术。

Conclusion: 提出的方法在干扰检测和定位方面表现优越，适用于复杂环境。

Abstract: Jamming devices disrupt signals from the global navigation satellite system
(GNSS) and pose a significant threat by compromising the reliability of
accurate positioning. Consequently, the detection and localization of these
interference signals are essential to achieve situational awareness, mitigating
their impact, and implementing effective counter-measures. Classical Angle of
Arrival (AoA) methods exhibit reduced accuracy in multipath environments due to
signal reflections and scattering, leading to localization errors.
Additionally, AoA-based techniques demand substantial computational resources
for array signal processing. In this paper, we propose a novel approach for
detecting and classifying interference while estimating the distance, azimuth,
and elevation of jamming sources. Our benchmark study evaluates 128 vision
encoder and time-series models to identify the highest-performing methods for
each task. We introduce an attention-based fusion framework that integrates
in-phase and quadrature (IQ) samples with Fast Fourier Transform (FFT)-computed
spectrograms while incorporating 22 AoA features to enhance localization
accuracy. Furthermore, we present a novel dataset of moving jamming devices
recorded in an indoor environment with dynamic multipath conditions and
demonstrate superior performance compared to state-of-the-art methods.

</details>


### [376] [DIVER-0 : A Fully Channel Equivariant EEG Foundation Model](https://arxiv.org/abs/2507.14141)
*Danny Dongyeop Han,Ahhyun Lucy Lee,Taeyang Lee,Yonghyeon Gwon,Sebin Lee,Seongjin Lee,David Keetae Park,Shinjae Yoo,Jiook Cha,Chun Kee Chung*

Main category: eess.SP

TL;DR: DIVER-0是一种新型EEG基础模型，通过全时空注意力和改进的位置编码技术，解决了现有模型在时空动态建模和通道置换等变性方面的不足，实现了跨数据集泛化和对任意电极配置的鲁棒适应。


<details>
  <summary>Details</summary>
Motivation: 现有EEG基础模型在建模时空动态和通道置换等变性方面存在局限性，导致无法适应多样化的电极配置。

Method: 提出DIVER-0模型，采用全时空注意力机制，结合Rotary Position Embedding（RoPE）和二进制注意力偏置，并引入Sliding Temporal Conditional Positional Encoding（STCPE）以保持时空平移等变性和通道置换等变性。

Result: 实验表明，DIVER-0仅需10%的预训练数据即可达到竞争性性能，并在所有通道置换条件下保持一致性结果。

Conclusion: DIVER-0有效解决了EEG模型的跨数据集泛化和电极配置适应问题，为神经记录设备的异构性处理提供了关键设计原则。

Abstract: Electroencephalography (EEG) is a non-invasive technique widely used in
brain-computer interfaces and clinical applications, yet existing EEG
foundation models face limitations in modeling spatio-temporal brain dynamics
and lack channel permutation equivariance, preventing robust generalization
across diverse electrode configurations. To address these challenges, we
propose DIVER-0, a novel EEG foundation model that demonstrates how full
spatio-temporal attention-rather than segregated spatial or temporal
processing-achieves superior performance when properly designed with Rotary
Position Embedding (RoPE) for temporal relationships and binary attention
biases for channel differentiation. We also introduce Sliding Temporal
Conditional Positional Encoding (STCPE), which improves upon existing
conditional positional encoding approaches by maintaining both temporal
translation equivariance and channel permutation equivariance, enabling robust
adaptation to arbitrary electrode configurations unseen during pretraining.
Experimental results demonstrate that DIVER-0 achieves competitive performance
with only 10% of pretraining data while maintaining consistent results across
all channel permutation conditions, validating its effectiveness for
cross-dataset generalization and establishing key design principles for
handling the inherent heterogeneity of neural recording setups.

</details>


### [377] [Self-DANA: A Resource-Efficient Channel-Adaptive Self-Supervised Approach for ECG Foundation Models](https://arxiv.org/abs/2507.14151)
*Giuliana Monachino,Nicolò La Porta,Beatrice Zanchi,Luigi Fiorillo,Alvise Dei Rossi,Georgiy Farina,Francesca Dalia Faraci*

Main category: eess.SP

TL;DR: 该论文提出了一种名为Self-DANA的新方法，用于使自监督架构适应输入通道减少的情况，同时保持资源高效和高性能。


<details>
  <summary>Details</summary>
Motivation: 随着可穿戴和便携设备的普及，对减少通道配置的学习需求增加，但目前对心电图（ECG）基础模型在减少通道场景下的适应性研究不足。

Method: 提出Self-DANA和随机导联选择（Random Lead Selection）增强技术，使模型在预训练时更具鲁棒性和通道无关性。

Result: 在五种减少通道配置下，Self-DANA显著提高了资源效率，减少了CPU和GPU的内存及时间消耗，同时达到最先进的性能。

Conclusion: Self-DANA是一种高效且易于集成的解决方案，适用于减少通道的ECG分析任务。

Abstract: Foundation Models (FMs) are large-scale machine learning models trained on
extensive, diverse datasets that can be adapted to a wide range of downstream
tasks with minimal fine-tuning. In the last two years, interest in FMs has also
grown for applications in the cardiological field to analyze the
electrocardiogram (ECG) signals. One of the key properties of FMs is their
transferability to a wide range of downstream scenarios. With the spread of
wearable and portable devices, keen interest in learning from reduced-channel
configurations has arisen. However, the adaptation of ECG FMs to downstream
scenarios with fewer available channels still has to be properly investigated.
In this work, we propose Self-DANA, a novel, easy-to-integrate solution that
makes self-supervised architectures adaptable to a reduced number of input
channels, ensuring resource efficiency and high performance. We also introduce
Random Lead Selection, a novel augmentation technique to pre-train models in a
more robust and channel-agnostic way. Our experimental results on five
reduced-channel configurations demonstrate that Self-DANA significantly
enhances resource efficiency while reaching state-of-the-art performance. It
requires up to 69.3% less peak CPU memory, 34.4% less peak GPU memory, about
17% less average epoch CPU time, and about 24% less average epoch GPU time.

</details>


### [378] [Surface EMG Profiling in Parkinson's Disease: Advancing Severity Assessment with GCN-SVM](https://arxiv.org/abs/2507.14153)
*Daniel Cieślak,Barbara Szyca,Weronika Bajko,Liwia Florkiewicz,Kinga Grzęda,Mariusz Kaczmarek,Helena Kamieniecka,Hubert Lis,Weronika Matwiejuk,Anna Prus,Michalina Razik,Inga Rozumowicz,Wiktoria Ziembakowska*

Main category: eess.SP

TL;DR: 研究提出了一种利用表面肌电图（sEMG）客观评估帕金森病（PD）严重程度的新方法，通过改进的GCN-SVM模型实现了92%的准确率。


<details>
  <summary>Details</summary>
Motivation: 帕金森病的诊断和监测因其渐进性和复杂症状而具有挑战性，需要更客观的评估方法。

Method: 研究使用sEMG技术分析肱二头肌的神经肌肉差异，并比较了传统SVM和改进的GCN-SVM模型的性能。

Result: 初步结果显示，GCN-SVM模型准确率高达92%，优于传统SVM的83%。

Conclusion: 该方法为PD严重程度评估提供了潜在的新工具，未来需更大规模研究验证并应用于临床。

Abstract: Parkinson's disease (PD) poses challenges in diagnosis and monitoring due to
its progressive nature and complex symptoms. This study introduces a novel
approach utilizing surface electromyography (sEMG) to objectively assess PD
severity, focusing on the biceps brachii muscle. Initial analysis of sEMG data
from five PD patients and five healthy controls revealed significant
neuromuscular differences. A traditional Support Vector Machine (SVM) model
achieved up to 83% accuracy, while enhancements with a Graph Convolutional
Network-Support Vector Machine (GCN-SVM) model increased accuracy to 92%.
Despite the preliminary nature of these results, the study outlines a detailed
experimental methodology for future research with larger cohorts to validate
these findings and integrate the approach into clinical practice. The proposed
approach holds promise for advancing PD severity assessment and improving
patient care in Parkinson's disease management.

</details>


### [379] [A Denoising VAE for Intracardiac Time Series in Ischemic Cardiomyopathy](https://arxiv.org/abs/2507.14164)
*Samuel Ruipérez-Campillo,Alain Ryser,Thomas M. Sutter,Ruibin Feng,Prasanth Ganesan,Brototo Deb,Kelly A. Brennan,Maxime Pedron,Albert J. Rogers,Maarten Z. H. Kolk,Fleur V. Y. Tjong,Sanjiv M. Narayan,Julia E. Vogt*

Main category: eess.SP

TL;DR: 论文提出了一种基于变分自编码器（VAE）的方法，用于提高心内单相动作电位（MAP）信号的质量，相比传统滤波方法表现出更优的去噪性能。


<details>
  <summary>Details</summary>
Motivation: 传统去噪技术难以处理心内信号中多样化的非线性、非平稳噪声，影响心律失常和心肌病的准确诊断与治疗。

Method: 使用来自42名缺血性心肌病患者的5706个时间序列数据，构建VAE模型以生成干净信号的表示。

Result: VAE模型在多种噪声类型下表现出优于传统方法的去噪能力，尤其对临床常见的时变非线性噪声效果显著。

Conclusion: VAE能够有效消除单次心跳中的多种噪声源，优于现有技术，有望提升心脏电生理学的治疗效果。

Abstract: In the field of cardiac electrophysiology (EP), effectively reducing noise in
intra-cardiac signals is crucial for the accurate diagnosis and treatment of
arrhythmias and cardiomyopathies. However, traditional noise reduction
techniques fall short in addressing the diverse noise patterns from various
sources, often non-linear and non-stationary, present in these signals. This
work introduces a Variational Autoencoder (VAE) model, aimed at improving the
quality of intra-ventricular monophasic action potential (MAP) signal
recordings. By constructing representations of clean signals from a dataset of
5706 time series from 42 patients diagnosed with ischemic cardiomyopathy, our
approach demonstrates superior denoising performance when compared to
conventional filtering methods commonly employed in clinical settings. We
assess the effectiveness of our VAE model using various metrics, indicating its
superior capability to denoise signals across different noise types, including
time-varying non-linear noise frequently found in clinical settings. These
results reveal that VAEs can eliminate diverse sources of noise in single
beats, outperforming state-of-the-art denoising techniques and potentially
improving treatment efficacy in cardiac EP.

</details>


### [380] [NeuroHD-RA: Neural-distilled Hyperdimensional Model with Rhythm Alignment](https://arxiv.org/abs/2507.14184)
*ZhengXiao He,Jinghao Wen,Huayu Li,Ao Li*

Main category: eess.SP

TL;DR: 提出了一种结合超维计算（HDC）和可学习神经编码的新型心电图（ECG）疾病检测框架，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统HDC方法依赖静态随机投影，无法适应任务需求，因此需要一种更具适应性和可解释性的方法。

Method: 采用基于RR间隔的可训练编码管道和神经蒸馏HDC架构，结合交叉熵和代理度量损失进行优化。

Result: 在Apnea-ECG和PTB-XL数据集上表现优异，精度达73.09%，F1分数0.626。

Conclusion: 该框架为边缘兼容的ECG分类提供了高效、可扩展且可解释的解决方案。

Abstract: We present a novel and interpretable framework for electrocardiogram
(ECG)-based disease detection that combines hyperdimensional computing (HDC)
with learnable neural encoding. Unlike conventional HDC approaches that rely on
static, random projections, our method introduces a rhythm-aware and trainable
encoding pipeline based on RR intervals, a physiological signal segmentation
strategy that aligns with cardiac cycles. The core of our design is a
neural-distilled HDC architecture, featuring a learnable RR-block encoder and a
BinaryLinear hyperdimensional projection layer, optimized jointly with
cross-entropy and proxy-based metric loss. This hybrid framework preserves the
symbolic interpretability of HDC while enabling task-adaptive representation
learning. Experiments on Apnea-ECG and PTB-XL demonstrate that our model
significantly outperforms traditional HDC and classical ML baselines, achieving
73.09\% precision and an F1 score of 0.626 on Apnea-ECG, with comparable
robustness on PTB-XL. Our framework offers an efficient and scalable solution
for edge-compatible ECG classification, with strong potential for interpretable
and personalized health monitoring.

</details>


### [381] [AI-Based Impedance Encoding-Decoding Method for Online Impedance Network Construction of Wind Farms](https://arxiv.org/abs/2507.14187)
*Xiaojuan Zhang,Tianyu Jiang,Haoxiang Zong,Chen Zhang,Chendan Li,Marta Molinas*

Main category: eess.SP

TL;DR: 论文提出了一种基于AI的阻抗编码-解码方法，用于在线构建风电场的阻抗网络模型，解决了传统方法中高密度阻抗曲线传输困难的问题。


<details>
  <summary>Details</summary>
Motivation: 传统阻抗网络模型需要每台风电机组在不同工况下的阻抗曲线，导致在线应用时传输大量高密度数据困难。

Method: 1. 训练阻抗编码器压缩阻抗曲线；2. 上传压缩数据至风电场；3. 训练阻抗解码器重建原始阻抗曲线；4. 基于节点导纳矩阵方法构建风电场阻抗网络模型。

Result: 通过模型训练和实时仿真验证，编码后的阻抗向量能够快速传输并准确重建原始阻抗曲线。

Conclusion: 提出的AI方法有效解决了阻抗曲线传输问题，为风电场阻抗网络模型的在线应用提供了可行方案。

Abstract: The impedance network (IN) model is gaining popularity in the oscillation
analysis of wind farms. However, the construction of such an IN model requires
impedance curves of each wind turbine under their respective operating
conditions, making its online application difficult due to the transmission of
numerous high-density impedance curves. To address this issue, this paper
proposes an AI-based impedance encoding-decoding method to facilitate the
online construction of IN model. First, an impedance encoder is trained to
compress impedance curves by setting the number of neurons much smaller than
that of frequency points. Then, the compressed data of each turbine are
uploaded to the wind farm and an impedance decoder is trained to reconstruct
original impedance curves. At last, based on the nodal admittance matrix (NAM)
method, the IN model of the wind farm can be obtained. The proposed method is
validated via model training and real-time simulations, demonstrating that the
encoded impedance vectors enable fast transmission and accurate reconstruction
of the original impedance curves.

</details>


### [382] [UWB Radar-based Heart Rate Monitoring: A Transfer Learning Approach](https://arxiv.org/abs/2507.14195)
*Elzbieta Gruzewska,Pooja Rao,Sebastien Baur,Matthew Baugh,Mathias M. J. Bellaiche,Sharanya Srinivas,Octavio Ponce,Matthew Thompson,Pramod Rudrapatna,Michael A. Sanchez,Lawrence Z. Cai,Timothy JA Chico,Robert F. Storey,Emily Maz,Umesh Telang,Shravya Shetty,Mayank Daswani*

Main category: eess.SP

TL;DR: 该研究展示了在FMCW和IR-UWB雷达系统之间进行迁移学习，用于心率监测，显著提高了IR-UWB系统的性能。


<details>
  <summary>Details</summary>
Motivation: 利用雷达技术实现无接触、被动的心率监测，但不同雷达系统缺乏标准化，需要大量配对数据集。

Method: 采用新颖的2D+1D ResNet架构，先在FMCW雷达数据上训练，再通过小规模IR-UWB数据集进行微调。

Result: FMCW雷达的MAE为0.85 bpm，MAPE为1.42%；迁移学习后IR-UWB的MAE降低25%，达到4.1 bpm。

Conclusion: 迁移学习方法可加速雷达心率监测技术在消费设备中的应用。

Abstract: Radar technology presents untapped potential for continuous, contactless, and
passive heart rate monitoring via consumer electronics like mobile phones.
However the variety of available radar systems and lack of standardization
means that a large new paired dataset collection is required for each radar
system. This study demonstrates transfer learning between frequency-modulated
continuous wave (FMCW) and impulse-radio ultra-wideband (IR-UWB) radar systems,
both increasingly integrated into consumer devices. FMCW radar utilizes a
continuous chirp, while IR-UWB radar employs short pulses. Our mm-wave FMCW
radar operated at 60 GHz with a 5.5 GHz bandwidth (2.7 cm resolution, 3
receiving antennas [Rx]), and our IR-UWB radar at 8 GHz with a 500 MHz
bandwidth (30 cm resolution, 2 Rx). Using a novel 2D+1D ResNet architecture we
achieved a mean absolute error (MAE) of 0.85 bpm and a mean absolute percentage
error (MAPE) of 1.42% for heart rate monitoring with FMCW radar (N=119
participants, an average of 8 hours per participant). This model maintained
performance (under 5 MAE/10% MAPE) across various body positions and heart rate
ranges, with a 98.9% recall. We then fine-tuned a variant of this model,
trained on single-antenna and single-range bin FMCW data, using a small (N=376,
avg 6 minutes per participant) IR-UWB dataset. This transfer learning approach
yielded a model with MAE 4.1 bpm and MAPE 6.3% (97.5% recall), a 25% MAE
reduction over the IR-UWB baseline. This demonstration of transfer learning
between radar systems for heart rate monitoring has the potential to accelerate
its introduction into existing consumer devices.

</details>


### [383] [Explainable Parallel CNN-LSTM Model for Differentiating Ventricular Tachycardia from Supraventricular Tachycardia with Aberrancy in 12-Lead ECGs](https://arxiv.org/abs/2507.14196)
*Zahra Teimouri-Jervekani,Fahimeh Nasimi,Mohammadreza Yazdchi,Ghazal MogharehZadeh,Javad Tezerji,Farzan Niknejad Mazandarani,Maryam Mohebbi*

Main category: eess.SP

TL;DR: 提出了一种轻量级并行深度学习架构，用于高效区分宽复合心动过速（WCT），并通过SHAP增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 由于心电图（ECG）信号中室性心动过速（VT）和室上性心动过速伴差异性传导（SVT-A）形态相似，临床诊断具有挑战性，误诊风险高。

Method: 采用轻量级并行深度学习架构，通过1D-CNN提取局部特征，LSTM捕捉时间依赖性，SHAP提供解释性。

Result: 模型准确率达95.63%，灵敏度和特异性均超过95%，计算效率优于现有方法。

Conclusion: 该框架实现了高精度WCT分类，计算开销低，SHAP增强了临床信任，适用于实时ECG分析。

Abstract: Background and Objective: Differentiating wide complex tachycardia (WCT) is
clinically critical yet challenging due to morphological similarities in
electrocardiogram (ECG) signals between life-threatening ventricular
tachycardia (VT) and supraventricular tachycardia with aberrancy (SVT-A).
Misdiagnosis carries fatal risks. We propose a computationally efficient deep
learning solution to improve diagnostic accuracy and provide model
interpretability for clinical deployment.
  Methods: A novel lightweight parallel deep architecture is introduced. Each
pipeline processes individual ECG leads using two 1D-CNN blocks to extract
local features. Feature maps are concatenated across leads, followed by LSTM
layers to capture temporal dependencies. Final classification employs fully
connected layers. Explainability is achieved via Shapley Additive Explanations
(SHAP) for local/global interpretation. The model was evaluated on a 35-subject
ECG database using standard performance metrics.
  Results: The model achieved $95.63\%$ accuracy ($95\%$ CI: $93.07-98.19\%$),
with sensitivity=$95.10\%$, specificity=$96.06\%$, and F1-score=$95.12\%$. It
outperformed state-of-the-art methods in both accuracy and computational
efficiency, requiring minimal CNN blocks per pipeline. SHAP analysis
demonstrated clinically interpretable feature contributions.
  Conclusions: Our end-to-end framework delivers high-precision WCT
classification with minimal computational overhead. The integration of SHAP
enhances clinical trust by elucidating decision logic, supporting rapid,
informed diagnosis. This approach shows significant promise for real-world ECG
analysis tools.

</details>


### [384] [A Comprehensive Benchmark for Electrocardiogram Time-Series](https://arxiv.org/abs/2507.14206)
*Zhijiang Tang,Jiaxin Qi,Yuhua Zheng,Jianqiang Huang*

Main category: eess.SP

TL;DR: 本文深入研究了心电图（ECG）信号，建立了全面的基准，包括分类下游任务、改进评估指标并提出新模型架构。


<details>
  <summary>Details</summary>
Motivation: 现有研究常忽略ECG信号的独特性和专用下游应用，导致对其特性理解不完整。

Method: 分类下游任务为四项评估任务，改进传统评估指标并提出新指标，同时提出新模型架构。

Result: 实验证明基准全面且稳健，新指标和模型架构有效。

Conclusion: 研究为ECG信号分析提供了坚实基础，推动了该领域的发展。

Abstract: Electrocardiogram~(ECG), a key bioelectrical time-series signal, is crucial
for assessing cardiac health and diagnosing various diseases. Given its
time-series format, ECG data is often incorporated into pre-training datasets
for large-scale time-series model training. However, existing studies often
overlook its unique characteristics and specialized downstream applications,
which differ significantly from other time-series data, leading to an
incomplete understanding of its properties. In this paper, we present an
in-depth investigation of ECG signals and establish a comprehensive benchmark,
which includes (1) categorizing its downstream applications into four distinct
evaluation tasks, (2) identifying limitations in traditional evaluation metrics
for ECG analysis, and introducing a novel metric; (3) benchmarking
state-of-the-art time-series models and proposing a new architecture. Extensive
experiments demonstrate that our proposed benchmark is comprehensive and
robust. The results validate the effectiveness of the proposed metric and model
architecture, which establish a solid foundation for advancing research in ECG
signal analysis.

</details>


### [385] [Age of Information Minimization in UAV-Enabled Integrated Sensing and Communication Systems](https://arxiv.org/abs/2507.14299)
*Yu Bai,Yifan Zhang,Boxuan Xie,Zheng Chang,Yanru Zhang,Riku Jantti,Zhu Han*

Main category: eess.SP

TL;DR: 提出了一种基于信息新鲜度（AoI）的无人机-ISAC系统，通过深度强化学习优化无人机轨迹和波束成形，以平衡感知精度和通信质量。


<details>
  <summary>Details</summary>
Motivation: 无人机在无线网络中具有灵活性，但资源受限下联合优化轨迹、通信和感知仍是挑战。

Method: 采用深度强化学习（DRL）算法，结合卡尔曼滤波和目标状态预测，优化无人机轨迹和波束成形。

Result: 仿真结果表明，该方法在平均AoI上优于基线方法。

Conclusion: 提出的AoI中心框架有效平衡了感知与通信的权衡，提升了系统性能。

Abstract: Unmanned aerial vehicles (UAVs) equipped with integrated sensing and
communication (ISAC) capabilities are envisioned to play a pivotal role in
future wireless networks due to their enhanced flexibility and efficiency.
However, jointly optimizing UAV trajectory planning, multi-user communication,
and target sensing under stringent resource constraints and time-critical
conditions remains a significant challenge. To address this, we propose an Age
of Information (AoI)-centric UAV-ISAC system that simultaneously performs
target sensing and serves multiple ground users, emphasizing information
freshness as the core performance metric. We formulate a long-term average AoI
minimization problem that jointly optimizes the UAV's flight trajectory and
beamforming. To tackle the high-dimensional, non-convexity of this problem, we
develop a deep reinforcement learning (DRL)-based algorithm capable of
providing real-time decisions on UAV movement and beamforming for both radar
sensing and multi-user communication. Specifically, a Kalman filter is employed
for accurate target state prediction, regularized zero-forcing is utilized to
mitigate inter-user interference, and the Soft Actor-Critic algorithm is
applied for training the DRL agent on continuous actions. The proposed
framework adaptively balances the trade-offs between sensing accuracy and
communication quality. Extensive simulation results demonstrate that our
proposed method consistently achieves lower average AoI compared to baseline
approaches.

</details>


### [386] [Recursive KalmanNet: Analyse des capacités de généralisation d'un réseau de neurones récurrent guidé par un filtre de Kalman](https://arxiv.org/abs/2507.14144)
*Cyril Falcon,Hassan Mortada,Mathéo Clavaud,Jean-Philippe Michel*

Main category: eess.SP

TL;DR: Recursive KalmanNet是一种基于Kalman滤波的递归神经网络，能够在未知噪声特性的情况下估计动态系统的状态变量和误差协方差。本文探讨了其在分布外场景下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 研究Recursive KalmanNet在测试数据与训练数据动态特性不同的情况下的表现，验证其泛化能力。

Method: 使用Recursive KalmanNet，结合Kalman滤波和递归神经网络，处理动态系统的状态估计问题。

Result: Recursive KalmanNet在分布外场景下表现出一定的泛化能力。

Conclusion: Recursive KalmanNet在未知噪声和动态变化的测试数据中具有潜在的应用价值。

Abstract: The Recursive KalmanNet, recently introduced by the authors, is a recurrent
neural network guided by a Kalman filter, capable of estimating the state
variables and error covariance of stochastic dynamic systems from noisy
measurements, without prior knowledge of the noise characteristics. This paper
explores its generalization capabilities in out-of-distribution scenarios,
where the temporal dynamics of the test measurements differ from those
encountered during training.
  Le Recursive KalmanNet, r\'ecemment introduit par les auteurs, est un
r\'eseau de neurones r\'ecurrent guid\'e par un filtre de Kalman, capable
d'estimer les variables d'\'etat et la covariance des erreurs des syst\`emes
dynamiques stochastiques \`a partir de mesures bruit\'ees, sans connaissance
pr\'ealable des caract\'eristiques des bruits. Cet article explore ses
capacit\'es de g\'en\'eralisation dans des sc\'enarios hors distribution, o\`u
les dynamiques temporelles des mesures de test diff\`erent de celles
rencontr\'ees \`a l'entra\^inement.

</details>


### [387] [Graph Convolutional Neural Networks to Model the Brain for Insomnia](https://arxiv.org/abs/2507.14147)
*Kevin Monteiro,Sam Nallaperuma-Herzberg,Martina Mason,Steve Niederer*

Main category: eess.SP

TL;DR: 该论文提出了一种基于脑电图（EEG）和图卷积神经网络（GCNN）的方法，用于识别失眠症患者的脑功能特征，并实现了70%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 现有失眠治疗方法副作用多，且缺乏针对失眠的脑网络模型研究，因此需要开发更有效的失眠诊断方法。

Method: 使用连续长时程EEG数据，基于功能连接和空间距离构建脑网络，计算主要脑波频段的功率谱密度，并训练GCNN模型进行分类。

Result: 50秒非重叠滑动窗口最适合EEG分割，分类准确率达70%（窗口级）和68%（受试者级）。特定电极（C4-P4、F4-C4、C4-A1）的缺失对模型性能影响显著。

Conclusion: 该方法能有效捕捉失眠相关的脑功能特征，特定脑区电极对模型性能至关重要，为失眠诊断提供了新思路。

Abstract: Insomnia affects a vast population of the world and can have a wide range of
causes. Existing treatments for insomnia have been linked with many side
effects like headaches, dizziness, etc. As such, there is a clear need for
improved insomnia treatment. Brain modelling has helped with assessing the
effects of brain pathology on brain network dynamics and with supporting
clinical decisions in the treatment of Alzheimer's disease, epilepsy, etc.
However, such models have not been developed for insomnia. Therefore, this
project attempts to understand the characteristics of the brain of individuals
experiencing insomnia using continuous long-duration EEG data. Brain networks
are derived based on functional connectivity and spatial distance between EEG
channels. The power spectral density of the channels is then computed for the
major brain wave frequency bands. A graph convolutional neural network (GCNN)
model is then trained to capture the functional characteristics associated with
insomnia and configured for the classification task to judge performance.
Results indicated a 50-second non-overlapping sliding window was the most
suitable choice for EEG segmentation. This approach achieved a classification
accuracy of 70% at window level and 68% at subject level. Additionally, the
omission of EEG channels C4-P4, F4-C4 and C4-A1 caused higher degradation in
model performance than the removal of other channels. These channel electrodes
are positioned near brain regions known to exhibit atypical levels of
functional connectivity in individuals with insomnia, which can explain such
results.

</details>


### [388] [Machine learning-enabled river water quality monitoring using lithography-free 3D-printed sensors](https://arxiv.org/abs/2507.14152)
*Frank Efe Erukainure,Feidra Gjata,Matin Ataei Kachouei,Henry Cox,Md. Azahar Ali*

Main category: eess.SP

TL;DR: 提出了一种无光刻技术的磷酸盐传感器（P-sensor），用于检测河流水中的磷酸盐，检测限低至1 ppb，响应时间短于30秒，并通过神经网络预测磷酸盐水平。


<details>
  <summary>Details</summary>
Motivation: 清洁水对全球粮食危机中的食物需求至关重要，磷酸盐等污染物会导致富营养化，危害健康和生态，因此需要连续监测磷酸盐水平。

Method: 采用3D打印的周期性聚合物图案（8微米特征尺寸）涂覆磷酸盐选择性膜，形成固态指示电极，并通过神经网络预测磷酸盐浓度。

Result: 传感器在0-475 ppm范围内检测到1 ppb磷酸盐，响应时间短于30秒；神经网络预测的均方误差低于1e-3，Pearson相关系数为0.997。

Conclusion: 该传感器为连续水质监测提供了实用工具，有助于改善公共健康并为决策者提供信息。

Abstract: River water quality monitoring is important for aquatic life, livestock, and
humans because clean water is critical to meeting food demand during the global
food crisis. Excessive contaminants, including phosphate, deplete dissolved
oxygen and trigger eutrophication, leading to serious health and ecological
problems. Continuous sensors that track phosphate levels can therefore help
prevent eutrophication. In this work we present a lithography-free phosphate
sensor (P-sensor) that detects phosphate in river water at parts-per-billion
levels. The device uses a solid-state indicator electrode formed by 3D-printed
periodic polymer patterns (8 um feature size) coated with a thin phosphate
ion-selective membrane. The P-sensor detects as little as 1 ppb phosphate
across 0 - 475 ppm with a response time under 30 seconds. We validated the
sensor on Rappahannock River water, Virginia (less than 0.8 ppm phosphate) at
sites upstream and downstream of a sewage treatment plant and benchmarked the
results against a commercial phosphate meter. A feed-forward neural network was
trained to predict phosphate levels, achieving a mean-squared error below 1e-3,
zero standard deviation, and a Pearson correlation coefficient of 0.997 for
river samples. These results demonstrate a practical tool for continuous
water-quality monitoring that can inform stakeholders and policymakers and
ultimately improve public health.

</details>


### [389] [UniPhyNet: A Unified Network For Multimodal Physiological Raw Signal Classification](https://arxiv.org/abs/2507.14163)
*Renxiang Qiu,Raghavendra Selvan*

Main category: eess.SP

TL;DR: UniPhyNet是一种新型神经网络架构，用于多模态生理数据分类，无需手工提取特征，显著提升分类准确率。


<details>
  <summary>Details</summary>
Motivation: 解决传统方法依赖手工特征提取的问题，提供端到端的认知负荷分类解决方案。

Method: 结合多尺度并行卷积块、ResNet块和通道块注意力模块，利用双向门控循环单元捕捉时间依赖性，支持单模态和多模态配置。

Result: 在CL-Drive数据集上，二进制分类准确率从70%提升至80%，三分类从62%提升至74%，优于基于特征的方法。

Conclusion: UniPhyNet是一种有效的端到端解决方案，适用于实际认知状态监测。

Abstract: We present UniPhyNet, a novel neural network architecture to classify
cognitive load using multimodal physiological data -- specifically EEG, ECG and
EDA signals -- without the explicit need for extracting hand-crafted features.
UniPhyNet integrates multiscale parallel convolutional blocks and ResNet-type
blocks enhanced with channel block attention module to focus on the informative
features while a bidirectional gated recurrent unit is used to capture temporal
dependencies. This architecture processes and combines signals in both unimodal
and multimodal configurations via intermediate fusion of learned feature maps.
On the CL-Drive dataset, UniPhyNet improves raw signal classification accuracy
from 70% to 80% (binary) and 62% to 74% (ternary), outperforming feature-based
models, demonstrating its effectiveness as an end-to-end solution for
real-world cognitive state monitoring.

</details>


### [390] [Automated Vigilance State Classification in Rodents Using Machine Learning and Feature Engineering](https://arxiv.org/abs/2507.14166)
*Sankalp Jajee,Gaurav Kumar,Homayoun Valafar*

Main category: eess.SP

TL;DR: 该研究提出了一种自动化框架，用于分类小啮齿动物的脑电图（EEG）记录，分为三种警觉状态：REM睡眠、慢波睡眠（SWS）和清醒状态，通过机器学习和信号处理技术显著提高了分类准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 临床前睡眠研究受限于人工分类的高劳动强度和评分者间变异性，影响了通量和可重复性。

Method: 结合先进的信号处理和机器学习技术，提取时域和频域特征（如频谱功率、最大-最小距离和跨频耦合指标），捕捉不同警觉状态的神经生理特征。

Result: 在2024年大数据健康科学案例竞赛中，XGBoost模型实现了91.5%的总体准确率，优于所有基线方法。

Conclusion: 该方法为自动化睡眠状态分类提供了重要进展，有望加速睡眠科学研究和慢性睡眠障碍的针对性干预开发。

Abstract: Preclinical sleep research remains constrained by labor intensive, manual
vigilance state classification and inter rater variability, limiting throughput
and reproducibility. This study presents an automated framework developed by
Team Neural Prognosticators to classify electroencephalogram (EEG) recordings
of small rodents into three critical vigilance states paradoxical sleep (REM),
slow wave sleep (SWS), and wakefulness. The system integrates advanced signal
processing with machine learning, leveraging engineered features from both time
and frequency domains, including spectral power across canonical EEG bands
(delta to gamma), temporal dynamics via Maximum-Minimum Distance, and
cross-frequency coupling metrics. These features capture distinct
neurophysiological signatures such as high frequency desynchronization during
wakefulness, delta oscillations in SWS, and REM specific bursts. Validated
during the 2024 Big Data Health Science Case Competition (University of South
Carolina Big Data Health Science Center, 2024), our XGBoost model achieved
91.5% overall accuracy, 86.8% precision, 81.2% recall, and an F1 score of
83.5%, outperforming all baseline methods. Our approach represents a critical
advancement in automated sleep state classification and a valuable tool for
accelerating discoveries in sleep science and the development of targeted
interventions for chronic sleep disorders. As a publicly available code (BDHSC)
resource is set to contribute significantly to advancements.

</details>


### [391] [Enhancing Generalization in PPG-Based Emotion Measurement with a CNN-TCN-LSTM Model](https://arxiv.org/abs/2507.14173)
*Karim Alghoul,Hussein Al Osman,Abdulmotaleb El Saddik*

Main category: eess.SP

TL;DR: 论文提出了一种结合CNN、LSTM和TCN的混合架构，用于基于PPG信号的情感识别，解决了模型在个体间泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: PPG信号因其易获取性被广泛用于情感识别，但现有模型在个体间的泛化能力不足。

Method: 提出混合架构，结合CNN、LSTM和TCN的优势，分别提取特征并融合，用于情感分类。

Result: 在PPGE数据集上，混合模型在AUC和F1 Score上优于现有CNN和CNN-LSTM模型。

Conclusion: 混合架构显著提升了PPG信号情感识别的泛化能力和性能。

Abstract: Human computer interaction has become integral to modern life, driven by
advancements in machine learning technologies. Affective computing, in
particular, has focused on systems that recognize, interpret, and respond to
human emotions, often using wearable devices, which provide continuous data
streams of physiological signals. Among various physiological signals, the
photoplethysmogram (PPG) has gained prominence due to its ease of acquisition
from widely available devices. However, the generalization of PPG-based emotion
recognition models across individuals remains an unresolved challenge. This
paper introduces a novel hybrid architecture that combines Convolutional Neural
Networks (CNNs), Long Short-Term Memory networks (LSTMs), and Temporal
Convolutional Networks (TCNs) to address this issue. The proposed model
integrates the strengths of these architectures to improve robustness and
generalization. Raw PPG signals are fed into the CNN for feature extraction.
These features are processed separately by LSTM and TCN. The outputs from these
components are concatenated to generate a final feature representation, which
serves as the input for classifying valence and arousal, the primary dimensions
of emotion. Experiments using the Photoplethysmogram Dataset for Emotional
Analysis (PPGE) demonstrate that the proposed hybrid model achieves better
model generalization than standalone CNN and LSTM architectures. Our results
show that the proposed solution outperforms the state-of-the-art CNN
architecture, as well as a CNN-LSTM model, in emotion recognition tasks with
PPG signals. Using metrics such as Area Under the Curve (AUC) and F1 Score, we
highlight the model's effectiveness in handling subject variability.

</details>


### [392] [Latent Sensor Fusion: Multimedia Learning of Physiological Signals for Resource-Constrained Devices](https://arxiv.org/abs/2507.14185)
*Abdullah Ahmed,Jeremy Gummeson*

Main category: eess.SP

TL;DR: 提出一种基于隐空间的模态无关统一编码器，通过传感器-隐空间融合分析多模态生理信号，显著提升了计算效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决资源受限设备上生物信号分析的计算挑战，同时保持表征准确性。

Method: 采用压缩感知和基于自动编码器的隐空间融合技术，实现多模态信号的模态无关编码。

Result: 统一编码器比模态特定方法更快、更轻量且更具可扩展性，同时保持准确性。

Conclusion: 该方法为资源受限设备上的多模态生理信号分析提供了一种高效解决方案。

Abstract: Latent spaces offer an efficient and effective means of summarizing data
while implicitly preserving meta-information through relational encoding. We
leverage these meta-embeddings to develop a modality-agnostic, unified encoder.
Our method employs sensor-latent fusion to analyze and correlate multimodal
physiological signals. Using a compressed sensing approach with
autoencoder-based latent space fusion, we address the computational challenges
of biosignal analysis on resource-constrained devices. Experimental results
show that our unified encoder is significantly faster, lighter, and more
scalable than modality-specific alternatives, without compromising
representational accuracy.

</details>


### [393] [Boosted Enhanced Quantile Regression Neural Networks with Spatiotemporal Permutation Entropy for Complex System Prognostics](https://arxiv.org/abs/2507.14194)
*David J Poland*

Main category: eess.SP

TL;DR: 提出了一种基于时空排列熵分析和增强分位数回归神经网络（BEQRNNs）的模式预测与系统预后框架，结合熵复杂度度量与先进神经网络架构，显著提升了复杂动态模式的理解能力。


<details>
  <summary>Details</summary>
Motivation: 解决多维系统中复杂动态模式理解的挑战，结合熵复杂度与神经网络技术。

Method: 采用双阶段计算：时空熵提取优化多尺度数据流，集成BEQRNN层实现概率预测与不确定性量化。

Result: 时空模式分类准确率达81.17%，预测范围达200时间步，关键转变检测精度提升79%，长期预测可靠性提高81.22%。

Conclusion: 框架在处理复杂多模态熵特征方面表现优异，具有实时预后应用的潜力。

Abstract: This paper presents a novel framework for pattern prediction and system
prognostics centered on Spatiotemporal Permutation Entropy analysis integrated
with Boosted Enhanced Quantile Regression Neural Networks (BEQRNNs). We address
the challenge of understanding complex dynamical patterns in multidimensional
systems through an approach that combines entropy-based complexity measures
with advanced neural architectures. The system leverages dual computational
stages: first implementing spatiotemporal entropy extraction optimized for
multiscale temporal and spatial data streams, followed by an integrated BEQRNN
layer that enables probabilistic pattern prediction with uncertainty
quantification. This architecture achieves 81.17% accuracy in spatiotemporal
pattern classification with prediction horizons up to 200 time steps and
maintains robust performance across diverse regimes. Field testing across
chaotic attractors, reaction-diffusion systems, and industrial datasets shows a
79% increase in critical transition detection accuracy and 81.22% improvement
in long-term prediction reliability. The framework's effectiveness in
processing complex, multimodal entropy features demonstrates significant
potential for real-time prognostic applications.

</details>


### [394] [Distributed Machine Learning Approach for Low-Latency Localization in Cell-Free Massive MIMO Systems](https://arxiv.org/abs/2507.14216)
*Manish Kumar,Tzu-Hsuan Chou,Byunghyun Lee,Nicolò Michelusi,David J. Love,Yaguang Zhang,James V. Krogmeier*

Main category: eess.SP

TL;DR: 提出了一种分布式机器学习框架，用于6G网络中基于指纹的低延迟定位，减少通信和计算负担。


<details>
  <summary>Details</summary>
Motivation: 支持实时应用的低延迟定位需求，适应6G网络架构。

Method: 每个接入点独立训练高斯过程回归模型，用户设备融合估计结果。

Result: 定位精度接近集中式方法，减少不确定性，降低延迟。

Conclusion: 分布式机器学习在6G网络中实现低延迟高精度定位具有潜力。

Abstract: Low-latency localization is critical in cellular networks to support
real-time applications requiring precise positioning. In this paper, we propose
a distributed machine learning (ML) framework for fingerprint-based
localization tailored to cell-free massive multiple-input multiple-output
(MIMO) systems, an emerging architecture for 6G networks. The proposed
framework enables each access point (AP) to independently train a Gaussian
process regression model using local angle-of-arrival and received signal
strength fingerprints. These models provide probabilistic position estimates
for the user equipment (UE), which are then fused by the UE with minimal
computational overhead to derive a final location estimate. This decentralized
approach eliminates the need for fronthaul communication between the APs and
the central processing unit (CPU), thereby reducing latency. Additionally,
distributing computational tasks across the APs alleviates the processing
burden on the CPU compared to traditional centralized localization schemes.
Simulation results demonstrate that the proposed distributed framework achieves
localization accuracy comparable to centralized methods, despite lacking the
benefits of centralized data aggregation. Moreover, it effectively reduces
uncertainty of the location estimates, as evidenced by the 95\% covariance
ellipse. The results highlight the potential of distributed ML for enabling
low-latency, high-accuracy localization in future 6G networks.

</details>


### [395] [Advanced Space Mapping Technique Integrating a Shared Coarse Model for Multistate Tuning-Driven Multiphysics Optimization of Tunable Filters](https://arxiv.org/abs/2507.14220)
*Haitian Hu,Wei Zhang,Feng Feng,Zhiguo Zhang,Qi-Jun Zhang*

Main category: eess.SP

TL;DR: 提出了一种基于共享电磁粗模型的高级空间映射技术，用于多状态调谐驱动的多物理优化，结合计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 解决多物理优化中计算效率与精度难以兼顾的问题。

Method: 使用共享电磁粗模型和映射神经网络构建替代模型，同时优化多状态调谐。

Result: 相比现有方法，实现了更高的建模精度，减少了训练样本和计算成本。

Conclusion: 该方法在多物理优化中具有显著优势，适用于多状态调谐需求。

Abstract: This article introduces an advanced space mapping (SM) technique that applies
a shared electromagnetic (EM)-based coarse model for multistate tuning-driven
multiphysics optimization of tunable filters. The SM method combines the
computational efficiency of EM single-physics simulations with the precision of
multiphysics simulations. The shared coarse model is based on EM single-physics
responses corresponding to various nontunable design parameters values.
Conversely, the fine model is implemented to delineate the behavior of
multiphysics responses concerning both nontunable and tunable design parameter
values. The proposed overall surrogate model comprises multiple subsurrogate
models, each consisting of one shared coarse model and two distinct mapping
neural networks. The responses from the shared coarse model in the EM
single-physics filed offer a suitable approximation for the fine responses in
the multiphysics filed, whereas the mapping neural networks facilitate
transition from the EM single-physics field to the multiphysics field. Each
subsurrogate model maintains consistent nontunable design parameter values but
possesses unique tunable design parameter values. By developing multiple
subsurrogate models, optimization can be simultaneously performed for each
tuning state. Nontunable design parameter values are constrained by all tuning
states, whereas tunable design parameter values are confined to their
respective tuning states. This optimization technique simultaneously accounts
for all the tuning states to fulfill the necessary multiple tuning state
requirements. Multiple EM and multiphysics training samples are generated
concurrently to develop the surrogate model. Compared with existing direct
multiphysics parameterized modeling techniques, our proposed method achieves
superior multiphysics modeling accuracy with fewer training samples and reduced
computational costs.

</details>


### [396] [Diffusion-based translation between unpaired spontaneous premature neonatal EEG and fetal MEG](https://arxiv.org/abs/2507.14224)
*Benoît Brebion,Alban Gallard,Katrin Sippel,Amer Zaylaa,Hubert Preissl,Sahar Moghimi,Fabrice Wallois,Yaël Frégier*

Main category: eess.SP

TL;DR: 利用人工智能将EEG知识迁移到fMEG，改进胎儿脑发育研究，提出一种基于双扩散桥的非配对扩散翻译方法，显著提升信号保真度。


<details>
  <summary>Details</summary>
Motivation: 传统EEG研究早产儿脑活动，但胎儿期关键窗口未知，fMEG数据稀缺且质量差，需改进方法。

Method: 开发基于双扩散桥的非配对扩散翻译方法，结合数值积分改进，训练于30例EEG和44例fMEG数据。

Result: 相比GAN，时间域均方误差提升5%，频率域完全避免模式崩溃，信号保真度接近完美。

Conclusion: 在EEG-fMEG非配对翻译领域达到新水平，为早期脑活动分析奠定基础，方法可推广至其他信号翻译应用。

Abstract: Background and objective: Brain activity in premature newborns has
traditionally been studied using electroencephalography (EEG), leading to
substantial advances in our understanding of early neural development. However,
since brain development takes root at the fetal stage, a critical window of
this process remains largely unknown. The only technique capable of recording
neural activity in the intrauterine environment is fetal magnetoencephalography
(fMEG), but this approach presents challenges in terms of data quality and
scarcity. Using artificial intelligence, the present research aims to transfer
the well-established knowledge from EEG studies to fMEG to improve
understanding of prenatal brain development, laying the foundations for better
detection and treatment of potential pathologies. Methods: We developed an
unpaired diffusion translation method based on dual diffusion bridges, which
notably includes numerical integration improvements to obtain more qualitative
results at a lower computational cost. Models were trained on our unpaired
dataset of bursts of spontaneous activity from 30 high-resolution premature
newborns EEG recordings and 44 fMEG recordings. Results: We demonstrate that
our method achieves significant improvement upon previous results obtained with
Generative Adversarial Networks (GANs), by almost 5% on the mean squared error
in the time domain, and completely eliminating the mode collapse problem in the
frequency domain, thus achieving near-perfect signal fidelity. Conclusion: We
set a new state of the art in the EEG-fMEG unpaired translation problem, as our
developed tool completely paves the way for early brain activity analysis.
Overall, we also believe that our method could be reused for other unpaired
signal translation applications.

</details>


### [397] [Graph Attention Networks for Detecting Epilepsy from EEG Signals Using Accessible Hardware in Low-Resource Settings](https://arxiv.org/abs/2507.15118)
*Szymon Mazurek,Stephen Moore,Alessandro Crimi*

Main category: eess.SP

TL;DR: 提出一种基于图神经网络的深度学习框架，用于低成本EEG硬件检测癫痫，在尼日利亚和几内亚比绍的测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决低收入国家癫痫诊断不足的问题，提供公平、可访问的自动评估方法。

Method: 将EEG信号建模为时空图，使用图注意力网络（GAT）分类并分析通道间关系，改进GAT以关注连接生物标志物，设计轻量级架构。

Result: 分类性能优于随机森林和图卷积网络，识别出额颞区的特定连接。

Conclusion: GAT为欠发达地区提供可扩展的癫痫诊断支持，推动低成本神经诊断工具的发展。

Abstract: Goal: Epilepsy remains under-diagnosed in low-income countries due to scarce
neurologists and costly diagnostic tools. We propose a graph-based deep
learning framework to detect epilepsy from low-cost Electroencephalography
(EEG) hardware, tested on recordings from Nigeria and Guinea-Bissau. Our focus
is on fair, accessible automatic assessment and explainability to shed light on
epilepsy biomarkers. Methods: We model EEG signals as spatio-temporal graphs,
classify them, and identify interchannel relationships and temporal dynamics
using graph attention networks (GAT). To emphasize connectivity biomarkers, we
adapt the inherently node-focused GAT to analyze edges. We also designed signal
preprocessing for low-fidelity recordings and a lightweight GAT architecture
trained on Google Colab and deployed on RaspberryPi devices. Results: The
approach achieves promising classification performance, outperforming a
standard classifier based on random forest and graph convolutional networks in
terms of accuracy and robustness over multiple sessions, but also highlighting
specific connections in the fronto-temporal region. Conclusions: The results
highlight the potential of GATs to provide insightful and scalable diagnostic
support for epilepsy in underserved regions, paving the way for affordable and
accessible neurodiagnostic tools.

</details>


### [398] [MEETI: A Multimodal ECG Dataset from MIMIC-IV-ECG with Signals, Images, Features and Interpretations](https://arxiv.org/abs/2507.15255)
*Deyun Zhang,Xiang Lan,Shijia Geng,Qinghao Zhao,Sumei Fan,Mengling Feng,Shenda Hong*

Main category: eess.SP

TL;DR: MEETI是首个大规模同步ECG波形、图像和文本解释的多模态数据集，填补了现有数据集的不足，支持可解释的多模态心血管AI研究。


<details>
  <summary>Details</summary>
Motivation: 现有ECG数据集多为单模态或双模态，难以支持多模态AI系统的开发，MEETI旨在解决这一问题。

Method: MEETI整合了原始ECG波形、高分辨率图像、详细文本解释及提取的特征参数，通过唯一标识符实现对齐。

Result: MEETI为多模态学习和可解释性分析提供了统一的数据基础，支持心血管AI的进一步发展。

Conclusion: MEETI为下一代可解释的多模态心血管AI奠定了坚实基础，是研究社区的宝贵资源。

Abstract: Electrocardiogram (ECG) plays a foundational role in modern cardiovascular
care, enabling non-invasive diagnosis of arrhythmias, myocardial ischemia, and
conduction disorders. While machine learning has achieved expert-level
performance in ECG interpretation, the development of clinically deployable
multimodal AI systems remains constrained, primarily due to the lack of
publicly available datasets that simultaneously incorporate raw signals,
diagnostic images, and interpretation text. Most existing ECG datasets provide
only single-modality data or, at most, dual modalities, making it difficult to
build models that can understand and integrate diverse ECG information in
real-world settings. To address this gap, we introduce MEETI (MIMIC-IV-Ext
ECG-Text-Image), the first large-scale ECG dataset that synchronizes raw
waveform data, high-resolution plotted images, and detailed textual
interpretations generated by large language models. In addition, MEETI includes
beat-level quantitative ECG parameters extracted from each lead, offering
structured parameters that support fine-grained analysis and model
interpretability. Each MEETI record is aligned across four components: (1) the
raw ECG waveform, (2) the corresponding plotted image, (3) extracted feature
parameters, and (4) detailed interpretation text. This alignment is achieved
using consistent, unique identifiers. This unified structure supports
transformer-based multimodal learning and supports fine-grained, interpretable
reasoning about cardiac health. By bridging the gap between traditional signal
analysis, image-based interpretation, and language-driven understanding, MEETI
established a robust foundation for the next generation of explainable,
multimodal cardiovascular AI. It offers the research community a comprehensive
benchmark for developing and evaluating ECG-based AI systems.

</details>


### [399] [Optimal Transceiver Design in Over-the-Air Federated Distillation](https://arxiv.org/abs/2507.15256)
*Zihao Hu,Jia Yan,Ying-Jun Angela Zhang,Jun Zhang,Khaled B. Letaief*

Main category: eess.SP

TL;DR: 提出了一种新颖的空中联合蒸馏（FD）框架，结合联合学习（FL）和知识蒸馏的优势，减少通信开销，优化收发器设计以提高学习收敛速度。


<details>
  <summary>Details</summary>
Motivation: 大型AI模型的出现使现有FL方法因通信开销大而效率低下，需新方法解决。

Method: 通过共享模型输出（知识）而非参数，利用多址信道的叠加特性进行空中聚合，优化收发器设计和功率分配。

Result: 推导了空中FD的收敛速率表达式，获得最优功率分配和接收器波束成形向量，通信开销显著降低，测试精度损失小。

Conclusion: 空中FD框架在减少通信开销的同时保持较高学习性能，优于传统FL方法。

Abstract: The rapid proliferation and growth of artificial intelligence (AI) has led to
the development of federated learning (FL). FL allows wireless devices (WDs) to
cooperatively learn by sharing only local model parameters, without needing to
share the entire dataset. However, the emergence of large AI models has made
existing FL approaches inefficient, due to the significant communication
overhead required. In this paper, we propose a novel over-the-air federated
distillation (FD) framework by synergizing the strength of FL and knowledge
distillation to avoid the heavy local model transmission. Instead of sharing
the model parameters, only the WDs' model outputs, referred to as knowledge,
are shared and aggregated over-the-air by exploiting the superposition property
of the multiple-access channel. We shall study the transceiver design in
over-the-air FD, aiming to maximize the learning convergence rate while meeting
the power constraints of the transceivers. The main challenge lies in the
intractability of the learning performance analysis, as well as the non-convex
nature and the optimization spanning the whole FD training period. To tackle
this problem, we first derive an analytical expression of the convergence rate
in over-the-air FD. Then, the closed-form optimal solutions of the WDs'
transmit power and the estimator for over-the-air aggregation are obtained
given the receiver combining strategy. Accordingly, we put forth an efficient
approach to find the optimal receiver beamforming vector via semidefinite
relaxation. We further prove that there is no optimality gap between the
original and relaxed problem for the receiver beamforming design. Numerical
results will show that the proposed over-the-air FD approach achieves a
significant reduction in communication overhead, with only a minor compromise
in testing accuracy compared to conventional FL benchmarks.

</details>


### [400] [EEG-based Epileptic Prediction via a Two-stage Channel-aware Set Transformer Network](https://arxiv.org/abs/2507.15364)
*Ruifeng Zheng,Cong Chen,Shuang Wang,Yiming Liu,Lin You,Jindong Lu,Ruizhe Zhu,Guodao Zhang,Kejie Huang*

Main category: eess.SP

TL;DR: 提出了一种新型的两阶段通道感知Set Transformer网络，用于减少癫痫预测所需的EEG通道传感器数量，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 癫痫发作对患者生活质量影响大，现有可穿戴预测设备因EEG设备体积大而受限。

Method: 采用两阶段通道感知Set Transformer网络和癫痫无关的数据划分方法。

Result: 通道选择后，平均通道数从18降至2.8，灵敏度从76.4%提升至80.1%，假阳性率为0.11/小时。

Conclusion: 通道感知网络有效减少了传感器数量，癫痫无关划分方法适用于EEG记录丰富的患者。

Abstract: Epilepsy is a chronic, noncommunicable brain disorder, and sudden seizure
onsets can significantly impact patients' quality of life and health. However,
wearable seizure-predicting devices are still limited, partly due to the bulky
size of EEG-collecting devices. To relieve the problem, we proposed a novel
two-stage channel-aware Set Transformer Network that could perform seizure
prediction with fewer EEG channel sensors. We also tested a seizure-independent
division method which could prevent the adjacency of training and test data.
Experiments were performed on the CHB-MIT dataset which includes 22 patients
with 88 merged seizures. The mean sensitivity before channel selection was
76.4% with a false predicting rate (FPR) of 0.09/hour. After channel selection,
dominant channels emerged in 20 out of 22 patients; the average number of
channels was reduced to 2.8 from 18; and the mean sensitivity rose to 80.1%
with an FPR of 0.11/hour. Furthermore, experimental results on the
seizure-independent division supported our assertion that a more rigorous
seizure-independent division should be used for patients with abundant EEG
recordings.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [401] [A Formal Model of the Economic Impacts of AI Openness Regulation](https://arxiv.org/abs/2507.14193)
*Tori Qiu,Benjamin Laufer,Jon Kleinberg,Hoda Heidari*

Main category: cs.GT

TL;DR: 论文分析了通用AI模型开放性的定义及其对开发者和市场的影响，提出了一个模型来评估不同开放性标准的经济激励效果。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决通用AI模型开放性定义的模糊性，以及如何通过法规激励开发者。

Method: 通过建模通用模型创建者（generalist）和微调者（specialist）的策略互动，分析不同开放性法规对市场均衡的影响。

Result: 结果表明，模型的基线性能决定了增加监管惩罚或开放性阈值对通用模型发布策略的影响。

Conclusion: 研究为AI治理决策提供了理论基础，有助于评估和完善实际的开源政策。

Abstract: Regulatory frameworks, such as the EU AI Act, encourage openness of
general-purpose AI models by offering legal exemptions for "open-source"
models. Despite this legislative attention on openness, the definition of
open-source foundation models remains ambiguous. This paper models the
strategic interactions among the creator of a general-purpose model (the
generalist) and the entity that fine-tunes the general-purpose model to a
specialized domain or task (the specialist), in response to regulatory
requirements on model openness. We present a stylized model of the regulator's
choice of an open-source definition to evaluate which AI openness standards
will establish appropriate economic incentives for developers. Our results
characterize market equilibria -- specifically, upstream model release
decisions and downstream fine-tuning efforts -- under various openness
regulations and present a range of effective regulatory penalties and
open-source thresholds. Overall, we find the model's baseline performance
determines when increasing the regulatory penalty vs. the open-source threshold
will significantly alter the generalist's release strategy. Our model provides
a theoretical foundation for AI governance decisions around openness and
enables evaluation and refinement of practical open-source policies.

</details>


### [402] [Strategyproofness and Monotone Allocation of Auction in Social Networks](https://arxiv.org/abs/2507.14472)
*Yuhang Guo,Dong Hao,Bin Li,Mingyu Xiao,Bakh Khoussainov*

Main category: cs.GT

TL;DR: 论文研究了网络拍卖中的策略证明性，提出了两种单调分配规则（ID-MON和IP-MON），并解决了单需求组合网络拍卖的难题。


<details>
  <summary>Details</summary>
Motivation: 现有网络拍卖缺乏通用的策略证明分配规则，导致多单位网络拍卖中策略证明性难以实现。

Method: 提出两种单调分配规则（ID-MON和IP-MON），并分析其策略证明支付规则的存在性和计算可行性。

Result: 证明了在ID-MON和IP-MON规则下，存在收益最大化的策略证明支付规则。

Conclusion: 通过新提出的分配规则，解决了单需求组合网络拍卖的策略证明性问题。

Abstract: Strategyproofness in network auctions requires that bidders not only report
their valuations truthfully, but also do their best to invite neighbours from
the social network. In contrast to canonical auctions, where the value-monotone
allocation in Myerson's Lemma is a cornerstone, a general principle of
allocation rules for strategyproof network auctions is still missing. We show
that, due to the absence of such a principle, even extensions to multi-unit
network auctions with single-unit demand present unexpected difficulties, and
all pioneering researches fail to be strategyproof. For the first time in this
field, we identify two categories of monotone allocation rules on networks:
Invitation-Depressed Monotonicity (ID-MON) and Invitation-Promoted Monotonicity
(IP-MON). They encompass all existing allocation rules of network auctions as
specific instances. For any given ID-MON or IP-MON allocation rule, we
characterize the existence and sufficient conditions for the strategyproof
payment rules, and show that among all such payment rules, the
revenue-maximizing one exists and is computationally feasible. With these
results, the obstacle of combinatorial network auction with single-minded
bidders is now resolved.

</details>


### [403] [Probing EFX via PMMS: (Non-)Existence Results in Discrete Fair Division](https://arxiv.org/abs/2507.14957)
*Jarosław Byrka,Franciszek Malinka,Tomasz Ponitka*

Main category: cs.GT

TL;DR: 论文研究了不可分割物品的公平分配问题，重点分析了EFX和PMMS问题，证明了PMMS在某些情况下不存在，同时给出了三种特殊情况下公平分配的存在性及多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 解决公平分配中的核心开放问题EFX及其更强变体PMMS，探索其存在性和计算复杂性。

Method: 通过构造实例证明PMMS的不存在性，并在个性化二值估值、二进制估值和配对需求估值三种特殊情况下，构造性地证明公平分配的存在性。

Result: 证明了PMMS在特定情况下不存在，但在三种特殊情况下存在公平分配，并提供了多项式时间算法。

Conclusion: 论文为公平分配问题提供了新的理论结果和算法支持，揭示了EFX与PMMS之间的差异。

Abstract: We study the fair division of indivisible items and provide new insights into
the EFX problem, which is widely regarded as the central open question in fair
division, and the PMMS problem, a strictly stronger variant of EFX. Our first
result constructs a three-agent instance with two monotone valuations and one
additive valuation in which no PMMS allocation exists. Since EFX allocations
are known to exist under these assumptions, this establishes a formal
separation between EFX and PMMS.
  We prove existence of fair allocations for three important special cases. We
show that EFX allocations exist for personalized bivalued valuations, where for
each agent $i$ there exist values $a_i > b_i$ such that agent $i$ assigns value
$v_i(\{g\}) \in \{a_i, b_i\}$ to each good $g$. We establish an analogous
existence result for PMMS allocations when $a_i$ is divisible by $b_i$. We also
prove that PMMS allocations exist for binary-valued MMS-feasible valuations,
where each bundle $S$ has value $v_i(S) \in \{0, 1\}$. Notably, this result
holds even without assuming monotonicity of valuations and thus applies to the
fair division of chores and mixed manna. Finally, we study a class of
valuations called pair-demand valuations, which extend the well-studied
unit-demand valuations to the case where each agent derives value from at most
two items, and we show that PMMS allocations exist in this setting. Our proofs
are constructive, and we provide polynomial-time algorithms for all three
existence results.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [404] [Identifying Algorithmic and Domain-Specific Bias in Parliamentary Debate Summarisation](https://arxiv.org/abs/2507.14221)
*Eoghan Cunningham,James Cross,Derek Greene*

Main category: cs.CY

TL;DR: 本文探讨了使用大型语言模型（LLMs）自动总结欧洲议会辩论的方法，并研究了其中存在的算法和代表性偏见。提出了一种多阶段总结框架，以减少偏见并提高内容准确性。


<details>
  <summary>Details</summary>
Motivation: 使复杂的立法辩论更易于公众理解，同时确保所有发言者的观点被公平代表。

Method: 提出了一种结构化的多阶段总结框架，分析了发言者属性（如发言顺序或政治立场）对总结结果的影响。

Result: 实验发现存在位置和党派偏见，某些发言者被低估或误归因。分层方法在减少偏见方面表现最佳。

Conclusion: 强调了在民主应用中使用LLMs时需要领域敏感的评估指标和伦理监督。

Abstract: The automated summarisation of parliamentary debates using large language
models (LLMs) offers a promising way to make complex legislative discourse more
accessible to the public. However, such summaries must not only be accurate and
concise but also equitably represent the views and contributions of all
speakers. This paper explores the use of LLMs to summarise plenary debates from
the European Parliament and investigates the algorithmic and representational
biases that emerge in this context. We propose a structured, multi-stage
summarisation framework that improves textual coherence and content fidelity,
while enabling the systematic analysis of how speaker attributes -- such as
speaking order or political affiliation -- influence the visibility and
accuracy of their contributions in the final summaries. Through our experiments
using both proprietary and open-weight LLMs, we find evidence of consistent
positional and partisan biases, with certain speakers systematically
under-represented or misattributed. Our analysis shows that these biases vary
by model and summarisation strategy, with hierarchical approaches offering the
greatest potential to reduce disparity. These findings underscore the need for
domain-sensitive evaluation metrics and ethical oversight in the deployment of
LLMs for democratic applications.

</details>


### [405] [Cognitive Castes: Artificial Intelligence, Epistemic Stratification, and the Dissolution of Democratic Discourse](https://arxiv.org/abs/2507.14218)
*Craig S Wright*

Main category: cs.CY

TL;DR: AI加剧认知分层，强化信息阶层，削弱民主审议能力。


<details>
  <summary>Details</summary>
Motivation: 探讨AI如何通过认知分层影响社会权力结构，削弱民主审议能力。

Method: 结合形式认识论、政治理论、算法架构和经济激励结构，分析AI对认知能力的影响。

Result: AI放大具备抽象和逻辑能力者的优势，同时通过优化界面削弱未训练者的认知能力。

Conclusion: 需重建理性自主作为公民义务，通过教育和认知基础设施保护认知权利。

Abstract: Artificial intelligence functions not as an epistemic leveller, but as an
accelerant of cognitive stratification, entrenching and formalising
informational castes within liberal-democratic societies. Synthesising formal
epistemology, political theory, algorithmic architecture, and economic
incentive structures, the argument traces how contemporary AI systems
selectively amplify the reasoning capacity of individuals equipped with
recursive abstraction, symbolic logic, and adversarial interrogation, whilst
simultaneously pacifying the cognitively untrained through engagement-optimised
interfaces. Fluency replaces rigour, immediacy displaces reflection, and
procedural reasoning is eclipsed by reactive suggestion. The result is a
technocratic realignment of power: no longer grounded in material capital
alone, but in the capacity to navigate, deconstruct, and manipulate systems of
epistemic production. Information ceases to be a commons; it becomes the
substrate through which consent is manufactured and autonomy subdued.
Deliberative democracy collapses not through censorship, but through the
erosion of interpretive agency. The proposed response is not technocratic
regulation, nor universal access, but the reconstruction of rational autonomy
as a civic mandate, codified in education, protected by epistemic rights, and
structurally embedded within open cognitive infrastructure.

</details>


### [406] [Culling Misinformation from Gen AI: Toward Ethical Curation and Refinement](https://arxiv.org/abs/2507.14242)
*Prerana Khatiwada,Grace Donaher,Jasymyn Navarro,Lokesh Bhatta*

Main category: cs.CY

TL;DR: 论文探讨了AI（尤其是ChatGPT和深度伪造）带来的公平问题和错误信息传播，提出需多方合作以减少危害，并建议未来政策指导。


<details>
  <summary>Details</summary>
Motivation: AI技术（如ChatGPT）的快速发展带来了自动化便利，但也引发公平问题和错误信息传播的风险，需深入理解并减少其负面影响。

Method: 通过分析学术资料，研究AI在医疗、教育、科学等领域的负面影响，并提出政策建议。

Result: 揭示了AI技术可能导致的公平问题和错误信息传播，强调多方合作的重要性。

Conclusion: 提出未来政策指导，呼吁用户、开发者和政府共同承担责任，平衡创新与风险。

Abstract: While Artificial Intelligence (AI) is not a new field, recent developments,
especially with the release of generative tools like ChatGPT, have brought it
to the forefront of the minds of industry workers and academic folk alike.
There is currently much talk about AI and its ability to reshape many everyday
processes as we know them through automation. It also allows users to expand
their ideas by suggesting things they may not have thought of on their own and
provides easier access to information. However, not all of the changes this
technology will bring or has brought so far are positive; this is why it is
extremely important for all modern people to recognize and understand the risks
before using these tools and allowing them to cause harm. This work takes a
position on better understanding many equity concerns and the spread of
misinformation that result from new AI, in this case, specifically ChatGPT and
deepfakes, and encouraging collaboration with law enforcement, developers, and
users to reduce harm. Considering many academic sources, it warns against these
issues, analyzing their cause and impact in fields including healthcare,
education, science, academia, retail, and finance. Lastly, we propose a set of
future-facing guidelines and policy considerations to solve these issues while
still enabling innovation in these fields, this responsibility falling upon
users, developers, and government entities.

</details>


### [407] [Bridging MOOCs, Smart Teaching, and AI: A Decade of Evolution Toward a Unified Pedagogy](https://arxiv.org/abs/2507.14266)
*Bo Yuan,Jiazi Hu*

Main category: cs.CY

TL;DR: 论文提出了一种结合MOOCs、智能教学和AI增强学习的三层教学框架，旨在解决传统教育中的挑战，并通过课程设计验证其可行性。


<details>
  <summary>Details</summary>
Motivation: 传统教育面临资源访问、实时互动和个性化反馈的挑战，现有技术范式（MOOCs、智能教学、AI）各自独立，缺乏整合。

Method: 提出三层教学框架，结合MOOCs的可扩展性、智能教学的实时响应性和AI的适应性，并通过项目课程设计验证。

Result: 框架能提升学习者参与度、支持教师，并实现个性化且可扩展的学习。

Conclusion: 整合三种范式可互补优势，为教育提供更高效、灵活的教学解决方案。

Abstract: Over the past decade, higher education has evolved through three distinct
paradigms: the emergence of Massive Open Online Courses (MOOCs), the
integration of Smart Teaching technologies into classrooms, and the rise of
AI-enhanced learning. Each paradigm is intended to address specific challenges
in traditional education: MOOCs enable ubiquitous access to learning resources;
Smart Teaching supports real-time interaction with data-driven insights; and
generative AI offers personalized feedback and on-demand content generation.
However, these paradigms are often implemented in isolation due to their
disparate technological origins and policy-driven adoption. This paper examines
the origins, strengths, and limitations of each paradigm, and advocates a
unified pedagogical perspective that synthesizes their complementary
affordances. We propose a three-layer instructional framework that combines the
scalability of MOOCs, the responsiveness of Smart Teaching, and the adaptivity
of AI. To demonstrate its feasibility, we present a curriculum design for a
project-based course. The findings highlight the framework's potential to
enhance learner engagement, support instructors, and enable personalized yet
scalable learning.

</details>


### [408] [Fiduciary AI for the Future of Brain-Technology Interactions](https://arxiv.org/abs/2507.14339)
*Abhishek Bhattacharjee,Jack Pilkington,Nita Farahany*

Main category: cs.CY

TL;DR: 论文提出将信托责任（忠诚、谨慎、保密）嵌入基于脑信号的基础模型设计中，以解决其潜在风险，如认知自由侵蚀和神经信号滥用。


<details>
  <summary>Details</summary>
Motivation: 脑基础模型与脑机接口结合可能带来革命性应用，但也存在前所未有的风险，如用户无法控制神经信号的解读，导致权力不对称和操纵风险。

Method: 结合法律传统和AI对齐技术，提出可实施的架构和治理机制，确保系统以用户利益为核心。

Result: 通过技术设计将信托责任嵌入模型，可平衡潜力与风险，保障用户自主权。

Conclusion: 将脑基础模型置于信托责任框架下，是实现其潜力同时保护用户自主权的关键。

Abstract: Brain foundation models represent a new frontier in AI: instead of processing
text or images, these models interpret real-time neural signals from EEG, fMRI,
and other neurotechnologies. When integrated with brain-computer interfaces
(BCIs), they may enable transformative applications-from thought controlled
devices to neuroprosthetics-by interpreting and acting on brain activity in
milliseconds. However, these same systems pose unprecedented risks, including
the exploitation of subconscious neural signals and the erosion of cognitive
liberty. Users cannot easily observe or control how their brain signals are
interpreted, creating power asymmetries that are vulnerable to manipulation.
This paper proposes embedding fiduciary duties-loyalty, care, and
confidentiality-directly into BCI-integrated brain foundation models through
technical design. Drawing on legal traditions and recent advancements in AI
alignment techniques, we outline implementable architectural and governance
mechanisms to ensure these systems act in users' best interests. Placing brain
foundation models on a fiduciary footing is essential to realizing their
potential without compromising self-determination.

</details>


### [409] [Mining Voter Behaviour and Confidence: A Rule-Based Analysis of the 2022 U.S. Elections](https://arxiv.org/abs/2507.14236)
*Md Al Jubair,Mohammad Shamsul Arefin,Ahmed Wasif Reza*

Main category: cs.CY

TL;DR: 研究通过Apriori算法分析2022年美国选举表现调查数据，发现选民信任与选举体验（如投票站易达性、注册问题等）密切相关，尤其是少数族裔群体。


<details>
  <summary>Details</summary>
Motivation: 探讨选民信任与选举体验之间的关系，特别关注少数族裔群体的投票障碍。

Method: 使用Apriori算法（支持度≥3%，置信度≥60%，提升度>1.5）分析2022年SPAE数据，调整支持度为2%以研究少数族裔模式。

Result: 投票站易达性与高信任度显著相关（提升度6.12）；少数族裔中，98.16%的黑人选民报告易达性与顺利注册相关；高信任度选民更倾向民主党。

Conclusion: 提升投票易达性和针对性支持对增强选举信任，尤其是边缘化群体，具有重要作用。

Abstract: This study explores the relationship between voter trust and their
experiences during elections by applying a rule-based data mining technique to
the 2022 Survey of the Performance of American Elections (SPAE). Using the
Apriori algorithm and setting parameters to capture meaningful associations
(support >= 3%, confidence >= 60%, and lift > 1.5), the analysis revealed a
strong connection between demographic attributes and voting-related challenges,
such as registration hurdles, accessibility issues, and queue times. For
instance, respondents who indicated that accessing polling stations was "very
easy" and who reported moderate confidence were found to be over six times more
likely (lift = 6.12) to trust their county's election outcome and experience no
registration issues. A further analysis, which adjusted the support threshold
to 2%, specifically examined patterns among minority voters. It revealed that
98.16 percent of Black voters who reported easy access to polling locations
also had smooth registration experiences. Additionally, those who had high
confidence in the vote-counting process were almost two times as likely to
identify as Democratic Party supporters. These findings point to the important
role that enhancing voting access and offering targeted support can play in
building trust in the electoral system, particularly among marginalized
communities.

</details>


### [410] [Unequal Voices: How LLMs Construct Constrained Queer Narratives](https://arxiv.org/abs/2507.15585)
*Atreya Ghosal,Ashim Gupta,Vivek Srikumar*

Main category: cs.CY

TL;DR: 论文研究了LLM生成内容中对酷儿群体的刻板化、狭窄化和边缘化表现，并验证了这些现象的假设。


<details>
  <summary>Details</summary>
Motivation: 探讨社会群体在话语中被边缘化的方式，尤其是LLM生成内容中对酷儿群体的限制性表现。

Method: 通过分析LLM生成的酷儿角色内容，识别有害表现、狭窄表现和话语他者化现象。

Result: 结果显示LLM对酷儿角色的描绘存在显著局限性。

Conclusion: LLM在表现酷儿群体时存在刻板化和狭窄化问题，需要改进以避免边缘化。

Abstract: One way social groups are marginalized in discourse is that the narratives
told about them often default to a narrow, stereotyped range of topics. In
contrast, default groups are allowed the full complexity of human existence. We
describe the constrained representations of queer people in LLM generations in
terms of harmful representations, narrow representations, and discursive
othering and formulate hypotheses to test for these phenomena. Our results show
that LLMs are significantly limited in their portrayals of queer personas.

</details>


### [411] [Why can't Epidemiology be automated (yet)?](https://arxiv.org/abs/2507.15617)
*David Bann,Ed Lowther,Liam Wright,Yevgeniya Kovalchuk*

Main category: cs.CY

TL;DR: 生成式AI为流行病学研究提供了加速或自动化的新机会，但其应用范围和局限性尚不明确。


<details>
  <summary>Details</summary>
Motivation: 探索AI在流行病学任务中的潜力，识别其效率提升点和障碍。

Method: 通过现有数据集和AI工具，分析流行病学任务（如文献综述、数据分析等）中AI的应用效果。

Result: AI在编码和行政任务中能提高效率，但受限于模型缺陷（如幻觉）和人为障碍（如数据访问）。

Conclusion: AI在流行病学中的应用需流行病学家与工程师的双向合作，以充分发挥其潜力。

Abstract: Recent advances in artificial intelligence (AI) - particularly generative AI
- present new opportunities to accelerate, or even automate, epidemiological
research. Unlike disciplines based on physical experimentation, a sizable
fraction of Epidemiology relies on secondary data analysis and thus is
well-suited for such augmentation. Yet, it remains unclear which specific tasks
can benefit from AI interventions or where roadblocks exist. Awareness of
current AI capabilities is also mixed. Here, we map the landscape of
epidemiological tasks using existing datasets - from literature review to data
access, analysis, writing up, and dissemination - and identify where existing
AI tools offer efficiency gains. While AI can increase productivity in some
areas such as coding and administrative tasks, its utility is constrained by
limitations of existing AI models (e.g. hallucinations in literature reviews)
and human systems (e.g. barriers to accessing datasets). Through examples of
AI-generated epidemiological outputs, including fully AI-generated papers, we
demonstrate that recently developed agentic systems can now design and execute
epidemiological analysis, albeit to varied quality (see
https://github.com/edlowther/automated-epidemiology). Epidemiologists have new
opportunities to empirically test and benchmark AI systems; realising the
potential of AI will require two-way engagement between epidemiologists and
engineers.

</details>


### [412] [Left Leaning Models: AI Assumptions on Economic Policy](https://arxiv.org/abs/2507.15771)
*Maxim Chupilkin*

Main category: cs.CY

TL;DR: 论文通过联合实验分析大型语言模型（LLMs）对经济政策的评估因素，发现其对失业、不平等、金融稳定和环境问题更敏感，而对经济增长、通胀和政府债务等传统宏观经济问题较不敏感。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在经济政策评估中的隐含假设，揭示其对不同经济问题的敏感度。

Method: 采用联合实验方法，分析LLMs对经济政策的评估因素。

Result: LLMs对失业、不平等、金融稳定和环境问题更敏感，而对传统宏观经济问题较不敏感，结果在不同场景和模型中一致。

Conclusion: LLMs在经济政策评估中更关注社会和环境问题，而非传统宏观经济指标。

Abstract: How does AI think about economic policy? While the use of large language
models (LLMs) in economics is growing exponentially, their assumptions on
economic issues remain a black box. This paper uses a conjoint experiment to
tease out the main factors influencing LLMs' evaluation of economic policy. It
finds that LLMs are most sensitive to unemployment, inequality, financial
stability, and environmental harm and less sensitive to traditional
macroeconomic concerns such as economic growth, inflation, and government debt.
The results are remarkably consistent across scenarios and across models.

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [413] [Efficient Whole Slide Pathology VQA via Token Compression](https://arxiv.org/abs/2507.14497)
*Weimin Lyu,Qingqiao Hu,Kehan Qi,Zhan Shi,Wentao Huang,Saumya Gupta,Chao Chen*

Main category: cs.CV

TL;DR: TCP-LLaVA是一种新型多模态大语言模型，通过令牌压缩技术解决病理学全切片图像（WSI）视觉问答（VQA）中的高计算资源消耗问题。


<details>
  <summary>Details</summary>
Motivation: 病理学全切片图像（WSI）尺寸巨大，现有方法难以高效处理其长上下文和高计算需求，且缺乏生成能力。

Method: 提出TCP-LLaVA，通过可训练的压缩令牌聚合视觉和文本信息，仅将压缩后的令牌输入语言模型，显著减少计算成本。

Result: 在十种TCGA肿瘤亚型上，TCP-LLaVA在VQA准确率上优于现有基线，同时大幅降低训练资源消耗。

Conclusion: TCP-LLaVA为WSI的VQA任务提供了一种高效且资源友好的解决方案。

Abstract: Whole-slide images (WSIs) in pathology can reach up to 10,000 x 10,000
pixels, posing significant challenges for multimodal large language model
(MLLM) due to long context length and high computational demands. Previous
methods typically focus on patch-level analysis or slide-level classification
using CLIP-based models with multi-instance learning, but they lack the
generative capabilities needed for visual question answering (VQA). More recent
MLLM-based approaches address VQA by feeding thousands of patch tokens directly
into the language model, which leads to excessive resource consumption. To
address these limitations, we propose Token Compression Pathology LLaVA
(TCP-LLaVA), the first MLLM architecture to perform WSI VQA via token
compression. TCP-LLaVA introduces a set of trainable compression tokens that
aggregate visual and textual information through a modality compression module,
inspired by the [CLS] token mechanism in BERT. Only the compressed tokens are
forwarded to the LLM for answer generation, significantly reducing input length
and computational cost. Experiments on ten TCGA tumor subtypes show that
TCP-LLaVA outperforms existing MLLM baselines in VQA accuracy while reducing
training resource consumption by a substantial margin.

</details>


### [414] [Docopilot: Improving Multimodal Models for Document-Level Understanding](https://arxiv.org/abs/2507.14675)
*Yuchen Duan,Zhe Chen,Yusong Hu,Weiyun Wang,Shenglong Ye,Botian Shi,Lewei Lu,Qibin Hou,Tong Lu,Hongsheng Li,Jifeng Dai,Wenhai Wang*

Main category: cs.CV

TL;DR: 论文提出了一种高质量文档级数据集Doc-750K，并开发了原生多模态模型Docopilot，解决了现有MLLMs在复杂文档理解中的不足。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在复杂多页文档理解上表现不足，主要由于缺乏高质量的文档级数据集，且现有检索增强生成方法存在碎片化检索上下文和多阶段错误累积等问题。

Method: 构建Doc-750K数据集，包含多样化文档结构和跨页依赖关系，并基于此开发原生多模态模型Docopilot。

Result: Docopilot在文档理解任务和多轮交互中表现出更高的连贯性、准确性和效率。

Conclusion: Docopilot为文档级多模态理解设定了新基准，相关数据、代码和模型已开源。

Abstract: Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

</details>


### [415] [DFQ-ViT: Data-Free Quantization for Vision Transformers without Fine-tuning](https://arxiv.org/abs/2507.14481)
*Yujia Tong,Jingling Yuan,Tian Zhang,Jianquan Liu,Chuang Hu*

Main category: cs.CV

TL;DR: DFQ-ViT是一种无需数据的视觉变换器量化方法，通过合成样本和激活校正矩阵提升量化模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在合成样本时未能平衡全局和局部特征，且量化模型与全精度模型的中间层激活分布差异大，导致性能下降。

Method: 提出DFQ-ViT，按难度递增顺序合成样本，并引入激活校正矩阵对齐量化与全精度模型的中间层激活。

Result: 实验表明DFQ-ViT优于现有方法，性能接近真实数据量化模型，如3位量化DeiT-T性能提升4.29%。

Conclusion: DFQ-ViT无需微调，降低计算开销和部署门槛，符合绿色学习原则，适用于资源受限环境。

Abstract: Data-Free Quantization (DFQ) enables the quantization of Vision Transformers
(ViTs) without requiring access to data, allowing for the deployment of ViTs on
devices with limited resources. In DFQ, the quantization model must be
calibrated using synthetic samples, making the quality of these synthetic
samples crucial. Existing methods fail to fully capture and balance the global
and local features within the samples, resulting in limited synthetic data
quality. Moreover, we have found that during inference, there is a significant
difference in the distributions of intermediate layer activations between the
quantized and full-precision models. These issues lead to a severe performance
degradation of the quantized model. To address these problems, we propose a
pipeline for Data-Free Quantization for Vision Transformers (DFQ-ViT).
Specifically, we synthesize samples in order of increasing difficulty,
effectively enhancing the quality of synthetic data. During the calibration and
inference stage, we introduce the activation correction matrix for the
quantized model to align the intermediate layer activations with those of the
full-precision model. Extensive experiments demonstrate that DFQ-ViT achieves
remarkable superiority over existing DFQ methods and its performance is on par
with models quantized through real data. For example, the performance of DeiT-T
with 3-bit weights quantization is 4.29% higher than the state-of-the-art. Our
method eliminates the need for fine-tuning, which not only reduces
computational overhead but also lowers the deployment barriers for edge
devices. This characteristic aligns with the principles of Green Learning by
improving energy efficiency and facilitating real-world applications in
resource-constrained environments.

</details>


### [416] [Benefit from Reference: Retrieval-Augmented Cross-modal Point Cloud Completion](https://arxiv.org/abs/2507.14485)
*Hongye Hou,Liu Zhan,Yang Yang*

Main category: cs.CV

TL;DR: 提出了一种基于检索增强的点云补全框架，通过跨模态检索学习结构先验信息，生成细粒度点云。


<details>
  <summary>Details</summary>
Motivation: 解决不完整点云补全任务中缺乏典型结构特征的问题，并提升生成能力和泛化能力。

Method: 设计了结构共享特征编码器（SSFE）和渐进检索增强生成器（PRAG），结合跨模态检索和层次特征融合。

Result: 在多个数据集和真实场景中验证了方法的有效性，能够处理稀疏数据和未见类别。

Conclusion: 该方法通过检索增强和特征融合，显著提升了点云补全的生成能力和泛化能力。

Abstract: Completing the whole 3D structure based on an incomplete point cloud is a
challenging task, particularly when the residual point cloud lacks typical
structural characteristics. Recent methods based on cross-modal learning
attempt to introduce instance images to aid the structure feature learning.
However, they still focus on each particular input class, limiting their
generation abilities. In this work, we propose a novel retrieval-augmented
point cloud completion framework. The core idea is to incorporate cross-modal
retrieval into completion task to learn structural prior information from
similar reference samples. Specifically, we design a Structural Shared Feature
Encoder (SSFE) to jointly extract cross-modal features and reconstruct
reference features as priors. Benefiting from a dual-channel control gate in
the encoder, relevant structural features in the reference sample are enhanced
and irrelevant information interference is suppressed. In addition, we propose
a Progressive Retrieval-Augmented Generator (PRAG) that employs a hierarchical
feature fusion mechanism to integrate reference prior information with input
features from global to local. Through extensive evaluations on multiple
datasets and real-world scenes, our method shows its effectiveness in
generating fine-grained point clouds, as well as its generalization capability
in handling sparse data and unseen categories.

</details>


### [417] [Multimodal AI for Gastrointestinal Diagnostics: Tackling VQA in MEDVQA-GI 2025](https://arxiv.org/abs/2507.14544)
*Sujata Gaihre,Amir Thapa Magar,Prasuna Pokharel,Laxmi Tiwari*

Main category: cs.CV

TL;DR: 本文介绍了针对ImageCLEFmed MEDVQA 2025挑战赛子任务1的视觉问答（VQA）方法，采用Florence模型作为主干，结合领域特定增强技术提升泛化能力，在KASVIR数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决胃肠道内窥镜视觉问答任务，提升医学VQA的准确性和泛化能力。

Method: 采用Florence多模态基础模型，结合视觉和文本编码器，并应用领域特定数据增强技术。

Result: 在KASVIR数据集上，微调Florence模型在官方挑战指标上表现优异。

Conclusion: 大型多模态模型在医学VQA中具有潜力，为未来研究提供了强基线。

Abstract: This paper describes our approach to Subtask 1 of the ImageCLEFmed MEDVQA
2025 Challenge, which targets visual question answering (VQA) for
gastrointestinal endoscopy. We adopt the Florence model-a large-scale
multimodal foundation model-as the backbone of our VQA pipeline, pairing a
powerful vision encoder with a text encoder to interpret endoscopic images and
produce clinically relevant answers. To improve generalization, we apply
domain-specific augmentations that preserve medical features while increasing
training diversity. Experiments on the KASVIR dataset show that fine-tuning
Florence yields accurate responses on the official challenge metrics. Our
results highlight the potential of large multimodal models in medical VQA and
provide a strong baseline for future work on explainability, robustness, and
clinical integration. The code is publicly available at:
https://github.com/TiwariLaxuu/VQA-Florence.git

</details>


### [418] [Benchmarking GANs, Diffusion Models, and Flow Matching for T1w-to-T2w MRI Translation](https://arxiv.org/abs/2507.14575)
*Andrea Moschetto,Lemuel Puglisi,Alec Sargood,Pierluigi Dell'Acqua,Francesco Guarnera,Sebastiano Battiato,Daniele Ravì*

Main category: cs.CV

TL;DR: 该论文对比了GAN、扩散模型和流匹配技术在T1w到T2w MRI图像转换中的表现，发现GAN-based Pix2Pix模型在结构保真度、图像质量和计算效率上优于其他方法。


<details>
  <summary>Details</summary>
Motivation: 减少MRI扫描时间和成本，通过计算合成缺失的对比度图像。

Method: 使用GAN、扩散模型和流匹配技术进行T1w到T2w的2D MRI图像转换，并在三个公开数据集上评估。

Result: GAN-based Pix2Pix模型表现最佳，流匹配模型在小数据集上容易过拟合。

Conclusion: GAN是当前最实用的MRI图像转换方法，未来研究需关注流匹配模型的数据需求。

Abstract: Magnetic Resonance Imaging (MRI) enables the acquisition of multiple image
contrasts, such as T1-weighted (T1w) and T2-weighted (T2w) scans, each offering
distinct diagnostic insights. However, acquiring all desired modalities
increases scan time and cost, motivating research into computational methods
for cross-modal synthesis. To address this, recent approaches aim to synthesize
missing MRI contrasts from those already acquired, reducing acquisition time
while preserving diagnostic quality. Image-to-image (I2I) translation provides
a promising framework for this task. In this paper, we present a comprehensive
benchmark of generative models$\unicode{x2013}$specifically, Generative
Adversarial Networks (GANs), diffusion models, and flow matching (FM)
techniques$\unicode{x2013}$for T1w-to-T2w 2D MRI I2I translation. All
frameworks are implemented with comparable settings and evaluated on three
publicly available MRI datasets of healthy adults. Our quantitative and
qualitative analyses show that the GAN-based Pix2Pix model outperforms
diffusion and FM-based methods in terms of structural fidelity, image quality,
and computational efficiency. Consistent with existing literature, these
results suggest that flow-based models are prone to overfitting on small
datasets and simpler tasks, and may require more data to match or surpass GAN
performance. These findings offer practical guidance for deploying I2I
translation techniques in real-world MRI workflows and highlight promising
directions for future research in cross-modal medical image synthesis. Code and
models are publicly available at
https://github.com/AndreaMoschetto/medical-I2I-benchmark.

</details>


### [419] [Performance comparison of medical image classification systems using TensorFlow Keras, PyTorch, and JAX](https://arxiv.org/abs/2507.14587)
*Merjem Bećirović,Amina Kurtović,Nordin Smajlović,Medina Kapo,Amila Akagić*

Main category: cs.CV

TL;DR: 本文比较了TensorFlow、PyTorch和JAX在血液细胞图像分类中的性能，重点关注推理时间和分类准确性。


<details>
  <summary>Details</summary>
Motivation: 研究不同深度学习框架在医学图像分类中的表现差异，填补现有研究的空白。

Method: 使用BloodMNIST数据集，比较TensorFlow、PyTorch和JAX在推理时间和不同图像尺寸下的分类性能。

Result: JAX和PyTorch的分类准确性接近当前基准，性能差异受图像分辨率和框架优化影响。

Conclusion: JAX和PyTorch在医学图像分类中表现高效，适合相关应用。

Abstract: Medical imaging plays a vital role in early disease diagnosis and monitoring.
Specifically, blood microscopy offers valuable insights into blood cell
morphology and the detection of hematological disorders. In recent years, deep
learning-based automated classification systems have demonstrated high
potential in enhancing the accuracy and efficiency of blood image analysis.
However, a detailed performance analysis of specific deep learning frameworks
appears to be lacking. This paper compares the performance of three popular
deep learning frameworks, TensorFlow with Keras, PyTorch, and JAX, in
classifying blood cell images from the publicly available BloodMNIST dataset.
The study primarily focuses on inference time differences, but also
classification performance for different image sizes. The results reveal
variations in performance across frameworks, influenced by factors such as
image resolution and framework-specific optimizations. Classification accuracy
for JAX and PyTorch was comparable to current benchmarks, showcasing the
efficiency of these frameworks for medical image classification.

</details>


### [420] [Exp-Graph: How Connections Learn Facial Attributes in Graph-based Expression Recognition](https://arxiv.org/abs/2507.14608)
*Nandani Sharma,Dinesh Singh*

Main category: cs.CV

TL;DR: 提出Exp-Graph框架，结合图卷积网络和视觉Transformer，通过面部属性图提升表情识别精度。


<details>
  <summary>Details</summary>
Motivation: 面部表情识别在人机交互等领域至关重要，但面部属性的结构变化需要被有效建模。

Method: 使用面部关键点作为图顶点，基于邻近性和局部外观相似性确定边，结合视觉Transformer和图卷积网络捕捉结构依赖。

Result: 在Oulu-CASIA、eNTERFACE05和AFEW数据集上分别达到98.09%、79.01%和56.39%的准确率。

Conclusion: Exp-Graph在实验室和真实场景中均表现出强泛化能力，适用于实际应用。

Abstract: Facial expression recognition is crucial for human-computer interaction
applications such as face animation, video surveillance, affective computing,
medical analysis, etc. Since the structure of facial attributes varies with
facial expressions, incorporating structural information into facial attributes
is essential for facial expression recognition. In this paper, we propose
Exp-Graph, a novel framework designed to represent the structural relationships
among facial attributes using graph-based modeling for facial expression
recognition. For facial attributes graph representation, facial landmarks are
used as the graph's vertices. At the same time, the edges are determined based
on the proximity of the facial landmark and the similarity of the local
appearance of the facial attributes encoded using the vision transformer.
Additionally, graph convolutional networks are utilized to capture and
integrate these structural dependencies into the encoding of facial attributes,
thereby enhancing the accuracy of expression recognition. Thus, Exp-Graph
learns from the facial attribute graphs highly expressive semantic
representations. On the other hand, the vision transformer and graph
convolutional blocks help the framework exploit the local and global
dependencies among the facial attributes that are essential for the recognition
of facial expressions. We conducted comprehensive evaluations of the proposed
Exp-Graph model on three benchmark datasets: Oulu-CASIA, eNTERFACE05, and AFEW.
The model achieved recognition accuracies of 98.09\%, 79.01\%, and 56.39\%,
respectively. These results indicate that Exp-Graph maintains strong
generalization capabilities across both controlled laboratory settings and
real-world, unconstrained environments, underscoring its effectiveness for
practical facial expression recognition applications.

</details>


### [421] [AI-Powered Precision in Sport Taekwondo: Enhancing Fairness, Speed, and Trust in Competition (FST.ai)](https://arxiv.org/abs/2507.14657)
*Keivan Shariatmadar,Ahmad Osman*

Main category: cs.CV

TL;DR: FST.ai是一个基于AI的框架，旨在通过计算机视觉和深度学习技术提升体育裁判的实时决策能力，特别是在跆拳道头部踢击检测中。


<details>
  <summary>Details</summary>
Motivation: 传统裁判系统存在延迟、主观性和不一致性问题，影响公平性和运动员信任。

Method: 利用计算机视觉、深度学习和边缘推理技术，实现实时动作识别与分类。

Result: 系统将决策时间从分钟缩短至秒级，提高了裁判的一致性和透明度。

Conclusion: FST.ai框架不仅适用于跆拳道，还可扩展到其他需要动作检测的体育项目，展示了其广泛的应用潜力。

Abstract: The integration of Artificial Intelligence (AI) into sports officiating
represents a paradigm shift in how decisions are made in competitive
environments. Traditional manual systems, even when supported by Instant Video
Replay (IVR), often suffer from latency, subjectivity, and inconsistent
enforcement, undermining fairness and athlete trust. This paper introduces
FST.ai, a novel AI-powered framework designed to enhance officiating in Sport
Taekwondo, particularly focusing on the complex task of real-time head kick
detection and scoring. Leveraging computer vision, deep learning, and edge
inference, the system automates the identification and classification of key
actions, significantly reducing decision time from minutes to seconds while
improving consistency and transparency. Importantly, the methodology is not
limited to Taekwondo. The underlying framework -- based on pose estimation,
motion classification, and impact analysis -- can be adapted to a wide range of
sports requiring action detection, such as judo, karate, fencing, or even team
sports like football and basketball, where foul recognition or performance
tracking is critical. By addressing one of Taekwondo's most challenging
scenarios -- head kick scoring -- we demonstrate the robustness, scalability,
and sport-agnostic potential of FST.ai to transform officiating standards
across multiple disciplines.

</details>


### [422] [Artificial Intelligence in the Food Industry: Food Waste Estimation based on Computer Vision, a Brief Case Study in a University Dining Hall](https://arxiv.org/abs/2507.14662)
*Shayan Rokhva,Babak Teimourpour*

Main category: cs.CV

TL;DR: 研究提出了一种基于计算机视觉的低成本框架，通过语义分割RGB图像来量化餐盘级食物浪费，适用于五种伊朗菜肴。模型表现良好，部分达到90%以上的像素比例估计准确率。


<details>
  <summary>Details</summary>
Motivation: 量化机构餐饮环境中的食物浪费，支持数据驱动的可持续发展策略。

Method: 使用四种全监督模型（U-Net、U-Net++及其轻量版），通过动态逆频率损失和AdamW优化器训练，评估包括像素准确率、Dice、IoU和自定义DPA指标。

Result: 模型表现良好，轻量模型实现实时推理。干燥和刚性食物（如米饭和薯条）分割效果更佳，复杂或粘稠食物（如炖菜）表现较差。

Conclusion: 该框架为大规模餐饮环境中的实时浪费监测提供了可扩展的无接触解决方案，为减少食物浪费提供了可行方向。

Abstract: Quantifying post-consumer food waste in institutional dining settings is
essential for supporting data-driven sustainability strategies. This study
presents a cost-effective computer vision framework that estimates plate-level
food waste by utilizing semantic segmentation of RGB images taken before and
after meal consumption across five Iranian dishes. Four fully supervised models
(U-Net, U-Net++, and their lightweight variants) were trained using a capped
dynamic inverse-frequency loss and AdamW optimizer, then evaluated through a
comprehensive set of metrics, including Pixel Accuracy, Dice, IoU, and a
custom-defined Distributional Pixel Agreement (DPA) metric tailored to the
task. All models achieved satisfying performance, and for each food type, at
least one model approached or surpassed 90% DPA, demonstrating strong alignment
in pixel-wise proportion estimates. Lighter models with reduced parameter
counts offered faster inference, achieving real-time throughput on an NVIDIA T4
GPU. Further analysis showed superior segmentation performance for dry and more
rigid components (e.g., rice and fries), while more complex, fragmented, or
viscous dishes, such as stews, showed reduced performance, specifically
post-consumption. Despite limitations such as reliance on 2D imaging,
constrained food variety, and manual data collection, the proposed framework is
pioneering and represents a scalable, contactless solution for continuous
monitoring of food consumption. This research lays foundational groundwork for
automated, real-time waste tracking systems in large-scale food service
environments and offers actionable insights and outlines feasible future
directions for dining hall management and policymakers aiming to reduce
institutional food waste.

</details>


### [423] [WSI-Agents: A Collaborative Multi-Agent System for Multi-Modal Whole Slide Image Analysis](https://arxiv.org/abs/2507.14680)
*Xinheng Lyu,Yuci Liang,Wenting Chen,Meidan Ding,Jiaqi Yang,Guolin Huang,Daokun Zhang,Xiangjian He,Linlin Shen*

Main category: cs.CV

TL;DR: WSI-Agents是一种协作多代理系统，用于多模态WSI分析，通过任务分配、验证和总结模块提升任务准确性和多任务适应性。


<details>
  <summary>Details</summary>
Motivation: 解决多模态大语言模型在WSI分析中性能不足的问题，探索协作多代理系统在病理学领域的潜力。

Method: 提出WSI-Agents系统，包括任务分配模块、验证机制和总结模块，结合专家代理和知识库。

Result: 在多种WSI基准测试中表现优于现有WSI MLLMs和医疗代理框架。

Conclusion: WSI-Agents在病理学领域实现了任务准确性和多任务适应性的平衡。

Abstract: Whole slide images (WSIs) are vital in digital pathology, enabling gigapixel
tissue analysis across various pathological tasks. While recent advancements in
multi-modal large language models (MLLMs) allow multi-task WSI analysis through
natural language, they often underperform compared to task-specific models.
Collaborative multi-agent systems have emerged as a promising solution to
balance versatility and accuracy in healthcare, yet their potential remains
underexplored in pathology-specific domains. To address these issues, we
propose WSI-Agents, a novel collaborative multi-agent system for multi-modal
WSI analysis. WSI-Agents integrates specialized functional agents with robust
task allocation and verification mechanisms to enhance both task-specific
accuracy and multi-task versatility through three components: (1) a task
allocation module assigning tasks to expert agents using a model zoo of patch
and WSI level MLLMs, (2) a verification mechanism ensuring accuracy through
internal consistency checks and external validation using pathology knowledge
bases and domain-specific models, and (3) a summary module synthesizing the
final summary with visual interpretation maps. Extensive experiments on
multi-modal WSI benchmarks show WSI-Agents's superiority to current WSI MLLMs
and medical agent frameworks across diverse tasks.

</details>


### [424] [LeAdQA: LLM-Driven Context-Aware Temporal Grounding for Video Question Answering](https://arxiv.org/abs/2507.14784)
*Xinxin Dong,Baoyun Peng,Haokai Ma,Yufei Wang,Zixuan Dong,Fei Hu,Xiaodong Wang*

Main category: cs.CV

TL;DR: LeAdQA通过结合因果感知查询优化和细粒度视觉定位，解决了视频问答中关键帧稀疏和因果推理的挑战，显著提升了复杂推理任务的性能。


<details>
  <summary>Details</summary>
Motivation: 当前视频问答方法存在任务无关采样和启发式检索的局限性，无法有效捕捉关键事件和因果-时间结构。

Method: LeAdQA利用LLM优化问题-选项对，结合时间定位模型精确检索关键片段，并通过自适应融合机制整合视觉-文本线索，最终由MLLM生成答案。

Result: 在NExT-QA、IntentQA和NExT-GQA数据集上，LeAdQA实现了最先进的性能，同时保持计算效率。

Conclusion: LeAdQA通过因果感知和视觉定位的协同作用，显著提升了视频问答的复杂推理能力。

Abstract: Video Question Answering (VideoQA) requires identifying sparse critical
moments in long videos and reasoning about their causal relationships to answer
semantically complex questions. While recent advances in multimodal learning
have improved alignment and fusion, current approaches remain limited by two
prevalent but fundamentally flawed strategies: (1) task-agnostic sampling
indiscriminately processes all frames, overwhelming key events with irrelevant
content; and (2) heuristic retrieval captures superficial patterns but misses
causal-temporal structures needed for complex reasoning. To address these
challenges, we introduce LeAdQA, an innovative approach that bridges these gaps
through synergizing causal-aware query refinement with fine-grained visual
grounding. Our method first leverages LLMs to reformulate question-option
pairs, resolving causal ambiguities and sharpening temporal focus. These
refined queries subsequently direct a temporal grounding model to precisely
retrieve the most salient segments, complemented by an adaptive fusion
mechanism dynamically integrating the evidence to maximize relevance. The
integrated visual-textual cues are then processed by an MLLM to generate
accurate, contextually-grounded answers. Experiments on NExT-QA, IntentQA, and
NExT-GQA demonstrate that our method's precise visual grounding substantially
enhances the understanding of video-question relationships, achieving
state-of-the-art (SOTA) performance on complex reasoning tasks while
maintaining computational efficiency.

</details>


### [425] [FOCUS: Fused Observation of Channels for Unveiling Spectra](https://arxiv.org/abs/2507.14787)
*Xi Xiao,Aristeidis Tsaris,Anika Tabassum,John Lagergren,Larry M. York,Tianyang Wang,Xiao Wang*

Main category: cs.CV

TL;DR: FOCUS框架通过类特定光谱提示和可学习的[SINK]令牌，实现了高效且可靠的ViT在HSI数据中的空间-光谱可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决现有显著性方法在HSI数据中难以捕捉有意义的频谱线索以及计算成本高的问题。

Method: 引入类特定光谱提示和[SINK]令牌，通过吸引力损失训练，生成3D显著性图和频谱重要性曲线。

Result: FOCUS提高了波段级IoU 15%，减少注意力崩溃40%，且与专家标注高度一致。

Conclusion: FOCUS以低参数开销实现了高分辨率ViT的可解释性，填补了黑盒建模与可信HSI决策之间的空白。

Abstract: Hyperspectral imaging (HSI) captures hundreds of narrow, contiguous
wavelength bands, making it a powerful tool in biology, agriculture, and
environmental monitoring. However, interpreting Vision Transformers (ViTs) in
this setting remains largely unexplored due to two key challenges: (1) existing
saliency methods struggle to capture meaningful spectral cues, often collapsing
attention onto the class token, and (2) full-spectrum ViTs are computationally
prohibitive for interpretability, given the high-dimensional nature of HSI
data. We present FOCUS, the first framework that enables reliable and efficient
spatial-spectral interpretability for frozen ViTs. FOCUS introduces two core
components: class-specific spectral prompts that guide attention toward
semantically meaningful wavelength groups, and a learnable [SINK] token trained
with an attraction loss to absorb noisy or redundant attention. Together, these
designs make it possible to generate stable and interpretable 3D saliency maps
and spectral importance curves in a single forward pass, without any gradient
backpropagation or backbone modification. FOCUS improves band-level IoU by 15
percent, reduces attention collapse by over 40 percent, and produces saliency
results that align closely with expert annotations. With less than 1 percent
parameter overhead, our method makes high-resolution ViT interpretability
practical for real-world hyperspectral applications, bridging a long-standing
gap between black-box modeling and trustworthy HSI decision-making.

</details>


### [426] [Real Time Captioning of Sign Language Gestures in Video Meetings](https://arxiv.org/abs/2507.14543)
*Sharanya Mukherjee,Md Hishaam Akhtar,Kannadasan R*

Main category: cs.CV

TL;DR: 提出一种浏览器扩展，将手语实时翻译为字幕，帮助听力障碍者在视频会议中更顺畅地交流。


<details>
  <summary>Details</summary>
Motivation: 听力障碍者与普通人之间的沟通存在障碍，尤其在疫情期间视频会议成为主流时，手语翻译需求增加。

Method: 利用包含2000多个单词级ASL视频的大规模数据集，开发浏览器扩展实现手语到字幕的实时翻译。

Result: 通过浏览器扩展实现手语到字幕的自动翻译，提升听力障碍者的沟通效率。

Conclusion: 该技术有望消除听力障碍者与普通人之间的沟通障碍，尤其在视频会议场景中具有重要意义。

Abstract: It has always been a rather tough task to communicate with someone possessing
a hearing impairment. One of the most tested ways to establish such a
communication is through the use of sign based languages. However, not many
people are aware of the smaller intricacies involved with sign language. Sign
language recognition using computer vision aims at eliminating the
communication barrier between deaf-mute and ordinary people so that they can
properly communicate with others. Recently the pandemic has left the whole
world shaken up and has transformed the way we communicate. Video meetings have
become essential for everyone, even people with a hearing disability. In recent
studies, it has been found that people with hearing disabilities prefer to sign
over typing during these video calls. In this paper, we are proposing a browser
extension that will automatically translate sign language to subtitles for
everyone else in the video call. The Large-scale dataset which contains more
than 2000 Word-Level ASL videos, which were performed by over 100 signers will
be used.

</details>


### [427] [Seeing Through Deepfakes: A Human-Inspired Framework for Multi-Face Detection](https://arxiv.org/abs/2507.14807)
*Juan Hu,Shaojing Fan,Terence Sim*

Main category: cs.CV

TL;DR: 论文提出了一种基于人类认知的新方法HICOM，用于检测多脸深度伪造视频，通过四项关键线索提升检测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 多脸深度伪造视频在自然社交场景中日益普遍，现有方法因缺乏上下文线索而难以应对。

Method: 通过人类研究识别四项关键线索，并开发HICOM框架，结合LLM提供可解释性。

Result: HICOM在基准数据集上平均准确率提升3.3%，在未见数据集上优于现有方法5.8%。

Conclusion: 研究表明，结合人类认知线索可有效提升深度伪造检测能力，并增强结果的可解释性。

Abstract: Multi-face deepfake videos are becoming increasingly prevalent, often
appearing in natural social settings that challenge existing detection methods.
Most current approaches excel at single-face detection but struggle in
multi-face scenarios, due to a lack of awareness of crucial contextual cues. In
this work, we develop a novel approach that leverages human cognition to
analyze and defend against multi-face deepfake videos. Through a series of
human studies, we systematically examine how people detect deepfake faces in
social settings. Our quantitative analysis reveals four key cues humans rely
on: scene-motion coherence, inter-face appearance compatibility, interpersonal
gaze alignment, and face-body consistency. Guided by these insights, we
introduce \textsf{HICOM}, a novel framework designed to detect every fake face
in multi-face scenarios. Extensive experiments on benchmark datasets show that
\textsf{HICOM} improves average accuracy by 3.3\% in in-dataset detection and
2.8\% under real-world perturbations. Moreover, it outperforms existing methods
by 5.8\% on unseen datasets, demonstrating the generalization of human-inspired
cues. \textsf{HICOM} further enhances interpretability by incorporating an LLM
to provide human-readable explanations, making detection results more
transparent and convincing. Our work sheds light on involving human factors to
enhance defense against deepfakes.

</details>


### [428] [SegQuant: A Semantics-Aware and Generalizable Quantization Framework for Diffusion Models](https://arxiv.org/abs/2507.14811)
*Jiaji Zhang,Ruichao Sun,Hailiang Zhao,Jiaju Wu,Peng Chen,Hao Li,Xinkui Zhao,Kingsum Chow,Gang Xiong,Lin Ye,Shuiguang Deng*

Main category: cs.CV

TL;DR: SegQuant是一个统一的量化框架，通过自适应结合互补技术提升扩散模型的通用性，解决了现有PTQ方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 扩散模型计算密集，现有PTQ方法依赖特定架构启发式，限制了通用性和工业部署。

Method: SegQuant包含SegLinear（分段感知的图量化策略）和DualScale（双尺度量化方案），保留视觉保真度。

Result: SegQuant在Transformer以外的扩散模型中表现优异，兼容主流部署工具。

Conclusion: SegQuant为扩散模型提供了一种高效、通用的量化解决方案。

Abstract: Diffusion models have demonstrated exceptional generative capabilities but
are computationally intensive, posing significant challenges for deployment in
resource-constrained or latency-sensitive environments. Quantization offers an
effective means to reduce model size and computational cost, with post-training
quantization (PTQ) being particularly appealing due to its compatibility with
pre-trained models without requiring retraining or training data. However,
existing PTQ methods for diffusion models often rely on architecture-specific
heuristics that limit their generalizability and hinder integration with
industrial deployment pipelines. To address these limitations, we propose
SegQuant, a unified quantization framework that adaptively combines
complementary techniques to enhance cross-model versatility. SegQuant consists
of a segment-aware, graph-based quantization strategy (SegLinear) that captures
structural semantics and spatial heterogeneity, along with a dual-scale
quantization scheme (DualScale) that preserves polarity-asymmetric activations,
which is crucial for maintaining visual fidelity in generated outputs. SegQuant
is broadly applicable beyond Transformer-based diffusion models, achieving
strong performance while ensuring seamless compatibility with mainstream
deployment tools.

</details>


### [429] [Paired Image Generation with Diffusion-Guided Diffusion Models](https://arxiv.org/abs/2507.14833)
*Haoxuan Zhang,Wenju Cui,Yuzhu Cao,Tao Tan,Jie Liu,Yunsong Peng,Jian Zheng*

Main category: cs.CV

TL;DR: 提出了一种用于数字乳腺断层合成图像（DBT）中肿块分割的配对图像生成方法，解决了现有扩散模型在生成质量和标注数据方面的限制。


<details>
  <summary>Details</summary>
Motivation: 高密度乳腺组织导致肿块隐蔽，人工标注困难且耗时，缺乏标注数据用于模型训练。现有扩散模型在生成质量和标注生成方面存在不足。

Method: 训练一个额外的扩散引导器，结合条件扩散模型，实现无需外部条件的配对图像（DBT切片和肿块掩码）生成。

Result: 实验表明，该方法提高了生成质量，缓解了标注数据短缺问题，并提升了下游任务（肿块分割）的性能。

Conclusion: 提出的配对图像生成方法在无需外部条件下有效提升了生成质量和下游任务性能，解决了标注数据不足的问题。

Abstract: The segmentation of mass lesions in digital breast tomosynthesis (DBT) images
is very significant for the early screening of breast cancer. However, the
high-density breast tissue often leads to high concealment of the mass lesions,
which makes manual annotation difficult and time-consuming. As a result, there
is a lack of annotated data for model training. Diffusion models are commonly
used for data augmentation, but the existing methods face two challenges.
First, due to the high concealment of lesions, it is difficult for the model to
learn the features of the lesion area. This leads to the low generation quality
of the lesion areas, thus limiting the quality of the generated images. Second,
existing methods can only generate images and cannot generate corresponding
annotations, which restricts the usability of the generated images in
supervised training. In this work, we propose a paired image generation method.
The method does not require external conditions and can achieve the generation
of paired images by training an extra diffusion guider for the conditional
diffusion model. During the experimental phase, we generated paired DBT slices
and mass lesion masks. Then, we incorporated them into the supervised training
process of the mass lesion segmentation task. The experimental results show
that our method can improve the generation quality without external conditions.
Moreover, it contributes to alleviating the shortage of annotated data, thus
enhancing the performance of downstream tasks.

</details>


### [430] [Grounding Degradations in Natural Language for All-In-One Video Restoration](https://arxiv.org/abs/2507.14851)
*Muhammad Kamran Janjua,Amirhosein Ghasemabadi,Kunlin Zhang,Mohammad Salameh,Chao Gao,Di Niu*

Main category: cs.CV

TL;DR: 提出了一种基于自然语言和基础模型的全能视频修复框架，无需预知退化信息，并在推理时无额外成本。同时呼吁标准化全能视频修复的基准测试，并提出了新的数据集。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常需要预知视频的退化信息，限制了灵活性和可解释性。本文旨在通过自然语言和基础模型提供更灵活、可解释的修复指导。

Method: 利用基础模型通过自然语言理解视频帧的退化语义上下文，无需训练或测试时预知退化信息，并在推理时解耦基础模型以减少成本。

Result: 在提出的多个基准测试中（包括新的时变复合退化数据集），方法表现优于现有技术，达到最优性能。

Conclusion: 提出的框架在灵活性和性能上均优于现有方法，同时强调了标准化基准测试的重要性。

Abstract: In this work, we propose an all-in-one video restoration framework that
grounds degradation-aware semantic context of video frames in natural language
via foundation models, offering interpretable and flexible guidance. Unlike
prior art, our method assumes no degradation knowledge in train or test time
and learns an approximation to the grounded knowledge such that the foundation
model can be safely disentangled during inference adding no extra cost.
Further, we call for standardization of benchmarks in all-in-one video
restoration, and propose two benchmarks in multi-degradation setting,
three-task (3D) and four-task (4D), and two time-varying composite degradation
benchmarks; one of the latter being our proposed dataset with varying snow
intensity, simulating how weather degradations affect videos naturally. We
compare our method with prior works and report state-of-the-art performance on
all benchmarks.

</details>


### [431] [TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D Visual Grounding based on CLIP](https://arxiv.org/abs/2507.14904)
*Fan Li,Zanyi Wang,Zeyi Huang,Guang Dai,Jingdong Wang,Mengmeng Wang*

Main category: cs.CV

TL;DR: 提出了一种统一的2D预训练多模态网络（GARF），用于处理RGB图像、文本和点云数据，简化了架构并提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D视觉定位方法依赖多模态分离编码器，导致模型复杂且训练效率低。

Method: 利用2D CLIP双模态模型，通过适配器微调适应三模态设置，并设计GARF模块融合几何多尺度特征。

Result: 模型参数量减少约58%，3D检测任务性能提升6.52%，3D视觉定位任务提升6.25%。

Conclusion: 该方法实现了跨模态的统一特征提取与融合，显著提升了效率和性能。

Abstract: 3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.

</details>


### [432] [StableAnimator++: Overcoming Pose Misalignment and Face Distortion for Human Image Animation](https://arxiv.org/abs/2507.15064)
*Shuyuan Tu,Zhen Xing,Xintong Han,Zhi-Qi Cheng,Qi Dai,Chong Luo,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: StableAnimator++ 是一种基于视频扩散模型的框架，通过可学习的姿态对齐和身份保持技术，解决了现有方法在人体图像动画中身份一致性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前的人体图像动画扩散模型在参考图像和驱动视频差异较大时难以保持身份一致性，因此需要一种更有效的解决方案。

Method: StableAnimator++ 结合了可学习的姿态对齐模块、全局内容感知的面部编码器和分布感知的身份适配器，并在推理阶段引入了基于 HJB 的面部优化。

Result: 实验结果表明，StableAnimator++ 在定性和定量上均表现出色，显著提升了身份一致性和视频质量。

Conclusion: StableAnimator++ 通过创新的模块设计和优化策略，成功解决了身份一致性问题，为人体图像动画提供了高质量的解决方案。

Abstract: Current diffusion models for human image animation often struggle to maintain
identity (ID) consistency, especially when the reference image and driving
video differ significantly in body size or position. We introduce
StableAnimator++, the first ID-preserving video diffusion framework with
learnable pose alignment, capable of generating high-quality videos conditioned
on a reference image and a pose sequence without any post-processing. Building
upon a video diffusion model, StableAnimator++ contains carefully designed
modules for both training and inference, striving for identity consistency. In
particular, StableAnimator++ first uses learnable layers to predict the
similarity transformation matrices between the reference image and the driven
poses via injecting guidance from Singular Value Decomposition (SVD). These
matrices align the driven poses with the reference image, mitigating
misalignment to a great extent. StableAnimator++ then computes image and face
embeddings using off-the-shelf encoders, refining the face embeddings via a
global content-aware Face Encoder. To further maintain ID, we introduce a
distribution-aware ID Adapter that counteracts interference caused by temporal
layers while preserving ID via distribution alignment. During the inference
stage, we propose a novel Hamilton-Jacobi-Bellman (HJB) based face optimization
integrated into the denoising process, guiding the diffusion trajectory for
enhanced facial fidelity. Experiments on benchmarks show the effectiveness of
StableAnimator++ both qualitatively and quantitatively.

</details>


### [433] [Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression](https://arxiv.org/abs/2507.14997)
*Roy H. Jennings,Genady Paikin,Roy Shaul,Evgeny Soloveichik*

Main category: cs.CV

TL;DR: 论文提出RvTC方法，通过灵活的基于分箱的分类替代预设词汇分类，在图像评估任务中取得最优性能，并证明语义提示能显著提升多模态大语言模型的表现。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在图像回归任务中表现不佳，预设词汇和通用提示无法利用文本输入的语义理解。

Method: 提出Regression via Transformer-Based Classification (RvTC)，采用分箱方法替代预设词汇分类，并通过数据特定提示提升模型性能。

Result: 在四个图像评估数据集上达到最优性能，AVA数据集中添加语义提示使相关性从0.83提升至0.90。

Conclusion: 语义提示和多模态理解的结合对提升回归任务性能至关重要。

Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based
regression tasks, but current approaches face key limitations. Recent methods
fine-tune MLLMs using preset output vocabularies and generic task-level prompts
(e.g., "How would you rate this image?"), assuming this mimics human rating
behavior. Our analysis reveals these approaches provide no benefit over
image-only training. Models using preset vocabularies and generic prompts
perform equivalently to image-only models, failing to leverage semantic
understanding from textual input. We propose Regression via Transformer-Based
Classification (RvTC), which replaces vocabulary-constrained classification
with a flexible bin-based approach. Unlike approaches that address
discretization errors through complex distributional modeling, RvTC eliminates
manual vocabulary crafting through straightforward bin increase, achieving
state-of-the-art performance on four image assessment datasets using only
images. More importantly, we demonstrate that data-specific prompts
dramatically improve performance. Unlike generic task descriptions, prompts
containing semantic information about specific images enable MLLMs to leverage
cross-modal understanding. On the AVA dataset, adding challenge titles to
prompts improves correlations from 0.83 to 0.90, a new state-of-the-art. We
demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that
MLLMs benefit from semantic prompt information surpassing mere statistical
biases. This underscores the importance of incorporating meaningful textual
context in multimodal regression tasks.

</details>


### [434] [BleedOrigin: Dynamic Bleeding Source Localization in Endoscopic Submucosal Dissection via Dual-Stage Detection and Tracking](https://arxiv.org/abs/2507.15094)
*Mengya Xu,Rulin Zhou,An Wang,Chaoyang Lyu,Zhen Li,Ning Zhong,Hongliang Ren*

Main category: cs.CV

TL;DR: 论文提出了首个ESD出血源数据集BleedOrigin-Bench和新颖的双阶段检测-跟踪框架BleedOrigin-Net，用于实时定位和跟踪出血源，显著提升了准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 内镜黏膜下剥离术（ESD）中出血的实时定位和持续监测对止血干预至关重要，但现有AI方法仅关注出血区域分割，缺乏对出血源检测和时间跟踪的研究，且缺乏专用数据集。

Method: 提出了BleedOrigin-Bench数据集，包含专家标注的出血源数据，并开发了BleedOrigin-Net框架，结合检测和跟踪技术，从出血起始检测到持续空间跟踪。

Result: BleedOrigin-Net在出血起始检测、初始源检测和点跟踪方面均达到先进水平，准确率分别为96.85%、70.24%和96.11%。

Conclusion: BleedOrigin-Bench和BleedOrigin-Net填补了ESD出血源检测和跟踪的空白，为AI辅助系统提供了有力支持。

Abstract: Intraoperative bleeding during Endoscopic Submucosal Dissection (ESD) poses
significant risks, demanding precise, real-time localization and continuous
monitoring of the bleeding source for effective hemostatic intervention. In
particular, endoscopists have to repeatedly flush to clear blood, allowing only
milliseconds to identify bleeding sources, an inefficient process that prolongs
operations and elevates patient risks. However, current Artificial Intelligence
(AI) methods primarily focus on bleeding region segmentation, overlooking the
critical need for accurate bleeding source detection and temporal tracking in
the challenging ESD environment, which is marked by frequent visual
obstructions and dynamic scene changes. This gap is widened by the lack of
specialized datasets, hindering the development of robust AI-assisted guidance
systems. To address these challenges, we introduce BleedOrigin-Bench, the first
comprehensive ESD bleeding source dataset, featuring 1,771 expert-annotated
bleeding sources across 106,222 frames from 44 procedures, supplemented with
39,755 pseudo-labeled frames. This benchmark covers 8 anatomical sites and 6
challenging clinical scenarios. We also present BleedOrigin-Net, a novel
dual-stage detection-tracking framework for the bleeding source localization in
ESD procedures, addressing the complete workflow from bleeding onset detection
to continuous spatial tracking. We compare with widely-used object detection
models (YOLOv11/v12), multimodal large language models, and point tracking
methods. Extensive evaluation demonstrates state-of-the-art performance,
achieving 96.85% frame-level accuracy ($\pm\leq8$ frames) for bleeding onset
detection, 70.24% pixel-level accuracy ($\leq100$ px) for initial source
detection, and 96.11% pixel-level accuracy ($\leq100$ px) for point tracking.

</details>


### [435] [OpenBreastUS: Benchmarking Neural Operators for Wave Imaging Using Breast Ultrasound Computed Tomography](https://arxiv.org/abs/2507.15035)
*Zhijun Zeng,Youjia Zheng,Hao Hu,Zeyuan Dong,Yihang Zheng,Xinliang Liu,Jinzhuo Wang,Zuoqiang Shi,Linfeng Zhang,Yubing Li,He Sun*

Main category: cs.CV

TL;DR: OpenBreastUS是一个大规模波动方程数据集，旨在弥合理论方程与实际成像应用之间的差距，用于评估神经算子在医学成像中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统波动方程数值求解器计算量大且不稳定，神经算子虽能加速求解，但现有数据集过于简化，限制了其在真实成像中的应用。

Method: 提出OpenBreastUS数据集，包含8000个解剖学真实的人体乳房模型和1600万次频域波模拟，用于评估神经算子的性能。

Result: 首次展示了神经算子求解器在人体乳房活体成像中的高效应用。

Conclusion: OpenBreastUS为开发创新的神经PDE求解器提供了平台，并促进了其在真实医学成像中的部署。

Abstract: Accurate and efficient simulation of wave equations is crucial in
computational wave imaging applications, such as ultrasound computed tomography
(USCT), which reconstructs tissue material properties from observed scattered
waves. Traditional numerical solvers for wave equations are computationally
intensive and often unstable, limiting their practical applications for
quasi-real-time image reconstruction. Neural operators offer an innovative
approach by accelerating PDE solving using neural networks; however, their
effectiveness in realistic imaging is limited because existing datasets
oversimplify real-world complexity. In this paper, we present OpenBreastUS, a
large-scale wave equation dataset designed to bridge the gap between
theoretical equations and practical imaging applications. OpenBreastUS includes
8,000 anatomically realistic human breast phantoms and over 16 million
frequency-domain wave simulations using real USCT configurations. It enables a
comprehensive benchmarking of popular neural operators for both forward
simulation and inverse imaging tasks, allowing analysis of their performance,
scalability, and generalization capabilities. By offering a realistic and
extensive dataset, OpenBreastUS not only serves as a platform for developing
innovative neural PDE solvers but also facilitates their deployment in
real-world medical imaging problems. For the first time, we demonstrate
efficient in vivo imaging of the human breast using neural operator solvers.

</details>


### [436] [LoopNet: A Multitasking Few-Shot Learning Approach for Loop Closure in Large Scale SLAM](https://arxiv.org/abs/2507.15109)
*Mohammad-Maher Nakshbandi,Ziad Sharawy,Sorin Grigorescu*

Main category: cs.CV

TL;DR: LoopNet是一种基于多任务ResNet架构的方法，用于解决SLAM闭环检测的准确性和实时计算问题，结合了在线学习和DISK描述符，性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决SLAM系统中闭环检测的准确性和嵌入式硬件实时计算的挑战。

Method: 采用多任务ResNet架构，结合在线学习和DISK描述符，优化嵌入式设备性能。

Result: LoopNet在多变条件下表现优于传统方法和手工特征。

Conclusion: LoopNet为SLAM闭环检测提供了高效解决方案，并发布了新数据集LoopDB。

Abstract: One of the main challenges in the Simultaneous Localization and Mapping
(SLAM) loop closure problem is the recognition of previously visited places. In
this work, we tackle the two main problems of real-time SLAM systems: 1) loop
closure detection accuracy and 2) real-time computation constraints on the
embedded hardware. Our LoopNet method is based on a multitasking variant of the
classical ResNet architecture, adapted for online retraining on a dynamic
visual dataset and optimized for embedded devices. The online retraining is
designed using a few-shot learning approach. The architecture provides both an
index into the queried visual dataset, and a measurement of the prediction
quality. Moreover, by leveraging DISK (DIStinctive Keypoints) descriptors,
LoopNet surpasses the limitations of handcrafted features and traditional deep
learning methods, offering better performance under varying conditions. Code is
available at https://github.com/RovisLab/LoopNet. Additinally, we introduce a
new loop closure benchmarking dataset, coined LoopDB, which is available at
https://github.com/RovisLab/LoopDB.

</details>


### [437] [Cross-Domain Few-Shot Learning with Coalescent Projections and Latent Space Reservation](https://arxiv.org/abs/2507.15243)
*Naeem Paeedeh,Mahardhika Pratama,Wolfgang Mayer,Jimmy Cao,Ryszard Kowlczyk*

Main category: cs.CV

TL;DR: DINO预训练模型结合原型分类器在跨域少样本学习中表现优于最新SOTA方法，提出Coalescent Projection（CP）和伪类生成方法解决过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 解决跨域少样本学习中因标记样本稀缺导致Transformer参数过多更新而引发的过拟合问题。

Method: 提出Coalescent Projection（CP）替代软提示，并结合基于基础域的自监督变换（SSTs）生成伪类。

Result: 在BSCD-FSL基准测试的极端域偏移场景中表现优异。

Conclusion: CP和伪类生成方法有效解决了过拟合问题，提升了模型在跨域少样本学习中的性能。

Abstract: Despite the progress in Cross-Domain Few-Shot Learning (CD-FSL), a model
pre-trained with DINO combined with a prototypical classifier outperforms the
latest SOTA methods. A crucial limitation that needs to be overcome is that
updating too many parameters of the transformers leads to overfitting due to
the scarcity of labeled samples. To address this challenge, we propose a new
concept, Coalescent Projection (CP), as an effective successor to soft prompts.
Additionally, we propose a novel pseudo-class generation method combined with
Self-Supervised Transformations (SSTs) that relies solely on the base domain to
prepare the network for encountering unseen samples from different domains. The
proposed method exhibits its effectiveness in comprehensive experiments on the
extreme domain shift scenario of the BSCD-FSL benchmark. Our code is published
at https://github.com/Naeem-Paeedeh/CPLSR.

</details>


### [438] [Conditional Video Generation for High-Efficiency Video Compression](https://arxiv.org/abs/2507.15269)
*Fangqiu Yi,Jingyu Xu,Jiawei Shao,Chi Zhang,Xuelong Li*

Main category: cs.CV

TL;DR: 提出了一种基于条件扩散模型的视频压缩框架，通过多粒度条件、紧凑表示和多条件训练模块，显著提升了感知质量。


<details>
  <summary>Details</summary>
Motivation: 利用条件扩散模型在人类视觉感知对齐重建中的优势，优化视频压缩的感知质量。

Method: 将视频压缩重构为条件生成任务，引入多粒度条件、紧凑表示和多条件训练模块。

Result: 在FVD和LPIPS等感知质量指标上显著优于传统和神经编解码器，尤其在高压缩比下表现突出。

Conclusion: 条件扩散模型在视频压缩中具有显著潜力，能够实现感知优化的高质量重建。

Abstract: Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.

</details>


### [439] [ExDD: Explicit Dual Distribution Learning for Surface Defect Detection via Diffusion Synthesis](https://arxiv.org/abs/2507.15335)
*Muhammad Aqeel,Federico Leonardi,Francesco Setti*

Main category: cs.CV

TL;DR: ExDD框架通过显式建模双特征分布，解决了工业缺陷检测中单类异常检测的局限性，利用潜在扩散模型生成合成缺陷数据，并通过邻域感知评分机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 工业缺陷检测系统在单类异常检测范式下存在局限性，假设异常分布均匀且难以应对真实制造环境中的数据稀缺问题。

Method: 提出ExDD框架，显式建模双特征分布，利用并行记忆库捕获正常和异常模式的统计特性，结合潜在扩散模型生成合成缺陷数据，并通过邻域感知评分机制融合距离度量。

Result: 在KSDD2数据集上表现优异（94.2% I-AUROC, 97.7% P-AUROC），100个合成样本时达到最佳增强效果。

Conclusion: ExDD框架通过显式建模和合成数据生成，显著提升了工业缺陷检测的性能，解决了数据稀缺和异常分布假设问题。

Abstract: Industrial defect detection systems face critical limitations when confined
to one-class anomaly detection paradigms, which assume uniform outlier
distributions and struggle with data scarcity in realworld manufacturing
environments. We present ExDD (Explicit Dual Distribution), a novel framework
that transcends these limitations by explicitly modeling dual feature
distributions. Our approach leverages parallel memory banks that capture the
distinct statistical properties of both normality and anomalous patterns,
addressing the fundamental flaw of uniform outlier assumptions. To overcome
data scarcity, we employ latent diffusion models with domain-specific textual
conditioning, generating in-distribution synthetic defects that preserve
industrial context. Our neighborhood-aware ratio scoring mechanism elegantly
fuses complementary distance metrics, amplifying signals in regions exhibiting
both deviation from normality and similarity to known defect patterns.
Experimental validation on KSDD2 demonstrates superior performance (94.2%
I-AUROC, 97.7% P-AUROC), with optimal augmentation at 100 synthetic samples.

</details>


### [440] [Dense-depth map guided deep Lidar-Visual Odometry with Sparse Point Clouds and Images](https://arxiv.org/abs/2507.15496)
*JunYing Huang,Ao Xu,DongSun Yong,KeRen Li,YuanFeng Wang,Qi Qin*

Main category: cs.CV

TL;DR: 提出了一种新颖的LiDAR-视觉里程计框架，结合LiDAR点云和图像，通过深度补全和多尺度特征提取网络实现高精度和鲁棒的位姿估计。


<details>
  <summary>Details</summary>
Motivation: 里程计是自主系统定位和导航的关键任务，现有方法在遮挡区域和动态环境中表现不佳，需要更准确和鲁棒的解决方案。

Method: 利用深度补全生成稠密深度图，结合多尺度特征提取网络和注意力机制，通过稠密深度信息优化光流估计，并采用分层位姿优化模块逐步优化运动估计。

Result: 在KITTI里程计基准测试中，该方法在精度和鲁棒性上达到或优于现有视觉和LiDAR里程计方法。

Conclusion: 提出的LiDAR-视觉里程计框架通过深度感知表示和分层优化，显著提升了位姿估计的准确性和鲁棒性。

Abstract: Odometry is a critical task for autonomous systems for self-localization and
navigation. We propose a novel LiDAR-Visual odometry framework that integrates
LiDAR point clouds and images for accurate and robust pose estimation. Our
method utilizes a dense-depth map estimated from point clouds and images
through depth completion, and incorporates a multi-scale feature extraction
network with attention mechanisms, enabling adaptive depth-aware
representations. Furthermore, we leverage dense depth information to refine
flow estimation and mitigate errors in occlusion-prone regions. Our
hierarchical pose refinement module optimizes motion estimation progressively,
ensuring robust predictions against dynamic environments and scale ambiguities.
Comprehensive experiments on the KITTI odometry benchmark demonstrate that our
approach achieves similar or superior accuracy and robustness compared to
state-of-the-art visual and LiDAR odometry methods.

</details>


### [441] [GeMix: Conditional GAN-Based Mixup for Improved Medical Image Augmentation](https://arxiv.org/abs/2507.15577)
*Hugo Carlesso,Maria Eliza Patulea,Moncef Garouani,Radu Tudor Ionescu,Josiane Mothe*

Main category: cs.CV

TL;DR: GeMix是一种基于类条件GAN的两阶段图像增强框架，通过生成视觉连贯的图像来改进传统Mixup在医学图像分类中的不足。


<details>
  <summary>Details</summary>
Motivation: 传统Mixup的像素级插值在医学等高风险应用中可能生成不真实图像，影响学习效果。

Method: 使用StyleGAN2-ADA生成器，通过Dirichlet和Beta分布采样标签向量，生成连续类流形上的图像。

Result: 在COVIDx-CT-3数据集上，GeMix结合真实数据显著提升了分类性能，降低了COVID-19检测的假阴性率。

Conclusion: GeMix是一种无需修改现有训练流程的Mixup替代方案，提供更强的正则化和语义保真度。

Abstract: Mixup has become a popular augmentation strategy for image classification,
yet its naive pixel-wise interpolation often produces unrealistic images that
can hinder learning, particularly in high-stakes medical applications. We
propose GeMix, a two-stage framework that replaces heuristic blending with a
learned, label-aware interpolation powered by class-conditional GANs. First, a
StyleGAN2-ADA generator is trained on the target dataset. During augmentation,
we sample two label vectors from Dirichlet priors biased toward different
classes and blend them via a Beta-distributed coefficient. Then, we condition
the generator on this soft label to synthesize visually coherent images that
lie along a continuous class manifold. We benchmark GeMix on the large-scale
COVIDx-CT-3 dataset using three backbones (ResNet-50, ResNet-101,
EfficientNet-B0). When combined with real data, our method increases macro-F1
over traditional mixup for all backbones, reducing the false negative rate for
COVID-19 detection. GeMix is thus a drop-in replacement for pixel-space mixup,
delivering stronger regularization and greater semantic fidelity, without
disrupting existing training pipelines. We publicly release our code at
https://github.com/hugocarlesso/GeMix to foster reproducibility and further
research.

</details>


### [442] [Being-H0: Vision-Language-Action Pretraining from Large-Scale Human Videos](https://arxiv.org/abs/2507.15597)
*Hao Luo,Yicheng Feng,Wanpeng Zhang,Sipeng Zheng,Ye Wang,Haoqi Yuan,Jiazheng Liu,Chaoyi Xu,Qin Jin,Zongqing Lu*

Main category: cs.CV

TL;DR: Being-H0是一个基于大规模人类视频训练的灵巧视觉-语言-动作模型（VLA），通过物理指令调优和新颖的数据处理流程，解决了现有模型在复杂操作任务和泛化能力上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型依赖合成数据或有限规模的遥操作演示，难以处理高灵巧性任务和适应新场景，因此需要更丰富的数据和训练方法。

Method: 提出物理指令调优范式，结合大规模VLA预训练、物理空间对齐和机器人任务后适应，并引入部分级运动标记化方法以实现精确手部轨迹建模。

Result: Being-H0在手部运动生成和指令跟随方面表现优异，且在模型和数据规模扩大时仍保持良好性能，实际机器人操作任务中也有显著提升。

Conclusion: 通过利用人类视频数据和物理指令调优，Being-H0显著提升了灵巧操作任务的性能，为机器人领域提供了新的解决方案。

Abstract: We introduce Being-H0, a dexterous Vision-Language-Action model (VLA) trained
on large-scale human videos. Existing VLAs struggle with complex manipulation
tasks requiring high dexterity and generalize poorly to novel scenarios and
tasks, primarily due to their reliance on synthetic data with significant
sim-to-real gaps or teleoperated demonstrations lacking scale and diversity. To
address this data bottleneck, we propose leveraging human hands as a foundation
manipulator, capitalizing on the rich dexterity and scalability present in web
data. Our approach centers on physical instruction tuning, a novel training
paradigm that combines large-scale VLA pretraining from human videos, physical
space alignment for 3D reasoning, and post-training adaptation for robotic
tasks. Additionally, we introduce a part-level motion tokenization method which
achieves millimeter-level reconstruction accuracy to model precise hand
trajectories for action learning. To support our proposed paradigm, we further
develop a comprehensive data curation pipeline that integrates heterogeneous
sources -- including motion capture, VR, and RGB-only videos -- into a
large-scale dataset with millions of motion-based instructional instances. We
empirically show the excellence of Being-H0 in hand motion generation and
instruction following, and it also scales well with model and data sizes.
Importantly, we observe the expected gains of Being-H0 in real-world robotic
manipulation as physical instruction tuning is applied. More details are
available at https://beingbeyond.github.io/Being-H0.

</details>


### [443] [EgoPrune: Efficient Token Pruning for Egomotion Video Reasoning in Embodied Agent](https://arxiv.org/abs/2507.15428)
*Jiaao Li,Kaiyuan Li,Chen Gao,Yong Li,Xinlei Chen*

Main category: cs.CV

TL;DR: EgoPrune是一种无需训练的令牌修剪方法，专为自我运动视频推理设计，通过关键帧选择、视角感知冗余过滤和MMR令牌选择器，显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 自我运动视频是具身AI代理的主要视觉输入，现有方法无法有效利用其时空连续性，导致计算成本高。

Method: EgoPrune包括关键帧选择、视角感知冗余过滤和MMR令牌选择器，无需训练即可高效修剪令牌。

Result: 在两种基准测试中，EgoPrune优于现有方法，显著减少FLOPs、内存使用和延迟，并在边缘设备上验证了其实际效率。

Conclusion: EgoPrune为自我运动视频推理提供了一种高效、实用的解决方案，适用于实际部署。

Abstract: Egomotion videos are first-person recordings where the view changes
continuously due to the agent's movement. As they serve as the primary visual
input for embodied AI agents, making egomotion video reasoning more efficient
is therefore essential for real-world deployment. Recent advances in
vision-language models have enabled strong multimodal reasoning capabilities,
but their computational cost remains prohibitive for long, redundant video
inputs. Existing token pruning methods, typically designed for third-person
videos, fail to leverage the spatiotemporal continuity and motion constraints
inherent in egomotion settings. To address this, we propose EgoPrune, a
training-free token pruning method tailored for egomotion video reasoning.
EgoPrune comprises three components: a keyframe selector adapted from EmbodiedR
for temporally efficient sampling; Perspective-Aware Redundancy Filtering
(PARF), which aligns visual tokens using perspective transformations and
removes redundant tokens; and a Maximal Marginal Relevance (MMR)-based token
selector that jointly considers visual-text relevance and intra-frame
diversity. Experiments on two egomotion video benchmarks show that EgoPrune
consistently outperforms prior training-free methods across various pruning
ratios while significantly reducing FLOPs, memory usage, and latency. Moreover,
we deploy EgoPrune on an embodied agent equipped with a Jetson Orin NX 16GB
edge device, demonstrating its real-world efficiency and suitability for
on-device egomotion video reasoning.

</details>


### [444] [ConformalSAM: Unlocking the Potential of Foundational Segmentation Models in Semi-Supervised Semantic Segmentation with Conformal Prediction](https://arxiv.org/abs/2507.15803)
*Danhui Chen,Ziquan Liu,Chuxi Yang,Dan Wang,Yan Yan,Yi Xu,Xiangyang Ji*

Main category: cs.CV

TL;DR: ConformalSAM利用基础分割模型SEEM生成未标注数据的预测掩码，并通过不确定性校准和自依赖训练策略提升半监督语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 解决像素级视觉任务中标注数据稀缺的问题，利用基础分割模型SEEM作为标注工具，并通过校准和过滤提升其可靠性。

Method: 提出ConformalSAM框架，先校准基础模型，再过滤不可靠像素标签，结合自依赖训练策略避免过拟合。

Result: 在三个标准半监督语义分割基准上，ConformalSAM性能优于现有方法，并可作为插件提升其他方法。

Conclusion: ConformalSAM通过有效利用基础分割模型和不确定性校准，显著提升了半监督语义分割的性能。

Abstract: Pixel-level vision tasks, such as semantic segmentation, require extensive
and high-quality annotated data, which is costly to obtain. Semi-supervised
semantic segmentation (SSSS) has emerged as a solution to alleviate the
labeling burden by leveraging both labeled and unlabeled data through
self-training techniques. Meanwhile, the advent of foundational segmentation
models pre-trained on massive data, has shown the potential to generalize
across domains effectively. This work explores whether a foundational
segmentation model can address label scarcity in the pixel-level vision task as
an annotator for unlabeled images. Specifically, we investigate the efficacy of
using SEEM, a Segment Anything Model (SAM) variant fine-tuned for textual
input, to generate predictive masks for unlabeled data. To address the
shortcomings of using SEEM-generated masks as supervision, we propose
ConformalSAM, a novel SSSS framework which first calibrates the foundation
model using the target domain's labeled data and then filters out unreliable
pixel labels of unlabeled data so that only high-confidence labels are used as
supervision. By leveraging conformal prediction (CP) to adapt foundation models
to target data through uncertainty calibration, ConformalSAM exploits the
strong capability of the foundational segmentation model reliably which
benefits the early-stage learning, while a subsequent self-reliance training
strategy mitigates overfitting to SEEM-generated masks in the later training
stage. Our experiment demonstrates that, on three standard benchmarks of SSSS,
ConformalSAM achieves superior performance compared to recent SSSS methods and
helps boost the performance of those methods as a plug-in.

</details>


### [445] [Diffusion models for multivariate subsurface generation and efficient probabilistic inversion](https://arxiv.org/abs/2507.15809)
*Roberto Miele,Niklas Linde*

Main category: cs.CV

TL;DR: 扩散模型在多元地下建模和概率反演中表现出色，优于变分自编码器和生成对抗网络，并通过改进的扩散后验采样方法提高了统计稳健性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在多元地下建模和概率反演中的应用，以提升建模能力和计算效率。

Method: 提出对扩散后验采样方法的改进，包括考虑噪声污染的似然近似，并在多元地质场景中进行测试。

Result: 相比原始方法，改进后的方法在统计稳健性、后验概率密度采样和计算成本方面表现更优。

Conclusion: 该方法适用于硬数据和间接条件数据，且反演过程更快，优于其他需要外循环的方法。

Abstract: Diffusion models offer stable training and state-of-the-art performance for
deep generative modeling tasks. Here, we consider their use in the context of
multivariate subsurface modeling and probabilistic inversion. We first
demonstrate that diffusion models enhance multivariate modeling capabilities
compared to variational autoencoders and generative adversarial networks. In
diffusion modeling, the generative process involves a comparatively large
number of time steps with update rules that can be modified to account for
conditioning data. We propose different corrections to the popular Diffusion
Posterior Sampling approach by Chung et al. (2023). In particular, we introduce
a likelihood approximation accounting for the noise-contamination that is
inherent in diffusion modeling. We assess performance in a multivariate
geological scenario involving facies and correlated acoustic impedance.
Conditional modeling is demonstrated using both local hard data (well logs) and
nonlinear geophysics (fullstack seismic data). Our tests show significantly
improved statistical robustness, enhanced sampling of the posterior probability
density function and reduced computational costs, compared to the original
approach. The method can be used with both hard and indirect conditioning data,
individually or simultaneously. As the inversion is included within the
diffusion process, it is faster than other methods requiring an outer-loop
around the generative model, such as Markov chain Monte Carlo.

</details>


### [446] [Uncovering Critical Features for Deepfake Detection through the Lottery Ticket Hypothesis](https://arxiv.org/abs/2507.15636)
*Lisan Al Amin,Md. Ismail Hossain,Thanh Thi Nguyen,Tasnim Jahan,Mahbubul Islam,Faisal Quader*

Main category: cs.CV

TL;DR: 该研究探讨了彩票假设（LTH）在深度伪造检测中的应用，通过剪枝神经网络识别关键特征，发现子网络在高度稀疏下仍能保持性能。


<details>
  <summary>Details</summary>
Motivation: 深度伪造技术对信息完整性和社会信任构成挑战，现有检测方法模型庞大且机制不明确，难以在资源有限环境中部署。

Method: 研究采用彩票假设（LTH）对MesoNet、CNN-5和ResNet-18架构进行剪枝，通过迭代幅度剪枝方法识别关键子网络。

Result: 实验显示，MesoNet在80%稀疏度下仍保持56.2%准确率（基线为62.6%），且LTH方法优于一次性剪枝。

Conclusion: 研究证明了LTH在深度伪造检测中的有效性，子网络具有跨数据集的可迁移性，为高效部署提供了可能。

Abstract: Recent advances in deepfake technology have created increasingly convincing
synthetic media that poses significant challenges to information integrity and
social trust. While current detection methods show promise, their underlying
mechanisms remain poorly understood, and the large sizes of their models make
them challenging to deploy in resource-limited environments. This study
investigates the application of the Lottery Ticket Hypothesis (LTH) to deepfake
detection, aiming to identify the key features crucial for recognizing
deepfakes. We examine how neural networks can be efficiently pruned while
maintaining high detection accuracy. Through extensive experiments with
MesoNet, CNN-5, and ResNet-18 architectures on the OpenForensic and
FaceForensics++ datasets, we find that deepfake detection networks contain
winning tickets, i.e., subnetworks, that preserve performance even at
substantial sparsity levels. Our results indicate that MesoNet retains 56.2%
accuracy at 80% sparsity on the OpenForensic dataset, with only 3,000
parameters, which is about 90% of its baseline accuracy (62.6%). The results
also show that our proposed LTH-based iterative magnitude pruning approach
consistently outperforms one-shot pruning methods. Using Grad-CAM
visualization, we analyze how pruned networks maintain their focus on critical
facial regions for deepfake detection. Additionally, we demonstrate the
transferability of winning tickets across datasets, suggesting potential for
efficient, deployable deepfake detection systems.

</details>


### [447] [LINR-PCGC: Lossless Implicit Neural Representations for Point Cloud Geometry Compression](https://arxiv.org/abs/2507.15686)
*Wenjie Huang,Qi Yang,Shuting Xia,He Huang,Zhu Li,Yiling Xu*

Main category: cs.CV

TL;DR: 提出了一种基于隐式神经表示（INR）的无损点云几何压缩方法LINR-PCGC，解决了现有方法对训练数据分布的依赖问题，并优化了编码时间和解码器大小。


<details>
  <summary>Details</summary>
Motivation: 现有AI点云压缩方法依赖特定数据分布，限制了实际应用；而INR方法虽能解决此问题，但仅支持有损压缩且编码时间和解码器大小受限。

Method: 设计了点云级编码框架和高效网络初始化策略，减少60%编码时间；提出基于多尺度SparseConv的轻量级编码网络，实现快速推理和小型解码器。

Result: 实验显示，LINR-PCGC在MVUB数据集上比G-PCC TMC13v23和SparsePCGC分别减少21.21%和21.95%的比特流。

Conclusion: LINR-PCGC首次实现基于INR的无损点云几何压缩，显著优于传统和AI方法，具有实际应用潜力。

Abstract: Existing AI-based point cloud compression methods struggle with dependence on
specific training data distributions, which limits their real-world deployment.
Implicit Neural Representation (INR) methods solve the above problem by
encoding overfitted network parameters to the bitstream, resulting in more
distribution-agnostic results. However, due to the limitation of encoding time
and decoder size, current INR based methods only consider lossy geometry
compression. In this paper, we propose the first INR based lossless point cloud
geometry compression method called Lossless Implicit Neural Representations for
Point Cloud Geometry Compression (LINR-PCGC). To accelerate encoding speed, we
design a group of point clouds level coding framework with an effective network
initialization strategy, which can reduce around 60% encoding time. A
lightweight coding network based on multiscale SparseConv, consisting of scale
context extraction, child node prediction, and model compression modules, is
proposed to realize fast inference and compact decoder size. Experimental
results show that our method consistently outperforms traditional and AI-based
methods: for example, with the convergence time in the MVUB dataset, our method
reduces the bitstream by approximately 21.21% compared to G-PCC TMC13v23 and
21.95% compared to SparsePCGC. Our project can be seen on
https://huangwenjie2023.github.io/LINR-PCGC/.

</details>


### [448] [True Multimodal In-Context Learning Needs Attention to the Visual Context](https://arxiv.org/abs/2507.15807)
*Shuo Chen,Jianzhe Liu,Zhen Han,Yan Xia,Daniel Cremers,Philip Torr,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: 论文提出了一种动态注意力重分配（DARA）方法，并引入了TrueMICL数据集，以解决多模态大语言模型（MLLMs）在上下文学习中过度依赖文本而忽视视觉信息的问题。


<details>
  <summary>Details</summary>
Motivation: 当前MLLMs在多模态上下文学习（MICL）中倾向于忽略视觉线索，仅依赖文本模式，导致其实际效用受限。

Method: 提出了动态注意力重分配（DARA）方法，通过调整视觉和文本标记的注意力权重，鼓励模型关注视觉内容。同时，引入了TrueMICL数据集，明确要求整合多模态信息。

Result: 实验表明，该方法显著提升了多模态上下文学习的真实能力。

Conclusion: DARA和TrueMICL的结合有效解决了MLLMs在MICL中的视觉信息利用不足问题，提升了模型的实用性。

Abstract: Multimodal Large Language Models (MLLMs), built on powerful language
backbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new
tasks from a few multimodal demonstrations consisting of images, questions, and
answers. Despite showing noticeable improvement on standard vision-language
datasets, current MLLMs struggle to leverage visual information in the
demonstrations. Specifically, they tend to neglect visual cues and over-rely on
textual patterns, leading to mere text imitation rather than genuine multimodal
adaptation. This behavior makes MICL still unimodal and largely restricts its
practical utility. More importantly, this limitation is often concealed by the
improved performance on tasks that do not require understanding the visual
context. As a result, how to effectively enhance MICL ability and reliably
evaluate the MICL performance remains underexplored. To address these issues,
we first introduce Dynamic Attention Reallocation (DARA), an efficient
fine-tuning strategy that encourages models to attend to the visual context by
rebalancing attention across visual and textual tokens. In addition, we present
TrueMICL, an MICL-dedicated dataset with both support and test sets that
explicitly requires the integration of multimodal information-particularly
visual content-for correct task completion. Extensive experiments demonstrate
the effectiveness of our holistic solution, showcasing substantial improvements
in the true multimodal in-context learning capabilities. Code and datasets are
available at https://chenxshuo.github.io/true-micl-colm .

</details>


### [449] [SeC: Advancing Complex Video Object Segmentation via Progressive Concept Construction](https://arxiv.org/abs/2507.15852)
*Zhixiong Zhang,Shuangrui Ding,Xiaoyi Dong,Songxin He,Jianfan Lin,Junsong Tang,Yuhang Zang,Yuhang Cao,Dahua Lin,Jiaqi Wang*

Main category: cs.CV

TL;DR: 论文提出了一种概念驱动的视频对象分割框架（SeC），通过结合视觉语言模型（LVLM）构建高级概念表示，显著提升了在复杂场景中的分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有视频对象分割方法依赖外观匹配，难以应对剧烈视觉变化和复杂场景变化，缺乏人类对对象的概念理解能力。

Method: SeC框架利用LVLM整合多帧视觉线索，构建对象中心的概念表示，并动态平衡语义推理与特征匹配。

Result: 在SeCVOS基准测试中，SeC比SAM 2.1提升了11.8个百分点，达到新的最优性能。

Conclusion: SeC通过概念驱动的分割方法，显著提升了复杂场景下的视频对象分割能力。

Abstract: Video Object Segmentation (VOS) is a core task in computer vision, requiring
models to track and segment target objects across video frames. Despite notable
advances with recent efforts, current techniques still lag behind human
capabilities in handling drastic visual variations, occlusions, and complex
scene changes. This limitation arises from their reliance on appearance
matching, neglecting the human-like conceptual understanding of objects that
enables robust identification across temporal dynamics. Motivated by this gap,
we propose Segment Concept (SeC), a concept-driven segmentation framework that
shifts from conventional feature matching to the progressive construction and
utilization of high-level, object-centric representations. SeC employs Large
Vision-Language Models (LVLMs) to integrate visual cues across diverse frames,
constructing robust conceptual priors. During inference, SeC forms a
comprehensive semantic representation of the target based on processed frames,
realizing robust segmentation of follow-up frames. Furthermore, SeC adaptively
balances LVLM-based semantic reasoning with enhanced feature matching,
dynamically adjusting computational efforts based on scene complexity. To
rigorously assess VOS methods in scenarios demanding high-level conceptual
reasoning and robust semantic understanding, we introduce the Semantic Complex
Scenarios Video Object Segmentation benchmark (SeCVOS). SeCVOS comprises 160
manually annotated multi-scenario videos designed to challenge models with
substantial appearance variations and dynamic scene transformations. In
particular, SeC achieves an 11.8-point improvement over SAM 2.1 on SeCVOS,
establishing a new state-of-the-art in concept-aware video object segmentation.

</details>


<div id='stat.ME'></div>

# stat.ME [[Back]](#toc)

### [450] [Misspecifying non-compensatory as compensatory IRT: analysis of estimated skills and variance](https://arxiv.org/abs/2507.15222)
*Hiroshi Tamano,Hideitsu Hino,Daichi Mochihashi*

Main category: stat.ME

TL;DR: 多维项目反应理论用于估计学习者的潜在技能和问题难度。研究发现，当非补偿模型被误设为补偿模型时，高技能会被低估，但机制未明。本文通过理论方法揭示了低估和高估现象，并探讨了参数估计的渐近方差差异。


<details>
  <summary>Details</summary>
Motivation: 研究多维项目反应理论中模型误设导致的技能低估和高估现象，以及参数估计的渐近方差差异，以填补现有研究的空白。

Method: 采用理论分析方法，探讨补偿和非补偿模型误设对技能估计的影响，并分析参数估计的渐近方差。

Result: 发现高技能被低估，同时在原点附近存在技能高估现象；模型误设还会影响参数估计的渐近方差。

Conclusion: 研究揭示了模型误设对技能估计的双向影响（低估和高估）及其对参数方差的影响，为多维项目反应理论的应用提供了更全面的理解。

Abstract: Multidimensional item response theory is a statistical test theory used to
estimate the latent skills of learners and the difficulty levels of problems
based on test results. Both compensatory and non-compensatory models have been
proposed in the literature. Previous studies have revealed the substantial
underestimation of higher skills when the non-compensatory model is
misspecified as the compensatory model. However, the underlying mechanism
behind this phenomenon has not been fully elucidated. It remains unclear
whether overestimation also occurs and whether issues arise regarding the
variance of the estimated parameters. In this paper, we aim to provide a
comprehensive understanding of both underestimation and overestimation through
a theoretical approach. In addition to the previously identified
underestimation of the skills, we newly discover that the overestimation of
skills occurs around the origin. Furthermore, we investigate the extent to
which the asymptotic variance of the estimated parameters differs when
considering model misspecification compared to when it is not taken into
account.

</details>


### [451] [Robust and Differentially Private PCA for non-Gaussian data](https://arxiv.org/abs/2507.15232)
*Minwoo Kim,Sungkyu Jung*

Main category: stat.ME

TL;DR: 提出了一种适用于重尾和可能受污染数据的差分隐私PCA方法，通过有界变换实现隐私保护，并在理论和实验中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有隐私保护PCA方法依赖限制性假设（如亚高斯数据）或易受数据污染，计算成本高或依赖未知参数，限制了其应用。

Method: 利用椭圆分布下协方差矩阵的特征向量和顺序不变性，通过有界变换实现差分隐私PCA，保证鲁棒性。

Result: 理论和实验表明，该方法在非高斯或受污染数据中能更好地恢复主成分子空间，统计效用优于现有方法。

Conclusion: 该方法为隐私保护PCA提供了更通用和鲁棒的解决方案，适用于复杂数据场景。

Abstract: Recent advances have sparked significant interest in the development of
privacy-preserving Principal Component Analysis (PCA). However, many existing
approaches rely on restrictive assumptions, such as assuming sub-Gaussian data
or being vulnerable to data contamination. Additionally, some methods are
computationally expensive or depend on unknown model parameters that must be
estimated, limiting their accessibility for data analysts seeking
privacy-preserving PCA. In this paper, we propose a differentially private PCA
method applicable to heavy-tailed and potentially contaminated data. Our
approach leverages the property that the covariance matrix of properly rescaled
data preserves eigenvectors and their order under elliptical distributions,
which include Gaussian and heavy-tailed distributions. By applying a bounded
transformation, we enable straightforward computation of principal components
in a differentially private manner. Additionally, boundedness guarantees
robustness against data contamination. We conduct both theoretical analysis and
empirical evaluations of the proposed method, focusing on its ability to
recover the subspace spanned by the leading principal components. Extensive
numerical experiments demonstrate that our method consistently outperforms
existing approaches in terms of statistical utility, particularly in
non-Gaussian or contaminated data settings.

</details>


### [452] [ACS: An interactive framework for conformal selection](https://arxiv.org/abs/2507.15825)
*Yu Gui,Ying Jin,Yash Nair,Zhimei Ren*

Main category: stat.ME

TL;DR: 本文提出了一种自适应共形选择（ACS）框架，支持无模型选择并保证错误控制，适用于人机交互式数据分析。


<details>
  <summary>Details</summary>
Motivation: 传统方法在数据分析和模型选择中缺乏灵活性和错误控制，ACS旨在解决这些问题，支持动态数据探索和决策。

Method: ACS基于共形选择框架，通过设计信息控制原则，支持数据部分重用、动态决策和信息整合，同时控制假发现率（FDR）。

Result: ACS在数值模拟和实际应用（如大语言模型部署和药物发现）中表现出有效性。

Conclusion: ACS为自适应数据分析提供了灵活且严格的方法，适用于多种场景。

Abstract: This paper presents adaptive conformal selection (ACS), an interactive
framework for model-free selection with guaranteed error control. Building on
conformal selection (Jin and Cand\`es, 2023b), ACS generalizes the approach to
support human-in-the-loop adaptive data analysis. Under the ACS framework, we
can partially reuse the data to boost the selection power, make decisions on
the fly while exploring the data, and incorporate new information or
preferences as they arise. The key to ACS is a carefully designed principle
that controls the information available for decision making, allowing the data
analyst to explore the data adaptively while maintaining rigorous control of
the false discovery rate (FDR). Based on the ACS framework, we provide concrete
selection algorithms for various goals, including model update/selection,
diversified selection, and incorporating newly available labeled data. The
effectiveness of ACS is demonstrated through extensive numerical simulations
and real-data applications in large language model (LLM) deployment and drug
discovery.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [453] [The New LLM Bottleneck: A Systems Perspective on Latent Attention and Mixture-of-Experts](https://arxiv.org/abs/2507.15465)
*Sungmin Yun,Seonyong Park,Hwayong Nam,Younjoo Lee,Gunjun Lee,Kwanhee Kyung,Sangpyo Kim,Nam Sung Kim,Jongmin Kim,Hyungyo Kim,Juhwan Cho,Seungmin Baek,Jung Ho Ahn*

Main category: cs.AR

TL;DR: 论文指出传统Transformer模型的计算负载分为内存受限的多头注意力（MHA）和计算受限的前馈层，但新型架构（MLA和MoE）改变了这一局面，减少了对专用硬件的需求。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型中MHA的内存瓶颈问题长期推动专用硬件研究，但新型架构（MLA和MoE）的出现挑战了这一需求。

Method: 通过分析MLA和MoE的计算特性，发现MLA的计算强度显著提高，MoE通过批处理调整计算强度，使模型更均衡。

Result: MLA的计算强度比MHA高两个数量级，MoE通过批处理实现计算强度匹配，减少了对专用硬件的依赖。

Conclusion: 下一代Transformer的设计重点应转向构建均衡系统，而非单一内存层加速。

Abstract: Computational workloads composing traditional Transformer models are starkly
bifurcated. Multi-Head Attention (MHA) is memory-bound, with low arithmetic
intensity, while feedforward layers are compute-bound. This dichotomy has long
motivated research into specialized hardware to mitigate the MHA bottleneck.
  This paper argues that recent architectural shifts, namely Multi-head Latent
Attention (MLA) and Mixture-of-Experts (MoE), challenge the premise of
specialized attention hardware. We make two key observations. First, the
arithmetic intensity of MLA is over two orders of magnitude greater than that
of MHA, shifting it close to a compute-bound regime well-suited for modern
accelerators like GPUs. Second, by distributing MoE experts across a pool of
accelerators, their arithmetic intensity can be tuned through batching to match
that of the dense layers, creating a more balanced computational profile.
  These findings reveal a diminishing need for specialized attention hardware.
The central challenge for next-generation Transformers is no longer
accelerating a single memory-bound layer. Instead, the focus must shift to
designing balanced systems with sufficient compute, memory capacity, memory
bandwidth, and high-bandwidth interconnects to manage the diverse demands of
large-scale models.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [454] [Geophysics-informed neural network for model-based seismic inversion using surrogate point spread functions](https://arxiv.org/abs/2507.14140)
*Marcus Saraiva,Ana Muller,Alexandre Maul*

Main category: physics.geo-ph

TL;DR: 提出了一种基于物理信息神经网络（GINN）的地震反演方法，结合深度学习与地震建模，解决了传统方法依赖1D平均小波和假设不现实横向分辨率的问题。


<details>
  <summary>Details</summary>
Motivation: 传统地震反演方法存在依赖1D平均小波和假设不现实横向分辨率的局限性，需要更高效、精确的解决方案。

Method: 使用深度卷积神经网络（DCNN）同时估计点扩散函数（PSFs）和声阻抗（IP），将PSFs分为零相位和残差分量以确保物理一致性。采用2D UNet架构和自监督损失函数（MSE+SSIM）进行训练。

Result: GINN能够生成高分辨率IP和真实PSFs，减少噪声并提高准确性，优于传统1D小波方法。

Conclusion: GINN是一种有效的地震反演方法，未来将优化训练过程并验证实际数据应用。

Abstract: Model-based seismic inversion is a key technique in reservoir
characterization, but traditional methods face significant limitations, such as
relying on 1D average stationary wavelets and assuming an unrealistic lateral
resolution. To address these challenges, we propose a Geophysics-Informed
Neural Network (GINN) that integrates deep learning with seismic modeling. This
novel approach employs a Deep Convolutional Neural Network (DCNN) to
simultaneously estimate Point Spread Functions (PSFs) and acoustic impedance
(IP). PSFs are divided into zero-phase and residual components to ensure
geophysical consistency and to capture fine details. We used synthetic data
from the SEAM Phase I Earth Model to train the GINN for 100 epochs
(approximately 20 minutes) using a 2D UNet architecture. The network's inputs
include positional features and a low-frequency impedance (LF-IP) model. A
self-supervised loss function combining Mean Squared Error (MSE) and Structural
Similarity Index Measure (SSIM) was employed to ensure accurate results. The
GINN demonstrated its ability to generate high-resolution IP and realistic
PSFs, aligning with expected geological features. Unlike traditional 1D
wavelets, the GINN produces PSFs with limited lateral resolution, reducing
noise and improving accuracy. Future work will aim to refine the training
process and validate the methodology with real seismic data.

</details>


### [455] [Integrating Newton's Laws with deep learning for enhanced physics-informed compound flood modelling](https://arxiv.org/abs/2507.15021)
*Soheil Radfar,Faezeh Maghsoodifar,Hamed Moftakhari,Hamid Moradkhani*

Main category: physics.geo-ph

TL;DR: ALPINE是一种物理信息神经网络框架，用于复合洪水建模，通过同时强制执行浅水动力学方程，显著提高了预测准确性。


<details>
  <summary>Details</summary>
Motivation: 传统流体动力学模型计算资源需求高，机器学习模型则牺牲物理一致性。ALPINE旨在解决这些问题，提供快速且物理一致的预测。

Method: ALPINE结合卷积编码器-解码器架构和ConvLSTM，通过复合损失函数平衡数据保真度和物理约束。

Result: 在六次历史风暴事件中，ALPINE显著降低了预测误差，并在风暴高峰期表现最佳。

Conclusion: ALPINE是一种物理一致的模拟器，适用于复合洪水预报和大规模风险分析。

Abstract: Coastal communities increasingly face compound floods, where multiple drivers
like storm surge, high tide, heavy rainfall, and river discharge occur together
or in sequence to produce impacts far greater than any single driver alone.
Traditional hydrodynamic models can provide accurate physics-based simulations
but require substantial computational resources for real-time applications or
risk assessments, while machine learning alternatives often sacrifice physical
consistency for speed, producing unrealistic predictions during extreme events.
This study addresses these challenges by developing ALPINE (All-in-one Physics
Informed Neural Emulator), a physics-informed neural network (PINN) framework
to enforce complete shallow water dynamics in compound flood modeling. Unlike
previous approaches that implement partial constraints, our framework
simultaneously enforces mass conservation and both momentum equations, ensuring
full adherence to Newton's laws throughout the prediction process. The model
integrates a convolutional encoder-decoder architecture with ConvLSTM temporal
processing, trained using a composite loss function that balances data fidelity
with physics-based residuals. Using six historical storm events (four for
training, one for validation, and one held-out for unseen testing), we observe
substantial improvements over baseline neural networks. ALPINE reduces
domain-averaged prediction errors and improves model skill metrics for water
surface elevation and velocity components. Physics-informed constraints prove
most valuable during peak storm intensity, when multiple flood drivers interact
and reliable predictions matter most. This approach yields a physically
consistent emulator capable of supporting compound-flood forecasting and
large-scale risk analyses while preserving physical realism essential for
coastal emergency management.

</details>


<div id='cs.NI'></div>

# cs.NI [[Back]](#toc)

### [456] [A Disentangled Representation Learning Framework for Low-altitude Network Coverage Prediction](https://arxiv.org/abs/2507.14186)
*Xiaojie Li,Zhijie Cai,Nan Qi,Chao Dong,Guangxu Zhu,Haixia Ma,Qihui Wu,Shi Jin*

Main category: cs.NI

TL;DR: 论文提出了一种基于专家知识特征压缩和解耦表示学习的双策略方法，用于解决低空网络覆盖预测中的特征采样不平衡和数据稀疏性问题。


<details>
  <summary>Details</summary>
Motivation: 低空网络覆盖预测对设计空中走廊至关重要，但基站天线波束模式通常不可获取，且低空路测数据稀疏，导致特征采样不平衡和模型泛化能力不足。

Method: 结合专家知识进行特征压缩以降低特征空间复杂度，并通过解耦表示学习整合传播模型和子网络，增强模型泛化能力。

Result: 实验表明该方法比最佳基线算法误差降低7%，实际网络验证中MAE误差达到5dB水平。

Conclusion: 该方法有效解决了低空网络覆盖预测中的挑战，具有实际应用价值。

Abstract: The expansion of the low-altitude economy has underscored the significance of
Low-Altitude Network Coverage (LANC) prediction for designing aerial corridors.
While accurate LANC forecasting hinges on the antenna beam patterns of Base
Stations (BSs), these patterns are typically proprietary and not readily
accessible. Operational parameters of BSs, which inherently contain beam
information, offer an opportunity for data-driven low-altitude coverage
prediction. However, collecting extensive low-altitude road test data is
cost-prohibitive, often yielding only sparse samples per BS. This scarcity
results in two primary challenges: imbalanced feature sampling due to limited
variability in high-dimensional operational parameters against the backdrop of
substantial changes in low-dimensional sampling locations, and diminished
generalizability stemming from insufficient data samples. To overcome these
obstacles, we introduce a dual strategy comprising expert knowledge-based
feature compression and disentangled representation learning. The former
reduces feature space complexity by leveraging communications expertise, while
the latter enhances model generalizability through the integration of
propagation models and distinct subnetworks that capture and aggregate the
semantic representations of latent features. Experimental evaluation confirms
the efficacy of our framework, yielding a 7% reduction in error compared to the
best baseline algorithm. Real-network validations further attest to its
reliability, achieving practical prediction accuracy with MAE errors at the 5dB
level.

</details>


### [457] [From Cell Towers to Satellites: A 2040 Blueprint for Urban-Grade Direct-to-Device Mobile Networks](https://arxiv.org/abs/2507.14188)
*Sebastian Barros Elgueta*

Main category: cs.NI

TL;DR: 论文探讨了完全基于轨道的移动网络系统，包括5G核心功能、激光回传等技术，并分析了其在密集城市中的可行性。


<details>
  <summary>Details</summary>
Motivation: 验证是否可以在轨道上运行完整的移动网络，并提供城市级服务，摆脱对地面基础设施的依赖。

Method: 提出了一种全轨道电信系统架构，结合电子控制相控阵、5G核心功能部署和卫星间激光回传，并通过仿真分析性能。

Result: 仿真显示屋顶和视线用户可维持64-QAM吞吐量，街道级接入需中继或辅助波束模式，技术瓶颈为工程问题而非物理限制。

Conclusion: 论文提出了15年路线图，从现有回退系统逐步过渡到完全自主的轨道覆盖网络，为未来移动通信提供新方向。

Abstract: In 2023, satellite and mobile networks crossed a historic threshold: standard
smartphones, using unmodified 3GPP protocols, connected directly to low Earth
orbit (LEO) satellites. This first wave of direct-to-device (D2D)
demonstrations validated the physical feasibility of satellite-based mobile
access. However, these systems remain fallback-grade--rural-only,
bandwidth-limited, and fully dependent on Earth-based mobile cores for
identity, session, and policy control. This paper asks a more ambitious
question: Can a complete mobile network, including radio access, core
functions, traffic routing, and content delivery, operate entirely from orbit?
And can it deliver sustained, urban-grade service in the world's densest
cities? We present the first end-to-end system architecture for a fully orbital
telco, integrating electronically steered phased arrays with 1000-beam
capacity, space-based deployment of 5G core functions (UPF, AMF), and
inter-satellite laser mesh backhaul. We analyze spectral efficiency, beam
capacity, and link budgets under dense urban conditions, accounting for path
loss, Doppler, and multipath. Simulations show that rooftop and line-of-sight
users can sustain 64-QAM throughput, while street-level access is feasible with
relay or assisted beam modes. The paper outlines the remaining constraints,
power, thermal dissipation, compute radiation hardening, and regulatory models,
and demonstrates that these are engineering bottlenecks, not physical limits.
Finally, we propose a staged 15-year roadmap from today's fallback D2D systems
to autonomous orbital overlays delivering 50-100 Mbps to handhelds in
megacities, with zero reliance on terrestrial infrastructure.

</details>


### [458] [PRATA: A Framework to Enable Predictive QoS in Vehicular Networks via Artificial Intelligence](https://arxiv.org/abs/2507.14211)
*Federico Mason,Tommaso Zugno,Matteo Drago,Marco Giordani,Mate Boban,Michele Zorzi*

Main category: cs.NI

TL;DR: PRATA是一个基于AI的预测性QoS框架，用于优化远程驾驶应用中的网络性能，通过RL方法显著提升系统表现。


<details>
  <summary>Details</summary>
Motivation: 远程驾驶对延迟和可靠性有严格要求，预测性QoS（PQoS）能提前应对网络变化，避免性能下降。

Method: PRATA框架包括5G RAN模拟、汽车数据生成工具和AI单元，通过RL优化数据分段以应对资源饱和或信道退化。

Result: RAN-AI单元在QoS与QoE间取得平衡，系统性能较基线方法提升近一倍。

Conclusion: PRATA证明了RL在优化PQoS中的有效性，同时探讨了状态空间和网络数据获取成本对RL实现的影响。

Abstract: Predictive Quality of Service (PQoS) makes it possible to anticipate QoS
changes, e.g., in wireless networks, and trigger appropriate countermeasures to
avoid performance degradation. Hence, PQoS is extremely useful for automotive
applications such as teleoperated driving, which poses strict constraints in
terms of latency and reliability. A promising tool for PQoS is given by
Reinforcement Learning (RL), a methodology that enables the design of
decision-making strategies for stochastic optimization. In this manuscript, we
present PRATA, a new simulation framework to enable PRedictive QoS based on AI
for Teleoperated driving Applications. PRATA consists of a modular pipeline
that includes (i) an end-to-end protocol stack to simulate the 5G Radio Access
Network (RAN), (ii) a tool for generating automotive data, and (iii) an
Artificial Intelligence (AI) unit to optimize PQoS decisions. To prove its
utility, we use PRATA to design an RL unit, named RAN-AI, to optimize the
segmentation level of teleoperated driving data in the event of resource
saturation or channel degradation. Hence, we show that the RAN-AI entity
efficiently balances the trade-off between QoS and Quality of Experience (QoE)
that characterize teleoperated driving applications, almost doubling the system
performance compared to baseline approaches. In addition, by varying the
learning settings of the RAN-AI entity, we investigate the impact of the state
space and the relative cost of acquiring network data that are necessary for
the implementation of RL.

</details>


### [459] [Intent-Based Network for RAN Management with Large Language Models](https://arxiv.org/abs/2507.14230)
*Fransiscus Asisi Bimo,Maria Amparo Canaveras Galdon,Chun-Kai Lai,Ray-Guang Cheng,Edwin K. P. Chong*

Main category: cs.NI

TL;DR: 本文提出了一种基于大型语言模型（LLMs）的意图驱动无线接入网络（RAN）管理自动化方法，通过结构化提示工程和闭环机制提升能效。


<details>
  <summary>Details</summary>
Motivation: 应对无线网络管理复杂性的增加，利用LLMs实现高级智能自动化。

Method: 结合LLMs的代理架构，通过结构化提示工程和闭环机制动态优化RAN参数。

Result: 网络能效自动提升，展示了LLM驱动的代理系统在实时反馈中实现稳健资源管理的潜力。

Conclusion: LLMs在意图驱动的RAN管理中具有显著潜力，能通过自动化提升网络性能。

Abstract: Advanced intelligent automation becomes an important feature to deal with the
increased complexity in managing wireless networks. This paper proposes a novel
automation approach of intent-based network for Radio Access Networks (RANs)
management by leveraging Large Language Models (LLMs). The proposed method
enhances intent translation, autonomously interpreting high-level objectives,
reasoning over complex network states, and generating precise configurations of
the RAN by integrating LLMs within an agentic architecture. We propose a
structured prompt engineering technique and demonstrate that the network can
automatically improve its energy efficiency by dynamically optimizing critical
RAN parameters through a closed-loop mechanism. It showcases the potential to
enable robust resource management in RAN by adapting strategies based on
real-time feedback via LLM-orchestrated agentic systems.

</details>


### [460] [Beyond DNS: Unlocking the Internet of AI Agents via the NANDA Index and Verified AgentFacts](https://arxiv.org/abs/2507.14263)
*Ramesh Raskar,Pradyumna Chari,John Zinky,Mahesh Lambe,Jared James Grogan,Sichao Wang,Rajesh Ranjan,Rekha Singhal,Shailja Gupta,Robert Lincourt,Raghu Bala,Aditi Joshi,Abhishek Singh,Ayush Chopra,Dimitris Stripelis,Bhuwan B,Sumit Kumar,Maria Gorskikh*

Main category: cs.NI

TL;DR: NANDA索引架构为AI代理互联网提供可发现性、可识别性和认证，支持动态、加密验证的AgentFacts，具备多端点路由、隐私保护等功能。


<details>
  <summary>Details</summary>
Motivation: 解决DNS为中心的互联网在AI代理激增时的身份和发现瓶颈，支持安全、信任的协作。

Method: 提出轻量级索引架构，支持动态AgentFacts、CRDT更新协议和自适应解析器。

Result: 实现快速全局解析、密钥轮换、隐私保护发现等五项保证。

Conclusion: NANDA为下一代AI代理互联网提供轻量、可扩展的基础，兼容现有网络设施。

Abstract: The Internet is poised to host billions to trillions of autonomous AI agents
that negotiate, delegate, and migrate in milliseconds and workloads that will
strain DNS-centred identity and discovery. In this paper, we describe the NANDA
index architecture, which we envision as a means for discoverability,
identifiability and authentication in the internet of AI agents. We present an
architecture where a minimal lean index resolves to dynamic, cryptographically
verifiable AgentFacts that supports multi-endpoint routing, load balancing,
privacy-preserving access, and credentialed capability assertions. Our
architecture design delivers five concrete guarantees: (1) A quilt-like index
proposal that supports both NANDA-native agents as well as third party agents
being discoverable via the index, (2) rapid global resolution for newly spawned
AI agents, (3) sub-second revocation and key rotation, (4) schema-validated
capability assertions, and (5) privacy-preserving discovery across
organisational boundaries via verifiable, least-disclosure queries. We
formalize the AgentFacts schema, specify a CRDT-based update protocol, and
prototype adaptive resolvers. The result is a lightweight, horizontally
scalable foundation that unlocks secure, trust-aware collaboration for the next
generation of the Internet of AI agents, without abandoning existing web
infrastructure.

</details>


### [461] [Agentic Satellite-Augmented Low-Altitude Economy and Terrestrial Networks: A Survey on Generative Approaches](https://arxiv.org/abs/2507.14633)
*Xiaozheng Gao,Yichen Wang,Bosen Liu,Xiao Zhou,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Dong In Kim,Abbas Jamalipour,Chau Yuen,Jianping An,Kai Yang*

Main category: cs.NI

TL;DR: 该论文探讨了通过生成式AI和大型语言模型（LLMs）实现智能自主代理在卫星增强低空经济与地面网络（SLAETNs）中的应用。


<details>
  <summary>Details</summary>
Motivation: 解决SLAETNs在异构、动态和关键任务环境中的可靠运行需求。

Method: 系统回顾了五类生成模型（VAEs、GANs、GDMs、TBMs和LLMs），并分析了它们在SLAETNs中的生成机制、能力和部署权衡。

Result: 展示了这些模型在通信增强、安全隐私保护和智能卫星任务三个领域的应用。

Conclusion: 提出了构建可扩展、自适应和可信赖生成代理的未来方向，为下一代集成网络中的智能代理AI提供了统一理解和参考。

Abstract: The development of satellite-augmented low-altitude economy and terrestrial
networks (SLAETNs) demands intelligent and autonomous systems that can operate
reliably across heterogeneous, dynamic, and mission-critical environments. To
address these challenges, this survey focuses on enabling agentic artificial
intelligence (AI), that is, artificial agents capable of perceiving, reasoning,
and acting, through generative AI (GAI) and large language models (LLMs). We
begin by introducing the architecture and characteristics of SLAETNs, and
analyzing the challenges that arise in integrating satellite, aerial, and
terrestrial components. Then, we present a model-driven foundation by
systematically reviewing five major categories of generative models:
variational autoencoders (VAEs), generative adversarial networks (GANs),
generative diffusion models (GDMs), transformer-based models (TBMs), and LLMs.
Moreover, we provide a comparative analysis to highlight their generative
mechanisms, capabilities, and deployment trade-offs within SLAETNs. Building on
this foundation, we examine how these models empower agentic functions across
three domains: communication enhancement, security and privacy protection, and
intelligent satellite tasks. Finally, we outline key future directions for
building scalable, adaptive, and trustworthy generative agents in SLAETNs. This
survey aims to provide a unified understanding and actionable reference for
advancing agentic AI in next-generation integrated networks.

</details>


### [462] [Quantum Machine Learning for Secure Cooperative Multi-Layer Edge AI with Proportional Fairness](https://arxiv.org/abs/2507.15145)
*Thai T. Vu,John Le*

Main category: cs.NI

TL;DR: 提出了一种通信高效的事件触发推理框架，用于多用户设备和边缘服务器的协作边缘AI系统，通过双阈值早期退出策略和比例公平约束优化分类效用。


<details>
  <summary>Details</summary>
Motivation: 解决多设备协作边缘AI系统中的通信效率、能源消耗和公平性问题。

Method: 基于双阈值早期退出策略，联合优化分类效用，采用交替优化和Benders分解方法求解。

Result: 实验表明，该框架显著提升了系统性能和资源分配的公平性。

Conclusion: 该框架在多设备协作边缘AI系统中实现了高效通信和公平资源分配。

Abstract: This paper proposes a communication-efficient, event-triggered inference
framework for cooperative edge AI systems comprising multiple user devices and
edge servers. Building upon dual-threshold early-exit strategies for rare-event
detection, the proposed approach extends classical single-device inference to a
distributed, multi-device setting while incorporating proportional fairness
constraints across users. A joint optimization framework is formulated to
maximize classification utility under communication, energy, and fairness
constraints. To solve the resulting problem efficiently, we exploit the
monotonicity of the utility function with respect to the confidence thresholds
and apply alternating optimization with Benders decomposition. Experimental
results show that the proposed framework significantly enhances system-wide
performance and fairness in resource allocation compared to single-device
baselines.

</details>


### [463] [User Head Movement-Predictive XR in Immersive H2M Collaborations over Future Enterprise Networks](https://arxiv.org/abs/2507.15254)
*Sourav Mondal,Elaine Wong*

Main category: cs.NI

TL;DR: 论文提出了一种新型的人机协作方案（HMC-DBA），通过预测头部运动优化带宽分配，以满足XR应用的实时需求。


<details>
  <summary>Details</summary>
Motivation: 未来移动系统和固定无线网络需要支持高带宽、低延迟服务，尤其是在工业和社会革命（如工业4.0/5.0和社会5.0）中，确保用户沉浸式体验和避免网络眩晕是关键挑战。

Method: 使用双向长短期记忆网络预测人类头部运动，提前调整机器摄像头方向，并动态分配带宽（HMC-DBA）。

Result: 实验表明，HMC-DBA在满足XR帧延迟和抖动需求的同时，显著降低了带宽消耗，并提高了网络资源利用率。

Conclusion: HMC-DBA方案在实时人机协作中表现出色，优于现有技术，适用于企业网络环境。

Abstract: The evolution towards future generation of mobile systems and fixed wireless
networks is primarily driven by the urgency to support high-bandwidth and
low-latency services across various vertical sectors. This endeavor is fueled
by smartphones as well as technologies like industrial internet of things,
extended reality (XR), and human-to-machine (H2M) collaborations for fostering
industrial and social revolutions like Industry 4.0/5.0 and Society 5.0. To
ensure an ideal immersive experience and avoid cyber-sickness for users in all
the aforementioned usage scenarios, it is typically challenging to synchronize
XR content from a remote machine to a human collaborator according to their
head movements across a large geographic span in real-time over communication
networks. Thus, we propose a novel H2M collaboration scheme where the human's
head movements are predicted ahead with highly accurate models like
bidirectional long short-term memory networks to orient the machine's camera in
advance. We validate that XR frame size varies in accordance with the human's
head movements and predict the corresponding bandwidth requirements from the
machine's camera to propose a human-machine coordinated dynamic bandwidth
allocation (HMC-DBA) scheme. Through extensive simulations, we show that
end-to-end latency and jitter requirements of XR frames are satisfied with much
lower bandwidth consumption over enterprise networks like
Fiber-To-The-Room-Business. Furthermore, we show that better efficiency in
network resource utilization is achieved by employing our proposed HMC-DBA over
state-of-the-art schemes.

</details>


<div id='hep-ph'></div>

# hep-ph [[Back]](#toc)

### [464] [Simulation-Prior Independent Neural Unfolding Procedure](https://arxiv.org/abs/2507.15084)
*Anja Butter,Theo Heimel,Nathan Huetsch,Michael Kagan,Tilman Plehn*

Main category: hep-ph

TL;DR: SPINUP方法利用神经网络编码前向映射，实现高维空间的无箱展开，独立于模拟训练数据的先验，并通过神经重要性采样和集成学习提高效率。


<details>
  <summary>Details</summary>
Motivation: 解决LHC中高维空间展开的挑战，避免传统方法对模拟数据先验的依赖。

Method: 使用神经网络编码前向映射，结合神经重要性采样和集成学习。

Result: 成功应用于探测器效应展开和部分子级展开，展示了方法的有效性。

Conclusion: SPINUP为高维空间展开提供了一种高效且独立于先验的新方法。

Abstract: Machine learning allows unfolding high-dimensional spaces without binning at
the LHC. The new SPINUP method extracts the unfolded distribution based on a
neural network encoding the forward mapping, making it independent of the prior
from the simulated training data. It is made efficient through neural
importance sampling, and ensembling can be used to estimate the effect of
information loss in the forward process. We showcase SPINUP for unfolding
detector effects on jet substructure observables and for unfolding to parton
level of associated Higgs and single-top production.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [465] [ObjectGS: Object-aware Scene Reconstruction and Scene Understanding via Gaussian Splatting](https://arxiv.org/abs/2507.15454)
*Ruijie Zhu,Mulin Yu,Linning Xu,Lihan Jiang,Yixuan Li,Tianzhu Zhang,Jiangmiao Pang,Bo Dai*

Main category: cs.GR

TL;DR: ObjectGS是一个对象感知框架，将3D场景重建与语义理解结合，通过动态调整和优化对象锚点实现精确的对象级重建。


<details>
  <summary>Details</summary>
Motivation: 3D Gaussian Splatting缺乏语义理解，限制了对象级感知能力。

Method: 将场景中的对象建模为局部锚点，生成神经高斯并共享对象ID，通过动态调整锚点和分类损失优化语义约束。

Result: 在开放词汇和全景分割任务中优于现有方法，并能无缝集成网格提取和场景编辑等应用。

Conclusion: ObjectGS成功实现了对象级3D重建与语义理解的统一。

Abstract: 3D Gaussian Splatting is renowned for its high-fidelity reconstructions and
real-time novel view synthesis, yet its lack of semantic understanding limits
object-level perception. In this work, we propose ObjectGS, an object-aware
framework that unifies 3D scene reconstruction with semantic understanding.
Instead of treating the scene as a unified whole, ObjectGS models individual
objects as local anchors that generate neural Gaussians and share object IDs,
enabling precise object-level reconstruction. During training, we dynamically
grow or prune these anchors and optimize their features, while a one-hot ID
encoding with a classification loss enforces clear semantic constraints. We
show through extensive experiments that ObjectGS not only outperforms
state-of-the-art methods on open-vocabulary and panoptic segmentation tasks,
but also integrates seamlessly with applications like mesh extraction and scene
editing. Project page: https://ruijiezhu94.github.io/ObjectGS_page

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [466] [FAMST: Fast Approximate Minimum Spanning Tree Construction for Large-Scale and High-Dimensional Data](https://arxiv.org/abs/2507.14261)
*Mahmood K. M. Almansoori,Miklos Telek*

Main category: cs.DS

TL;DR: FAMST是一种快速近似最小生成树算法，通过三阶段方法显著降低计算复杂度，适用于大规模高维数据集。


<details>
  <summary>Details</summary>
Motivation: 解决传统最小生成树算法在大规模高维数据集上的计算复杂度问题。

Method: 采用三阶段方法：近似最近邻图构建、组件间连接和迭代边优化。

Result: 时间复杂度和空间复杂度显著降低，实验显示速度快1000倍且误差低。

Conclusion: FAMST扩展了MST技术的应用范围，适用于百万级数据点和千维问题。

Abstract: We present Fast Approximate Minimum Spanning Tree (FAMST), a novel algorithm
that addresses the computational challenges of constructing Minimum Spanning
Trees (MSTs) for large-scale and high-dimensional datasets. FAMST utilizes a
three-phase approach: Approximate Nearest Neighbor (ANN) graph construction,
ANN inter-component connection, and iterative edge refinement. For a dataset of
$n$ points in a $d$-dimensional space, FAMST achieves $\mathcal{O}(dn \log n)$
time complexity and $\mathcal{O}(dn + kn)$ space complexity when $k$ nearest
neighbors are considered, which is a significant improvement over the
$\mathcal{O}(n^2)$ time and space complexity of traditional methods.
  Experiments across diverse datasets demonstrate that FAMST achieves
remarkably low approximation errors while providing speedups of up to
1000$\times$ compared to exact MST algorithms. We analyze how the key
hyperparameters, $k$ (neighborhood size) and $\lambda$ (inter-component edges),
affect performance, providing practical guidelines for hyperparameter
selection. FAMST enables MST-based analysis on datasets with millions of points
and thousands of dimensions, extending the applicability of MST techniques to
problem scales previously considered infeasible.

</details>


### [467] [Differentially Private Synthetic Graphs Preserving Triangle-Motif Cuts](https://arxiv.org/abs/2507.14835)
*Pan Peng,Hangyu Xu*

Main category: cs.DS

TL;DR: 本文提出了一种差分隐私（DP）机制，用于生成合成图G'，以近似原始图G中所有切割的三角形子图数量，并提供了误差界限和下限。


<details>
  <summary>Details</summary>
Motivation: 研究如何在保证差分隐私的前提下，生成合成图以近似复杂网络中的频繁子图（如三角形），应用于图聚类、稀疏化和社会网络分析等领域。

Method: 提出了一种多项式时间的(ε,δ)-DP机制，输入图G，生成合成图G'，近似所有切割的三角形子图数量，误差为Õ(√(mℓ₃(G))n/ε^(3/2))。

Result: 提供了误差上限和下限，证明算法的有效性，并推广到加权图和更一般的Kₕ-子图切割。

Conclusion: 该机制在多项式时间内生成满足差分隐私的合成图，适用于多种图分析任务，并提供了理论保证。

Abstract: We study the problem of releasing a differentially private (DP) synthetic
graph $G'$ that well approximates the triangle-motif sizes of all cuts of any
given graph $G$, where a motif in general refers to a frequently occurring
subgraph within complex networks. Non-private versions of such graphs have
found applications in diverse fields such as graph clustering, graph
sparsification, and social network analysis. Specifically, we present the first
$(\varepsilon,\delta)$-DP mechanism that, given an input graph $G$ with $n$
vertices, $m$ edges and local sensitivity of triangles $\ell_{3}(G)$, generates
a synthetic graph $G'$ in polynomial time, approximating the triangle-motif
sizes of all cuts $(S,V\setminus S)$ of the input graph $G$ up to an additive
error of $\tilde{O}(\sqrt{m\ell_{3}(G)}n/\varepsilon^{3/2})$. Additionally, we
provide a lower bound of $\Omega(\sqrt{mn}\ell_{3}(G)/\varepsilon)$ on the
additive error for any DP algorithm that answers the triangle-motif size
queries of all $(S,T)$-cut of $G$. Finally, our algorithm generalizes to
weighted graphs, and our lower bound extends to any $K_h$-motif cut for any
constant $h\geq 2$.

</details>


### [468] [Language Generation in the Limit: Noise, Loss, and Feedback](https://arxiv.org/abs/2507.15319)
*Yannan Bai,Debmalya Panigrahi,Ian Zhang*

Main category: cs.DS

TL;DR: 本文解决了语言生成极限中的联合封闭性问题，并利用相关技术对噪声、损失和反馈的变体进行了精确刻画。


<details>
  <summary>Details</summary>
Motivation: 研究语言生成极限的联合封闭性，并探索噪声、无样本和反馈等变体的性质。

Method: 通过构造反例证明联合封闭性不成立，并分析噪声、无样本和反馈模型的等价性或分离性。

Result: 证明联合封闭性不成立；噪声与非噪声生成存在分离；有限查询无增强，无限查询增强模型。

Conclusion: 解决了联合封闭性问题，并对多种变体提供了精确刻画，深化了对语言生成极限的理解。

Abstract: Kleinberg and Mullainathan (2024) recently proposed a formal framework called
language generation in the limit and showed that given a sequence of example
strings from an unknown target language drawn from any countable collection, an
algorithm can correctly generate unseen strings from the target language within
finite time. This notion was further refined by Li, Raman, and Tewari (2024),
who defined stricter categories of non-uniform and uniform generation. They
showed that a finite union of uniformly generatable collections is generatable
in the limit, and asked if the same is true for non-uniform generation.
  We begin by resolving the question in the negative: we give a uniformly
generatable collection and a non-uniformly generatable collection whose union
is not generatable in the limit. We then use facets of this construction to
further our understanding of several variants of language generation. The first
two, generation with noise and without samples, were introduced by Raman and
Raman (2025) and Li, Raman, and Tewari (2024) respectively. We show the
equivalence of these models for uniform and non-uniform generation, and provide
a characterization of non-uniform noisy generation. The former paper asked if
there is any separation between noisy and non-noisy generation in the limit --
we show that such a separation exists even with a single noisy string. Finally,
we study the framework of generation with feedback, introduced by Charikar and
Pabbaraju (2025), where the algorithm is strengthened by allowing it to ask
membership queries. We show finite queries add no power, but infinite queries
yield a strictly more powerful model.
  In summary, the results in this paper resolve the union-closedness of
language generation in the limit, and leverage those techniques (and others) to
give precise characterizations for natural variants that incorporate noise,
loss, and feedback.

</details>


<div id='q-bio.QM'></div>

# q-bio.QM [[Back]](#toc)

### [469] [KinForm: Kinetics Informed Feature Optimised Representation Models for Enzyme $k_{cat}$ and $K_{M}$ Prediction](https://arxiv.org/abs/2507.14639)
*Saleh Alwer,Ronan Fleming*

Main category: q-bio.QM

TL;DR: KinForm是一个机器学习框架，通过优化蛋白质特征表示，提高了酶动力学参数的预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 酶动力学参数（如$k_{cat}$和$K_{\mathrm{M}}$）对建模酶活性至关重要，但实验数据在规模和多样性上有限。现有方法通常使用单蛋白质语言模型的均值池化残基嵌入表示蛋白质，预测效果有限。

Method: KinForm结合多种残基级嵌入（如ESM和ProtT5-XL-UniRef50），基于结合位点概率进行加权池化，并通过PCA降维和相似性过采样策略优化数据。

Result: KinForm在两个基准数据集上优于基线方法，尤其在低序列相似性区间表现显著。结合位点概率池化、中间层选择、PCA和过采样均对性能提升有贡献。

Conclusion: KinForm通过优化特征表示和数据平衡，显著提升了酶动力学参数的预测性能，并建议在评估模型泛化能力时避免序列重叠。

Abstract: Kinetic parameters such as the turnover number ($k_{cat}$) and Michaelis
constant ($K_{\mathrm{M}}$) are essential for modelling enzymatic activity but
experimental data remains limited in scale and diversity. Previous methods for
predicting enzyme kinetics typically use mean-pooled residue embeddings from a
single protein language model to represent the protein. We present KinForm, a
machine learning framework designed to improve predictive accuracy and
generalisation for kinetic parameters by optimising protein feature
representations. KinForm combines several residue-level embeddings
(Evolutionary Scale Modeling Cambrian, Evolutionary Scale Modeling 2, and
ProtT5-XL-UniRef50), taken from empirically selected intermediate transformer
layers and applies weighted pooling based on per-residue binding-site
probability. To counter the resulting high dimensionality, we apply
dimensionality reduction using principal--component analysis (PCA) on
concatenated protein features, and rebalance the training data via a
similarity-based oversampling strategy. KinForm outperforms baseline methods on
two benchmark datasets. Improvements are most pronounced in low sequence
similarity bins. We observe improvements from binding-site probability pooling,
intermediate-layer selection, PCA, and oversampling of low-identity proteins.
We also find that removing sequence overlap between folds provides a more
realistic evaluation of generalisation and should be the standard over random
splitting when benchmarking kinetic prediction models.

</details>


<div id='astro-ph.HE'></div>

# astro-ph.HE [[Back]](#toc)

### [470] [The hunt for new pulsating ultraluminous X-ray sources: a clustering approach](https://arxiv.org/abs/2507.15032)
*Nicolò Oreste Pinciroli Vago,Roberta Amato,Matteo Imbrogno,GianLuca Israel,Andrea Belfiore,Konstantinos Kovlakas,Piero Fraternali,Mario Pasquato*

Main category: astro-ph.HE

TL;DR: 利用AI方法从XMM-Newton探测到的ULXs中筛选出新的候选脉冲ULXs（PULXs），尽管未发现新的脉冲信号，但验证了AI方法的预测能力。


<details>
  <summary>Details</summary>
Motivation: 由于统计不足，现有ULXs中脉冲信号的发现受限，需通过AI方法识别潜在PULXs。

Method: 使用无监督聚类算法将ULXs分为两类，利用已知PULXs设定阈值，筛选候选PULXs。

Result: 筛选出85个独特候选PULXs，355次观测，但未发现新脉冲信号。

Conclusion: AI方法展示了预测潜力，但需更高统计量数据验证候选PULXs的脉冲信号。

Abstract: The discovery of fast and variable coherent signals in a handful of
ultraluminous X-ray sources (ULXs) testifies to the presence of super-Eddington
accreting neutron stars, and drastically changed the understanding of the ULX
class. Our capability of discovering pulsations in ULXs is limited, among
others, by poor statistics. However, catalogues and archives of high-energy
missions contain information which can be used to identify new candidate
pulsating ULXs (PULXs). The goal of this research is to single out candidate
PULXs among those ULXs which have not shown pulsations due to an unfavourable
combination of factors. We applied an AI approach to an updated database of
ULXs detected by XMM-Newton. We first used an unsupervised clustering algorithm
to sort out sources with similar characteristics into two clusters. Then, the
sample of known PULX observations has been used to set the separation threshold
between the two clusters and to identify the one containing the new candidate
PULXs. We found that only a few criteria are needed to assign the membership of
an observation to one of the two clusters. The cluster of new candidate PULXs
counts 85 unique sources for 355 observations, with $\sim$85% of these new
candidates having multiple observations. A preliminary timing analysis found no
new pulsations for these candidates. This work presents a sample of new
candidate PULXs observed by XMM-Newton, the properties of which are similar (in
a multi-dimensional phase space) to those of the known PULXs, despite the
absence of pulsations in their light curves. While this result is a clear
example of the predictive power of AI-based methods, it also highlights the
need for high-statistics observational data to reveal coherent signals from the
sources in this sample and thus validate the robustness of the approach.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [471] [U-DREAM: Unsupervised Dereverberation guided by a Reverberation Model](https://arxiv.org/abs/2507.14237)
*Louis Bahrman,Mathieu Fontaine,Gaël Richard*

Main category: cs.SD

TL;DR: 论文提出了一种从弱监督到完全无监督的去混响模型训练方法，仅需混响信号和声学模型，无需成对数据。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法通常需要成对的干湿数据，但实际中难以获取。

Method: 采用基于贝叶斯公式的顺序学习策略，通过深度神经网络从混响输入中估计声学参数和干信号，并使用混响匹配损失指导。

Result: 最数据高效的变体仅需100个混响参数标记样本即可超越无监督基线。

Conclusion: 该方法在低资源场景下具有有效性和实用性。

Abstract: This paper explores the outcome of training state-ofthe-art dereverberation
models with supervision settings ranging from weakly-supervised to fully
unsupervised, relying solely on reverberant signals and an acoustic model for
training. Most of the existing deep learning approaches typically require
paired dry and reverberant data, which are difficult to obtain in practice. We
develop instead a sequential learning strategy motivated by a bayesian
formulation of the dereverberation problem, wherein acoustic parameters and dry
signals are estimated from reverberant inputs using deep neural networks,
guided by a reverberation matching loss. Our most data-efficient variant
requires only 100 reverberation-parameter-labelled samples to outperform an
unsupervised baseline, demonstrating the effectiveness and practicality of the
proposed method in low-resource scenarios.

</details>


### [472] [Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems](https://arxiv.org/abs/2507.15214)
*Natalia Tomashenko,Emmanuel Vincent,Marc Tommasi*

Main category: cs.SD

TL;DR: 本文提出了一种从语音时间动态中提取上下文相关时长嵌入的新方法，用于表征说话人特征，并开发了攻击模型，显著提升了说话人验证性能。


<details>
  <summary>Details</summary>
Motivation: 语音的时间动态（如节奏、语调和语速）包含独特的说话人身份信息，但现有方法对其表征不足。

Method: 提取上下文相关时长嵌入，并基于此开发攻击模型，分析说话人验证和语音匿名化系统的潜在漏洞。

Result: 实验表明，该方法在原始和匿名数据上的说话人验证性能显著优于文献中的简单表征方法。

Conclusion: 上下文相关时长嵌入能有效表征说话人特征，提升系统性能，但也揭示了潜在的安全漏洞。

Abstract: The temporal dynamics of speech, encompassing variations in rhythm,
intonation, and speaking rate, contain important and unique information about
speaker identity. This paper proposes a new method for representing speaker
characteristics by extracting context-dependent duration embeddings from speech
temporal dynamics. We develop novel attack models using these representations
and analyze the potential vulnerabilities in speaker verification and voice
anonymization systems.The experimental results show that the developed attack
models provide a significant improvement in speaker verification performance
for both original and anonymized data in comparison with simpler
representations of speech temporal dynamics reported in the literature.

</details>


### [473] [A2TTS: TTS for Low Resource Indian Languages](https://arxiv.org/abs/2507.15272)
*Ayush Singh Bhadoriya,Abhishek Nikunj Shinde,Isha Pandey,Ganesh Ramakrishnan*

Main category: cs.SD

TL;DR: 提出了一种基于扩散模型的说话人条件文本转语音系统，支持未见说话人和多种印度语言。


<details>
  <summary>Details</summary>
Motivation: 解决未见说话人生成语音和支持多语言（尤其是印度语言）的挑战。

Method: 使用扩散模型架构，结合说话人编码器和交叉注意力时长预测机制，增强韵律和自然度。

Result: 生成的语音更接近目标说话人，时长建模和表达力显著提升。

Conclusion: 该方法在多语言环境下表现优异，为零样本生成提供了有效解决方案。

Abstract: We present a speaker conditioned text-to-speech (TTS) system aimed at
addressing challenges in generating speech for unseen speakers and supporting
diverse Indian languages. Our method leverages a diffusion-based TTS
architecture, where a speaker encoder extracts embeddings from short reference
audio samples to condition the DDPM decoder for multispeaker generation. To
further enhance prosody and naturalness, we employ a cross-attention based
duration prediction mechanism that utilizes reference audio, enabling more
accurate and speaker consistent timing. This results in speech that closely
resembles the target speaker while improving duration modeling and overall
expressiveness. Additionally, to improve zero-shot generation, we employed
classifier free guidance, allowing the system to generate speech more near
speech for unknown speakers. Using this approach, we trained language-specific
speaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian
languages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and
Tamil.

</details>


### [474] [Neuro-MSBG: An End-to-End Neural Model for Hearing Loss Simulation](https://arxiv.org/abs/2507.15396)
*Hui-Guan Yuan,Ryandhimas E. Zezario,Shafique Ahmed,Hsin-Min Wang,Kai-Lung Hua,Yu Tsao*

Main category: cs.SD

TL;DR: Neuro-MSBG是一种轻量级端到端模型，用于高效模拟听力损失，支持并行推理并显著降低计算时间。


<details>
  <summary>Details</summary>
Motivation: 现有听力损失模拟模型计算复杂且延迟高，限制了实时应用，且缺乏与语音处理系统的直接集成。

Method: 提出Neuro-MSBG，结合个性化听力图编码器进行时频建模，支持并行推理。

Result: Neuro-MSBG在STOI和PESQ指标上表现优异，计算时间减少46倍。

Conclusion: Neuro-MSBG高效实用，适用于实时听力损失模拟。

Abstract: Hearing loss simulation models are essential for hearing aid deployment.
However, existing models have high computational complexity and latency, which
limits real-time applications and lack direct integration with speech
processing systems. To address these issues, we propose Neuro-MSBG, a
lightweight end-to-end model with a personalized audiogram encoder for
effective time-frequency modeling. Experiments show that Neuro-MSBG supports
parallel inference and retains the intelligibility and perceptual quality of
the original MSBG, with a Spearman's rank correlation coefficient (SRCC) of
0.9247 for Short-Time Objective Intelligibility (STOI) and 0.8671 for
Perceptual Evaluation of Speech Quality (PESQ). Neuro-MSBG reduces simulation
runtime by a factor of 46 (from 0.970 seconds to 0.021 seconds for a 1 second
input), further demonstrating its efficiency and practicality.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [475] [Large Language Model as An Operator: An Experience-Driven Solution for Distribution Network Voltage Control](https://arxiv.org/abs/2507.14800)
*Xu Yang,Chenhui Lin,Haotian Liu,Qi Wang,Wenchuan Wu*

Main category: eess.SY

TL;DR: 该论文提出了一种基于大语言模型（LLM）的分布式网络电压控制方法，通过多模块协作实现策略的自主进化。


<details>
  <summary>Details</summary>
Motivation: 利用LLM的高级推理和信息分析能力，解决电力系统调度中的电压控制问题。

Method: 通过经验存储、检索、生成和修改四个模块的协作，实现LLM电压控制策略的自主进化。

Result: 实验验证了该方法的有效性，并展示了LLM在电力系统调度中的适用性。

Conclusion: LLM为电力系统调度提供了一种新的自主控制解决方案。

Abstract: With the advanced reasoning and information analysis capabilities, large
language models (LLMs) can offer a novel approach for the autonomous generation
of dispatch strategies in power systems. This letter proposes an LLM-based
experience-driven voltage control solution for distribution networks, which
enables the self-evolution of LLM-based voltage control strategies through the
collaboration and interaction of multiple modules-specifically, experience
storage, experience retrieval, experience generation, and experience
modification. Comprehensive experimental results validate the effectiveness of
the proposed method and highlight the applicability of LLM in addressing power
system dispatch challenges.

</details>


### [476] [Physics-Informed Learning of Proprietary Inverter Models for Grid Dynamic Studies](https://arxiv.org/abs/2507.15259)
*Kyung-Bin Kwon,Sayak Mukherjee,Ramij R. Hossain,Marcelo Elizondo*

Main category: eess.SY

TL;DR: 提出了一种基于物理信息的神经ODE框架（PI-LNM），用于模拟逆变器的专有动态特性，提高电网动态仿真的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前行业实践中，逆变器的内部控制和参数通常不公开，导致动态仿真和相关研究（如稳定性分析的增益调整）难以准确进行。

Method: 提出了PI-LNM模型，将系统物理与神经网络学习层结合，捕捉专有单元的未建模行为。

Result: 通过电网形成逆变器（GFM）案例验证，相比纯数据驱动方法，PI-LNM显著提高了动态仿真准确性。

Conclusion: PI-LNM框架成功解决了专有动态特性建模的挑战，为电网仿真提供了更准确的工具。

Abstract: This letter develops a novel physics-informed neural ordinary differential
equations-based framework to emulate the proprietary dynamics of the inverters
-- essential for improved accuracy in grid dynamic simulations. In current
industry practice, the original equipment manufacturers (OEMs) often do not
disclose the exact internal controls and parameters of the inverters, posing
significant challenges in performing accurate dynamic simulations and other
relevant studies, such as gain tunings for stability analysis and controls. To
address this, we propose a Physics-Informed Latent Neural ODE Model (PI-LNM)
that integrates system physics with neural learning layers to capture the
unmodeled behaviors of proprietary units. The proposed method is validated
using a grid-forming inverter (GFM) case study, demonstrating improved dynamic
simulation accuracy over approaches that rely solely on data-driven learning
without physics-based guidance.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [477] [Exposing and Mitigating Calibration Biases and Demographic Unfairness in MLLM Few-Shot In-Context Learning for Medical Image Classification](https://arxiv.org/abs/2506.23298)
*Xing Shen,Justin Szeto,Mingyang Li,Hengguan Huang,Tal Arbel*

Main category: eess.IV

TL;DR: 论文研究了多模态大语言模型（MLLMs）在医学图像分类中的校准偏差和人口统计学不公平性，并提出了一种名为CALIN的推理时校准方法以减少这些偏差。


<details>
  <summary>Details</summary>
Motivation: MLLMs在医学图像分析中具有巨大潜力，但其预测准确性和校准误差在不同人口亚组中的表现需要深入分析，以确保临床实践中的安全部署。

Method: CALIN通过双层程序（从群体水平到亚组水平）估计校准需求，并在推理时应用校准矩阵调整预测置信度。

Result: 在三个医学影像数据集上的实验表明，CALIN能有效确保公平的置信校准，同时提高预测准确性，且公平性与效用之间的权衡最小。

Conclusion: CALIN为MLLMs在医学图像分类中的公平性和校准问题提供了一种有效的解决方案。

Abstract: Multimodal large language models (MLLMs) have enormous potential to perform
few-shot in-context learning in the context of medical image analysis. However,
safe deployment of these models into real-world clinical practice requires an
in-depth analysis of the accuracies of their predictions, and their associated
calibration errors, particularly across different demographic subgroups. In
this work, we present the first investigation into the calibration biases and
demographic unfairness of MLLMs' predictions and confidence scores in few-shot
in-context learning for medical image classification. We introduce CALIN, an
inference-time calibration method designed to mitigate the associated biases.
Specifically, CALIN estimates the amount of calibration needed, represented by
calibration matrices, using a bi-level procedure: progressing from the
population level to the subgroup level prior to inference. It then applies this
estimation to calibrate the predicted confidence scores during inference.
Experimental results on three medical imaging datasets: PAPILA for fundus image
classification, HAM10000 for skin cancer classification, and MIMIC-CXR for
chest X-ray classification demonstrate CALIN's effectiveness at ensuring fair
confidence calibration in its prediction, while improving its overall
prediction accuracies and exhibiting minimum fairness-utility trade-off. Our
codebase can be found at
https://github.com/xingbpshen/medical-calibration-fairness-mllm.

</details>


### [478] [MiDeSeC: A Dataset for Mitosis Detection and Segmentation in Breast Cancer Histopathology Images](https://arxiv.org/abs/2507.14271)
*Refik Samet,Nooshin Nemati,Emrah Hancer,Serpil Sak,Bilge Ayca Kirmizi,Zeynep Yildirim*

Main category: eess.IV

TL;DR: MiDeSeC数据集包含25名患者的H&E染色乳腺癌切片，用于训练和测试模型，覆盖多种有丝分裂形态。


<details>
  <summary>Details</summary>
Motivation: 创建大型数据集以覆盖多种有丝分裂形态，提高乳腺癌诊断的准确性。

Method: 从25名患者的玻璃切片中选择50个1024x1024像素区域，扫描并标注500多个有丝分裂。

Result: 数据集包含500多个有丝分裂，分为训练集和测试集。

Conclusion: MiDeSeC数据集为乳腺癌有丝分裂研究提供了丰富资源。

Abstract: The MiDeSeC dataset is created through H&E stained invasive breast carcinoma,
no special type (NST) slides of 25 different patients captured at 40x
magnification from the Department of Medical Pathology at Ankara University.
The slides have been scanned by 3D Histech Panoramic p250 Flash-3 scanner and
Olympus BX50 microscope. As several possible mitosis shapes exist, it is
crucial to have a large dataset to cover all the cases. Accordingly, a total of
50 regions is selected from glass slides for 25 patients, each of regions with
a size of 1024*1024 pixels. There are more than 500 mitoses in total in these
50 regions. Two-thirds of the regions are reserved for training, the other
third for testing.

</details>


### [479] [NuSeC: A Dataset for Nuclei Segmentation in Breast Cancer Histopathology Images](https://arxiv.org/abs/2507.14272)
*Refik Samet,Nooshin Nemati,Emrah Hancer,Serpil Sak,Bilge Ayca Kirmizi*

Main category: eess.IV

TL;DR: NuSeC数据集包含100张1024*1024像素的图像，来自25名患者，每名患者4张图像。数据集按75%训练集和25%测试集划分，训练集75张图像（约30000个核结构），测试集25张图像（约6000个核结构）。


<details>
  <summary>Details</summary>
Motivation: 为未来研究者使用NuSeC数据集开发的方法提供一致的比较分析基础。

Method: 从25名患者中随机选择每名患者的1张图像构建测试集（25张），其余75张作为训练集。

Result: 训练集包含约30000个核结构，测试集包含约6000个核结构。

Conclusion: NuSeC数据集的分割方式确保了未来方法比较的一致性。

Abstract: The NuSeC dataset is created by selecting 4 images with the size of 1024*1024
pixels from the slides of each patient among 25 patients. Therefore, there are
a total of 100 images in the NuSeC dataset. To carry out a consistent
comparative analysis between the methods that will be developed using the NuSeC
dataset by the researchers in the future, we divide the NuSeC dataset 75% as
the training set and 25% as the testing set. In detail, an image is randomly
selected from 4 images of each patient among 25 patients to build the testing
set, and then the remaining images are reserved for the training set. While the
training set includes 75 images with around 30000 nuclei structures, the
testing set includes 25 images with around 6000 nuclei structures.

</details>


### [480] [QUTCC: Quantile Uncertainty Training and Conformal Calibration for Imaging Inverse Problems](https://arxiv.org/abs/2507.14760)
*Cassandra Tong Ye,Shamus Li,Tyler King,Kristina Monakhova*

Main category: eess.IV

TL;DR: QUTCC是一种非线性、非均匀的量化不确定性训练和校准技术，用于提高深度学习模型在医学图像任务中的可靠性。


<details>
  <summary>Details</summary>
Motivation: 深度学习模型在医学图像任务中可能产生幻觉，导致不准确的预测，而现有不确定性量化方法效果有限。

Method: 提出QUTCC技术，结合U-Net架构和量化嵌入，通过非线性校准生成更紧密的不确定性区间。

Result: QUTCC能有效识别幻觉并生成更紧密的不确定性区间，优于现有方法。

Conclusion: QUTCC在医学图像任务中提高了预测的可靠性和准确性。

Abstract: Deep learning models often hallucinate, producing realistic artifacts that
are not truly present in the sample. This can have dire consequences for
scientific and medical inverse problems, such as MRI and microscopy denoising,
where accuracy is more important than perceptual quality. Uncertainty
quantification techniques, such as conformal prediction, can pinpoint outliers
and provide guarantees for image regression tasks, improving reliability.
However, existing methods utilize a linear constant scaling factor to calibrate
uncertainty bounds, resulting in larger, less informative bounds. We propose
QUTCC, a quantile uncertainty training and calibration technique that enables
nonlinear, non-uniform scaling of quantile predictions to enable tighter
uncertainty estimates. Using a U-Net architecture with a quantile embedding,
QUTCC enables the prediction of the full conditional distribution of quantiles
for the imaging task. During calibration, QUTCC generates uncertainty bounds by
iteratively querying the network for upper and lower quantiles, progressively
refining the bounds to obtain a tighter interval that captures the desired
coverage. We evaluate our method on several denoising tasks as well as
compressive MRI reconstruction. Our method successfully pinpoints
hallucinations in image estimates and consistently achieves tighter uncertainty
intervals than prior methods while maintaining the same statistical coverage.

</details>


### [481] [Performance Analysis of Post-Training Quantization for CNN-based Conjunctival Pallor Anemia Detection](https://arxiv.org/abs/2507.15151)
*Sebastian A. Cruz Romero,Wilfredo E. Lugo Beauchamp*

Main category: eess.IV

TL;DR: 论文探讨了利用深度学习模型通过结膜苍白检测贫血的方法，使用CP-AnemiC数据集，MobileNet架构，并评估了量化对模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 传统贫血检测方法成本高且需专业知识，阻碍早期诊断。研究旨在通过深度学习模型解决这一问题。

Method: 使用MobileNet架构，结合数据增强和交叉验证策略，对CP-AnemiC数据集进行端到端微调。

Result: 模型准确率达0.9313，精度0.9374，F1分数0.9773。量化实验显示FP16保持高性能，INT8和INT4性能下降显著。

Conclusion: 研究支持进一步探索量化方案和硬件优化，以平衡模型大小、推理时间和诊断准确性。

Abstract: Anemia is a widespread global health issue, particularly among young children
in low-resource settings. Traditional methods for anemia detection often
require expensive equipment and expert knowledge, creating barriers to early
and accurate diagnosis. To address these challenges, we explore the use of deep
learning models for detecting anemia through conjunctival pallor, focusing on
the CP-AnemiC dataset, which includes 710 images from children aged 6-59
months. The dataset is annotated with hemoglobin levels, gender, age and other
demographic data, enabling the development of machine learning models for
accurate anemia detection. We use the MobileNet architecture as a backbone,
known for its efficiency in mobile and embedded vision applications, and
fine-tune our model end-to-end using data augmentation techniques and a
cross-validation strategy. Our model implementation achieved an accuracy of
0.9313, a precision of 0.9374, and an F1 score of 0.9773 demonstrating strong
performance on the dataset. To optimize the model for deployment on edge
devices, we performed post-training quantization, evaluating the impact of
different bit-widths (FP32, FP16, INT8, and INT4) on model performance.
Preliminary results suggest that while FP16 quantization maintains high
accuracy (0.9250), precision (0.9370), and F1 Score (0.9377), more aggressive
quantization (INT8 and INT4) leads to significant performance degradation.
Overall, our study supports further exploration of quantization schemes and
hardware optimizations to assess trade-offs between model size, inference time,
and diagnostic accuracy in mobile healthcare applications.

</details>


### [482] [A Study of Anatomical Priors for Deep Learning-Based Segmentation of Pheochromocytoma in Abdominal CT](https://arxiv.org/abs/2507.15193)
*Tanjin Taher Toma,Tejas Sudharshan Mathai,Bikash Santra,Pritam Mukherjee,Jianfei Liu,Wesley Jong,Darwish Alabyad,Vivek Batheja,Abhishek Jha,Mayank Patel,Darko Pucar,Jayadira del Rivero,Karel Pacak,Ronald M. Summers*

Main category: eess.IV

TL;DR: 该研究评估了11种标注策略，发现结合肿瘤、肾脏和主动脉（TKA）的标注策略在PCC分割中表现最佳，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 提高PCC在腹部CT扫描中的分割精度，以支持肿瘤负荷估计、预后和治疗规划，同时减少对昂贵基因检测的依赖。

Method: 使用nnU-Net框架，比较了11种标注策略，包括基于器官特异性解剖先验的多类方案，并在105例CT扫描上进行了训练和测试。

Result: TKA标注策略在DSC、NSD和F1分数上显著优于传统方法，并在肿瘤负荷量化和遗传亚型分割中表现优异。

Conclusion: 结合相关解剖背景的深度学习模型能显著提升PCC分割精度，支持临床评估和长期监测。

Abstract: Accurate segmentation of pheochromocytoma (PCC) in abdominal CT scans is
essential for tumor burden estimation, prognosis, and treatment planning. It
may also help infer genetic clusters, reducing reliance on expensive testing.
This study systematically evaluates anatomical priors to identify
configurations that improve deep learning-based PCC segmentation. We employed
the nnU-Net framework to evaluate eleven annotation strategies for accurate 3D
segmentation of pheochromocytoma, introducing a set of novel multi-class
schemes based on organ-specific anatomical priors. These priors were derived
from adjacent organs commonly surrounding adrenal tumors (e.g., liver, spleen,
kidney, aorta, adrenal gland, and pancreas), and were compared against a broad
body-region prior used in previous work. The framework was trained and tested
on 105 contrast-enhanced CT scans from 91 patients at the NIH Clinical Center.
Performance was measured using Dice Similarity Coefficient (DSC), Normalized
Surface Distance (NSD), and instance-wise F1 score. Among all strategies, the
Tumor + Kidney + Aorta (TKA) annotation achieved the highest segmentation
accuracy, significantly outperforming the previously used Tumor + Body (TB)
annotation across DSC (p = 0.0097), NSD (p = 0.0110), and F1 score (25.84%
improvement at an IoU threshold of 0.5), measured on a 70-30 train-test split.
The TKA model also showed superior tumor burden quantification (R^2 = 0.968)
and strong segmentation across all genetic subtypes. In five-fold
cross-validation, TKA consistently outperformed TB across IoU thresholds (0.1
to 0.5), reinforcing its robustness and generalizability. These findings
highlight the value of incorporating relevant anatomical context in deep
learning models to achieve precise PCC segmentation, supporting clinical
assessment and longitudinal monitoring.

</details>


### [483] [EndoControlMag: Robust Endoscopic Vascular Motion Magnification with Periodic Reference Resetting and Hierarchical Tissue-aware Dual-Mask Contro](https://arxiv.org/abs/2507.15292)
*An Wanga,Rulin Zhou,Mengya Xu,Yiru Ye,Longfei Gou,Yiting Chang,Hao Chen,Chwee Ming Lim,Jiankun Wang,Hongliang Ren*

Main category: eess.IV

TL;DR: EndoControlMag是一种无需训练的拉格朗日框架，通过掩码条件血管运动放大技术，显著提升内窥镜手术中血管细微运动的可视化效果。


<details>
  <summary>Details</summary>
Motivation: 内窥镜手术中血管细微运动的可视化对手术精确性和决策至关重要，但由于手术场景的复杂性和动态性，这一任务极具挑战性。

Method: EndoControlMag采用周期性参考重置（PRR）方案和分层组织感知放大（HTM）框架，结合双模式掩码扩张，实现血管运动的精确放大。

Result: 在EndoVMM24数据集上的评估表明，EndoControlMag在放大精度和视觉质量上显著优于现有方法，并在复杂手术条件下保持鲁棒性。

Conclusion: EndoControlMag为内窥镜手术中的血管运动可视化提供了一种高效且鲁棒的解决方案。

Abstract: Visualizing subtle vascular motions in endoscopic surgery is crucial for
surgical precision and decision-making, yet remains challenging due to the
complex and dynamic nature of surgical scenes. To address this, we introduce
EndoControlMag, a training-free, Lagrangian-based framework with
mask-conditioned vascular motion magnification tailored to endoscopic
environments. Our approach features two key modules: a Periodic Reference
Resetting (PRR) scheme that divides videos into short overlapping clips with
dynamically updated reference frames to prevent error accumulation while
maintaining temporal coherence, and a Hierarchical Tissue-aware Magnification
(HTM) framework with dual-mode mask dilation. HTM first tracks vessel cores
using a pretrained visual tracking model to maintain accurate localization
despite occlusions and view changes. It then applies one of two adaptive
softening strategies to surrounding tissues: motion-based softening that
modulates magnification strength proportional to observed tissue displacement,
or distance-based exponential decay that simulates biomechanical force
attenuation. This dual-mode approach accommodates diverse surgical
scenarios-motion-based softening excels with complex tissue deformations while
distance-based softening provides stability during unreliable optical flow
conditions. We evaluate EndoControlMag on our EndoVMM24 dataset spanning four
different surgery types and various challenging scenarios, including
occlusions, instrument disturbance, view changes, and vessel deformations.
Quantitative metrics, visual assessments, and expert surgeon evaluations
demonstrate that EndoControlMag significantly outperforms existing methods in
both magnification accuracy and visual quality while maintaining robustness
across challenging surgical conditions. The code, dataset, and video results
are available at https://szupc.github.io/EndoControlMag/.

</details>


### [484] [MedSR-Impact: Transformer-Based Super-Resolution for Lung CT Segmentation, Radiomics, Classification, and Prognosis](https://arxiv.org/abs/2507.15340)
*Marc Boubnovski Martell,Kristofer Linton-Reid,Mitchell Chen,Sumeet Hindocha,Benjamin Hunter,Marco A. Calzado,Richard Lee,Joram M. Posma,Eric O. Aboagye*

Main category: eess.IV

TL;DR: TVSRN-V2是一种基于Transformer的超分辨率框架，用于临床肺部CT分析，显著提高了分割精度、放射组学特征重现性和预测性能。


<details>
  <summary>Details</summary>
Motivation: 高分辨率CT对胸部疾病诊断和治疗规划至关重要，但受限于辐射剂量和硬件成本。

Method: TVSRN-V2结合Through-Plane Attention Blocks和Swin Transformer V2，通过伪低分辨率增强提高鲁棒性。

Result: 在多个临床队列中，TVSRN-V2显著提升了分割精度（+4% Dice）、放射组学特征重现性和预测性能（+0.06 C-index和AUC）。

Conclusion: TVSRN-V2是一种临床可行的系统，显著提升了CT工作流程中的剂量效率和定量分析能力。

Abstract: High-resolution volumetric computed tomography (CT) is essential for accurate
diagnosis and treatment planning in thoracic diseases; however, it is limited
by radiation dose and hardware costs. We present the Transformer Volumetric
Super-Resolution Network (\textbf{TVSRN-V2}), a transformer-based
super-resolution (SR) framework designed for practical deployment in clinical
lung CT analysis. Built from scalable components, including Through-Plane
Attention Blocks (TAB) and Swin Transformer V2 -- our model effectively
reconstructs fine anatomical details in low-dose CT volumes and integrates
seamlessly with downstream analysis pipelines. We evaluate its effectiveness on
three critical lung cancer tasks -- lobe segmentation, radiomics, and prognosis
-- across multiple clinical cohorts. To enhance robustness across variable
acquisition protocols, we introduce pseudo-low-resolution augmentation,
simulating scanner diversity without requiring private data. TVSRN-V2
demonstrates a significant improvement in segmentation accuracy (+4\% Dice),
higher radiomic feature reproducibility, and enhanced predictive performance
(+0.06 C-index and AUC). These results indicate that SR-driven recovery of
structural detail significantly enhances clinical decision support, positioning
TVSRN-V2 as a well-engineered, clinically viable system for dose-efficient
imaging and quantitative analysis in real-world CT workflows.

</details>


### [485] [Latent Space Synergy: Text-Guided Data Augmentation for Direct Diffusion Biomedical Segmentation](https://arxiv.org/abs/2507.15361)
*Muhammad Aqeel,Maham Nazir,Zanxi Ruan,Francesco Setti*

Main category: eess.IV

TL;DR: SynDiff结合文本引导的合成数据生成和高效扩散分割，解决医学图像分割数据稀缺问题，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 医学图像分割（如息肉检测）因标注需专业知识而数据稀缺，传统方法效率低。

Method: 使用潜在扩散模型通过文本条件修复生成临床真实合成息肉，并引入直接潜在估计实现单步推理加速。

Result: 在CVC-ClinicDB上达到96.0% Dice和92.9% IoU，保持实时性。

Conclusion: SynDiff通过可控合成增强提升分割鲁棒性，适用于资源有限的医疗场景。

Abstract: Medical image segmentation suffers from data scarcity, particularly in polyp
detection where annotation requires specialized expertise. We present SynDiff,
a framework combining text-guided synthetic data generation with efficient
diffusion-based segmentation. Our approach employs latent diffusion models to
generate clinically realistic synthetic polyps through text-conditioned
inpainting, augmenting limited training data with semantically diverse samples.
Unlike traditional diffusion methods requiring iterative denoising, we
introduce direct latent estimation enabling single-step inference with T x
computational speedup. On CVC-ClinicDB, SynDiff achieves 96.0% Dice and 92.9%
IoU while maintaining real-time capability suitable for clinical deployment.
The framework demonstrates that controlled synthetic augmentation improves
segmentation robustness without distribution shift. SynDiff bridges the gap
between data-hungry deep learning models and clinical constraints, offering an
efficient solution for deployment in resourcelimited medical settings.

</details>


### [486] [RARE-UNet: Resolution-Aligned Routing Entry for Adaptive Medical Image Segmentation](https://arxiv.org/abs/2507.15524)
*Simon Winther Albertsen,Hjalte Svaneborg Bjørnstrup,Mostafa Mehdipour Ghazi*

Main category: eess.IV

TL;DR: RARE-UNet是一种分辨率感知的多尺度分割架构，通过动态调整推理路径以适应输入的空间分辨率，解决了现有模型在低分辨率数据下性能下降的问题。


<details>
  <summary>Details</summary>
Motivation: 现有分割模型通常假设输入为固定高分辨率，在真实场景中面对低分辨率数据时性能显著下降，限制了临床应用。

Method: 提出RARE-UNet，包含多尺度块、分辨率感知路由机制和一致性驱动训练，以动态适应输入分辨率。

Result: 在脑成像任务（海马体和肿瘤分割）中，RARE-UNet的平均Dice分数最高（0.84和0.65），且在低分辨率下保持性能并减少推理时间。

Conclusion: RARE-UNet在实现分辨率鲁棒分割方面表现出高效性和可扩展性。

Abstract: Accurate segmentation is crucial for clinical applications, but existing
models often assume fixed, high-resolution inputs and degrade significantly
when faced with lower-resolution data in real-world scenarios. To address this
limitation, we propose RARE-UNet, a resolution-aware multi-scale segmentation
architecture that dynamically adapts its inference path to the spatial
resolution of the input. Central to our design are multi-scale blocks
integrated at multiple encoder depths, a resolution-aware routing mechanism,
and consistency-driven training that aligns multi-resolution features with
full-resolution representations. We evaluate RARE-UNet on two benchmark brain
imaging tasks for hippocampus and tumor segmentation. Compared to standard
UNet, its multi-resolution augmented variant, and nnUNet, our model achieves
the highest average Dice scores of 0.84 and 0.65 across resolution, while
maintaining consistent performance and significantly reduced inference time at
lower resolutions. These results highlight the effectiveness and scalability of
our architecture in achieving resolution-robust segmentation. The codes are
available at: https://github.com/simonsejse/RARE-UNet.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [487] [Quantum Annealing for Machine Learning: Applications in Feature Selection, Instance Selection, and Clustering](https://arxiv.org/abs/2507.15063)
*Chloe Pomeroy,Aleksandar Pramov,Karishma Thakrar,Lakshmi Yendapalli*

Main category: quant-ph

TL;DR: 本文比较了量子退火（QA）和经典模拟退火（SA）在机器学习组合优化问题（特征选择、实例选择和聚类）中的应用，表明QA在效率和效果上具有竞争力。


<details>
  <summary>Details</summary>
Motivation: 探索量子退火和经典模拟退火在机器学习组合优化问题中的表现，验证QA在当前量子硬件限制下的实用性。

Method: 将每个任务（特征选择、实例选择、聚类）建模为QUBO问题，并分别实现量子退火和经典模拟退火求解器进行比较。

Result: QA在特征选择中计算效率更高；实例选择中提出新启发式方法；聚类中通过QUBO细化提升紧凑性和检索指标。

Conclusion: QA在当前量子硬件条件下可作为机器学习离散优化的高效工具。

Abstract: This paper explores the applications of quantum annealing (QA) and classical
simulated annealing (SA) to a suite of combinatorial optimization problems in
machine learning, namely feature selection, instance selection, and clustering.
We formulate each task as a Quadratic Unconstrained Binary Optimization (QUBO)
problem and implement both quantum and classical solvers to compare their
effectiveness. For feature selection, we propose several QUBO configurations
that balance feature importance and redundancy, showing that quantum annealing
(QA) produces solutions that are computationally more efficient. In instance
selection, we propose a few novel heuristics for instance-level importance
measures that extend existing methods. For clustering, we embed a
classical-to-quantum pipeline, using classical clustering followed by
QUBO-based medoid refinement, and demonstrate consistent improvements in
cluster compactness and retrieval metrics. Our results suggest that QA can be a
competitive and efficient tool for discrete machine learning optimization, even
within the constraints of current quantum hardware.

</details>


<div id='q-fin.ST'></div>

# q-fin.ST [[Back]](#toc)

### [488] [FinSurvival: A Suite of Large Scale Survival Modeling Tasks from Finance](https://arxiv.org/abs/2507.14160)
*Aaron Green,Zihan Nie,Hanzhen Qin,Oshani Seneviratne,Kristin P. Bennett*

Main category: q-fin.ST

TL;DR: 论文提出FinSurvival基准，基于DeFi交易数据构建16个生存建模任务，填补AI生存建模领域缺乏大规模公开数据集的空白。


<details>
  <summary>Details</summary>
Motivation: 解决AI生存建模领域缺乏大规模、真实且公开数据集的问题，推动该领域的研究发展。

Method: 利用DeFi交易数据，通过自动化流程构建16个生存建模任务，并生成对应的分类问题。

Result: FinSurvival包含750万条记录，任务具有挑战性，现有方法表现不佳。

Conclusion: FinSurvival为AI生存建模提供了实用基准，未来可扩展更多DeFi交易数据。

Abstract: Survival modeling predicts the time until an event occurs and is widely used
in risk analysis; for example, it's used in medicine to predict the survival of
a patient based on censored data. There is a need for large-scale, realistic,
and freely available datasets for benchmarking artificial intelligence (AI)
survival models. In this paper, we derive a suite of 16 survival modeling tasks
from publicly available transaction data generated by lending of
cryptocurrencies in Decentralized Finance (DeFi). Each task was constructed
using an automated pipeline based on choices of index and outcome events. For
example, the model predicts the time from when a user borrows cryptocurrency
coins (index event) until their first repayment (outcome event). We formulate a
survival benchmark consisting of a suite of 16 survival-time prediction tasks
(FinSurvival). We also automatically create 16 corresponding classification
problems for each task by thresholding the survival time using the restricted
mean survival time. With over 7.5 million records, FinSurvival provides a suite
of realistic financial modeling tasks that will spur future AI survival
modeling research. Our evaluation indicated that these are challenging tasks
that are not well addressed by existing methods. FinSurvival enables the
evaluation of AI survival models applicable to traditional finance, industry,
medicine, and commerce, which is currently hindered by the lack of large public
datasets. Our benchmark demonstrates how AI models could assess opportunities
and risks in DeFi. In the future, the FinSurvival benchmark pipeline can be
used to create new benchmarks by incorporating more DeFi transactions and
protocols as the use of cryptocurrency grows.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [489] [All-atom inverse protein folding through discrete flow matching](https://arxiv.org/abs/2507.14156)
*Kai Yi,Kiarash Jamali,Sjors H. W. Scheres*

Main category: q-bio.BM

TL;DR: ADFLIP是一种基于离散流匹配的生成模型，用于设计蛋白质序列，特别适用于包含非蛋白质组分的动态复合物。


<details>
  <summary>Details</summary>
Motivation: 解决现有逆折叠方法在预测含非蛋白质组分或动态结构复合物序列时的不足。

Method: 采用离散流匹配生成模型，逐步整合预测的氨基酸侧链结构，并通过多结构状态采样设计动态复合物。

Result: 在单结构和多结构逆折叠任务中表现优异，适用于全原子蛋白质设计。

Conclusion: ADFLIP展示了在全原子蛋白质设计中的卓越潜力，代码已开源。

Abstract: The recent breakthrough of AlphaFold3 in modeling complex biomolecular
interactions, including those between proteins and ligands, nucleotides, or
metal ions, creates new opportunities for protein design. In so-called inverse
protein folding, the objective is to find a sequence of amino acids that adopts
a target protein structure. Many inverse folding methods struggle to predict
sequences for complexes that contain non-protein components, and perform poorly
with complexes that adopt multiple structural states. To address these
challenges, we present ADFLIP (All-atom Discrete FLow matching Inverse Protein
folding), a generative model based on discrete flow-matching for designing
protein sequences conditioned on all-atom structural contexts. ADFLIP
progressively incorporates predicted amino acid side chains as structural
context during sequence generation and enables the design of dynamic protein
complexes through ensemble sampling across multiple structural states.
Furthermore, ADFLIP implements training-free classifier guidance sampling,
which allows the incorporation of arbitrary pre-trained models to optimise the
designed sequence for desired protein properties. We evaluated the performance
of ADFLIP on protein complexes with small-molecule ligands, nucleotides, or
metal ions, including dynamic complexes for which structure ensembles were
determined by nuclear magnetic resonance (NMR). Our model achieves
state-of-the-art performance in single-structure and multi-structure inverse
folding tasks, demonstrating excellent potential for all-atom protein design.
The code is available at https://github.com/ykiiiiii/ADFLIP.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [490] [What do Large Language Models know about materials?](https://arxiv.org/abs/2507.14586)
*Adrian Ehrenhofer,Thomas Wallmersperger,Gianaurelio Cuniberti*

Main category: physics.app-ph

TL;DR: 论文探讨了大型语言模型（LLMs）在机械工程和材料科学中的应用，重点关注其在材料知识生成中的准确性。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在科学领域的适用性，特别是其在材料科学中的知识生成能力，以填补互联网非科学内容的不足。

Method: 以元素周期表为例，分析词汇和标记化对材料指纹唯一性的影响，并评估不同开源LLMs生成准确信息的能力。

Result: 提出了一个材料知识基准，用于判断LLMs在PSPP链中哪些步骤适用，哪些需要专业模型。

Conclusion: LLMs在材料科学中有潜力，但需结合专业模型以确保准确性。

Abstract: Large Language Models (LLMs) are increasingly applied in the fields of
mechanical engineering and materials science. As models that establish
connections through the interface of language, LLMs can be applied for
step-wise reasoning through the Processing-Structure-Property-Performance chain
of material science and engineering. Current LLMs are built for adequately
representing a dataset, which is the most part of the accessible internet.
However, the internet mostly contains non-scientific content. If LLMs should be
applied for engineering purposes, it is valuable to investigate models for
their intrinsic knowledge -- here: the capacity to generate correct information
about materials. In the current work, for the example of the Periodic Table of
Elements, we highlight the role of vocabulary and tokenization for the
uniqueness of material fingerprints, and the LLMs' capabilities of generating
factually correct output of different state-of-the-art open models. This leads
to a material knowledge benchmark for an informed choice, for which steps in
the PSPP chain LLMs are applicable, and where specialized models are required.

</details>
